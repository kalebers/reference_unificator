Database,Authors,Title,Journal,Year,Volume,Issue,Pages,DOI,URL,Abstract,Keywords
ScienceDirect,"Amir Khosravian, Abdollah Amirkhani, Hossein Kashiani, Masoud Masih-Tehrani,","Generalizing state-of-the-art object detectors for autonomous vehicles in unseen environments,","Expert Systems with Applications,","115417,","183,",,,https://doi.org/10.1016/j.eswa.2021.115417.,https://www.sciencedirect.com/science/article/pii/S0957417421008368,"In scene understanding for autonomous vehicles (AVs), models trained on the available datasets fail to generalize well to the complex, real-world scenarios with higher dynamics. In this work, we attempt to handle the distribution mismatch by employing the generative adversarial network (GAN) and weather modeling to strengthen the intra-domain data. We also alleviate the fragility of our trained models against natural distortions with state-of-the-art augmentation approaches. Finally, we assess our method for cross-domain object detection through CARLA simulation. Our experiments demonstrate that: (1) Augmenting training class with even limited intra-domain data captured from the adverse weather conditions boosts the generalization of the two kinds of object detectors; (2) Exploiting GANs and weather modeling to elaborately simulate the adverse, intra-domain weather conditions manages to surmount the adverse data scarcity issue for intra-domain object detection; (3) A combination of Augmix and style augmentations not only can promote the robustness of our trained models against different natural distortions but also can boost their performance in the cross-domain object detection; (4) Training GANs for unsupervised image-to-image translation by means of the existing, large-scale datasets outside of our training domain is found beneficial to alleviate image-based and instance-based domain shifts.",Autonomous vehicles; Object detection; Distribution mismatch; Natural distortions; Cross-domain robustness
ScienceDirect,"Carmen Gheorghe, Mihai Duguleana, Razvan Gabriel Boboc, Cristian Cezar Postelnicu,","Analyzing Real-Time Object Detection with YOLO Algorithm in Automotive Applications: A Review,","CMES - Computer Modeling in Engineering and Sciences,","2024,","141, Issue 3,",,"1939-1981,",https://doi.org/10.32604/cmes.2024.054735.,https://www.sciencedirect.com/science/article/pii/S1526149224003011,"Identifying objects in real-time is a technology that is developing rapidly and has a huge potential for expansion in many technical fields. Currently, systems that use image processing to detect objects are based on the information from a single frame. A video camera positioned in the analyzed area captures the image, monitoring in detail the changes that occur between frames. The You Only Look Once (YOLO) algorithm is a model for detecting objects in images, that is currently known for the accuracy of the data obtained and the fast-working speed. This study proposes a comprehensive literature review of YOLO research, as well as a bibliometric analysis to map the trends in the automotive field from 2020 to 2024. Object detection applications using YOLO were categorized into three primary domains: road traffic, autonomous vehicle development, and industrial settings. A detailed analysis was conducted for each domain, providing quantitative insights into existing implementations. Among the various YOLO architectures evaluated (v2–v8, H, X, R, C), YOLO v8 demonstrated superior performance with a mean Average Precision (mAP) of 0.99.",YOLO; automotive; autonomous vehicles; traffic; industry
ScienceDirect,"Alireza Ghasemieh, Rasha Kashef,","3D object detection for autonomous driving: Methods, models, sensors, data, and challenges,","Transportation Engineering,","100115,","8,",,,https://doi.org/10.1016/j.treng.2022.100115.,https://www.sciencedirect.com/science/article/pii/S2666691X22000136,"Detection of the surrounding objects of a vehicle is the most crucial step in autonomous driving. Failure to identify those objects correctly in a timely manner can cause irreparable damage, impacting our safety and society. Several studies have been introduced to identify these objects in the two-dimensional (2D) and three-dimensional (3D) vector space. The 2D object detection method has achieved remarkable success; however, in the last few years, detecting objects in 3D have received more remarkable adoption. 3D object recognition has several advantages over 2D detection methods, as more accurate information about the environment is obtained for better detection. For example, the depth of the images is not considered in the 2D detection, which reduces the detection accuracy. Despite considerable efforts in 3D object detection, it has not yet reached the stage of maturity. Therefore, in this paper, we aim at providing a comprehensive overview of the state-of-the-art 3D object detection methods, with a focus on 1) identifying advantages and limitations, 2) revelling a novel categorization of the literature, 3) outlying the various training procedures, 4) highlighting the research gap in the existing methods and 5) building a road map for future directions.",Autonomous vehicles; Sensors; LiDAR; Point cloud; Stereo images; 3D object detection
ScienceDirect,"Wensheng Zhang, Hongli Shi, Yunche Zhao, Zhenan Feng, Ruggiero Lovreglio,","MMAF-Net: Multi-view multi-stage adaptive fusion for multi-sensor 3D object detection,","Expert Systems with Applications,","122716,","242,",,,https://doi.org/10.1016/j.eswa.2023.122716.,https://www.sciencedirect.com/science/article/pii/S0957417423032189,"In this paper, we propose a 3D object detection method called MMAF-Net that is based on the multi-view and multi-stage adaptive fusion of RGB images and LiDAR point cloud data. This is an end-to-end architecture, which combines the characteristics of RGB images, the front view of point clouds based on reflection intensity, and the bird’s eye view of point clouds. It also adopts a multi-stage fusion approach of “data-level fusion + feature-level fusion” to fully exploit the strength of multimodal information. Our proposed method addresses key challenges found in current 3D object detection methods for autonomous driving, including insufficient feature extraction from multimodal data, rudimentary fusion techniques, and sensitivity to distance and occlusion. To ensure the comprehensive integration of multimodal information, we present a series of targeted fusion methods. Firstly, we propose a novel input form that encodes dense point cloud reflectivity information into the image to enhance its representational power. Secondly, we design the Region Attention Adaptive Fusion module utilizing an attention mechanism to guide the network in adaptively adjusting the importance of different features. Finally, we extend the 2D DIOU (Distance Intersection over Union) loss function to 3D and develop a joint regression loss based on 3D_DIOU and SmoothL1 to optimize the similarity between detected and ground truth boxes. The experimental results on the KITTI dataset demonstrate that MMAF-Net effectively addresses the challenges posed by highly obscured or crowded scenes while maintaining real-time performance and improving the detection accuracy of smaller and more difficult objects that are occluded at far distances.",3D object detection; Multi-sensor fusion; Attention mechanism; Joint regression loss; Autonomous driving
ScienceDirect,"Linh Trinh, Siegfried Mercelis, Ali Anwar,","A comprehensive review of datasets and deep learning techniques for vision in unmanned surface vehicles,","Ocean Engineering,","121501,","334,",,,https://doi.org/10.1016/j.oceaneng.2025.121501.,https://www.sciencedirect.com/science/article/pii/S0029801825011850,"Unmanned Surface Vehicles (USVs) have emerged as a major platform in maritime operations, capable of supporting a wide range of applications. USVs allow for difficult unmanned tasks in harsh maritime environments. With the rapid development of USVs, many vision tasks such as detection and segmentation become increasingly important. Datasets play an important role in encouraging and improving the research and development of reliable vision algorithms for USVs. In this regard, a large number of recent studies have focused on the release of vision datasets for USVs. Along with the development of datasets, a variety of deep learning techniques have also been studied, with a focus on USVs. However, there is a lack of a systematic review of recent studies in both datasets and vision techniques to provide a comprehensive picture of the current development of vision on USVs, including limitations and trends. In this study, we provide a comprehensive review of both USV datasets and deep learning techniques for vision tasks. Our review was conducted using a large number of vision datasets from USVs. We elaborate several challenges and potential opportunities for research and development in USV vision based on a thorough analysis of current datasets and deep learning techniques.",Unmanned surface vessels; Datasets; Deep learning; Computer vision
ScienceDirect,"Yifan Liu, Yong Zhang, Rukai Lan, Cheng Cheng, Zhaolong Wu,","AWARDistill: Adaptive and robust 3D object detection in adverse conditions through knowledge distillation,","Expert Systems with Applications,","126032,","266,",,,https://doi.org/10.1016/j.eswa.2024.126032.,https://www.sciencedirect.com/science/article/pii/S0957417424028999,"3D object detection is a crucial component of autonomous vehicle perception, but adverse weather conditions can affect sensor performance, leading to a deterioration in data quality, thereby posing significant challenges to the further development of perception. This paper presents a novel, robust 3D object detection framework to address this issue. Firstly, to tackle the problem of lacking adverse weather datasets, we propose the Multi-modal Adverse-Weather Data Simulation Theory (MIST), which employs optical models to simulate fog and replicates the dynamic properties of rain and snow to recreate real-world circumstances. Secondly, we propose the Adaptive and Robust 3D Object Detection Framework in Adverse Conditions through Knowledge Distillation (AWARDistill), which employs staged knowledge distillation to enable the model to adapt to adverse weather conditions, significantly enhancing detection accuracy and robustness. Additionally, we designed two modules that can be integrated into other detection frameworks to enhance robustness. We evaluated the performance of AWARDistill on multiple datasets. On the KITTI dataset, our model attained an average precision of about 88% and can efficiently adapt to extreme weather. Extensive experiments demonstrate our model’s effectiveness and superiority, providing strong support for autonomous driving in challenging weather environments.",Autonomous driving; 3D object detection; Adverse weather; Knowledge distillation
ScienceDirect,"Bharat Mahaur, K.K. Mishra, Anoj Kumar,","An improved lightweight small object detection framework applied to real-time autonomous driving,","Expert Systems with Applications,","121036,","234,",,,https://doi.org/10.1016/j.eswa.2023.121036.,https://www.sciencedirect.com/science/article/pii/S0957417423015385,"Recent deep learning-based object detectors have shown compelling performance for the detection of large objects in autonomous driving applications. However, the detection of small objects like traffic signs and traffic lights is challenging owing to the complex nature of such objects. This article investigates how an existing object detector can be adjusted to address specific tasks and how these modifications can impact the detection of small objects. In particular, we explore and introduce architectural changes to the different components of the popular YOLOv5 model in order to improve its performance in the detection of small objects for autonomous driving. Initially, we propose group depthwise separable convolution as the improved convolution unit to replace standard convolution. We then integrate this unit to create the attention-based dilated CSP block. Lastly, this block is combined with several proposed modules, including the improved SPP, improved PANet, and improved information paths, to form our IS-YOLOv5 model. We also integrate kernel pruning on the network to accelerate the model deployment on vehicle-mounted mobile platform due to limited computing resources and real-time constraints. Specifically, we propose the versatile network pruning (VNP) technique based on Taylor criterion ranking to prune less-essential kernels in the network. We will show that our modifications barely increase the complexity but significantly improve the detection accuracy and speed. Compared to the conventional YOLOv5, the proposed IS-YOLOv5 model increases the mAP by 8.35% on the BDD100K dataset. Besides, our proposed model improves the detection speed in FPS by 3.10% compared to the YOLOv5 model. When using the VNP scheme, FPS is further increased by 52.14%, while the model size and complexity are reduced by 39.29% and 47.81%, with almost no change in mAP. Nevertheless, when compared to state-of-the-art models, IS-YOLOv5+VNP is found to be conducive to the deployment in autonomous driving systems.",Small object detection; Architectural changes; YOLOv5; Kernel pruning; Lightweight design; Autonomous driving
ScienceDirect,"Wei Chen, Yan Li, Zijian Tian, Fan Zhang,","2D and 3D object detection algorithms from images: A Survey,","Array,","100305,","19,",,,https://doi.org/10.1016/j.array.2023.100305.,https://www.sciencedirect.com/science/article/pii/S2590005623000309,"Object detection is a crucial branch of computer vision that aims to locate and classify objects in images. Using deep convolutional neural networks (CNNs) as the primary framework for object detection can efficiently extract features, which is closer to real-time performance than the traditional model that extracts features manually. In recent years, the rise of Transformer with powerful self-attention mechanisms has further enhanced performance to a new level. However, when it comes to specific vision tasks in the real world, it is necessary to obtain 3D information about the spatial coordinates, orientation, and velocity of objects, which makes research on object detection in 3D scenes more active. Although LiDAR-based 3D object detection algorithms have excellent performance, they are difficult to popularize in practical applications due to their high price. Hence, we summarize the development process, different frameworks, contributions, advantages, disadvantages, and development trends of image-based 2D and 3D object detection algorithms in recent years to help more researchers better understand this field. Besides, representative datasets，evaluation metrics，related techniques and applications are introduced, and some valuable research directions are discussed.",Image; Object detection; CNNs; Transformer; 3D
ScienceDirect,"Li Wang, Dawei Zhao, Tao Wu, Hao Fu, Zhiyu Wang, Liang Xiao, Xin Xu, Bin Dai,","Drosophila-inspired 3D moving object detection based on point clouds,","Information Sciences,","2020,","534,",,"154-171,",https://doi.org/10.1016/j.ins.2020.05.006.,https://www.sciencedirect.com/science/article/pii/S0020025520303960,"3D moving object detection is one of the most critical tasks in dynamic scene analysis. In this paper, we propose a novel Drosophila-inspired 3D moving object detection method using Lidar sensors. According to the theory of elementary motion detector, we have developed a motion detector based on the shallow visual neural pathway of Drosophila. This detector is sensitive to the movement of objects and can well suppress background noise. Designing neural circuits with different connection modes, the approach searches for motion areas in a coarse-to-fine fashion and extracts point clouds of each motion area to form moving object proposals. An improved 3D object detection network is then used to estimate the point clouds of each proposal and efficiently generates the 3D bounding boxes and the object categories. We evaluate the proposed approach on the widely-used KITTI benchmark, and state-of-the-art performance was obtained by using the proposed approach on the task of motion detection.",3D moving object detection; Elementary motion detector; Drosophila-inspired model; Neural network; Autonomous driving
ScienceDirect,"Kishore Kumar Anguchamy, Venketesh Palanisamy,","Real-time object detection using improvised YOLOv4 and feature mapping technique for autonomous driving,","Expert Systems with Applications,","127452,","280,",,,https://doi.org/10.1016/j.eswa.2025.127452.,https://www.sciencedirect.com/science/article/pii/S0957417425010747,"Drivable area detection and object detection are the primary actors in any autonomous driving technology. The proposed solution in this article contemplates a model for object detection in real-time with an improvised deep learning solution implemented with a modified YOLO algorithm. The proposed algorithm is replaced with the feature pyramid in place of spatial dimension features, for reducing the computational complexity of the standard YOLOv4 algorithm. The attention span of the YOLOv4 algorithm is improved by CBAM (Convolutional Block Attention Module) structures, where the speed of detecting the objects is associated with the keyframes and feature pyramid. BDD100K data set is available open-source and considered to be the benchmark for autonomous driving technology. This dataset is used for evaluating the performance of the proposed technique and the speed of detection significantly improved 4.72 frames per second when compared against other techniques.",Convolutional neural network; Deep learning; Object detection; Self-driving car; Feature mapping
ScienceDirect,"Qihuai Chen, Jianhe Wen, Tianliang Lin, Haoling Ren,","Target recognition and localization of environmental perception system based on binocular cameras for unmanned cleaning vehicle,","Engineering Applications of Artificial Intelligence,","109994,","143,",,,https://doi.org/10.1016/j.engappai.2024.109994.,https://www.sciencedirect.com/science/article/pii/S0952197624021535,"Cleaning vehicles, vital in municipal projects, automated via unmanned driving, boost urban cleaning efficiency and sustainability, cutting costs and minimizing environmental impact. Environmental perception is key to their automation, differing from passenger cars. Cleaning vehicles often encounter issues such as low cleaning efficiency, high energy consumption, and suboptimal cleaning results when operating in complex and variable scenarios. The automated cleaning vehicle uses an environmental perception system to identify waste and provides vital information for controlling its cleaning devices, boosting both cleaning and energy efficiency. However, existing object detection algorithms struggle with challenges such as background interference and the small size of road debris, leading to limited detection capabilities. This paper proposes an environmental perception solution specifically tailored for the operational context of cleaning vehicles. The solution uses a binocular camera system, integrating stereo vision technology with object detection algorithms to identify and locate road debris. Attention mechanisms are incorporated to enhance model detection accuracy. An additional layer for small object detection is implemented to improve the ability to detect smaller targets, increasing the mean average precision by 5.1% and the recall rate by 6.61%. These improvements could lead to more accurate and efficient automated cleaning systems in urban environments, with far-reaching implications for the future of smart cities.To validate the feasibility of the proposed solution, tests are conducted using a road debris dataset and real-vehicle application experiments. The experimental results demonstrate that the system effectively performs debris identification and localization tasks on cleaning vehicles.",Cleaning vehicle; Autonomous driving; Binocular cameras; Binocular stereo vision; Object detection; Deep learning
ScienceDirect,"Zeyang Cheng, Xiaojie Du, Qingyu Liu, Shuguang Zhan, Bo Yu, He Wang, Xiaojun Zhu, Can Xu,","Object detection in autonomous driving scenario using YOLOv8-SimAM: a robust test in different datasets,","Transportmetrica A Transport Science,","2025,",,,,https://doi.org/10.1080/23249935.2025.2511818.,https://www.sciencedirect.com/science/article/pii/S2324993525000545,"With the advancement of deep learning, AI applications in transportation, particularly in target detection and tracking, have made significant strides. However, real-time vehicle detection of autonomous driving scenario still faces challenges of low accuracy and efficiency. This study proposes an improved YOLOv8-SimAM network, aiming to enhance detection accuracy. The SimAM attention module is introduced and the loss function is optimised to enhance the features of target vehicle without increasing network parameters. YOLOv8-SimAM is combined with DeepSORT for efficient tracking. Experiments show that the YOLOv8-SimAM improves mAP by 1%, accuracy by 4%, and FPS by 3.36% on the UA-DETRAC dataset compared to YOLOv8. It also performs excellently on BDD100k and MIT vehicle datasets, addressing multi-target detection issues in complex scenarios. The model’s improved accuracy and efficiency, along with its strong generalisability and reliability, make it suitable for various driving environments, providing robust support for full-scale autonomous driving.",Object detection; YOLOv8-SimAM; DeepSORT; autonomous driving
ScienceDirect,"Rashmi, Rashmi Chaudhry,","SD-YOLO-AWDNet: A hybrid approach for smart object detection in challenging weather for self-driving cars,","Expert Systems with Applications,","124942,","256,",,,https://doi.org/10.1016/j.eswa.2024.124942.,https://www.sciencedirect.com/science/article/pii/S0957417424018098,"Several deep learning algorithms are currently focused on object detection in adverse weather scenarios for autonomous driving systems. However, these algorithms face challenges in real-time scenarios, leading to a reduction in detection accuracy. To tackle these issues, this paper introduces a lightweight object detection model named Self Driving Cars You Only Look Once Adverse Weather Detection Network (SD-YOLO-AWDNet), derived from enhancements to the YOLOv5 algorithm. The model incorporates four progressive improvement levels within the YOLOv5 framework. This includes integrating C3Ghost and GhostConv modules in the backbone to enhance detection speed by reducing computational overhead during feature extraction. To address potential accuracy issues arising from these modules, Depthwise-Separable Dilated Convolutions (DSDC) are introduced, striking a balance between accuracy and parameter reduction. The model further incorporates a Coordinate Attention (CA) module in the GhostBottleneck to enhance feature extraction and eliminate unnecessary features, improving precision in object detection. Additionally, a novel “Focal Distribution Loss” replaces CIoU Loss, accelerating bounding box regression and loss reduction. Test dataset experiments demonstrate that SD-YOLO-AWDNet outperforms YOLOv5 with a 54% decrease in FLOPs, a 52.53% decrease in model parameters, a 2.24% increase in mAP, and a threefold improvement in detection speed.",Object detection; Self driving cars; Deep neural network; YOLO
ScienceDirect,"Robert Fonod, Haechan Cho, Hwasoo Yeo, Nikolas Geroliminis,","Advanced computer vision for extracting georeferenced vehicle trajectories from drone imagery,","Transportation Research Part C: Emerging Technologies,","105205,","178,",,,https://doi.org/10.1016/j.trc.2025.105205.,https://www.sciencedirect.com/science/article/pii/S0968090X25002098,"This paper presents a comprehensive framework for extracting georeferenced vehicle trajectories from high-altitude drone imagery, addressing key challenges in urban traffic monitoring and the limitations of traditional ground-based systems. Our approach integrates several novel contributions, including a tailored object detector optimized for high-altitude bird’s-eye view perspectives, a unique track stabilization method that uses detected vehicle bounding boxes as exclusion masks during image registration, and an orthophoto and master frame-based georeferencing strategy that enhances consistent alignment across multiple drone viewpoints. Additionally, our framework features robust vehicle dimension estimation and detailed road segmentation, enabling comprehensive traffic dynamics analysis. Conducted in the Songdo International Business District, South Korea, the study utilized a multi-drone experiment covering 20 intersections, capturing approximately 12TB of ultra-high-definition video data over four days. The framework produced two high-quality datasets: the Songdo Traffic dataset, comprising approximately 700,000 unique vehicle trajectories, and the Songdo Vision dataset, containing over 5000 human-annotated images with about 300,000 vehicle instances categorized into four classes. Comparisons with high-precision sensor data from an instrumented probe vehicle highlight the accuracy and consistency of our extraction pipeline in dense urban environments. The public release of the Songdo Traffic and Songdo Vision datasets, along with the complete source code for the extraction pipeline, establishes new benchmarks in data quality, reproducibility, and scalability in traffic research. The results demonstrate the potential of integrating drone technology with advanced computer vision methods for precise and cost-effective urban traffic monitoring, providing valuable resources for developing intelligent transportation systems and enhancing traffic management strategies.",Traffic monitoring; Machine vision; Vehicle tracking; Video image processing; Georeferenced vehicle trajectories; Multi-drone data collection; Urban traffic
ScienceDirect,"Jiaqi Wang, Jian Su,","A Review of Object Detection Techniques in IoT-Based Intelligent Transportation Systems,","Computers, Materials and Continua,","2025,","84, Issue 1,",,"125-152,",https://doi.org/10.32604/cmc.2025.064309.,https://www.sciencedirect.com/science/article/pii/S1546221825005211,"The Intelligent Transportation System (ITS), as a vital means to alleviate traffic congestion and reduce traffic accidents, demonstrates immense potential in improving traffic safety and efficiency through the integration of Internet of Things (IoT) technologies. The enhancement of its performance largely depends on breakthrough advancements in object detection technology. However, current object detection technology still faces numerous challenges, such as accuracy, robustness, and data privacy issues. These challenges are particularly critical in the application of ITS and require in-depth analysis and exploration of future improvement directions. This study provides a comprehensive review of the development of object detection technology and analyzes its specific applications in ITS, aiming to thoroughly explore the use and advancement of object detection technologies in IoT-based intelligent transportation systems. To achieve this objective, we adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach to search, screen, and assess the eligibility of relevant literature, ultimately including 88 studies. Through an analysis of these studies, we summarized the characteristics, advantages, and limitations of object detection technology across the traditional methods stage and the deep learning-based methods stage. Additionally, we examined its applications in ITS from three perspectives: vehicle detection, pedestrian detection, and traffic sign detection. We also identified the major challenges currently faced by these technologies and proposed future directions for addressing these issues. This review offers researchers a comprehensive perspective, identifying potential improvement directions for object detection technology in ITS, including accuracy, robustness, real-time performance, data annotation cost, and data privacy. In doing so, it provides significant guidance for the further development of IoT-based intelligent transportation systems.",Intelligent transportation systems; Internet of Things; object detection; deep learning
ScienceDirect,"Ke Wang, Gang Li, Junlan Chen, Yan Long, Tao Chen, Long Chen, Qin Xia,","The adaptability and challenges of autonomous vehicles to pedestrians in urban China,","Accident Analysis & Prevention,","105692,","145,",,,https://doi.org/10.1016/j.aap.2020.105692.,https://www.sciencedirect.com/science/article/pii/S0001457520302475,"China is the world's largest automotive market and is ambitious for autonomous vehicles (AVs) development. As one of the key goals of AVs, pedestrian safety is an important issue in China. Despite the rapid development of driverless technologies in recent years, there is a lack of researches on the adaptability of AVs to pedestrians. To fill the gap, this study would discuss the adaptability of current driverless technologies to China urban pedestrians by reviewing the latest researches. The paper firstly analyzed typical Chinese pedestrian behaviors and summarized the safety demands of pedestrians for AVs through articles and open database data, which are worked as the evaluation criteria. Then, corresponding driverless technologies are carefully reviewed. Finally, the adaptability would be given combining the above analyses. Our review found that autonomous vehicles have trouble in the occluded pedestrian environment and Chinese pedestrians do not accept AVs well. And more explorations should be conducted on standard human-machine interaction, interaction information overload avoidance, occluded pedestrians detection and nation-based receptivity research. The conclusions are very useful for motor corporations and driverless car researchers to place more attention on the complexity of the Chinese pedestrian environment, for transportation experts to protect pedestrian safety in the context of AVs, and for governors to think about making new pedestrians policies to welcome the upcoming driverless cars.",Chinese pedestrian environment; Adaptability; Autonomous vehicles; Road safety; Urban China
ScienceDirect,"Hao Du, Lu Ren, Yuanda Wang, Xiang Cao, Changyin Sun,","Advancements in perception system with multi-sensor fusion for embodied agents,","Information Fusion,","102859,","117,",,,https://doi.org/10.1016/j.inffus.2024.102859.,https://www.sciencedirect.com/science/article/pii/S1566253524006377,"The multi-sensor data fusion perception technology, as a pivotal technique for achieving complex environmental perception and decision-making, has been garnering extensive attention from researchers. To date, there has been a lack of comprehensive review articles discussing the research progress of multi-sensor fusion perception systems for embodied agents, particularly in terms of analyzing the agent’s perception of itself and the surrounding scene. To address this gap and encourage further research, this study defines key terminology and analyzes datasets from the past two decades, focusing on advancements in multi-sensor fusion SLAM and multi-sensor scene perception. These key designs can aid researchers in gaining a better understanding of the field and initiating research in the domain of multi-sensor fusion perception for embodied agents. In this survey, we begin with a brief introduction to common sensor types and their characteristics. We then delve into the multi-sensor fusion perception datasets tailored for the domains of autonomous driving, drones, unmanned ground vehicles, and unmanned surface vehicles. Following this, we discuss the classification and fundamental principles of existing multi-sensor data fusion SLAM algorithms, and present the experimental outcomes of various classical fusion frameworks. Subsequently, we comprehensively review the technologies of multi-sensor data fusion scene perception, including object detection, semantic segmentation, instance segmentation, and panoramic understanding. Finally, we summarize our findings and discuss potential future developments in multi-sensor fusion perception technology.",Embodied agents; Multi-sensor; State estimation; Scene perception; Datasets; SLAM
ScienceDirect,"Nusaybah M. Alahdal, Felwa Abukhodair, Leila Haj Meftah, Asma Cherif,","Real-time Object Detection in Autonomous Vehicles with YOLO,","Procedia Computer Science,","2024,","246,",,"2792-2801,",https://doi.org/10.1016/j.procs.2024.09.392.,https://www.sciencedirect.com/science/article/pii/S1877050924024293,"AI analytics enables autonomous cars to detect and recognize objects, such as other vehicles, pedestrians, traffic signs, and obsta- cles, in real-time. Deep learning models, notably the You Only Look Once (YOLO) model, have demonstrated accuracy and speed in obstacle avoidance. However, current datasets are limited, lacking diversity and labeling, hindering their ability to represent real- world scenarios accurately. Besides, previous studies have focused extensively on specific object classes, such as pedestrians and vehicles, often neglecting other objects like bikes and road signs. To address this, we introduce a novel dataset tailored for AV envi- ronments, encompassing various road object types under different conditions. Our innovative methodology relies on self-supervised learning using the late YOLO version to improve model robustness with limited labeled data and AI-driven adaptive model opti- mization based on real-time feedback. We evaluate three YOLO architectures—YOLOv5, YOLOv7, and YOLOv8—customized for AV object detection. Our assessment covers everyday AV objects such as cars, pedestrians, bicycles, and road signs, empha- sizing early detection. We employ the VSim-AV simulator dataset to ensure robust evaluation, augmented with preprocessing techniques to optimize data quality and model generalization. The study reveals that YOLOv5 and YOLOv8 outperform YOLOv7 regarding precision and recall across various object classes, with YOLOv5 leading at 1.3 ms/image and YOLOv8 at 3.3 ms/image. The mean average precision was 0.94 for YOLOv5, 0.441 for YOLOv7, and 0.927 for YOLOv8, highlighting the limitations in current literature and challenges in YOLO model performance.",Smart city; Simulation environment; Deep learning; Artificial intelligence; YOLO; Object detection; Autonomous vehicles
ScienceDirect,"Peng Ping, Zhengpeng Yang, Lu Tao, Quan Shi, Weiping Ding,","BEV-TinySpotter: A novel BEV perception method considering multi-dimensional feature fusion of small target,","Information Fusion,","102793,","116,",,,https://doi.org/10.1016/j.inffus.2024.102793.,https://www.sciencedirect.com/science/article/pii/S1566253524005712,"The BEV perceptual integrity depends on the ability to accurately capture multi-scale targets. However, the detection of multi-scale targets, especially small targets, is often affected by perceptual noise, information sparsity, and target occlusion, which subsequently affecting the accuracy of BEV perception and causing certain hidden danger to driving safety. To address these issues, we propose a novel BEV perception method BEV-TinySpotter (BEV-TS), which introduced two feature detection enhancement modules and one analytical causal reasoning module to highlighting small object features and improving the accuracy and robustness of small object detection. Specifically, we first enhance the feature recognition of small targets using the multi-scale spatial information acquisition module, and introduce the distribution and migration weights of small targets reflected in the time-series information. Then, we jointly employ global pooling and local pooling to differentiate the acquisition paths of the information to improve the sensitivity of capturing information for small targets. Additionally, we design causal inference graphs to post-fuse the recognition results of BEV, which utilizes causal reasoning to construct a state description mechanism for small targets, further enhancing the accuracy of BEV perception from a logical reasoning perspective. Compared with other SOTA BEV methods, the proposed method effectively enhances the ability to capture and infer the status of small targets during BEV perception, thus significantly improves the completeness of driving environment sensing.",Autonomous driving; BEV perception; Small object detection; Feature fusion; Causal reasoning
ScienceDirect,"Md Mohsin Kabir, Jamin Rahman Jim, Zoltán Istenes,","Terrain detection and segmentation for autonomous vehicle navigation: A state-of-the-art systematic review,","Information Fusion,","102644,","113,",,,https://doi.org/10.1016/j.inffus.2024.102644.,https://www.sciencedirect.com/science/article/pii/S1566253524004226,"This review comprehensively investigates the current state and emerging trends of autonomous vehicle terrain detection and segmentation. By systematically reviewing literature from various databases, this study outlines the evolution of detection and segmentation techniques from traditional computer vision methods to advanced machine learning and deep learning approaches. It identifies critical technological advancements, evaluates their performance, and discusses the challenges faced under various environmental conditions, data acquisition, and integration with vehicle systems. This study also highlights the need for standardized benchmarks and datasets to facilitate the development and testing of robust terrain detection systems. This review encompasses terrain detection and segmentation in structured environments, such as urban roads and highways, and unstructured environments, including rural paths and off-road terrains, to comprehensively analyze autonomous vehicle navigation challenges. By analyzing recent research findings, this review provides insights into future directions for overcoming these limitations and fostering innovation in the autonomous driving domain.",Autonomous vehicles; Terrain detection; Segmentation techniques; Machine learning; Deep learning; Computer vision
ScienceDirect,"Udink Aulia, Iskandar Hasanuddin, Muhammad Dirhamsyah, Nasaruddin Nasaruddin,","A new CNN-BASED object detection system for autonomous mobile robots based on real-world vehicle datasets,","Heliyon,","2024,","10, Issue 15,",,,https://doi.org/10.1016/j.heliyon.2024.e35247.,https://www.sciencedirect.com/science/article/pii/S2405844024112789,"Recently, autonomous mobile robots (AMRs) have begun to be used in the delivery of goods, but one of the biggest challenges faced in this field is the navigation system that guides a robot to its destination. The navigation system must be able to identify objects in the robot's path and take evasive actions to avoid them. Developing an object detection system for an AMR requires a deep learning model that is able to achieve a high level of accuracy, with fast inference times, and a model with a compact size that can be run on embedded control systems. Consequently, object recognition requires a convolutional neural network (CNN)-based model that can yield high object classification accuracy and process data quickly. This paper introduces a new CNN-based object detection system for an AMR that employs real-world vehicle datasets. First, we create original real-world datasets of images from Banda Aceh city. We then develop a new CNN-based object identification system that is capable of identifying cars, motorcycles, people, and rickshaws under morning, afternoon, and evening lighting conditions. An SSD Mobilenetv2 FPN Lite 320 × 320 architecture is employed for retraining using these real-world datasets. Quantitative and qualitative performance indicators are then applied to evaluate the CNN model. Training the pre-trained SSD Mobilenetv2 FPN Lite 320 × 320 model improves its classification and detection accuracy, as indicated by its performance results. We conclude that the proposed CNN-based object detection system has the potential for use in an AMR.",Autonomous mobile robot (AMR); Transfer learning; CNN; Real-world datasets; Object detection
ScienceDirect,"Husnain Mushtaq, Xiaoheng Deng, Roohallah Alizadehsani, Muhammad Shahid Iqbal, Tamoor Khan, Adeel Ahmed Abbasi,","SC3D: Semantic-guided and Class-adaptive cross-domain fusion for 3D object detection in autonomous vehicles,","Expert Systems with Applications,","126359,","268,",,,https://doi.org/10.1016/j.eswa.2024.126359.,https://www.sciencedirect.com/science/article/pii/S0957417424032263,"Accurate 3D object detection is critical for safe autonomous driving and smart mobility applications. Existing fusion methods that combine camera and LiDAR data face inefficiencies, including disjointed feature extraction, spatial misalignment, and issues with detecting smaller objects due to road vibrations and clock synchronization errors. Furthermore, heuristic sampling often overlooks key foreground points, diminishing detection accuracy for smaller, occluded objects. To address these challenges, we propose the Semantic-guided and Class-adaptive Hierarchical Cross-domain Fusion (SG-CAD Fusion) framework, which enhances 3D object detection in autonomous driving with three core innovations: (1) a Point Transformer Module that enriches raw LiDAR data with probabilistic features aligned with image pixels, enabling accurate feature synchronization across domains; (2) a Semantic Class-Adaptive Sampling mechanism that preserves essential foreground points as anchor points, facilitating effective discriminative feature learning; and (3) a Progressive Cross-Domain Fusion and Multi-Layer Refinement Network that integrates CNN and Transformer features within a structured progressive hierarchy, combining CNN’s local feature detail with the global contextual depth of Transformers. On the KITTI and NuScenes benchmarks, SG-CAD Fusion achieves impressive Average Precision (AP) scores, with Bird’s Eye View (BEV) detection at 94.08% (easy), 90.11% (moderate), and 85.92% (hard), resulting in a mean AP of 90.03%. For 3D detection, SG-CAD Fusion attains 89.52% (easy), 82.65% (moderate), and 77.92% (hard), with a mean AP of 83.27%. These results validate SG-CAD Fusion’s effectiveness in boosting detection accuracy and segmentation across complex urban scenarios, highlighting its real-world applicability in autonomous vehicles and smart mobility.",3D object dejection; Semantic point sampling; Self-attention; Spatial features learning; Multi-modal fusion
ScienceDirect,"Juan Wang, Hao Yang, Zizhen Zhang, Nan Zhao, Jixiang Shao, Minghua Wu, Zhigang Ma, Jialu Zhu, Xu An Wang, Haina Song,","Detection of moving small targets in infrared images for urban traffic monitoring,","Internet of Things,","101673,","33,",,,https://doi.org/10.1016/j.iot.2025.101673.,https://www.sciencedirect.com/science/article/pii/S2542660525001878,"The Internet of Vehicles (IoV) and autonomous driving technologies require increasingly robust object detection capabilities, especially for small objects. However, reliably detecting small objects in urban traffic scenarios remains technically challenging under adverse weather conditions, including low illumination, rain, and snow. To address these challenges, we propose a fused IR–visible imaging approach using an enhanced YOLOv9 architecture. The proposed method employs a dual-branch semantic enhancement architecture, which achieves dynamic inter-modal feature weighting through a channel attention mechanism. The visible branch preserves texture details, while the infrared branch extracts thermal radiation characteristics, followed by multi-scale feature-level fusion. Firstly, we present UR-YOLO designed for detecting small targets in urban traffic environments. Secondly, we propose a novel DeeperFuse module that incorporates dual-branch semantic enhancement and channel attention mechanisms for effective multimodal feature fusion. Finally, by jointly optimizing fusion and detection losses, the method preserves critical details, enhances clarity and contrast. Experimental evaluation on the M\relax \special {t4ht=3}FD dataset demonstrates improved detection performance relative to the baseline YOLOv9 model. The results show an increase of 1.4 percentage points in mAP (from 83.3% to 84.7%) and 2.2 percentage points in APsmall (from 51.6% to 53.8%). Furthermore, our method achieves real-time processing at 30 FPS, making it suitable for deployment in urban autonomous driving scenarios. Future work will focus on enhancing model performance via multimodal fusion, lightweight design, and multi-scale feature learning. We will also develop diverse datasets to advance autonomous driving perception in complex environments.",Small object detection; Urban roads; Infrared images; Feature extraction; Feature fusion
ScienceDirect,"José M. Molina, Juan P. Llerena, Luis Usero, Miguel A. Patricio,","Advances in instance segmentation: Technologies, metrics and applications in computer vision,","Neurocomputing,","129584,","625,",,,https://doi.org/10.1016/j.neucom.2025.129584.,https://www.sciencedirect.com/science/article/pii/S0925231225002565,"Instance segmentation is an advanced technique in computer vision that focuses on identifying and classifying each individual object in an image at the pixel level. Unlike semantic segmentation, which groups pixels of similar objects without distinguishing between different instances, instance segmentation assigns unique labels to each object, even if they are of the same class. This makes it possible not only to detect the presence and category of objects in an image but also to locate each specific instance and clearly distinguish them from each other. This problem not only advances the technical and theoretical understanding of how machines see and process digital images, but also has a direct impact on various industries and sectors where computer vision is an essential part of the system. In this paper, we present the current deep learning-based technologies, the metrics used for their evaluation, and a review of general and concrete datasets in general and drone-specific contexts. The results of this study provide a compendium of easily deployable deep learning-based technologies. This review paper aims to accelerate the process of understanding and using instance segmentation technologies for the reader.",Computer Vision; Instance Segmentation; Evaluation Metrics; Datasets
ScienceDirect,"Nicolas Bustos, Mehrsa Mashhadi, Susana K. Lai-Yuen, Sudeep Sarkar, Tapas K. Das,","A systematic literature review on object detection using near infrared and thermal images,","Neurocomputing,","126804,","560,",,,https://doi.org/10.1016/j.neucom.2023.126804.,https://www.sciencedirect.com/science/article/pii/S092523122300927X,"Significant advances have been achieved in object detection techniques using visible images for applications that include military operations, autonomous driving, and security surveillance. However, the quality of visible images suffers from various environmental and illumination conditions resulting in poor detection outcomes. To remedy this, a wide range of new methodologies using visual images together with infrared (IR) images of various wavelengths (including those referred to as thermal images) are being developed and presented to the open literature. Despite this progress, many challenges of object detection still prevail, and it is important to understand them. In this paper, we present a systematic literature review documenting recent advances in object detection using predominantly IR data. We discuss our systematic review process for the identification, filtering, screening, and selection of the relevant methodologies to include in the literature review. The selected methodologies are analyzed and organized into three main groups: (1) object detection in IR images, which includes detection of labeled objects, small target detection, and background subtraction, (2) object detection on multispectral data, and (3) data fusion approaches. Reviewed studies consider different types of objects, environmental conditions, and types of images, particularly in the IR domain. Finally, we discuss some of the key limitations of the current literature and opportunities for future research for improving object detection using both visible and IR data as well as LiDAR and radar data, when applicable.",Object detection; Target recognition; Visible images; Thermal images; Infrared images
Scopus,"Mo, L.; Leng, S.; Luo, Z.; Ou, D.; Lin, X.",A study of unmanned deep learning target detection,,2025,,,,10.1145/3718751.3718803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007599085&doi=10.1145%2f3718751.3718803&partnerID=40&md5=ec79a41ddb51788f4841fbc458ec2591,"Aiming at the technical problems of dynamic adjustment and path planning in the process of unmanned target detection, deep learning, target dataset, and target dataset-based detection model research were carried out from three aspects, namely, detection principle, model training, and model experimentation, and technical methods such as deep learning, neural network, and YOLOv5 were applied to achieve environment sensing and target detection. The YOLOv5 target detection model was applied to conduct unmanned deep learning target detection experiments, and the results show that the YOLOv5 target detection model based on the ArgoVerse training set is feasible as a technical solution for unmanned deep learning target detection system. © 2024 Copyright held by the owner/author(s).",Deep learning; Self-driving; Target detection; YOLO; Deep learning; Detection models; Dynamic adjustment; Dynamic paths; Model experimentations; Model training; Principle modeling; Self drivings; Targets detection; YOLO
Scopus,"Wang, B.; Luo, P.; Yang, Y.; Zhao, Z.; Dong, R.; Guan, Y.",A Review and Prospect of Cybersecurity Research on Air Traffic Management Systems,,2025,,,,10.11999/JEIT240966,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007507863&doi=10.11999%2fJEIT240966&partnerID=40&md5=5fcc7db825f56a27cfc3ebf41acc473b,"Significance The air traffic management system is a critical national infrastructure that impacts both aerospace security and the safety of lives and property. With the widespread adoption of information, networking, and intelligent technologies, the modern air traffic management system has evolved into a space-air-ground-sea integrated network, incorporating heterogeneous systems and multiple stakeholders. The network security of the system can no longer be effectively ensured by device redundancy, physical isolation, security by obscurity, or human-in-the-loop strategies. Due to the stringent requirements for aviation airworthiness certification, the implementation of new cybersecurity technologies is often delayed. New types of cyberattacks, such as advanced persistent threats and supply chain attacks, are increasingly prevalent. Vulnerabilities in both hardware and software, particularly in embedded systems and industrial control systems, are continually being exposed, widening the attack surface and increasing the number of potential attack vectors. Cyberattack incidents are frequent, and the network security situation remains critical. Progress The United States’ Next Generation Air Transportation System (NextGen), the European Commission’s Single European Sky Air Traffic Management Research (SESAR), and the Civil Aviation Administration of China have prioritized cybersecurity in their development plans for next-generation air transportation systems. Several countries and organizations, including the United States, Japan, China, the European Union, and Germany, have established frameworks for the information security of air traffic management systems. Although network and information security for air traffic management systems is gaining attention, many countries prioritize operational safety over cybersecurity concerns. Existing security specifications and industry standards are limited in addressing network and information security. Most of them focus on top-level design and strategic directions, with insufficient attention to fundamental theories, core technologies, and key methodologies. Current review literature lacks a comprehensive assessment of assets within air traffic management systems, often focusing only on specific components such as aircraft or airports. Furthermore, research on aviation information security mainly addresses traditional concerns, without fully considering the intelligent and dynamic security challenges facing next-generation air transportation systems. Conclusions This paper comprehensively examines the complexity of the cybersecurity ecosystem in air traffic management systems, considering various entities such as electronic-enabled aircraft, communication, navigation, Surveillance/Air Traffic Management (CNS/ATM), smart airports, and intelligent computing. It focuses on asset categorization, information flow, threat analysis, attack modeling, and defense mechanisms, integrating dynamic flight phases to systematically review the current state of cybersecurity in air traffic management systems. Several scientific issues are identified that must be addressed in constructing a secure ecological framework for air traffic management. Based on the Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) model, this paper analyzes typical attack examples related to the four ecological entities (Figs. 7, 9, 12, and 14) and constructs an ATT&CK matrix for air traffic management systems (Fig. 15). Additionally, with the intelligent development goal of next-generation air transportation systems as a guide, ten typical applications of intelligent air traffic management are outlined (Fig. 13, Table 11), with a systematic analysis of the attack patterns and defense mechanisms of their intelligent algorithms (Tables 12, 13). These findings provide theoretical references for the development of smart civil aviation and the assurance of cybersecurity in China. Prospects Currently, the cybersecurity ecosystem of air traffic management systems is highly complex, with unclear mechanisms, indistinct boundaries for cybersecurity assets, and incomplete security assurance requirements. Moreover, there is a lack of comprehensive, systematic, and holistic cybersecurity design and defense mechanisms, which limits the ability to counter various subjective, human-driven, and emerging types of malicious cyberattacks. This paper highlights key research challenges in areas such as dynamic cybersecurity analysis, attack impact propagation modeling, human-in-the-loop cybersecurity analysis, and distributed intrusion detection systems. Cybersecurity analysis of air traffic management systems should be conducted within the dynamic operational environment of a space-air-ground-sea integrated network, accounting for the cybersecurity ecosystem and analyzing it across different spatial and temporal dimensions. As aircraft are cyber-physical systems, cybersecurity threat analysis should focus on the interrelated propagation mechanisms between security and safety, as well as their cascading failure models. Furthermore, humans serve as the last line of defense in cybersecurity. When performing threat modeling and risk assessment for avionics systems, it is crucial to fully incorporate “human-in-the-loop” characteristics to derive comprehensive and objective conclusions. Finally, the design, testing, certification, and updating of civil aviation avionics systems are constrained by strict airworthiness requirements, preventing the rapid implementation of advanced cybersecurity technologies. Distributed anomaly detection systems, however, currently represent an effective technical approach for combating cyberattacks in air traffic management systems. © 2025 Science Press. All rights reserved.",Air traffic management system; Cyber security; Research challenges; Research review; Air navigation; Airport vehicular traffic; Authentication; Helicopter services; Information management; Medium access control; Production control; Research and development management; Risk management; Sensitive data; Strategic planning; Supply chain management; Air Traffic Management; Air Traffic Management Systems; Air-traffic management system; Cyber security; Cyber-attacks; Defence mechanisms; Human-in-the-loop; Next-generation air transportation systems; Research challenges; Research review; Cyber attacks
Scopus,"Zhang, H.; Zhang, Q.; Gong, Y.; Yao, F.; Xiao, P.",MDCFVit-YOLO: A model for nighttime infrared small target vehicle and pedestrian detection,,2025,,,,10.1371/journal.pone.0324700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008297656&doi=10.1371%2fjournal.pone.0324700&partnerID=40&md5=3a0e8442e113260f4b7361390542ed72,"An MDCFVit-YOLO model based on the YOLOv8 algorithm is proposed to address issues in nighttime infrared object detection such as low visibility, high interference, and low precision in detecting small objects. The backbone network uses the lightweight Repvit model, improving detection performance and reducing model weight through transfer learning. The proposed MPA module integrates multi-scale contextual information, capturing complex dependencies between spatial and channel dimensions, thereby enhancing the representation capability of the neural network. The CSM module dynamically adjusts the weights of feature maps, enhancing the model of sensitivity to small targets. The dynamic automated detection head DAIH improves the accuracy of infrared target detection by dynamically adjusting regression feature maps. Additionally, three innovative loss functions—focalerDIoU, focalerGIOU and focalerShapeIoU are proposed to reduce losses during the training process. Experimental results show that the detection accuracy of 78% for small infrared nighttime targets, with a recall rate of 58.6%, an mAP value of 67%. and a parameter count of 20.9M for the MDCFVit-YOLO model. Compared to the baseline model YOLOv8, the mAP increased by 6.4%, with accuracy and recall rates improved by 4.5% and 5.7%, respectively. This research provides new ideas and methods for infrared target detection, enhancing the detection accuracy and real-time performance. © 2025 Zhang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Algorithms; Humans; Infrared Rays; Models, Theoretical; Neural Networks, Computer; Pedestrians; algorithm; article; controlled study; diagnosis; diagnostic test accuracy study; female; human; infrared radiation; nerve cell network; pedestrian; transfer of learning; visibility; artificial neural network; theoretical model"
Scopus,"Şahin, D.; Torkul, O.; Şişci, M.; Diren, D.D.; Yılmaz, R.; Kibar, A.",Real-Time Classification of Chicken Parts in the Packaging Process Using Object Detection Models Based on Deep Learning,,2025,,,,10.3390/pr13041005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003719534&doi=10.3390%2fpr13041005&partnerID=40&md5=17e224354041d7279300df289e2c726e,"Chicken meat plays an important role in the healthy diets of many people and has a large global trade volume. In the chicken meat sector, in some production processes, traditional methods are used. Traditional chicken part sorting methods are often manual and time-consuming, especially during the packaging process. This study aimed to identify and classify the chicken parts for their input during the packaging process with the highest possible accuracy and speed. For this purpose, deep-learning-based object detection models were used. An image dataset was developed for the classification models by collecting the image data of different chicken parts, such as legs, breasts, shanks, wings, and drumsticks. The models were trained by the You Only Look Once version 8 (YOLOv8) algorithm variants and the Real-Time Detection Transformer (RT-DETR) algorithm variants. Then, they were evaluated and compared based on precision, recall, F1-Score, mean average precision (mAP), and Mean Inference Time per frame (MITF) metrics. Based on the obtained results, the YOLOv8s model outperformed the other models developed with other YOLOv8 versions and the RT-DETR algorithm versions by obtaining values of 0.9969, 0.9950, and 0.9807 for the F1-score, mAP@0.5, and mAP@0.5:0.95, respectively. It has been proven suitable for real-time applications with an MITF value of 10.3 ms/image. © 2025 by the authors.",chicken parts; deep learning; image processing; object detection; reducing waste and costs; RT-DETR; YOLOv8; Image coding; Motion analysis; Optical flows; Poultry; Chicken part; Deep learning; Images processing; Objects detection; Packaging process; Real-time detection; Real-time detection transformer; Reducing costs; Reducing waste; You only look once version 8; Photointerpretation
Scopus,"Zhou, S.; Zhou, H.; Qian, L.",A multi-scale small object detection algorithm SMA-YOLO for UAV remote sensing images,,2025,,,,10.1038/s41598-025-92344-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000297089&doi=10.1038%2fs41598-025-92344-7&partnerID=40&md5=c3f1c2a079398ca95f659fa98c6a2e3c,"Detecting small objects in complex remote sensing environments presents significant challenges, including insufficient extraction of local spatial information, rigid feature fusion, and limited global feature representation. In addition, improving model performance requires a delicate balance between improving accuracy and managing computational complexity. To address these challenges, we propose the SMA-YOLO algorithm. First, we introduce the Non-Semantic Sparse Attention (NSSA) mechanism in the backbone network, which efficiently extracts non-semantic features related to the task, thus improving the model’s sensitivity to small objects. In the model’s throat, we design a Bidirectional Multi-Branch Auxiliary Feature Pyramid Network (BIMA-FPN), which integrates high-level semantic information with low-level spatial details, improving small object detection while expanding multi-scale receptive fields. Finally, we incorporate a Channel-Space Feature Fusion Adaptive Head (CSFA-Head), which fully handles multi-scale features and adaptively handles consistency problems of different scales, further improving the robustness of the model in complex scenarios. Experimental results on the VisDrone2019 dataset show that SMA-YOLO achieves a 13% improvement in mAP compared to the baseline model, demonstrating exceptional adaptability in small object detection tasks for remote sensing imagery. These results provide valuable insights and new approaches to further advance research in this area. © The Author(s) 2025.",Feature fusion; Multi-branch auxiliary; Object detection; Remote sensing images; algorithm; article; controlled study; detection algorithm; diagnosis; human; imagery; receptive field; remote sensing
Scopus,"Liu, Z.; Wu, J.; Cai, Y.; Wang, H.; Chen, L.; Liu, Q.",Dual-stage feature specialization network for robust visual object detection in autonomous vehicles,,2025,,,,10.1038/s41598-025-99363-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004205826&doi=10.1038%2fs41598-025-99363-4&partnerID=40&md5=3669b8b1b4d6a6a9029d9e699567395d,"Efficient feature representation is critical for accurate visual perception in autonomous vehicles. Existing two-stage object detection methods often suffer from feature interference between candidate region generation and classification regression tasks, leading to suboptimal performance in complex scenes. To address this, we propose a Dual-Stage Feature Specialization Network (DSFSN) that decouples feature extraction: MobileNetV3 is employed for lightweight candidate region generation, while ResNet-FPN enhances multi-scale feature fusion for precise classification. Extensive experiments on PASCAL VOC and MS COCO datasets demonstrate state-of-the-art performance, achieving 81.6% mAP (9.3% higher than Faster R-CNN) and 29.3% AP on MS COCO, with a 14.9% improvement in small object detection. Real-world tests under diverse conditions (e.g., rain, night) validate the robustness of our method for autonomous driving applications. This work provides a novel framework for balancing accuracy and efficiency in visual perception systems. © The Author(s) 2025.",Autonomous vehicle; Complex environment perception; Complex problem application; Two-stage feature extraction; Visual object detection; article; autonomous vehicle; controlled study; diagnosis; feature extraction; female; human; human experiment; night; normal human; rain; residual neural network; specialization
Scopus,"Zhang, L.; Gong, K.; Yin, X.; Fu, T.; Shangguan, Q.",Development of a car-following model incorporating the oppression effects of large trucks,,2025,,,,10.1016/j.physa.2025.130793,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009508694&doi=10.1016%2fj.physa.2025.130793&partnerID=40&md5=01a43ee5da089d1aca3d8e475e844d40,"The presence of large trucks on highways significantly alters the driving behavior of surrounding vehicles, especially by disrupting the car-following patterns of smaller vehicles due to their size, speed differences, and visibility constraints. This study focuses on investigating the mechanism of how large trucks on highways impact the car-following behavior of surrounding drivers. The research begins by utilizing unmanned aerial vehicles (UAV) to collect vehicle trajectory data. A novel concept, termed the “oppression effects” of large trucks, is introduced, and its influence is characterized using potential field theory. Subsequently, a car-following model is developed that incorporates the oppression effects of large trucks. To illustrate the distribution of these effects, intensity contour maps are employed based on various motion states of the large truck. Finally, the proposed model is then calibrated using real-world trajectory data, and its predictive accuracy is assessed against benchmark car-following models. The proposed model improves trajectory prediction accuracy by over 40.9 % in RMSE and 22.4 % in MAE compared to classical models. The results demonstrate that the car-following model, which accounts for the oppression effects of large trucks, yields more accurate predictions of the driving behavior of vehicles following large trucks on highways. This research contributes to the theoretical foundation for behavior modeling and risk control in mixed traffic environments involving trucks and cars, ultimately enhancing safety for drivers in proximity to large trucks. © 2025 Elsevier B.V.",Car-following model; Highway; Large trucks; Oppression effects; Potential field theory; Automobiles; Behavioral research; Risk assessment; Trajectories; Trucks; Unmanned aerial vehicles (UAV); Car following; Car-following modeling; Driving behaviour; Field theory; Highway; Large-trucks; Oppression effect; Potential field; Potential field theory; Small vehicle; Antennas
Scopus,"Alesaily, Z.; Albialy, A.","Future cities: A bibliometric review, 1875 to 2024",,2025,,,,10.1016/j.sftr.2025.100801,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007872906&doi=10.1016%2fj.sftr.2025.100801&partnerID=40&md5=ed2bb979bca171d89ce65f6d461a5c82,"The term ""future cities"" represents a paradigm shift in urban development. It aims to create sustainable urban environments that integrate technology, strategic management, and citizen participation in order to address challenges such as rapid urbanization and climate change. It attains its objectives through the integration of interdisciplinary research, systems thinking, and continuous practice. Despite considerable efforts in the field of future cities' research, there is currently no single comprehensive review that can adequately address the full range of aspects related to the topic. Reviews, on the other hand, tend to be fragmented, concentrating on specific topics like smart cities, sustainable smart cities, livable cities, digitization, the impact of the COVID-19 pandemic, the environment, natural resources, and new innovations and technologies. Therefore, we conducted a bibliometric review of future cities, which included all documents listed in Scopus, Web of Science, and Dimensions from 1875 to 2024. This review identified publication trends, annual growth, and the most prolific authors, institutions, and countries in the field. We also identified networks of co-authorship, as well as the most influential citations by authors, journals, and popular documents. Furthermore, the analysis revealed the key works that established the intellectual and conceptual framework for the field of future cities. The conceptual structure of future cities studies revealed four main clusters: first, smart cities and decision-making; second, urban planning and future development; third, sustainable development; and fourth, city and human studies. This process identified research gaps and potential future directions. In terms of future research directions, the findings suggest a lack of studies that explicitly link the theme of ""future cities"" with concepts such as urban identity and artificial intelligence, despite AI's growing importance in recent years. This observation highlights the need for increased research efforts in these areas. Furthermore, addressing concerns about intelligence requires research of a contradictory nature. The study concludes with a valuable reference for researchers, urban planners, architects, social scientists, and specialists in related fields of research, including technology and the environment. It emphasizes that the major challenge facing urban planners is to integrate these disciplines into a unified and coherent system that promotes human well-being in a sustainable future. © 2025 The Author(s)",Cities; City; Future; Future cities; Smart cities; Smart sustainable cities; Sustainable development; Sustainable futures; Urban planning; Urbanization
Scopus,"Ye, X.; Li, S.; Gong, W.; Li, X.; Li, X.; Dadashova, B.; Li, W.; Du, J.; Wu, D.",Street View Imagery in Traffic Crash and Road Safety Analysis: A Review,,2025,,,,10.1007/s12061-025-09653-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003804687&doi=10.1007%2fs12061-025-09653-7&partnerID=40&md5=cbb2e192a12ec49c3542e5bcebfe8609,"Preventing traffic crashes presents a formidable challenge due to the intricate interplay between drivers and other participants within a complex urban infrastructure. In recent years, increasing studies on road safety involved computer vision and machine learning to detect visual features from street view imagery (SVI) and explore their impacts on crashes, though the recent progress is poorly understood. This paper conducted a comprehensive review of existing literature to investigate how SVI has been used in traffic crashes and road safety studies, utilizing a broad database collection including Scopus, Web of Science, and Transport Research International Documentation. We categorized SVI-generated features into two types of factors, explored their relationship with traffic crashes, and examined the prevalent detection models. Our review demonstrated that SVI plays an important role in capturing road design and driving environment factors, which significantly influence the frequency and risk of traffic crashes. These findings underscore the significant impact of these street visual factors on road safety. Through a systematic review of recent progress, we also identified challenges and future research opportunities for SVI applications in traffic crash study, such as the potential use of large language models. © The Author(s), under exclusive licence to Springer Nature B.V. 2025.",Machine learning; Road safety; Street view; Streetscape features; Traffic crashes; computer vision; environmental factor; machine learning; road traffic; safety; satellite imagery
Scopus,"Chen, Y.; Li, X.; Luan, C.; Hou, W.; Liu, H.; Zhu, Z.; Xue, L.; Zhang, J.; Liu, D.; Wu, X.; Wei, L.; Jian, C.; Li, J.",Cross-level interaction fusion network-based RGB-T semantic segmentation for distant targets,,2025,,,,10.1016/j.patcog.2024.111218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211574922&doi=10.1016%2fj.patcog.2024.111218&partnerID=40&md5=30c286bd6de2e0cb279b581c791628c5,"RGB-T segmentation represents an innovative approach driven by advancements in multispectral detection and is poised to replace traditional RGB segmentation methods. An effective cross-modality feature fusion module is essential for this technology. The precise segmentation of distant objects is another significant challenge. Focused on these two areas, we propose an end-to-end distant object feature fusion network (DOFFNet) for RGB-T segmentation. Initially, we introduce a cross-level interaction fusion strategy (CLIF) and an inter-correlation fusion method (IFFM) in the encoder to enhance multi-scale feature expression and improve fusion accuracy. Subsequently, we propose a residual dense pixel convolution (R-DPC) in the decoder with a trainable upsampling unit that dynamically reconstructs information lost during encoding, particularly for distant objects whose features may vanish after pooling. Experimental results show that our DOFFNet achieves a top mean pixel accuracy of 75.8% and dramatically improves accuracy for four classes, including objects occupying as little as 0.2%–2% of total pixels. This improvement ensures more reliable and effective performance in practical applications, particularly in scenarios where small object detection is critical. Moreover, it demonstrates potential applicability in other fields like medical imaging and remote sensing. © 2024 The Authors",Cross modality; Distant object; Feature fusion; Multi-scale information; Semantic segmentation; Image coding; Medical imaging; Object detection; Proximity sensors; Signal encoding; Cross levels; Cross modality; Distant object; Features fusions; Innovative approaches; Multiscale information; Multispectral detection; Network-based; Segmentation methods; Semantic segmentation; Semantic Segmentation
Scopus,"Azizi, A.; Charalambous, P.; Chrysanthou, Y.",DeepSafe:Two-level deep learning approach for disaster victims detection,,2025,,,,10.1016/j.vrih.2024.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002685579&doi=10.1016%2fj.vrih.2024.08.005&partnerID=40&md5=98cd7776fc4740b4eb6791106b682f14,"Background: Efficient disaster victim detection (DVD) in urban areas after natural disasters is crucial for minimizing losses. However, conventional search and rescue (SAR) methods often experience delays, which can hinder the timely detection of victims. SAR teams face various challenges, including limited access to debris and collapsed structures, safety risks due to unstable conditions, and disrupted communication networks. Methods: In this paper, we present DeepSafe, a novel two-level deep learning approach for multilevel classification and object detection using a simulated disaster victim dataset. DeepSafe first employs YOLOv8 to classify images into victim and non-victim categories. Subsequently, Detectron2 is used to precisely locate and outline the victims. Results: Experimental results demonstrate the promising performance of DeepSafe in both victim classification and detection. The model effectively identified and located victims under the challenging conditions presented in the dataset. Conclusion: DeepSafe offers a practical tool for real-time disaster management and SAR operations, significantly improving conventional methods by reducing delays and enhancing victim detection accuracy in disaster-stricken urban areas. © 2024 Beijing Zhongke Journal Publishing Co. Ltd",Deep learning; Disaster management; Victims detection; Victims identification; YOLO; Classification (of information); Deep learning; Disaster prevention; Collapsed structures; Deep learning; Disaster management; Learning approach; Natural disasters; Search and rescue; Urban areas; Victim detections; Victim identifications; YOLO; Disasters
Scopus,"Pagire, V.; Chavali, M.; Kale, A.",A comprehensive review of object detection with traditional and deep learning methods,,2025,,,,10.1016/j.sigpro.2025.110075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004257960&doi=10.1016%2fj.sigpro.2025.110075&partnerID=40&md5=53c64d962a3804252a619801ad9a7d13,"Object detection is one of the most important and challenging tasks of computer vision. It has numerous applications in the fields of agriculture, defence, retail markets and manufacturing units, transportation, social media platforms, medical, wildlife monitoring and conservation. This survey aims to give researchers a comprehensive understanding of the current state of object detection algorithms. In this review, object detection and its different aspects have been covered in detail. This review paper starts with a quick overview of object detection followed by traditional and deep learning models for object detection. The section on deep learning models provides a comprehensive overview of one-stage and two-stage object detectors. A detailed discussion is given of the transformer-based detectors and lightweight networks category. Additionally, the evaluation metrics used for object detection methods are discussed systematically. The best object detection algorithms for different applications are discussed at the end of the survey. This survey is useful for beginners who want to study different object detection algorithms and their use in different applications. © 2025 Elsevier B.V.",Classification; Deep learning; Feature extraction; Lightweight networks; Object detection; Traditional methods; Object recognition; Deep learning; Features extraction; Learning methods; Learning models; Lightweight network; Market units; Object detection algorithms; Objects detection; Retail market; Traditional method; Object detection
Scopus,"Liu, G.; Jiang, W.; Sun, C.; Ning, N.; Wang, R.; Buhari, A.",Object detection algorithm for autonomous driving: Design and real-time performance analysis of AttenRetina model,,2025,,,,10.1016/j.aej.2025.02.063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001072362&doi=10.1016%2fj.aej.2025.02.063&partnerID=40&md5=f62890f3197244dfbbb6517537b662d1,"With the continuous advancement of autonomous driving technology, how to efficiently and accurately detect objects (such as pedestrians, cyclists, traffic signs, etc.) has become a core challenge to improve the safety and reliability of the system. Existing object detection models still face the problem of insufficient accuracy and robustness when dealing with complex backgrounds and occlusions. To this end, this paper proposes the AttenRetina object detection model for autonomous driving, which combines the multi-scale feature fusion module (FPN) and the attention mechanism to significantly improve the detection ability of the model in various scenarios. Experimental results show that AttenRetina performs well on the KITTI and MS COCO datasets, and significantly outperforms other mainstream models in key indicators such as Precision, Recall and mAP. The mAP on the KITTI dataset reaches 0.86, which is more than 12% higher than the basic model, showing its great potential in autonomous driving object detection. The research in this paper provides an effective solution to the object detection problem in autonomous driving systems, and provides an important reference for future algorithm optimization and application. © 2025 The Authors",Autonomous driving; Deep learning; Multi-scale features; Object localization; Small object detection; Autonomous driving; Deep learning; Design time; Detection models; Multi-scale features; Object detection algorithms; Object localization; Objects detection; Real time performance; Small object detection; Traffic signs
Scopus,"Dai, W.; lv, J.; Xiang, R.; Jin, S.",Study on end-to-end detection method for surface defects of automotive sheet metal parts,,2025,,,,10.1007/s11554-025-01656-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000056862&doi=10.1007%2fs11554-025-01656-4&partnerID=40&md5=80939a1d181f48ffa7807cb556fd01b9,"Sheet metal parts account for more than 60% of the total automotive parts, and their defects can seriously affect the safety of automobile operations. Therefore, it is very important to detect defects in sheet metal parts during the production process. Due to the small size of defects in sheet metal parts, and high detection precision required, the traditional detection method cannot meet the requirements. And the factory production speed is fast, if the detection speed is low, it will cause defects to escape. Therefore, we propose an end-to-end detection method for automotive sheet metal parts surface defects. To effectively improve the detection speed, the dual regression classification strategy is proposed, which removes the NMS post-processing. Gradient information branch is added to provide rich gradient information for the model and mitigate the information loss during long convolution. Use the SPD-Conv module, optimized for small-size defects detection, to retain complete space information. Finally, the model is evaluated on the automotive sheet metal parts defect dataset. The experimental results show that the proposed method is superior to the benchmark methods in precision and speed, with mAP of 92.32% and FPS of 39.06, which achieves end-to-end detection. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.",Automotive sheet metal part; convolutional neural network; Deep learning; Defect detection; End-to-end; Deep neural networks; Automotive sheet metal part; Automotive sheet metals; Convolutional neural network; Deep learning; Defect detection; Detection methods; Detection speed; End to end; Gradient informations; Sheet metal parts; Convolutional neural networks
Scopus,"Liu, Z.; Zhang, Z.",The Research on an Improved YOLOX-Based Algorithm for Small-Object Road Vehicle Detection,,2025,,,,10.3390/electronics14112179,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007694460&doi=10.3390%2felectronics14112179&partnerID=40&md5=35a83152fc5b3f354956d4e01b3fb3d6,"To address the challenges of missed detections and false positives caused by dense vehicle distribution, occlusions, and small object sizes in complex traffic scenarios, this paper proposes an improved YOLOX-based vehicle detection algorithm with three key innovations. First, we design a novel Wavelet-Enhanced Convolution (WEC) module that expands the receptive field to enhance the model’s global perception capability. Building upon this foundation, we integrate the SimAM attention mechanism, which improves feature saturation by adaptively fusing semantic features across different channels and spatial locations, thereby strengthening the network’s multi-scale generalization ability. Furthermore, we develop a Varifocal Intersection over Union (VIoU) bounding-box regression loss function that optimizes convergence in multi-scale feature learning while enhancing global feature extraction capabilities. The experimental results on the VisDrone dataset demonstrate that our improved model achieves performance gains of 0.9% mAP and 1.8% mAP75 compared to the baseline version, effectively improving vehicle detection accuracy. © 2025 by the authors.",object detection; SimAM attention; VIoU; YOLOX; Multi-task learning; Object recognition; False positive; Missed detections; Object size; Objects detection; Road vehicles; SimAM attention; Small objects; Varifocal intersection over union; Vehicles detection; YOLOX; Object detection
Scopus,"Arrasmith, W.W.","Handbook of systems engineering and analysis of electro-optical and infrared systems: Concepts, principles, and methods: Second edition",,2025,,,,10.1201/9781003624097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009318236&doi=10.1201%2f9781003624097&partnerID=40&md5=28e4855a0b6d00e906fe3bf85c842e58,"There has been a lot of innovation in systems engineering and some fundamental advances in the fields of optics, imaging, lasers, and photonics that warrant attention. This volume focuses on concepts, principles, and methods of systems engineering-related topics from government, industrial, and academic settings such as development and operations (DevOps), agile methods, and the concept of the ""digital twin."" Handbook of Systems Engineering and Analysis of Electro-Optical and Infrared Systems: Concepts, Principles, and Methods offers more information on decision and risk analysis and statistical methods in systems engineering such as design of experiments (DOX) methods, hypothesis testing, analysis of variance, blocking, 2k factorial analysis, and regression analysis. It includes new material on systems architecture to properly guide the evolving system design and bridge the gap between the requirements generation and design efforts. The integration of recent high-speed atmospheric turbulence research results in the optical technical examples and case studies to illustrate the new developments is also included. A presentation of new optical technical materials on adaptive optics (AO), atmospheric turbulence compensation (ATC), and laser systems along with more are also key updates that are emphasized in the second edition 2-volume set. Because this volume blends modern-day systems engineering methods with detailed optical systems analysis and applies these methodologies to EO/IR systems, this new edition is an excellent text for professionals in STEM disciplines who work with optical or infrared systems. It's also a great practical reference text for practicing engineers and a solid educational text for graduate-level systems engineering, engineering, science, and technology students. This book is also available as a set Handbook of Systems Engineering and Analysis of Electro-Optical and Infrared Systems (978-1-032-22242-4). © 2025 William Wolfgang Arrasmith. All rights reserved.","Adaptive optics; Atmospheric turbulence; Engineering education; Engineering research; Infrared devices; Optical systems; Statistical methods; STEM (science, technology, engineering and mathematics); Students; Systems analysis; Systems thinking; Agile methods; Development and operations; Electro-optical systems; Hypothesis testing; Infrared systems; Optical-; Risk analyze; System concepts; System methods; Systems principles; Design of experiments"
Scopus,"Yang, M.; Han, S.",ETS-YOLO: An Efficient YOLO-based Model for Real-Time Traffic Sign Recognition,,2025,,,,10.1007/s11760-025-04276-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006762553&doi=10.1007%2fs11760-025-04276-4&partnerID=40&md5=4dc35886c0e5557757463a7e932141d3,"In addressing the challenges inherent in traffic sign recognition, such as susceptibility to interference, difficulty in detecting small targets, and the trade-off between real-time performance and accuracy, we propose an Efficient Traffic Sign YOLO (ETS-YOLO) model to fulfill the demands of real-time sign recognition. Built on the baseline YOLOv5 model, we adopt Partial Convolution (PConv) to redesign the Cross Stage Partial bottleneck including 3 convolutional layers (C3) module of the network, resulting in the proposed C3Efficient module, which reduces redundant calculations and memory accesses, making the model more efficient and lightweight. Accordingly, we utilize the Dynamic Upsampler (DySample) up-sampling to enhance the up-sampling effect during the Feature Pyramid Network (FPN) stage and design a new Normalized Wasserstein Distance (NWD) loss to redefine the anchor positional loss function mechanism, leading to better detection accuracy for tiny objects. Also, applying the Soft-NMS algorithm facilitates anchor filtering optimization, notably improving the detection accuracy of adjacent occluded targets. As a result of these improvements, the ETS-YOLO model achieves a mean Average Precision (mAP_0.5) of 0.805 when trained and tested on 45 types of traffic signs within the TT100K dataset, demonstrating a noteworthy improvement of 0.052 compared to the baseline model. In terms of model complexity, the ETS-YOLO model has 6.0 million parameters and a computational load of 14.3 GFLOPs, which represent reductions of 16.0% in model size and 12.3% in computational load, respectively, compared to the baseline model. Meanwhile, the model achieves an inference latency of 26.3 ms, showing a decrease of 10% in inference time over the baseline model. Ultimately, our model achieves a favorable balance between lightweight and accuracy compared to other state-of-the-art models. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",Anchor filtering optimization; Model light-weighting; Tiny object detection; Traffic sign recognition; You only look once (YOLO); Anchor filtering; Anchor filtering optimization; Lightweighting; Model light; Model light-weighting; Objects detection; Optimisations; Tiny object detection; Traffic sign recognition; You only look once; NP-hard
Scopus,"Kang, S.; Sun, Y.; Li, S.; Xu, Y.; Li, Y.; Chen, G.; Xue, F.",A lightweight neural network search algorithm based on in-place distillation and performance prediction for hardware-aware optimization,,2025,,,,10.1016/j.engappai.2025.110775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001817504&doi=10.1016%2fj.engappai.2025.110775&partnerID=40&md5=4f5fb52a8eaeacfe8bdfe7e598ca2272,"Due to the limited computing resources of edge devices, traditional object detection algorithms struggle to meet the efficiency and accuracy requirements of autonomous driving. Consequently, designing a neural network model that balances hardware resource requirements, operating speed, and accuracy is crucial. To address this, by integrating algorithm with hardware characteristics, we propose a lightweight neural network architecture search algorithm based on in-place distillation and performance predictor (LNIP). Initially, we focus on optimizing the operators of the you only look once version 8 nano (YOLOv8n) and dynamically adjust its network structure. Then, we trained a super-network using a progressive shrinking strategy, the sandwich rule, and in-place distillation. Subsequently, we employed a Gaussian process to model the relationship between network architecture and accuracy, utilizing encoding methods and custom kernel function to develop high-performance predictor. Finally, during the search process, we introduce a reward function based on Pareto optimality to balance the performance of the model with hardware constraints. Building upon this foundation, we design an efficient search algorithm based on the performance predictor to progressively explore the optimal network structure tailored to hardware characteristics. We compared our lightweight network with state-of-the-art methods on the BDD100K, COCO, and PASCAL VOC datasets and deployed it on the Black Sesame A1000 and NVIDIA Xavier for comprehensive evaluation. On the NVIDIA Xavier, the lightweight network achieves a latency of 11.81 ms and an edge precision of 46.1 %. These experimental results demonstrate that our method outperforms existing methods in balancing hardware constraints and model performance. © 2025 Elsevier Ltd",In-place distillation; Neural network architecture search; Object detection; Pareto optimality; Performance predictor; Hardware characteristics; Hardware constraints; In-place distillation; Neural network architecture; Neural network architecture search; Neural-networks; Objects detection; Pareto-optimality; Performance predictor; Search Algorithms; Pareto principle
Scopus,"Shi, P.; Wu, W.; Yang, A.",MPVF: Multi-Modal 3D Object Detection Algorithm with Pointwise and Voxelwise Fusion,,2025,,,,10.3390/a18030172,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001304732&doi=10.3390%2fa18030172&partnerID=40&md5=ff35a35273858c7415e72a7769b13646,"3D object detection plays a pivotal role in achieving accurate environmental perception, particularly in complex traffic scenarios where single-modal detection methods often fail to meet precision requirements. This highlights the necessity of multi-modal fusion approaches to enhance detection performance. However, existing camera-LiDAR intermediate fusion methods suffer from insufficient interaction between local and global features and limited fine-grained feature extraction capabilities, which results in inadequate small object detection and unstable performance in complex scenes. To address these issues, the multi-modal 3D object detection algorithm with pointwise and voxelwise fusion (MPVF) is proposed, which enhances multi-modal feature interaction and optimizes feature extraction strategies to improve detection precision and robustness. First, the pointwise and voxelwise fusion (PVWF) module is proposed to combine local features from the pointwise fusion (PWF) module with global features from the voxelwise fusion (VWF) module, enhancing the interaction between features across modalities, improving small object detection capabilities, and boosting model performance in complex scenes. Second, an expressive feature extraction module, improved ResNet-101 and feature pyramid (IRFP), is developed, comprising the improved ResNet-101 (IR) and feature pyramid (FP) modules. The IR module uses a group convolution strategy to inject high-level semantic features into the PWF and VWF modules, improving extraction efficiency. The FP module, placed at an intermediate stage, captures fine-grained features at various resolutions, enhancing the model’s precision and robustness. Finally, evaluation on the KITTI dataset demonstrates a mean Average Precision (mAP) of 69.24%, a 2.75% improvement over GraphAlign++. Detection accuracy for cars, pedestrians, and cyclists reaches 85.12%, 48.61%, and 70.12%, respectively, with the proposed method excelling in pedestrian and cyclist detection. © 2025 by the authors.",3D object detection; autonomous driving; image; multi-modal fusion; point cloud; Adaptive boosting; Image coding; Image fusion; 3D object; 3d object detection; Autonomous driving; Fusion modules; Image; Multi-modal; Multi-modal fusion; Objects detection; Point wise; Point-clouds; Bicycles
Scopus,"Song, Y.; Chen, Z.; Yang, H.; Liao, J.",GS-LinYOLOv10: A drone-based model for real-time construction site safety monitoring,,2025,,,,10.1016/j.aej.2025.01.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280077&doi=10.1016%2fj.aej.2025.01.021&partnerID=40&md5=a7748bfc0241b41b8c8137c9281c3d63,"Real-time safety monitoring on construction sites is essential for ensuring worker safety, but traditional detection methods face challenges in dynamic environments with moving objects, occlusions, and complex conditions. To address these limitations, we propose GS-LinYOLOv10, an improved model based on YOLOv10, specifically designed for drone-based safety monitoring. The GSConv module introduces a lightweight feature extraction mechanism, reducing computational complexity without compromising detection accuracy. The Linformer-based attention mechanism efficiently captures global context, addressing challenges in dynamic and complex environments. The model integrates IoT sensor data for real-time feedback, incorporates the GSConv module for lightweight feature extraction, and utilizes a Linformer-based attention mechanism to efficiently capture global context. These innovations reduce computational complexity while significantly improving detection accuracy. Experimental results show that GS-LinYOLOv10 achieves a precision of 91.2% and a mean average precision (mAP) of 89.4%, outperforming existing models. The integration of IoT sensors allows the drone system to dynamically adjust its monitoring focus, improving adaptability to changing environments and enhancing hazard detection. This research provides an advanced, drone-based IoT-enhanced solution for real-time construction site safety monitoring, offering a more effective and efficient approach to safety management. © 2025",Construction site safety; Drone-based monitoring; GSConv; IoT integration; Linformer; Real-time safety detection; Aircraft; Safety engineering; Construction site safety; Drone-based monitoring; Dynamic environments; Gsconv; IoT integration; Linformer; Real- time; Real-time construction; Real-time safety detection; Safety monitoring; Aircraft detection
Scopus,"Zhou, J.; Chen, X.; Gao, J.; Tang, Y.",Lightweight detection model for infrared targets of traffic participants in complex environments,,2025,,,,10.13637/j.issn.1009-6094.2024.1652,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009100962&doi=10.13637%2fj.issn.1009-6094.2024.1652&partnerID=40&md5=da8ddb18111104ffe1696894991020d8,"To tackle the challenges of low recognition accuracy, subpar real-time performance, and difficulties in lightweight deployment for multi-class infrared target detection of traffic participants In Intelligent Connected Vehicles (ICVs) and traffic monitoring systems, we propose a lightweight detection model called CNDS YOLO, which is based on an enhanced YOLOv9m architecture. First, we designed a lightweight CE MobileNetv3 structure that incorporates the Generalized Efficient Layer Aggregation Network (C ELAN) to replace the entire backbone network. This approach significantly reduces the parameter count while enhancing detection accuracy. Second, Depthwise Separable Convolution (DSConv) was integrated to replace conventional convolution in the neck feature fusion layer, auxiliary reversible CBL module, and DS SPPF module. This modification further improves real-time performance and enables lightweight deployment capabilities. Additionally, a normalized N Swin Transformer module was incorporated into the neck feature fusion layer, significantly enhancing the model’s ability to capture and fuse features related to infrared targets, which in turn improves detection accuracy and stability. Furthermore, a new 160 × 160 small target detection head was developed at the detection end to increase sensitivity to small and occluded targets. Finally, experiments conducted on the FLIR2 dataset demonstrated that the CNDS YOLO model outperformed the baseline YOLOv9m model, reducing parameters and FLOPs by 31. 0% and 35. 5%, respectively, while enhancing detection speed (FS) and mean Average Precision (AP) by 24. 2% and 7. 0%, respectively. Compared to mainstream models, the CNDS YOLO model achieved the highest values in both Average Precision (AP) and detection speed (FS) . In the ablation experiments, each of the four improved modules within CNDS YOLO was quantitatively assessed. For example, the DSConv module resulted in a reduction of parameters and FLOPs by 24. 5% and 37. 6%, respectively, while the N Swin Transformer module improved Recall (R) and AP by 4. 1% and 4. 3%, respectively. The visual detection experiments effectively validated the model’s sensitivity to real-world infrared targets, demonstrating its robustness and suitability for deployment. The CNDS YOLO model significantly improves the speed and accuracy of embedded devices in detecting traffic participants within complex environments, thereby offering theoretical support for the safe operation of intelligent transportation systems. © 2025 Science China Press. All rights reserved.",CNDS YOLO model; infrared target; lightweight; safety engineering; small target detection; traffic participants
Scopus,"Xu, H.; Wang, X.",Perception of distance and speed of front vehicle based on vehicle image features,,2025,,,,10.3785/j.issn.1008-973X.2025.06.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006945786&doi=10.3785%2fj.issn.1008-973X.2025.06.013&partnerID=40&md5=d038ef0579e9824c311da68be5f6fb4a,"A multimodal perception method for distance and speed of front vehicle integrating vehicle image features was proposed for front vehicle detection and operational state perception in driving scenarios. The position features of vehicles in images were detected by an improved SW-YOLOv8n model, and the relative lateral and longitudinal distances to the front vehicle were calculated using geometric algorithms. A feature extraction network was designed to extract vehicle features, where image feature vectors were fused through serial concatenation, and a neural network for vehicle distance measurement was established. The multi-feature fusion module was integrated with the distance measurement neural network to construct an end-to-end front vehicle distance perception model and a vehicle tracking-based speed estimation model, which synchronously output precise distance estimations and stable speed tracking results. Experimental results demonstrated that on the test dataset, the SW-YOLOv8n model achieved improvements of 1.6 percentage points in mAP50 and 2.3 percentage points in mAP50−95 compared to the baseline YOLOv8n, while maintaining a detection speed of 260.11 frames per second. Within a lateral range of 9.5 m and a longitudinal range of 50 m, under unobstructed conditions, the preceding vehicle distance perception model exhibited an average relative error of 1.87% between predicted and actual distances, while under occluded conditions, the average relative error was 2.02%. The speed measurement results of the tracking-based model exhibited significant stability, confirming the method’s effectiveness for front vehicle distance and speed perception tasks. © 2025 Zhejiang University. All rights reserved.",deep learning; object detection; state perception; vehicle distance measurement; vehicle speed measurement; Aircraft detection; Deep learning; Distance perception; Image features; Neural-networks; Objects detection; Perception model; State perception; Vehicle distance measurement; Vehicle images; Vehicle speed measurement; Distance measurement
Scopus,"Nan, Z.; Liu, W.; Zhu, G.; Zhao, H.; Xia, W.; Lin, X.; Yang, Y.",LiDAR-Camera joint obstacle detection algorithm for railway track area,,2025,,,,10.1016/j.eswa.2025.127089,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219499447&doi=10.1016%2fj.eswa.2025.127089&partnerID=40&md5=980d23a81478dfb062e42b8eaf810c62,"The application of multi-sensor systems in railway security holds significant research potential. We propose a joint decision-making strategy for obstacle detection (OD) in track areas that integrates LiDAR and camera sensors. LiDAR can accurately detect the geometric information of obstacles in the region without being affected by lighting conditions. To accurately assess the danger level of obstacles, we introduce a camera sensor to classify obstacles based on known locations, thereby enhancing the accuracy of detecting potentially hazardous obstacles. We first use LiDAR to obtain the point cloud data for the detection area. A point cloud algorithm designed explicitly for static obstacle recognition is applied to extract obstacle point cloud information. Using an external transformation relationship, we capture the corresponding obstacle images, fusing them with the point cloud data to serve as input images for a neural network. Subsequently, we introduce MS-YOLO-DLKA, an image OD network that combines a multi-scale feature extraction module (MS-Block) and a large convolution kernel module (D-LKA) based on YOLOv5. On our railway track obstacle dataset, the network achieved an accuracy of 85 %, a recall rate of 95.8 %, and a mAP value of 0.91, outperforming several SOTA (state-of-the-art) networks regarding comprehensive application performance. In test scenarios, our equipment has achieved OD within a range of 50 m for obstacles as small as 20 cm × 20 cm × 20 cm, providing a new railway security and monitoring solution. © 2025 Elsevier Ltd",Camera; Data fusion; LiDAR; MS-YOLO-DLKA; ROI; Metadata; Network security; Railroad tracks; Railroad transportation; Railroads; Sensor data fusion; Camera sensor; Detection algorithm; LiDAR; MS-YOLO-DLKA; Obstacles detection; Point cloud data; Point-clouds; Railway securities; Railway track; ROI; Obstacle detectors
Scopus,"Aromoye, I.A.; Hiung, L.H.; Sebastian, P.",P-DETR: A transformer-based algorithm for pipeline structure detection,,2025,,,,10.1016/j.rineng.2025.104652,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000323559&doi=10.1016%2fj.rineng.2025.104652&partnerID=40&md5=e0eda7cad7df22a428a7b3699d2eb320,"Pipelines are essential transportation infrastructure for oil and gas, but they are vulnerable to defects such as cracks, joint failure, and corrosion due to extreme weather conditions. These defects can result in oil and gas leakage, which prompts environmental and economic damages. Hence, regular inspection of pipelines is necessary. The industry has increasingly relied on using drones for pipeline inspections, though the inspection is still done manually by the drone operator or offline via recorded video footage from the drone. This paper proposes using the Pipe Detection Transformer (P-DETR), a novel transformer-based model designed for pipeline detection and potential integration with aerial robots or drones to enable autonomous pipeline inspection. P-DETR introduces significant improvements to the original Detection Transformer (DETR) framework to enhance its detection performance, particularly for small-sized pipes - a key limitation of the baseline DETR. The major contribution is a Feature Normalization and Transformation (FNT) module, which fuses multiple layers of the convolutional backbone to provide a focused representation of small-sized features before processing by the transformer module. Experimental results validate the superiority of P-DETR, achieving an overall mAP of 55 %, a 3 AP improvement over DETR, and significantly increasing precision for small-sized pipe detection by 8.6 AP (from 1.9 to 10.5). Additionally, precision improvements for medium- and large-sized pipes were 10.8 AP (from 10.8 to 21.6) and 2.2AP (from 64.4 to 66.6), respectively, with an overall recall of 73.9 %, a 4 AP improved performance over DETR. The results from extensive experiments highlight the superior performance of the proposed P-DETR model over the original DETR, UP-DETR, R-DETR, Skip-DETR, and other standard object detection models, including YOLOv3 and SSD. © 2025 The Author(s)","DETR; Machine learning; Oil and gas; Pipe detection; Pipelines, Deep learning; Transformers; Electric towers; Electric transformer testing; Gas piping systems; Petroleum transportation; Pipelines; Detection transformer; Machine-learning; Oil and gas; Performance; Pipe detection; Pipeline inspection; Pipeline structure; Pipeline, deep learning; Structure detection; Transformer; Pipeline corrosion"
Scopus,"Jin, Z.; He, T.; Qiao, L.; Duan, J.; Shi, X.; Yan, B.; Guo, C.",MES-YOLO: An efficient lightweight maritime search and rescue object detection algorithm with improved feature fusion pyramid network,,2025,,,,10.1016/j.jvcir.2025.104453,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002037003&doi=10.1016%2fj.jvcir.2025.104453&partnerID=40&md5=7d962112ed89e485504ede67c26fabb1,"Maritime Search and Rescue (SAR) object detection is challenged by environmental complexity, variability in object scales, and real-time computation constraints of Unmanned Aerial Vehicles (UAVs). Our MES-YOLO algorithm, designed for maritime UAV imagery, employs an innovative Multi Asymptotic Feature Pyramid Network (MAFPN) to enhance detection accuracy across scales. It integrates an Efficient Module (EMO) and Inverted Residual Mobile Blocks (iRMB) to maintain a lightweight model while enhancing key information perception.The SIoU loss function is used to optimize the detection performance of the model. Tests on the SeaDronesSee dataset show that MES-YOLO increased average precision (mAP50) from 81.5% to 87.1%, reduced parameter count by 43.3%, and improved the F1 score by 6.8%, with a model size only 58.3% that of YOLOv8, surpassing YOLO series and other mainstream algorithms in robustness to background illumination and imaging angles. © 2025 Elsevier Inc.",Adaptive spatial fusion; Feature enhancement; Maritime search and rescue; Object detection; Transformer; Yolov8; Aircraft detection; Image enhancement; Adaptive spatial fusion; Aerial vehicle; Feature enhancement; Maritime search and rescue; Object detection algorithms; Objects detection; Pyramid network; Search and rescue; Transformer; Yolov8; Unmanned aerial vehicles (UAV)
Scopus,"Jafari, A.A.; Agarwal, A.; Ozcinar, C.; Anbarjafari, G.",An Integral-Differential Probabilistic Fusion Framework of YOLO v8 and GPT-4o for High-Fidelity Tiny Object Recognition and Collision Threat Confidence in Autonomous Driving,,2025,,,,10.1007/s11760-025-04243-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006689176&doi=10.1007%2fs11760-025-04243-z&partnerID=40&md5=9e1eba8c3b45ecfb0e4a099a825cbed4,"We present a novel Integral-Differential Probabilistic Fusion Framework that synergistically integrates a state-of-the-art deep object detection network (YOLO v8) with a multimodal large language model (GPT-4o) to tackle the longstanding challenge of tiny object recognition and collision threat assessment in autonomous driving. Our central hypothesis is that the fusion of precise spatial detection with rich semantic and relational reasoning-mathematically formalized using probability theory, differential equations, and integral calculus-can yield a continuous Threat Confidence Level that robustly quantifies collision risk in real time. To validate our approach, we conduct extensive experiments on the 100K Vehicle Dashcam Image Dataset, demonstrating that our framework achieves an overall classification accuracy exceeding 90% across diverse scenarios, including varying illumination and occlusion conditions. The results underscore the potential of dynamic thresholding in reducing false positives while preserving sensitivity to true threats, paving the way for more reliable and safe autonomous driving systems. This work not only advances the state-of-the-art in tiny object detection but also lays a rigorous theoretical foundation for future research in multimodal sensor fusion and real-time risk assessment. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",Autonomous driving; Deep learning; Multimodal large language model; Probabilistic fusion; Tiny object detection; Autonomous vehicles; Deep learning; Differentiation (calculus); Integral equations; Autonomous driving; Deep learning; Language model; Multi-modal; Multimodal large language model; Objects detection; Objects recognition; Probabilistic fusion; Probabilistics; Tiny object detection; Risk assessment
Scopus,"Bae, C.; Choi, E.; Lee, S.",DT-CAS: Collision Avoidance System for Unmanned Aerial Vehicles in Digital Twin Environments,,2025,,,,10.5139/JKSAS.2025.53.4.445,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001268704&doi=10.5139%2fJKSAS.2025.53.4.445&partnerID=40&md5=888405e97d7f8f1012d85042f255dc84,"A collision avoidance system is essential for the safe operation of multiple Unmanned Aerial Vehicles(UAVs). Previous studies utilizing sensors for UAV collision avoidance may exhibit performance variations depending on the sensor used, while reinforcement learning-based approaches face challenges related to hardware limitations and power consumption due to computational demands. This paper proposes a Digital Twin-based Collision Avoidance System(DT-CAS) that predicts potential collision points based on pre-determined flight paths in a digital twin environment and controls UAVs to avoid collisions. DT-CAS extracts flight path intersections of UAVs within the digital twin environment and controls the entry order to avoid collisions. DT-CAS can monitor UAVs for collision detection and avoidance with an average error of about 6.75cm. Also, DT-CAS was able to avoid collisions in scenarios with varying flight distances, speeds, and wind conditions, with only an about 5% increase in flight time and battery usage. © 2025 The Korean Society for Aeronautical and Space Sciences.",Collision Avoidance System; Control; Digital Twin; Monitoring; Unmanned Aerial Vehicles
Scopus,"Aibibu, T.; Lan, J.; Zeng, Y.; Hu, J.; Yong, Z.",Multiview angle UAV infrared image simulation with segmented model and object detection for traffic surveillance,,2025,,,,10.1038/s41598-025-89585-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218816176&doi=10.1038%2fs41598-025-89585-x&partnerID=40&md5=dfbb79bfe88d28460dd8cb5959a4192b,"With the rapid development of infrared (IR) imaging UAV technology, infrared aerial image processing technology has been applied in different fields. But it is not very convenient to obtain real aerial images in some cases because of flight limitations, acquisition costs and other factors. So, it is necessary to simulate UAV infrared images by computer. This paper proposed an improved infrared aerial image simulation method based on open source AirSim. By improving the original AirSim infrared image simulation method, the simulation quality of the infrared image is improved via 3-dimensional segmented model processing. The infrared aerial images of the traffic scene with different viewing angles are simulated via the proposed method in this paper and we constructed infrared traffic scene simulation dataset (IR-TSS) containing seven types of objects. We propose the efficient EfficientNCSP-Net net for the IR-TSS dataset and use popular methods for comparative experiments. The experimental results show that the proposed EfficientNCSP-Net has an mAP50 greater than 96% for object detection on IR-TSS dataset, which is better than those of the existing methods. This paper not only contributes to research on infrared image simulations of traffic scenes, but also has referential significance in other aerial image simulation fields. © The Author(s) 2025.",article; diagnosis; human experiment; image processing; infrared radiation; male; simulation; thermography; traffic
Scopus,"Huang, D.; Huang, H.; Huang, D.; Liu, Z.",Review of Application of BEV Perceptual Learning in Autonomous Driving,,2025,,,,10.3778/j.issn.1002-8331.2407-0501,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007330614&doi=10.3778%2fj.issn.1002-8331.2407-0501&partnerID=40&md5=c8bc15f3a058c389c3750e7baa074e20,"As the types of sensors used as acquisition inputs in the autonomous driving perception module continue to develop, it becomes more and more difficult to represent the multi-modal data uniformly. BEV perception learning in the automatic driving perception task module can make multi-modal data unified integration into a feature space, which has better development potential compared with other perception learning models. The reasons for the good development potential of BEV perception model are summarized from five aspects: research significance, spatial deployment, preparation work, algorithm development, and evaluation index. The BEV perception model can be summarized into four series from a framework perspective: Lift-Splat-Lss series, IPM reverse perspective conversion, MLP view conversion and Transformer view conversion. The input data can be summarized into two categories: the first type of pure image feature input includes monocular camera input and multi-camera input; the second type of fusion data input is not only the simple data fusion of point cloud data and image features, but also the knowledge distillation fusion guided or supervised by point cloud data and the fusion of height segmentation by guided slice. It provides an overview of the application of four kinds of automatic driving tasks in BEV perception model, such as multi-target tracking, map segmentation, lane detection and 3D target detection, and summarizes the shortcomings of the four series of current BEV perception learning frameworks. © 2025 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",3D target detection; BEV perception learning; lane detection; map segmentation; multi-modal data fusion; multi-target tracking; view conversion; Automatic target recognition; Data integration; Depth perception; Image fusion; Image segmentation; Information fusion; Metadata; Modal analysis; Network security; Photointerpretation; Sensor data fusion; 3d target detection; BEV perception learning; Lane detection; MAP segmentation; Multi-modal data; Multi-modal data fusion; Multi-target-tracking; Perception model; Targets detection; View conversion; Target tracking
Scopus,"Tian, Y.; Lin, F.; Li, Y.; Zhang, T.; Zhang, Q.; Fu, X.; Huang, J.; Dai, X.; Wang, Y.; Tian, C.; Li, B.; Lv, Y.; Kovács, L.; Wang, F.-Y.",UAVs meet LLMs: Overviews and perspectives towards agentic low-altitude mobility,,2025,,,,10.1016/j.inffus.2025.103158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002285252&doi=10.1016%2fj.inffus.2025.103158&partnerID=40&md5=5bcb4e0cccd08b4c389aa7cd6128f3f3,"Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced transformative advancements across various domains, like transportation, logistics, and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs extend traditional systems’ perception and action capabilities, garnering widespread attention from academia and industry. However, current UAV operations primarily depend on human control, with only limited autonomy in simple scenarios, and lack the intelligence and adaptability needed for more complex environments and tasks. The emergence of large language models (LLMs) demonstrates remarkable problem-solving and generalization capabilities, offering a promising pathway for advancing UAV intelligence. This paper explores the integration of LLMs and UAVs, beginning with an overview of UAV systems’ fundamental components and functionalities, followed by an overview of the state-of-the-art LLM technology. Subsequently, it systematically highlights the multimodal data resources available for UAVs, which provide critical support for training and evaluation. Furthermore, key tasks and application scenarios where UAVs and LLMs converge are categorized and analyzed. Finally, a reference roadmap towards agentic UAVs is proposed to enable UAVs to achieve agentic intelligence through autonomous perception, memory, reasoning, and tool utilization. Related resources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs. © 2025",Foundation intelligence; Large language models; Low altitude mobility systems; Unmanned aerial vehicles; Maneuverability; Street traffic control; Aerial vehicle; Foundation intelligence; Language model; Large language model; Low altitude mobility system; Low altitudes; Manoeuvrability; Mobility systems; Transportation-logistics; Unmanned aerial vehicle; Unmanned aerial vehicles (UAV)
Scopus,"Mahmud, B.U.; Hong, G.; Lalwani, V.R.; Brown, N.; Asher, Z.D.",Real-Time Identification of Look-Alike Medical Vials Using Mixed Reality-Enabled Deep Learning,,2025,,,,10.3390/fi17050223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006463451&doi=10.3390%2ffi17050223&partnerID=40&md5=7aafb804915b4b81dd523fdcc0e45169,"The accurate identification of look-alike medical vials is essential for patient safety, particularly when similar vials contain different substances, volumes, or concentrations. Traditional methods, such as manual selection or barcode-based identification, are prone to human error or face reliability issues under varying lighting conditions. This study addresses these challenges by introducing a real-time deep learning-based vial identification system, leveraging a Lightweight YOLOv4 model optimized for edge devices. The system is integrated into a Mixed Reality (MR) environment, enabling the real-time detection and annotation of vials with immediate operator feedback. Compared to standard barcode-based methods and the baseline YOLOv4-Tiny model, the proposed approach improves identification accuracy while maintaining low computational overhead. The experimental evaluations demonstrate a mean average precision (mAP) of 98.76 percent, with an inference speed of 68 milliseconds per frame on HoloLens 2, achieving real-time performance. The results highlight the model’s robustness in diverse lighting conditions and its ability to mitigate misclassifications of visually similar vials. By combining deep learning with MR, this system offers a more reliable and efficient alternative for pharmaceutical and medical applications, paving the way for AI-driven MR-assisted workflows in critical healthcare environments. © 2025 by the authors.",AI; AI in healthcare; deep learning; HoloLens; look-alike vial; mixed reality; object detection; real time detection; Patient treatment; AI in healthcare; Deep learning; Hololens; Lighting conditions; Look-alike vial; Mixed reality; Objects detection; Patient safety; Real-time detection; Real-time identification; Deep learning
Scopus,"Chang, B.R.; Tsai, H.-F.; Syu, J.-S.",Implementing High-Speed Object Detection and Steering Angle Prediction for Self-Driving Control,,2025,,,,10.3390/electronics14091874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004848133&doi=10.3390%2felectronics14091874&partnerID=40&md5=01e6acf5e6cb833eda3fdf8de6bdbdd8,"In the previous work, we proposed LWGSE-YOLOv4-tiny and LWDSG-ResNet18, leveraging depthwise separable and Ghost Convolutions for fast self-driving control while achieving a detection speed of 24.9 FPS. However, the system fell short of Level 4 autonomous driving safety requirements. That is, the control response speed of object detection integrated with steering angle prediction must exceed 39.2 FPS. This study enhances YOLOv11n with dual convolution and RepGhost bottleneck, forming DuCRG-YOLOv11n, significantly improving the object detection speed while maintaining accuracy. Similarly, DuC-ResNet18 improves steering angle prediction speed and accuracy. Our approach achieves 50.7 FPS, meeting Level 4 safety standards. Compared to previous work, DuCRG-YOLOv11n boosts feature extraction speed by 912.97%, while DuC-ResNet18 enhances prediction speed by 45.37% and accuracy by 12.26%. © 2025 by the authors.",Dual Conv; DuC-ResNet18; DuCRG-YOLOv11n; object detection; reparameterization; RepGhost bottleneck; steering angle prediction; Object recognition; Driving control; Dual conv; DuC-resnet18; DuCRG-yolov11n; Objects detection; Reparameterization; Repghost bottleneck; Self drivings; Steering angle prediction; Steering angles; Object detection
Scopus,"Zhang, X.; Yang, D.; Song, T.; Ye, Y.; Song, Y.; Zhou, J.; Chen, J.",A lightweight object detector based on changeable-size lightweight convolution and context augmentation module for images captured by UAVs,,2025,,,,10.1007/s00371-024-03749-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213706543&doi=10.1007%2fs00371-024-03749-w&partnerID=40&md5=fc0105327af0839595648716b5313517,"Object detection based on images captured by UAVs has become a hot research topic. However, images have a rich variety of small targets and complex backgrounds. This poses significant challenges for existing object detectors. Furthermore, due to the limitations of UAV platforms, it is difficult to deploy complicated models. Therefore, a novel lightweight detection network LDN-UAV is introduced in this work. Firstly, the YOLOv5s network is redesigned to obtain the YOLOv5ss basic network, which reduces the complexity of the model while improving the detection performance. Moreover, a lightweight feature enhancement module is devised, which enhances the spatial utilization of features. Next, we propose a novel lightweight convolutional operation that maps receptive-field features to a specific size via a shared MLP, which simplifies the model and improves the performance. Moreover, a context augmentation module is created, which aggregates contextual information to increase the benefits of features through MLP branching and Softmax. Finally, a lightweight Decoupled-Head is designed to ensure efficient performance of the detection. To validate the advantages of proposed lightweight convolutions, we conduct extensive experiments on COCO2017 and VOC 7+12. Additionally, for images captured by UAVs, relevant experiments are performed based on VisDrone-DET2021. The results of all experiments demonstrate that the proposed method achieves better detection performance compared to state-of-the-art lightweight detectors. Compared to the original baseline model, LDN-UAV uses only 2.46M parameters and increased the metrics mAP50 and mAP by 8.6 % and 5.9 %, respectively. Code is available at https://github.com/CV-ZhangXin/LDN-UAV. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Changeable-size convolution; Context augmentation module; Images captured by UAVs; Object detection; Aircraft detection; Change detection; Image enhancement; Object detection; Object recognition; Object tracking; Changeable-size convolution; Context augmentation module; Detection performance; Hot research topics; Image captured by UAV; Object detectors; Objects detection; Small targets; Target background; Unmanned aerial vehicles (UAV)
Scopus,"Peng, R.; Liao, C.; Pan, W.; Gou, X.; Zhang, J.; Lin, Y.",Improved YOLOv7 for small object detection in airports: Task-oriented feature learning with Gaussian Wasserstein loss and attention mechanisms,,2025,,,,10.1016/j.neucom.2025.129844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000608898&doi=10.1016%2fj.neucom.2025.129844&partnerID=40&md5=f5a4c5ba731d3c28cb6f383721b1e8ce,"Small objects in the airport scene, such as Persons and Vehicles, can lead to low accuracy and robustness of the object detection task. To address the above problems, an improved YOLOv7 model is proposed to detect certain objects on the airport scene. Considering the perspective distortion of the monitoring camera on the airport surface, the deformable convolutional feature extractor (DCFE) is first designed to adaptively extract features from input images for irregular occlusion objects. To learn task-oriented features from different channels, the attention mechanism is incorporated into the backbone to focus on informative concepts in a data-driven manner, formulating an attention feature extractor (AttFE). During the model training, the Normalized Gaussian Wasserstein distance (NWD) is considered as the loss function to measure the prediction errors after converting the bounding boxes into Gaussian distribution, thereby enhancing the ability to fit the small objects. A real-world airport surface dataset (ASD) is constructed to validate the proposed model. Extensive experimental results demonstrate that the proposed model outperforms selective baselines, achieving a 1.2% absolute improvement in mAP over the original YOLOv7 network. Experiments conducted on multiple common datasets and the results demonstrate that the proposed model exhibits superior performance in terms of mAP. All proposed technical modules contribute to expected performance improvement. Most importantly, the proposed model achieves higher performance for small objects and has the desired robustness over occluded objects. © 2025 Elsevier B.V.",Airport object detection; Deformable convolution; Normalized Gaussian Wasserstein distance; Small objects; YOLOv7; Airport runways; Object detection; Object recognition; Object tracking; Airport object; Airport object detection; Deformable convolution; Gaussians; Normalized gaussian wasserstein distance; Objects detection; Performance; Small objects; Wasserstein distance; YOLOv7; ablation therapy; aircraft; airport; Article; attention network; back propagation; convolution algorithm; convolutional neural network; feature extraction; human; kernel method; learning; motor vehicle; normal distribution; normalized gaussian wasserstein distance; novel object recognition test; outcomes research; prediction error; quantitative analysis; response generalization; sensitivity analysis; yolov7 model; Gaussian distribution
Scopus,"Zhao, J.; Wu, Y.; Deng, R.; Xu, S.; Gao, J.; Burke, A.",A Survey of Autonomous Driving from a Deep Learning Perspective,,2025,,,,10.1145/3729420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009652283&doi=10.1145%2f3729420&partnerID=40&md5=ef50f8e07db173ebeb72c6fc74d0bf4c,"Autonomous driving represents a significant advancement in the transportation industry, enhancing vehicle intelligence, optimizing traffic management, and improving user experiences. Central to these innovations is deep learning, which enables systems to handle complex data and make informed decisions. Our survey explores critical applications of deep learning in autonomous driving, such as perception and detection, localization and mapping, and decision-making and control. We investigate specialized deep learning techniques, including convolutional neural networks, recurrent neural networks, self-attention transformers, and their variants, among others. These methods are applied within various learning paradigms - supervised, unsupervised, and reinforcement learning - to suit the specific needs of autonomous driving. Our analysis evaluates the effectiveness, benefits, and limitations of these technologies, focusing on their integration with other intelligent algorithms to enhance system performance. Furthermore, we examine the architectures of autonomous systems, analyzing how knowledge and information are organized from modular, pipeline-based frameworks to comprehensive end-to-end models. By presenting an exhaustive overview of the progressing domain of autonomous driving and bridging various research areas, our survey aims to synthesize diverse research threads into a unified narrative. This effort not only aims to enhance our understanding but also pushes the boundaries of what is achievable in this interdisciplinary field.  © 2025 Copyright held by the owner/author(s).",Autonomous driving; decision-making; deep learning; end-to-end; perception; real-world; reinforcement learning; sensor fusion; simulation; trajectory planning; Automobile drivers; Autonomous vehicles; Behavioral research; Convolutional neural networks; Decision making; Deep neural networks; Deep reinforcement learning; Human computer interaction; Human engineering; Intelligent vehicle highway systems; Learning systems; Traffic control; User experience; Autonomous driving; Decisions makings; Deep learning; End to end; Real-world; Reinforcement learnings; Sensor fusion; Simulation; Trajectory Planning; Transportation industry; Reinforcement learning
Scopus,"Ullah, R.; Zhang, S.; Asif, M.; Wahab, F.","Multimodal learning-based speech enhancement and separation, recent innovations, new horizons, challenges and real-world applications",,2025,,,,10.1016/j.compbiomed.2025.110082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001492793&doi=10.1016%2fj.compbiomed.2025.110082&partnerID=40&md5=fed4e3bdc349dccef8592fb3b276071d,"With the increasing global prevalence of disabling hearing loss, speech enhancement technologies have become crucial for overcoming communication barriers and improving the quality of life for those affected. Multimodal learning has emerged as a powerful approach for speech enhancement and separation, integrating information from various sensory modalities such as audio signals, visual cues, and textual data. Despite substantial progress, challenges remain in synchronizing modalities, ensuring model robustness, and achieving scalability for real-time applications. This paper provides a comprehensive review of the latest advances in the most promising strategy, multimodal learning for speech enhancement and separation. We underscore the limitations of various methods in noisy and dynamic real-world environments and demonstrate how multimodal systems leverage complementary information from lip movements, text transcripts, and even brain signals to enhance performance. Critical deep learning architectures are covered, such as Transformers, Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and generative models like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models. Various fusion strategies, including early and late fusion and attention mechanisms, are explored to address challenges in aligning and integrating multimodal inputs effectively. Furthermore, the paper explores important real-world applications in areas like automatic driver monitoring in autonomous vehicles, emotion recognition for mental health monitoring, augmented reality in interactive retail, smart surveillance for public safety, remote healthcare and telemedicine, and hearing assistive devices. Additionally, critical advanced procedures, comparisons, future challenges, and prospects are discussed to guide future research in multimodal learning for speech enhancement and separation, offering a roadmap for new horizons in this transformative field. © 2025 Elsevier Ltd","Audio-visual signal processing; Deep learning; Fusion techniques; Multimodal learning; Speech denoising; Speech separation; Deep Learning; Humans; Neural Networks, Computer; Speech; Convolutional neural networks; Emotion Recognition; Graph neural networks; Hearing aids; Speech enhancement; Audio-visual; Audio-visual signal processing; Deep learning; Fusion techniques; Hearing loss; Multi-modal learning; Real-world; Speech denoising; Speech separation; Visual signal processing; augmented reality; autoencoder; autonomous vehicle; communication barrier; convolutional neural network; deep learning; diffusion; generative adversarial network; generative model; graph neural network; health survey; hearing; hearing impairment; human; learning; mental health; prevalence; quality of life; review; self help device; signal processing; speech; telemedicine; artificial neural network; Generative adversarial networks"
Scopus,"Wang, T.; Liu, B.; Chen, P.",Dynamic Cascade Detector for Storage Tanks and Ships in Optical Remote Sensing Images,,2025,,,,10.3390/rs17111882,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007731419&doi=10.3390%2frs17111882&partnerID=40&md5=3c0d00cdc28ec140dc6e7f4326fb0f94,"Regional Convolutional Neural Network (RCNN)−based detectors have played a crucial role in object detection in remote sensing images due to their exceptional detection capabilities. Some studies have shown that different stages should have different Intersections of Union (IoU) thresholds to distinguish positive and negative samples because each stage has different IoU distributions. However, these studies have overlooked the fact that the IoU distribution at each stage changes continuously during the training process. Therefore, the IoU threshold at each stage should also be adjusted continuously to adapt to the changes in the IoU distribution. We realized that the IoU distribution at each stage is very similar to a Gaussian skewed distribution. In this paper, we introduce a novel dynamic IoU threshold method based on the Cascade RCNN architecture, called the Dynamic Cascade detector, with reference to the Gaussian skewed distribution. We tested the effectiveness of this method by detecting horizontal storage tanks and rotated ships in optical remote sensing images. Our experiments demonstrated that this technique can significantly improve detection results, as evaluated based on the COCO metric. In addition, the threshold range of the last stage impacts other stages, so the threshold range of one stage may change significantly when the number of stages changes. Furthermore, the threshold may not always increase during the training process and may decrease when the IoU distribution resembles a negatively skewed distribution. © 2025 by the authors.",Dynamic RCNN; Gaussian skewed distribution; object detection; optical remote sensing image; Convolutional neural networks; Gaussian distribution; Vehicle detection; Convolutional neural network; Dynamic regional convolutional neural network; Gaussian skewed distribution; Gaussians; Objects detection; Optical remote sensing; Optical remote sensing image; Remote sensing images; Skewed distribution; Storage tank; Optical remote sensing
Scopus,"Asamoah, J.K.; Agyei Kyem, B.; Obeng-Amoako, N.D.; Aboah, A.",SAAM-ReflectNet: Sign-aware attention-based multitasking framework for integrated traffic sign detection and retroreflectivity estimation,,2025,,,,10.1016/j.eswa.2025.128003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005071568&doi=10.1016%2fj.eswa.2025.128003&partnerID=40&md5=4a14c7126837522de293ced9635f4386,"Traffic sign retroreflectivity is essential for roadway safety, particularly in low-light and adverse weather conditions. Traditional methods, such as handheld retroreflectometers and nighttime inspections, are labor-intensive, costly, and unsuitable for large-scale implementation. To address these limitations, we developed SAAM-ReflectNet, a deep learning framework that unifies traffic sign detection, classification, and retroreflectivity estimation into a single automated pipeline. Our RetroNet backbone, developed as part of this study, extracts robust spatial and semantic features to enhance feature representation. The Sign-Aware Attention Module we designed prioritizes critical traffic sign regions, improving detection and classification accuracy by focusing on the most relevant areas. Additionally, our multimodal fusion layers seamlessly integrate RGB imagery with LiDAR intensity data, enabling reliable retroreflectivity estimation. ReflectNet achieved a mean Average Precision (mAP) of 0.635 at IoU=0.5 and 0.522 across IoU thresholds from 0.5 to 0.95, alongside Root Mean Squared Errors (RMSE) of 0.169 for foreground and 0.147 for background reflectivity. Across 15 evaluation runs, performance improvements were statistically significant compared to all baselines (p < 0.05), underscoring the consistency and reliability of ReflectNet.These findings underscore the reliability, scalability, and transferability of our approach, establishing ReflectNet as a transformative tool for intelligent transportation systems and proactive traffic sign maintenance. © 2025 Elsevier Ltd",Computer vision; LiDAR; Multi-task learning; Object detection; Retroreflectivity; Traffic signs; Transportation systems; Advanced driver assistance systems; Air traffic control; Deep learning; Highway traffic control; Multi-task learning; Multimodal transportation; Street traffic control; Variable message signs; Adverse weather; Condition; LiDAR; Low light; Multitask learning; Objects detection; Retroreflectivity; Roadway safety; Traffic sign detection; Transportation system; Road and street markings
Scopus,"Ren, J.; Wen, C.; Zhang, L.; Su, H.; Yang, C.; Lv, Y.; Yang, N.; Qin, X.",High performance point-Voxel feature set abstraction with mamba for 3D object detection,,2025,,,,10.1016/j.eswa.2025.128127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005225379&doi=10.1016%2fj.eswa.2025.128127&partnerID=40&md5=7a75a007a506bfe9b339fde987426d0f,"In the field of autonomous driving, a two-stage three-dimensional object detection approach has seen significant advancements. However, challenges persist in terms of detection accuracy, which can have a profound impact on the safety of autonomous vehicles. This study examined four critical issues that impair the accuracy and efficiency of the model: limited acceptance fields, slow acquisition of global features from voxels, challenges in capturing keypoint features, and uncertainties associated with network post-processing. To address these challenges, we propose four novel techniques: (1) a non-empty voxel feature extraction method that utilises linear angular attention to broaden the receptive field; (2) an efficient voxel feature extraction and downsampling approach based on Mamba2, designed to accelerate the acquisition of global voxel features; (3) a node extraction strategy that employs the Kolmogorov-Arnold Network (KAN) to extract key point features via segmented farthest point sampling (S-FPS); (4) a fuzzy non-maximum suppression (Fuzzy-NMS) method that refines suppression thresholds during the post-processing phase. By integrating these techniques, we introduce a High-Performance Point-Voxel Region Convolutional Neural Network (HP-PV-RCNN) algorithm specifically tailored for precise 3D object detection. We validated the effectiveness of the HP-PV-RCNN algorithm through comprehensive experiments using the Kitti, NuScenes, and Waymo open datasets. Specifically, our proposed network attained average precisions of 83.73 % for vehicles, 76.32 % for bicycles, and 53.52 % for pedestrians in the medium-difficulty category of the Kitti dataset for detecting these entities. The code and model are available at https://github.com/jlauwcj/HP-PV-RCNN. © 2025 Elsevier Ltd",3D Object detection; Automatic driving; Hybrid point-voxel approach; Mamba; Point cloud; Fuzzy neural networks; Image segmentation; Pedestrian safety; 3D object; 3d object detection; Automatic driving; Hybrid point-voxel approach; Keypoints; Mamba; Objects detection; Performance points; Point-clouds; Post-processing; Convolutional neural networks
Scopus,"Yang, B.; Tao, T.; Wu, W.; Zhang, Y.; Meng, X.; Yang, J.",MultiDistiller: Efficient Multimodal 3D Detection via Knowledge Distillation for Drones and Autonomous Vehicles,,2025,,,,10.3390/drones9050322,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006653864&doi=10.3390%2fdrones9050322&partnerID=40&md5=4caf598885092898a9e674c3323a1da0,"Real-time 3D object detection is a cornerstone for the safe operation of drones and autonomous vehicles (AVs)—drones must avoid millimeter-scale power lines in cluttered airspace, while AVs require instantaneous recognition of pedestrians and vehicles in dynamic urban environments. Although significant progress has been made in detection methods based on point clouds, cameras, and multimodal fusion, the computational complexity of existing high-precision models struggles to meet the real-time requirements of vehicular edge devices. Additionally, during the model lightweighting process, issues such as multimodal feature coupling failure and the imbalance between classification and localization performance often arise. To address these challenges, this paper proposes a knowledge distillation framework for multimodal 3D object detection, incorporating attention guidance, rank-aware learning, and interactive feature supervision to achieve efficient model compression and performance optimization. Specifically: To enhance the student model’s ability to focus on key channel and spatial features, we introduce attention-guided feature distillation, leveraging a bird’s-eye view foreground mask and a dual-attention mechanism. To mitigate the degradation of classification performance when transitioning from two-stage to single-stage detectors, we propose ranking-aware category distillation by modeling anchor-level distribution. To address the insufficient cross-modal feature extraction capability, we enhance the student network’s image features using the teacher network’s point cloud spatial priors, thereby constructing a LiDAR-image cross-modal feature alignment mechanism. Experimental results demonstrate the effectiveness of the proposed approach in multimodal 3D object detection. On the KITTI dataset, our method improves network performance by 4.89% even after reducing the number of channels by half. © 2025 by the authors.",3D object detection; autonomous vehicles; drones; intelligent perception; knowledge distillation; LiDAR; multimodal fusion; Distillation equipment; Image analysis; Image enhancement; Learning to rank; Students; 3D object; 3d object detection; Autonomous Vehicles; Intelligent perception; Knowledge distillation; LiDAR; Multi-modal; Multi-modal fusion; Objects detection; Point-clouds; Vehicle detection
Scopus,"Yao, X.; Liu, P.; Zhou, J.; Wang, Z.; Fan, S.; Wang, Y.",MAT-PointPillars: Enhanced PointPillars algorithm based on multi-scale attention mechanisms and transformer,,2025,,,,10.1371/journal.pone.0325373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009074468&doi=10.1371%2fjournal.pone.0325373&partnerID=40&md5=2c0feb52fb841da5da2dea545dfc455a,"Aiming at the problem that small and irregular detection targets such as cyclists have low detection accuracy and inaccurate recognition by existing 3D target detection algorithms, MAT-PointPillars (Multi-scale Attention and Transformer PointPillars), a 3D object detection algorithm, extends PointPillars with multi-scale vision Transformers and attention mechanisms. First, the algorithm employs pillar coding for semantic point cloud encoding and introduces an attention mechanism to refine the backbone’s upsampling process. Furthermore, the Transformer Encoder is introduced to improve the upsampling structure of the third stage of the backbone. On the KITTI dataset, our algorithm achieved 3D average detection accuracy (AP3D) of 81.15%, 62.02%, and 58.68% across three difficulty levels. Compared with the baseline model, the proposed algorithm improves AP3D by 2.44%, 1.19%, and 1.23% respectively. The real-time 3D object detection system is built based on ROS, and average running frames per second of the system is 22.63, which is higher than the sampling frequency of conventional LiDAR. By ensuring sufficient detection speed, the MAT-PointPillars algorithm can increase detection accuracy of cyclists in real-world scenarios. © 2025 Yao et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",algorithm; article; cyclist; detection algorithm; diagnosis; human; human experiment; Mien (people); velocity
Scopus,"Giri, K.J.",SO-YOLOv8: A novel deep learning-based approach for small object detection with YOLO beyond COCO,,2025,,,,10.1016/j.eswa.2025.127447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002047156&doi=10.1016%2fj.eswa.2025.127447&partnerID=40&md5=7e4ad25415622f5d7f111d04f3ce6ad9,"Small object detection in images is a significant challenge in computer vision due to issues like low resolution, occlusion, and scale variation, often resulting in existing models missing important details or requiring complex, large-scale setups. This paper introduces SO-YOLOv8, an enhanced version of the YOLO model that focuses on small object detection. The proposed model uses advanced hyperparameter optimization, sophisticated data augmentation, and multi-scale training to improve detection accuracy. SO-YOLOv8 also includes a Squeeze-and-Excitation (SE) block, which helps the model better recognize features of small objects. Experimental results on the PASCAL VOC 2012 dataset, a benchmark known for diverse and challenging object scales, demonstrate substantial improvements, achieving a precision of 1.0, showing an increase of 6% and an enhanced mean Average Precision (mAP) score of 0.79, reflecting a 1% increase in mAP compared to YOLOv8. While the mAP gain may seem marginal, even a slight improvement in small object detection significantly impacts real-world applications such as autonomous vehicles (detecting distant pedestrians or small road hazards), surveillance and security (identifying concealed objects in crowded environments), medical imaging (spotting small anomalies like tumors), and remote sensing (detecting small objects in satellite or drone imagery). Also, the 6% increase in precision indicates a significant reduction in false positives, making the detection system more reliable and reducing misclassifications that could otherwise lead to critical errors. These findings confirm that targeted customization of YOLO's architecture can effectively address the challenges associated with small object detection. This research contributes to the ongoing development of object detection methodologies and establishes a robust foundation for future work in small object detection. © 2025 Elsevier Ltd",Computer vision; Deep learning; Optimization; Precision; Small object detection; YOLOv8; Deep learning; Large-scales; Learning-based approach; Lower resolution; Model use; Optimisations; Precision; Small object detection; Small objects; YOLOv8
Scopus,"Xie, B.J.; Li, H.; Luan, Z.; Li, X.X.; Lei, Z.",A lightweight coal mine pedestrian detector for video surveillance systems with multi-level feature fusion and channel pruning,,2025,,,,10.1038/s41598-025-87157-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218947395&doi=10.1038%2fs41598-025-87157-7&partnerID=40&md5=afb3894a7cc74242857e7e14245cc8c2,"Pedestrian detection in coal mines is crucial for video surveillance systems. Limited computational resources pose challenges to deploying large models, affecting detection efficiency. To address this, we propose a lightweight pedestrian in coal mine detector with multi-level feature fusion. Our approach integrates the backbone network with coordinate attention, introducing a bidirectional feature pyramid network and a thin neck technique to enhance multi-scale detection capability while reducing computational load. We also employ regression loss with a dynamic focus mechanism for bounding box regression to minimize model errors. The Linkage Channel Pruning method enforces channel-level sparsity on the designed detector to achieve network slimming and secondary lightweight development. Results on a proprietary dataset demonstrate our method’s parameters (0.61 M), computational load (2.0 GFLOPs), model size (1.48 MB), detection accuracy (0.966), and inference time (2.1 ms). Compared to the baseline, our method achieves a 4.96 × reduction in parameters, a 4.05 × reduction in computational load, a 4.02 × reduction in model size, a 59.62% reduction in inference time, and a 1.2% accuracy improvement. Experimental validation on proprietary and public datasets confirms that our method exhibits state-of-the-art lightweight performance, accuracy, and real-time capability, demonstrating significant potential in practical engineering applications. The insights gained provide technical references and real-time accident prevention for coal mine video surveillance systems. © The Author(s) 2025.",Accident prevention; Channel pruning; Coal mine pedestrian detection; Lightweight architecture; Video surveillance; accident prevention; article; coal mining; human; pedestrian; video surveillance
Scopus,"Seidaliyeva, U.; Ilipbayeva, L.; Utebayeva, D.; Smailov, N.; Matson, E.T.; Tashtay, Y.; Turumbetov, M.; Sabibolda, A.",LiDAR Technology for UAV Detection: From Fundamentals and Operational Principles to Advanced Detection and Classification Techniques,,2025,,,,10.3390/s25092757,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004919528&doi=10.3390%2fs25092757&partnerID=40&md5=b3324ff0bb470f63f9c65933eaede5dd,"As unmanned aerial vehicles (UAVs) are increasingly employed across various industries, the demand for robust and accurate detection has become crucial. Light detection and ranging (LiDAR) has developed as a vital sensor technology due to its ability to provide rich 3D spatial information, particularly in applications such as security and airspace monitoring. This review systematically explores recent innovations in LiDAR-based drone detection, deeply focusing on the principles and components of LiDAR sensors, their classifications based on different parameters and scanning mechanisms, and the approaches for processing LiDAR data. The review briefly compares recent research works in LiDAR-based only and its fusion with other sensor modalities, the real-world applications of LiDAR with deep learning, as well as the major challenges in sensor fusion-based UAV detection. © 2025 by the authors.",3D object detection; deep learning; deep learning for point cloud processing; drone detection; LiDAR; LiDAR classifications; object detection; point clouds; scanning mechanism; UAV detection; unmanned aerial vehicles (UAVs); Air navigation; Micro air vehicle (MAV); Target drones; 3D object; 3d object detection; Aerial vehicle; Cloud processing; Deep learning; Deep learning for point cloud processing; Drone detection; Light detection and ranging; Light detection and ranging classification; Objects detection; Point-clouds; Scanning mechanisms; Unmanned aerial vehicle; Unmanned aerial vehicle detection; Vehicles detection; classification; cloud computing; deep learning; diagnosis; drone; human; review; sensor; unmanned aerial vehicle; Drones
Scopus,"Trinh, L.; Mercelis, S.; Anwar, A.",A comprehensive review of datasets and deep learning techniques for vision in unmanned surface vehicles,,2025,,,,10.1016/j.oceaneng.2025.121501,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005586392&doi=10.1016%2fj.oceaneng.2025.121501&partnerID=40&md5=061e8d05c620f68b5bf8249becd089b9,"Unmanned Surface Vehicles (USVs) have emerged as a major platform in maritime operations, capable of supporting a wide range of applications. USVs allow for difficult unmanned tasks in harsh maritime environments. With the rapid development of USVs, many vision tasks such as detection and segmentation become increasingly important. Datasets play an important role in encouraging and improving the research and development of reliable vision algorithms for USVs. In this regard, a large number of recent studies have focused on the release of vision datasets for USVs. Along with the development of datasets, a variety of deep learning techniques have also been studied, with a focus on USVs. However, there is a lack of a systematic review of recent studies in both datasets and vision techniques to provide a comprehensive picture of the current development of vision on USVs, including limitations and trends. In this study, we provide a comprehensive review of both USV datasets and deep learning techniques for vision tasks. Our review was conducted using a large number of vision datasets from USVs. We elaborate several challenges and potential opportunities for research and development in USV vision based on a thorough analysis of current datasets and deep learning techniques. © 2025 Elsevier Ltd",Computer vision; Datasets; Deep learning; Unmanned surface vessels; 'current; Dataset; Deep learning; Learning techniques; Maritime environment; Maritime operation; Research and development; Surface vehicles; Unmanned surface vessels; Vision algorithms; algorithm; computer vision; data set; machine learning; unmanned vehicle; vessel; Deep learning
Scopus,"Qiu, J.; Zhang, W.; Xu, S.; Zhou, H.",DP-YOLO: A lightweight traffic sign detection model for small object detection,,2025,,,,10.1016/j.dsp.2025.105311,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005082446&doi=10.1016%2fj.dsp.2025.105311&partnerID=40&md5=9d09d118f701fc06916e6002e84eab5e,"Autonomous driving is a critical area in artificial intelligence, with vast potential for development. While current object detection algorithms have shown strong performance in traffic sign detection, they still face difficulties with small object recognition, often resulting in missed or false detections. To address this, we propose DP-YOLO, a traffic sign detection algorithm based on YOLOv8s. To enhance detection accuracy for small objects and reduce the model's parameter count, we first removed the large object detection layer from the baseline model and added a small object detection layer. In the feature extraction stage, we design the DBBNCSPELAN4 module to boost the network's feature extraction capability. Additionally, we propose the PTCSP module, incorporating Transformer technology into the model's feature processing network and reducing both parameters and computational cost. Finally, we introduce the W3F_MPDIoU loss to mitigate the impact of low-quality samples on the model and enhance its robustness. Experiments demonstrate that, compared to YOLOv8s, DP-YOLO reduces the model's parameter count by 77.0%, while achieving improvements in mAP0.5 by 5.8% on the TT100K dataset, 2.7% on the GTSDB dataset, and 1.3% on the CCTSDB dataset. Experimental results demonstrate that the proposed method effectively enhances the detection capability for small-sized traffic signs and exhibits high potential for edge deployment. © 2025 The Author(s)",Autonomous driving; Lightweight; Small object detection; Traffic sign detection; TT100K; Vehicle detection; 'current; Autonomous driving; Detection models; Features extraction; Lightweight; Modeling parameters; Small object detection; Small objects; Traffic sign detection; Tt100k; Object detection
Scopus,"Ye, Z.; You, J.; Gu, J.; Kou, H.; Li, G.",Modeling and Simulation of Urban Laser Countermeasures Against Low-Slow-Small UAVs,,2025,,,,10.3390/drones9060419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009043503&doi=10.3390%2fdrones9060419&partnerID=40&md5=965933aa268971c4e31fdf3ee0d2e006,"This study addresses the modeling and simulation challenges of urban laser countermeasure systems against Low-Slow-Small (LSS) UAVs by proposing a physics simulation framework integrating Geographic Information System (GIS)-based dynamic 3D real-world scenes and constructing a hybrid Anti-UAV dataset combining real and simulated data. A three-stage target tracking system is developed, encompassing target acquisition, coarse tracking, and precise tracking. Furthermore, the UAV-D-Fine detection algorithm is introduced, significantly improving small-target detection accuracy and efficiency. The simulation platform achieves dynamic fusion between target models and GIS real-scene models, enabling a full physical simulation of UAV takeoff, tracking, aiming, and laser engagement, with additional validation of laser antenna tracking performance. Experimental results demonstrate the superior performance of the proposed algorithm in both simulated and real-world environments, ensuring accurate UAV detection and sustained tracking, thereby providing robust support for low-altitude UAV laser countermeasure missions. © 2025 by the authors.",drone countermeasures; drone detection; maneuvering target; real-time tracking; Antennas; Clutter (information theory); Drones; Geographic information systems; Information use; Simulation platform; Target drones; Drone countermeasure; Drone detection; Geographic information; Manoeuvring target; Model and simulation; Physics simulation; Real time tracking; Real-world; Simulation framework; Small UAV; Aircraft detection
Scopus,"Rabecka, D.V.; Pari, B.J.",A HYBRID FRAMEWORK FOR OBJECT DETECTION AND SEGMENTATION IN AUTONOMOUS VEHICLES USING YOLO NAS AND MASK R-CNN,,2025,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001986858&partnerID=40&md5=7a8243e1c6fa45983908c97f50d6c3f9,"A leading opinion prescription that can precisely establish and segregate objects in complex environments is necessary for the hurried development of self-driving autos. The new hybrid framework shown in this work improves object detection and segmentation performance by combining Mask R-CNN with You Only Look Once Neural Architecture Search (YOLO NAS). With the neck and head of YOLO-NAS retained, this study tries to boost the performance of YOLO-NAS by substituting a combination of Res Net and Feature Pyramid Network (FPN) for the default Rep Ne X t backbone. Additionally, to increase segmentation capabilities, the study integrates Mask R-CNN. Our methodology leverages the efficiency of YOLO NAS for fast object detection and the precision of Mask R-CNN for complex segmentation tasks using the KITTI dataset, a leading benchmark in autonomous driving research. This approach resolves issues such as disparate object sizes, obstructions, and complex backgrounds that are frequently encountered when driving in urban areas. Our cloud-based approach outperforms previous approaches in terms of precision, recall, and F1 scores. The results of our experiments show that this combination approach could greatly contribute to the development of more reliable and safer autonomous driving systems, paving the way for advancements in real-time perception technology. © Little Lion Scientific.",Autonomous Vehicles; Convolution Neural Network; Object Detection; You Only Look Once
Scopus,"Singh, N.; Maurya, C.P.; Mahaur, B.; Singh, S.K.",Improved YOLOv11 with weights pruning for road object detection in rainy environment,,2025,,,,10.1007/s11760-025-04070-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002983615&doi=10.1007%2fs11760-025-04070-2&partnerID=40&md5=c82889b6199f1fe69d6977ff4bd3477d,"Object detection on roadways is crucial for autonomous driving and advanced driver assistance systems. However, adverse weather conditions, notably rain, significantly degrade the performance of these systems. This paper presents a novel approach to improve the detection of road objects in rainy weather scenarios by applying YOLOv11 model. This includes specialized data augmentation techniques to simulate rainy conditions, adjustments in network architecture to improve resilience to rain-induced noise, and optimized training strategies to enhance model performance. The study leverages BDD100K, Cityscapes, and DAWN-Rainy datasets of various road scenarios under different rain intensities. We systematically augment these datasets to ensure the model learns to identify objects obscured by rain streaks and reflections. Such enhancements enable better handling of occlusions and reduced visibility in the feature extraction layers. Also, to ensure the model’s efficiency and suitability for real-time applications, we apply a network pruning technique, which reduces the model size and computational requirements without sacrificing performance. Extensive experiments demonstrate that our model has a comparable mean Average Precision with the baseline YOLOv11 but at a 2x compression ratio under rainy conditions. This research contributes to the field of autonomous driving by providing a more reliable object detection system for adverse weather conditions, improving overall road safety. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",Deep learning; Rain generation and augmentation; Road object detection; Weight pruning; Data compression ratio; Deep learning; Driver licensing; Motor transportation; Network security; Rain; Weather forecasting; Adverse weather; Autonomous driving; Condition; Deep learning; Objects detection; Performance; Rain generation and augmentation; Rainy conditions; Road object detection; Weight pruning; Advanced driver assistance systems
Scopus,"Wu, P.; Li, H.; Luo, X.; Hu, L.; Yang, R.; Zeng, N.",From data analysis to intelligent maintenance: a survey on visual defect detection in aero-engines,,2025,,,,10.1088/1361-6501/add6c8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005735634&doi=10.1088%2f1361-6501%2fadd6c8&partnerID=40&md5=c2db984d4f1cfb158423a49a7fbd5538,"In this paper, a systematic review of aero-engine defect detection methods is presented, encompassing the general procedure, traditional and intelligent detection algorithms, performance optimization, and future trends. The complete process and innovative theories of aero-engine visual defect detection are analyzed in this overview. Specifically, a five-level taxonomy is designed, with each level further subdivided to provide deeper insights, from data acquisition and task-oriented detection with nondestructive testing (NDT), to practical applications. By leveraging multiscale feature fusion-based detection, these methods achieve enhanced precision in identifying defects across varying scales and complexities. Moreover, in-depth discussions and outlooks on performance optimization and efficient deployment strategies are provided to promote advanced intelligent maintenance solutions for high-end equipment, which may encourage more multidisciplinary collaborations. Compared to other existing surveys, this work comprehensively outlines how computer vision (CV)-based methods can assist in aero-engine defect detection for intelligent decision-making, and a connection between NDT technology and CV-based inspection has been established, thereby drawing greater attention to the application of artificial intelligence to further enhance the development of industrial predictive maintenance. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",aero-engine; computer vision; defect detection; industrial artificial intelligence; multiscale feature fusion; Leak detection; Predictive maintenance; Aero-engine; Defect detection; Engine defects; Features fusions; Industrial artificial intelligence; Intelligent maintenance; Multi-scale features; Multiscale feature fusion; Performance optimizations; Visual defects; Taxonomies
Scopus,"He, S.; Chen, H.; He, L.; Xu, E.; Tang, T.",Active collision avoidance system based on TimesNet behavioral game model and ABAPF risk quantification map,,2025,,,,10.1016/j.measurement.2025.116670,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214681386&doi=10.1016%2fj.measurement.2025.116670&partnerID=40&md5=c22b09a6653b384f205dbf643f5fb55a,"Active collision avoidance is necessary to avoid vehicle collisions. However, existing methods often have difficulties in detecting the potential risks of nearby vehicles and do not fully consider the impact of collision avoidance behavior on nearby vehicles. In order to address the two major challenges of collision avoidance behaviors interacting with each other and short accident response time, a vehicle active collision avoidance system based on TimesNet to predict vehicle intentions with time dependence and Adaptive Boundary Artificial Potential Field (ABAPF) is proposed. Firstly, the vehicles communicate through intelligent connected vehicles, and the surrounding road environment and vehicle motion are used as the risk sources to predict the travelling intentions of the surrounding vehicles for vehicle intentions using two TimesNet networks based on vehicle chassis information and driver habits. Then, a risk map containing potential risks is generated by ABAPF, and a safety factor assessment method is proposed to generate collision avoidance paths. The simulation and real vehicle test results show that the accuracy of the system is 94.2%, and the collision avoidance path has excellent performance in the evaluation. Avoids collisions well in the event of an accident. © 2025 Elsevier Ltd",Collision avoidance; Driver habit; Intent interaction; Safety factor assessment; TimesNet; Risk assessment; Vehicle safety; Artificial potential fields; Avoidance behaviour; Collision avoidance systems; Collisions avoidance; Driver habit; Game models; Intent interaction; Potential risks; Safety factor assessment; Timesnet
Scopus,"Ren, Z.; Yao, K.; Sheng, S.; Wang, B.; Lang, X.; Wan, D.; Fu, W.",YOLO-SDH: improved YOLOv5 using scaled decoupled head for object detection,,2025,,,,10.1007/s13042-024-02357-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205894324&doi=10.1007%2fs13042-024-02357-3&partnerID=40&md5=43af934027e3b4aa343ab0d4c0610960,"As a crucial component of computer vision tasks, object detection serves a significant role in a variety of fields, including autonomous driving, defect detection, and remote sensing image recognition. However, the majority of current object detection networks fail to achieve a decent balance between detection accuracy and detection efficiency, and there is room for improvement in terms of detection accuracy. In response, to improve detection accuracy, a more efficient network framework, YOLO-SDH, was proposed in this paper based on You Only Look Once v5 (YOLOv5). In addition, a decoupled head that automatically adjusts the number of channels according to the model size was proposed, which can enhance the network’s detection effect by separating the classification and regression tasks.On the premise of requiring less computation, a lightweight deformable convolution module is proposed so that the convolution can extract ROI over a wider range, thereby enhancing the accuracy of the object detection network. Experiments were run on the datasets of PASCAL VOC2012, NEU-DET, AW, and RSOD. In comparison to the original YOLOv5, the mAP 0.5 of YOLO-SDH improved by 1.29–3.03%, the F1-score improved by 1.2–3.2%, the Precision improved by 0.7–4.2%, demonstrating the algorithm’s efficacy and superiority. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Convolutional neural network; Decoupled head; Deep learning algorithm; Deformable CovNets; Object detection; Computer vision; Convolutional neural networks; Deep neural networks; Image recognition; Object detection; Object recognition; Regression analysis; Autonomous driving; Convolutional neural network; Decoupled head; Deep learning algorithm; Defect detection; Deformable covnet; Detection accuracy; Detection networks; Driving defects; Objects detection; Convolution
Scopus,"Liu, W.; Zhang, S.; Liu, H.; Zou, J.",A multimodal automatic generation and annotation framework for prohibited and restricted goods in online transactions,,2025,,,,10.1016/j.engappai.2025.111000,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004022351&doi=10.1016%2fj.engappai.2025.111000&partnerID=40&md5=a4d5ebc9eaa91de3089ca7ef0499f3cb,"Transactions of prohibited and restricted goods on e-commerce platforms threaten consumer safety and hinder Internet economy growth. However, the lack of such datasets hinders intelligent identification and effective regulation. To address this issue, a large-scale multimodal dataset of prohibited and restricted goods is constructed, comprising 18,446 images and 36,892 texts. Nevertheless, this is insufficient due to the diverse forms and deep concealment of prohibited and restricted products in online transactions. Therefore, we propose a multimodal automatic generation and annotation framework for prohibited and restricted goods in online transactions. This framework consists of an image generation module, a text description module, and an image annotation module. The image generation module is utilized to generate more diverse images for prohibited and restricted goods. The text description module can generate text descriptions for the generated images. The image annotation module is employed to annotate the generated images for various visual tasks. This framework achieved a 24.71 user rating for image generation, a 92.72 % mean average precision for image annotation, and an 81.67 % semantic score for image description. Experimental results demonstrate the effectiveness and accuracy of the proposed automatic annotation framework, which can enhance the effective supervision of prohibited and restricted goods. © 2025 Elsevier Ltd",Automatic annotation; Image generation; Large language model; Multimodal dataset; Online transaction; Prohibited and restricted goods; Marketplaces; Automatic annotation; Automatic Generation; Image annotation; Image generations; Language model; Large language model; Multi-modal; Multi-modal dataset; Online transaction; Prohibited and restricted good; Electronic money
Scopus,"Baumann, N.; Ghignone, E.; Kühne, J.; Bastuck, N.; Becker, J.; Imholz, N.; Kränzlin, T.; Lim, T.Y.; Lötscher, M.; Schwarzenbach, L.; Tognoni, L.; Vogt, C.; Carron, A.; Magno, M.",ForzaETH Race Stack—Scaled Autonomous Head-to-Head Racing on Fully Commercial Off-the-Shelf Hardware,,2025,,,,10.1002/rob.22429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204468559&doi=10.1002%2frob.22429&partnerID=40&md5=12a274c729ec6dcaf6427338e19cb546,"Autonomous racing in robotics combines high-speed dynamics with the necessity for reliability and real-time decision-making. While such racing pushes software and hardware to their limits, many existing full-system solutions necessitate complex, custom hardware and software, and usually focus on Time-TrIals rather than full unrestricted Head-to-head racing, due to financial and safety constraints. This limits their reproducibility, making advancements and replication feasible mostly for well-resourced laboratories with comprehensive expertise in mechanical, electrical, and robotics fields. Researchers interested in the autonomy domain but with only partial experience in one of these fields, need to spend significant time with familiarization and integration. The ForzaETH Race Stack addresses this gap by providing an autonomous racing software platform designed for F1TENTH, a 1:10 scaled Head-to-Head autonomous racing competition, which simplifies replication by using commercial off-the-shelf hardware. This approach enhances the competitive aspect of autonomous racing and provides an accessible platform for research and development in the field. The ForzaETH Race Stack is designed with modularity and operational ease of use in mind, allowing customization and adaptability to various environmental conditions, such as track friction and layout, which is exemplified by the various modularly implemented state estimation and control systems. Capable of handling both Time-Trials and Head-to-Head racing, the stack has demonstrated its effectiveness, robustness, and adaptability in the field by winning the official F1TENTH international competition multiple times. Furthermore, the stack demonstrated its reliability and performance at unprecedented scales, up to over (Formula presented.) on tracks up to 150 m in length. © 2024 Wiley Periodicals LLC.",autonomous driving; autonomous racing; motion control; open source software; path planning; robotic perception; state estimation; Commercial off-the-shelf; Motion planning; Robot programming; Robustness (control systems); State estimation; Autonomous driving; Autonomous racing; Commercial off-the-shelf hardwares; High Speed; Open-source softwares; Real time decision-making; Real-time decision making; Robotic perception; Software and hardwares; Speed dynamics; Open source software
Scopus,"P, M.P.; K, U.",Video prediction based on temporal aggregation and recurrent propagation for surveillance videos,,2025,,,,10.1016/j.mex.2025.103402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007893407&doi=10.1016%2fj.mex.2025.103402&partnerID=40&md5=c039dbbce93eaf39f0c78caed5da3d5c,"Video prediction is essential for recreating absent frames in video sequences while maintaining temporal and spatial coherence. This procedure, known as video inpainting, seeks to reconstruct missing segments by utilizing data from available frames. Frame interpolation, a fundamental component of this methodology, detects and produces intermediary frames between input sequences. The suggested methodology presents a Bidirectional Video Prediction Network (BVPN) for precisely forecasting absent frames that occur before, after, or between specified input frames. The BVPN framework incorporates temporal aggregation and recurrent propagation to improve forecast accuracy. Temporal aggregation employs a series of reference frames to generate absent content by harnessing existing spatial and temporal data, hence assuring seamless coherence. Recurrent propagation enhances temporal consistency by integrating pertinent information from prior time steps to progressively improve predictions. The timing of frames is constantly controlled through intermediate activations in the BVPN, allowing for accurate synchronization and improved temporal alignment. A fusion module integrates intermediate interpretations to generate cohesive final outputs. Experimental assessments indicate that the suggested method surpasses current state-of-the-art techniques in video inpainting and prediction, attaining enhanced smoothness and precision. Surveillance video datasets demonstrate substantial enhancements in predictive accuracy, highlighting the strength and efficacy of the suggested strategy in practical application. • The proposed method integrates bidirectional video prediction, temporal aggregation, and recurrent propagation to effectively reconstruct missing intermediate video frames with enhanced accuracy. • Comparative analysis using the UCF-Crime dataset demonstrates higher PSNR and SSIM values for the proposed method, indicating improved frame quality and temporal consistency over existing techniques. • This research provides a robust framework for future advancements in video frame prediction, contributing to applications in anomaly detection, surveillance, and video restoration. © 2025 The Author(s)",Frames; Inpainting; Interpolation; Prediction; Recurrent propagation; Temporal aggregation; Time steps; article; crime; female; forecasting; human; middle aged; outlier detection; prediction; videorecording
Scopus,"Cheng, S.; Chen, L.; Yang, K.",DGSS-YOLOv8s: A Real-Time Model for Small and Complex Object Detection in Autonomous Vehicles,,2025,,,,10.3390/a18060358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009135853&doi=10.3390%2fa18060358&partnerID=40&md5=151710d8105d67c424f1a5a670f1f281,"Object detection in complex road scenes is vital for autonomous driving, facing challenges such as object occlusion, small target sizes, and irregularly shaped targets. To address these issues, this paper introduces DGSS-YOLOv8s, a model designed to enhance detection accuracy and high-FPS performance within the You Only Look Once version 8 small (YOLOv8s) framework. The key innovation lies in the synergistic integration of several architectural enhancements: the DCNv3_LKA_C2f module, leveraging Deformable Convolution v3 (DCNv3) and Large Kernel Attention (LKA) for better the capture of complex object shapes; an Optimized Feature Pyramid Network structure (Optimized-GFPN) for improved multi-scale feature fusion; the Detect_SA module, incorporating spatial Self-Attention (SA) at the detection head for broader context awareness; and an Inner-Shape Intersection over Union (IoU) loss function to improve bounding box regression accuracy. These components collectively target the aforementioned challenges in road environments. Evaluations on the Berkeley DeepDrive 100K (BDD100K) and Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) datasets demonstrate the model’s effectiveness. Compared to baseline YOLOv8s, DGSS-YOLOv8s achieves mean Average Precision (mAP)@50 improvements of 2.4% (BDD100K) and 4.6% (KITTI). Significant gains were observed for challenging categories, notably 87.3% mAP@50 for cyclists on KITTI, and small object detection (AP-small) improved by up to 9.7% on KITTI. Crucially, DGSS-YOLOv8s achieved high processing speeds suitable for autonomous driving, operating at 103.1 FPS (BDD100K) and 102.5 FPS (KITTI) on an NVIDIA GeForce RTX 4090 GPU. These results highlight that DGSS-YOLOv8s effectively balances enhanced detection accuracy for complex scenarios with high processing speed, demonstrating its potential for demanding autonomous driving applications. © 2025 by the authors.",autonomous driving; high FPS detection; multi-scale feature fusion; small object detection; YOLOv8 optimization; Automobile drivers; Autonomous vehicles; Complex networks; Feature extraction; Object recognition; Roads and streets; Traffic control; Vehicle detection; Autonomous driving; Complex objects; Features fusions; High FPS detection; Institutes of technologies; Multi-scale feature fusion; Multi-scale features; Optimisations; Small object detection; You only look once version 8 small optimization; Object detection
Scopus,"Li, C.; Jiang, S.; Cao, X.",Small-Target Detection Algorithm Based on STDA-YOLOv8,,2025,,,,10.3390/s25092861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004827736&doi=10.3390%2fs25092861&partnerID=40&md5=73a3d6cddd5ab9a7193385b2eb449ac6,"Due to the inherent limitations of detection networks and the imbalance in training data, small-target detection has always been a challenging issue in the field of target detection. To address the issues of false positives and missed detections in small-target detection scenarios, a new algorithm based on STDA-YOLOv8 is proposed for small-target detection. A novel network architecture for small-target detection is designed, incorporating a Contextual Augmentation Module (CAM) and a Feature Refinement Module (FRM) to enhance the detection performance for small targets. The CAM introduces multi-scale dilated convolutions, where convolutional kernels with different dilation rates capture contextual information from various receptive fields, enabling more accurate extraction of small-target features. The FRM performs adaptive feature fusion in both channel and spatial dimensions, significantly improving the detection precision for small targets. Addressing the issue of a significant disparity in the number of annotations between small and larger objects in existing classic public datasets, a new data augmentation method called Copy–Reduce–Paste is introduced. Ablation and comparative experiments conducted on the proposed STDA-YOLOv8 model demonstrate that on the VisDrone dataset, its accuracy improved by 5.3% compared to YOLOv8, reaching 93.5%; on the PASCAL VOC dataset, its accuracy increased by 5.7% compared to YOLOv8, achieving 94.2%, outperforming current mainstream target detection models and small-target detection algorithms like QueryDet, effectively enhancing small-target detection capabilities. © 2025 by the authors.",contextual augmentation; feature refinement; small-target detection; YOLOv8; Feature extraction; Contextual augmentation; Detection networks; Feature refinement; Inherent limitations; Small target detection; Small targets; Target detection algorithm; Targets detection; Training data; YOLOv8; algorithm; article; controlled study; detection algorithm; diagnosis; diagnostic test accuracy study; false positive result; human; Radar target recognition
Scopus,"Yu, K.; Zhang, H.; Lyu, W.; Guo, Q.; Deng, Z.; Xu, W.",Efficient progressive aggregation enhancement network for defect detection,,2025,,,,10.1088/1361-6501/adbf86,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000529369&doi=10.1088%2f1361-6501%2fadbf86&partnerID=40&md5=93588d14de9dc806fc085c64bc8fb24e,"Computer vision-based deep learning models are of great significance in industrial defect quality detection. Unlike natural objects, defects in industrial products are typically quite small and exhibit highly uneven scales, resulting in the suboptimal performance of conventional object detectors when encountered with complex defect detection. Hence, this paper introduces an efficient progressive aggregation enhanced network (EPAE-Net) with the goal of strengthening defect detection performance in complex scenarios. Firstly, a global context feature enhancement module is designed to model the global context of images, enhancing the model’s ability to perceive key information. Secondly, a downsampling module is designed using self-calibrated convolution to improve the detection performance of small targets. Subsequently, multiplex aggregation FPN is constructed to alleviate the interference caused by information conflicts during feature fusion, further enhance the interaction between cross-layer features, and enhance the detection ability of the model for defects with extreme aspect ratio. Finally, the efficient complete intersection over union loss function is introduced to refine the network and further enhance the performance of network defect detection. The mAP of the proposed EPAE-Net on the Tianchi fabric dataset, printed circuit board dataset, and NEU-DET dataset reaches 77.1%, 98.7%, and 81.5%, respectively. Compared with other state-of-the-art methods, EPAE-Net shows strong competitiveness. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",attention mechanism; deep learning models; feature augmentation; global context feature; industrial defect detection; Contrastive Learning; Deep learning; Attention mechanisms; Context features; Deep learning model; Defect detection; Detection performance; Feature augmentation; Global context; Global context feature; Industrial defect detection; Learning models; Aspect ratio
Scopus,"CAO, X.; NING, X.; LIU, S.; LIAN, X.; WANG, H.; ZHANG, G.; CHEN, F.; ZHANG, J.; LIU, B.; CHEN, Z.",Spacecraft intelligent orbital game technology: A review,,2025,,,,10.1016/j.cja.2025.103480,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005082994&doi=10.1016%2fj.cja.2025.103480&partnerID=40&md5=fa0db1e06584bc73730374e9984d5134,"In recent years, the availability of space orbital resources has been declining, and the increasing frequency of spacecraft close approach events has heightened the urgency for enhanced space security measures. This paper establishes a comprehensive framework for intelligent orbital game technology in space, encompassing four core technologies: threat perception of non-cooperative targets, intent recognition, situation assessment, and intelligent orbital game countermeasures. The concepts of multi-turn, multi-round and multi-match in space orbital games are defined, clarifying the core technological requirements for intelligent space orbital games and establishing a cohesive technological framework. Subsequently, the current status of research on these four core technologies is investigated. The challenges faced in the existing research are analyzed, and potential solutions for future studies are proposed. This paper aims to provide readers with a thorough understanding of the latest advancements in space intelligent orbital game technology, along with insights into the future directions and challenges in this field. © 2025 The Author(s)",Game confrontation; Intelligent orbital game; Intent recognition; Situation assessment; Threat perception; Core technology; Four-core; Game confrontation; Game technologies; Intelligent orbital game; Intent recognition; Orbitals; Situation assessment; Space security; Threat perception
Scopus,"Wang, C.; Song, C.; Xu, T.; Jiang, R.",Precision Weeding in Agriculture: A Comprehensive Review of Intelligent Laser Robots Leveraging Deep Learning Techniques,,2025,,,,10.3390/agriculture15111213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007793342&doi=10.3390%2fagriculture15111213&partnerID=40&md5=5c66772df048d6c99acebbe80b03ed60,"With the advancement of modern agriculture, intelligent laser robots driven by deep learning have emerged as an effective solution to address the limitations of traditional weeding methods. These robots offer precise and efficient weed control, crucial for boosting agricultural productivity. This paper provides a comprehensive review of recent research on laser weeding applications using intelligent robots. Firstly, we introduce the content analysis method employed to organize the reviewed literature. Subsequently, we present the workflow of weeding systems, emphasizing key technologies such as the perception, decision-making, and execution layers. A detailed discussion follows on the application of deep learning algorithms, including Convolutional Neural Networks (CNNs), YOLO, and Faster R-CNN, in weed control. Here, we show that these algorithms can achieve high accuracy in weed detection, with YOLO demonstrating particularly fast and accurate performance. Furthermore, we analyze the challenges and open problems associated with deep learning detection systems and explore future trends in this research field. By summarizing the role of intelligent laser robots powered by deep learning, we aim to provide insights for researchers and practitioners in agriculture, fostering further innovation and development in this promising area. © 2025 by the authors.",artificial intelligence in agriculture; autonomous weeding; deep learning; intelligent laser robots; precision agriculture; weeding in agriculture; agricultural development; artificial intelligence; artificial neural network; laser method; machine learning; precision agriculture; robotics
Scopus,"Ma, Q.-L.; Li, Z.-H.; Yan, H.; Zou, Z.",Object Detection During Vehicle Operation Based on Self-attention and Multi-point Cloud Feature Fusion,,2025,,,,10.19721/j.cnki.1001-7372.2025.05.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008444767&doi=10.19721%2fj.cnki.1001-7372.2025.05.021&partnerID=40&md5=3c00c74eccc6eb2679dd3b8d42026b1e,"This study proposes a multi-feature fusion object detection model called MF-Point, based on the optimization of the PointPillars and PointRCNN detection methods. The purpose is to enhance the perception capabilities of autonomous vehicles in terms of the size, position, and orientation of objects in motion. The model integrates Full Self-Attention (FSA) mechanism to enhance network context awareness. Additionally, Dynamic Self-Attention (DSA) mechanism is employed during the upsampling process to effectively propagate structural information throughout the node network, thereby improving the spatial semantic representation of point-cloud features. Moreover, the baseline algorithm's network incorporates a new pillar feature extraction branch parallel to its original point feature extraction branch. Both branches adopt self-attention variants to generate comprehensive point features that are embedded into the baseline framework to improve the capability of the model to process and interpret point-cloud data. Experiments were conducted using the KITTI 3D object detection dataset, focusing on three target classes: vehicles, cyclists, and pedestrians. The results demonstrate that the improved MF-Point algorithm achieved detection accuracy improvements of 2.19%, 2.68%, and 2.12% for vehicles, pedestrians, and cyclists, respectively, in the 3D mode. In the Bird's Eye View (BEV) mode, the detection accuracies for these three target classes improved by 0.59%, 0.88%, and 4.67%, respectively. To further evaluate the generalizability and practical performance of the MF-point model, it was deployed in the ROS system. The model was validated using the 64-line KITTI dataset and 32-line real-world LiDAR data, achieving detection speeds of 8.4 frame·s-1 and 10.7 frame·s-1, respectively. Experiments demonstrate that the MF-Point model improves the detection performance for occluded moving objects, reducing the miss rate by 2.1%. The model exhibits high real-time performance and reliable detection accuracy on both public datasets and real-world tests. © 2025 Chang'an University. All rights reserved.",3D target detection; automotive engineering; autonomous driving; LiDAR point cloud; self-attention mechanism; Automobile drivers; Autonomous vehicles; Extraction; Feature extraction; Object detection; Object recognition; Optical radar; Pillar extraction; Target tracking; Three dimensional computer graphics; Vehicle detection; 3d target detection; Attention mechanisms; Automotives; Autonomous driving; Detection accuracy; LiDAR point cloud; Objects detection; Point-clouds; Self-attention mechanism; Targets detection; Automotive engineering
Scopus,"Li, Y.; Li, X.; Lin, M.",FE-YOLO: Fourier enhancement YOLO for end-to-end object detection in low-light conditions,,2025,,,,10.1016/j.dsp.2025.105355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006988873&doi=10.1016%2fj.dsp.2025.105355&partnerID=40&md5=027023ee1de00b76c5fda73d7d011727,"Current low-light image object detection algorithms using deep learning often have low accuracy, high computational load, and poor model generalization. This paper presents an efficient end-to-end low-light image object detection network called FE-YOLO, which combines the amplitude and phase information from the Fourier transform with the object detection ability of YOLO. Firstly, a Fourier Enhanced Network (FENet) is proposed, which incorporates a Frequency Domain Processing Block (FPB). The FPB extracts frequency domain information and enhances image brightness and contrast by expanding the amplitude, improving image quality in low-light conditions. Secondly, a new approach to optimize both image enhancement and object detection simultaneously is presented. It combines the enhancement loss and the detection loss into a joint loss function, using two specific loss functions: amplitude difference loss and phase similarity loss. The functions accurately constrain the amplitude and phase of the image, balancing image enhancement and structure information preservation, thus improving the performance of object detection. Thirdly, a Joint comprehensive training strategy is used on FE-YOLO to improve its generalization through an end-to-end joint training approach. Experiments are conducted using low-light image datasets such as ExDark and DarkFace. The experimental results demonstrate that FENet outperforms other low-light image enhancement models and shows better noise immunity. FE-YOLO also performs excellently in low-light object detection. Furthermore, experiments using YOLOv8 confirm that it outperforms current state-of-the-art low-light image object detection algorithms. The detailed experimental results and program code have been made public at: https://github.com/tgliyang1985/FE-YOLO. © 2025",Amplitude difference loss; Fourier enhancement network; Low-light image; Object detection; Phase similarity loss; Fourier transforms; Frequency domain analysis; Image coding; Object detection; Photointerpretation; Amplitude difference; Amplitude difference loss; End to end; Fourier; Fourier enhancement network; Image object detection; Low light conditions; Low-light images; Objects detection; Phase similarity loss; Image enhancement
Scopus,"Norouzi, M.; Hosseini, S.H.; Khoshnevisan, M.; Moshiri, B.",Applications of pre-trained CNN models and data fusion techniques in Unity3D for connected vehicles,,2025,,,,10.1007/s10489-024-06213-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218211183&doi=10.1007%2fs10489-024-06213-3&partnerID=40&md5=af9feb64217344de50b6c1ce6e6e9e72,"Intelligent Transportation Systems (ITS) aim to enhance road safety and Internet of Things (IoT)-related solutions are crucial in achieving this objective. By leveraging Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) technologies, drivers can access valuable information about their surroundings. This research utilized the Unity 3D game engine to simulate various traffic scenarios, exploring a stochastic environment with two data sources: camera and road sign labels. We developed a full-duplex communication system to enable the communication between Python and Unity. This allows the vehicle to capture images in Unity and classify them using Convolutional Neural Network (CNN) models coded in Python. To improve road sign detection accuracy, we applied multi-sensor Data Fusion (DF) techniques to fuse the information received from the sources. We applied DF methods such as the Kalman filter, Dempster-Shafer theory, and Fuzzy Integral Operators to combine the two sources of information. Furthermore, our proposed CNN model incorporates an Ordered Weighted Averaging (OWA) layer to fuse information from three pre-trained CNN models. Our results show that the proposed model integrating the OWA layer achieved an accuracy of 98.81%, outperforming six state-of-the-art models. We compared the Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). In our work, EKF exhibited a lower execution time (0.02 seconds), yielding less accurate results. UKF, however, provided a more accurate estimate while being more computationally complex. Furthermore, the Dempster-Shafer model showed approximately 30% better accuracy compared to the Fuzzy Integral Operator. Using this methodology on autonomous vehicles in our virtual environment led to making more accurate decisions, even in a variety of weather conditions and accident scenarios. The findings of this research contribute to the development of more efficient and safer vehicles. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",Connected vehicles; Convolutional Neural Networks (CNN); Data fusion; Dempster-shafer theory; Kalman filter; OWA operator; Unity game engine; Autonomous vehicles; Biocommunications; Convolutional neural networks; Highway accidents; Image segmentation; Motor transportation; Multilayer neural networks; Problem oriented languages; Procedure oriented languages; Road and street markings; Sensor data fusion; Steganography; Connected vehicle; Convolutional neural network; Data fusion technique; Dempster-Shafer theory; Fuzzy integral; Game Engine; Neural network model; Ordered weighted averaging operator; Unity game engine; Extended Kalman filters
Scopus,"Adam, M.A.A.; Tapamo, J.R.",Survey on Image-Based Vehicle Detection Methods,,2025,,,,10.3390/wevj16060303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008902787&doi=10.3390%2fwevj16060303&partnerID=40&md5=8963876b2bb11d9c07865fb65a3191c6,"Vehicle detection is essential for real-world applications such as road surveillance, intelligent transportation systems, and autonomous driving, where high accuracy and real-time performance are critical. However, achieving robust detection remains challenging due to scene complexity, occlusion, scale variation, and varying lighting conditions. Over the past two decades, numerous studies have been proposed to address these issues. This study presents a comprehensive and structured survey of image-based vehicle detection methods, systematically comparing classical machine learning techniques based on handcrafted features with modern deep learning approaches. Deep learning methods are categorized into one-stage detectors (e.g., YOLO, SSD, FCOS, CenterNet), two-stage detectors (e.g., Faster R-CNN, Mask R-CNN), transformer-based detectors (e.g., DETR, Swin Transformer), and GAN-based methods, highlighting architectural trade-offs concerning speed, accuracy, and practical deployment. We analyze widely adopted performance metrics from recent studies, evaluate characteristics and limitations of popular vehicle detection datasets, and explicitly discuss technical challenges, including domain generalization, environmental variability, computational constraints, and annotation quality. The survey concludes by clearly identifying open research challenges and promising future directions, such as efficient edge deployment strategies, multimodal data fusion, transformer-based enhancements, and integration with Vehicle-to-Everything (V2X) communication systems. © 2025 by the authors.",classical and deep learning; object detection; real-time detection; vehicle detection; Automobile drivers; Data communication systems; Deep learning; Human computer interaction; Intelligent vehicle highway systems; Learning systems; Motor transportation; Object detection; Object recognition; Security systems; Signal detection; Traffic control; Vehicle detection; Vehicle performance; Vehicle to vehicle communications; Autonomous driving; Classical and deep learning; Detection methods; High-accuracy; Image-based; Intelligent transportation systems; Objects detection; Real-time detection; Real-world; Vehicles detection; Economic and social effects
Scopus,"Mudavath, T.; Mamidi, A.",Object detection challenges: Navigating through varied weather conditions—Acomprehensive survey,,2025,,,,10.1007/s12652-025-04956-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001483013&doi=10.1007%2fs12652-025-04956-6&partnerID=40&md5=392bd506047ba6313295540d5c830d2e,"Detecting objects in computer vision is a challenging task, especially under varied weather conditions such as rain, fog, snow, etc. which degrades the visibility, and illumination changes in the image. These conditions create specific challenges in object detection in recognizing the distinct objects in the image. The influence of changing weather conditions remains the cause for concern although deep learning has revolutionized object detection through multi-stage detectors that offer improved accuracy over one-stage detectors, which allow rapid inference. This survey offers a comprehensive exploration of the various object detection methods, and datasets, and emphasizes the challenges posed under the challenging weather conditions. By analyzing current methodologies in object detection and identifying gaps in existing research, this paper provides the limitations under the weather conditions and highlights the research opportunities under weather conditions for the development of better object detection methods that can be suitable for surveillance applications. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.",And YOLO; Classification; Deep learning; Localization; Machine learning; Object detection; Weather conditions; Computer vision; Object detection; Object recognition; Security systems; And YOLO; Condition; Deep learning; Detecting objects; Illumination changes; Localisation; Machine-learning; Object detection method; Objects detection; Weather condition; Deep learning
Scopus,"Wutke, M.; Debiasi, D.; Tomar, S.; Probst, J.; Kemper, N.; Gevers, K.; Lieboldt, M.-A.; Traulsen, I.",Multistage pig identification using a sequential ear tag detection pipeline,,2025,,,,10.1038/s41598-025-05283-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008710490&doi=10.1038%2fs41598-025-05283-8&partnerID=40&md5=d6e2ae34dbd984f5811a881bb390b203,"Reliable animal identification in livestock husbandry is essential for various applications, including behavioral monitoring, welfare assessment, and the analysis of social structures. Although recent advancements in deep learning models have improved animal identification using biometric markers, their applicability remains limited for species without distinctive traits like pigs. Consequently, synthetic features such as ear tags have become widely adopted. However, challenges such as poor lighting conditions and the complexity of ear tag coding continue to restrict the effectiveness of Computer Vision and Deep Learning techniques. In this study, we introduce a robust, lighting-invariant method for individual pig identification that leverages commercially available ear tags within a sequential detection pipeline. Our approach employs four object detection models in succession to detect pigs, localize ear tags, perform rotation correction via pin detection, and recognize digits, ultimately generating a reliable ID proposal. In a first evaluation stage, we assessed the performance of each model independently, achieving a mAP0.95 value of 0.970, 0.979, 0.974 and 0.979 for the pig detection, ear tag detection, pin detection and ID classification model, respectively. In addition, our method was further evaluated in two different camera environments to assess its performance in both familiar and unfamiliar conditions. The results demonstrate that the proposed approach achieves a very high precision of 0.996 in a familiar top-down camera scenario and maintained a strong generalization performance in an unfamiliar, close-up setup with a precision of 0.913 and a recall of 0.903. Furthermore, by publicly proposing three custom datasets for ear tag, pin, and digit detection, we aim to support reproducibility and further research in automated animal identification for precision livestock farming. The findings of this study demonstrate the effectiveness of ID-based animal identification and the proposed method could be integrated within advanced multi-object tracking systems to enable continuous animal observation and for monitoring specific target areas, thereby significantly enhancing overall livestock management systems. © The Author(s) 2025.",Animal identification; Computer vision; YOLOv10; Animal Husbandry; Animal Identification Systems; Animals; Deep Learning; Ear; Swine; animal; animal husbandry; animal identification; deep learning; ear; pig; procedures
Scopus,"Mohammed, A.; Ibrahim, H.M.; Omar, N.M.",Optimizing RetinaNet anchors using differential evolution for improved object detection,,2025,,,,10.1038/s41598-025-02888-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008583584&doi=10.1038%2fs41598-025-02888-x&partnerID=40&md5=f0eb35854be8d576186b6d9dadee7fe3,"Object detection is a fundamental task in computer vision. It has two primary types: one-stage detectors known for their high speed and efficiency, and two-stage detectors, which offer higher accuracy but are often slower due to their complex architecture. Balancing these two aspects has been a significant challenge in the field. RetinaNet, a premier single-stage object detector, is renowned for its remarkable balance between speed and accuracy. Its success is largely due to the groundbreaking focal loss function, which adeptly addresses the issue of class imbalance prevalent in object detection tasks. This innovative approach significantly enhances detection accuracy while maintaining high speed, making RetinaNet an ideal choice for a wide range of real-world applications. However, its performance decreases when applied to datasets containing objects with unique characteristics, such as objects with elongated or squat shapes. In such cases, the default anchor parameters may not fully meet the requirements of these specialized objects. To overcome this limitation, we present an enhancement to the RetinaNet model to improve its ability to handle variations in objects across different domains. Specifically, we propose an optimization algorithm based on Differential Evolution (DE) that adjusts anchor scales and ratios while determining the most appropriate number of these parameters for each dataset based on the annotated data. Through extensive experiments on datasets spanning diverse domains such as the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI), the Unconstrained Face Detection Dataset (UFDD), the TomatoPlantFactoryDataset, and the widely used Common Objects in Context (COCO) 2017 benchmark, we demonstrate that our proposed method significantly outperforms both the original RetinaNet and anchor-free methods by a considerable margin. © The Author(s) 2025.",Anchor optimization; Computer vision; Deep learning; Differential evolution; Object detection; RetinaNet; algorithm; article; benchmarking; computer vision; deep learning; diagnosis; evolution; velocity
Scopus,"Zhou, Y.; Wu, X.; Li, Y.; Sun, H.; Fan, D.",Algorithm for surface flow velocity measurement in trunk canal based on improved YOLOv8 and DeepSORT,,2025,,,,10.1016/j.engappai.2025.110344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219056857&doi=10.1016%2fj.engappai.2025.110344&partnerID=40&md5=d845ff914925a27cb83fe49e27019bd0,"The velocity measurement of trunk canal and river plays an important role in agriculture and forestry irrigation scheduling, water resources management and flood prediction. Particle flow measurement technology can realize non-contact and high-precision flow measurement, but in practical application, the particle size is small, the shape is different and the dynamic change brings great challenges to the application of this method. To solve these problems, this paper proposed the surface velocity measurement method of trunk canal based on improved YOLOv8(You Only Look Once Version 8) and DeepSORT(Deep Simple Online and Realtime Tracking), and introduced tiny detection layer and channel attention mechanism to improve YOLOv8's detection capability of small targets. In DeepSORT, IBN-Net(Intent-Based Networking-Network) network structure and GIoU(Generalized Intersection over Union) matching are introduced to solve the problem of discontinuity or loss of target tracking in complex cases, which improves the accuracy and robustness of target tracking. The experimental results show that the improved YOLOv8 improves AP(Average Precision) and mAP(mean Average Precision) by nearly 5% and 0.2% respectively. The performance of the improved DeepSORT has been improved across the board, especially IDP and MOTA, which have improved by 25.2% and 5.6% respectively. The algorithm also has good accuracy in actual velocity measurement. © 2025 The Authors",Deep simple Online and Realtime Tracking(DeepSORT) target tracking; Generalized intersection over Union(GIoU); Intent-based Networking-network(IBN-Net); Surface velocity measurement of trunk canal; You only look once Version 8(YOLOV8) target detection; Acceleration measurement; Flood control; Flow measurement; Flow velocity; Flowmeters; Irrigation canals; Water management; Deep simple online and realtime tracking target tracking; Generalized intersection; Generalized intersection over union; Intent-based networking-network; Measurements of; On-line tracking; Real time tracking; Simple++; Surface velocity; Surface velocity measurement of trunk canal; Targets detection; Targets tracking; You only look once version 8 target detection; Velocity measurement
Scopus,"Gu, Q.; Han, Z.; Kong, S.; Huang, H.; Li, Y.; Fan, Q.; Wu, R.",DCYOLO: Dual negative weighting label assignment and cross-layer decouple head for YOLO in remote sensing images,,2025,,,,10.1016/j.eswa.2025.127595,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002487689&doi=10.1016%2fj.eswa.2025.127595&partnerID=40&md5=63ed929cdcd53d3323a0f4c379461ece,"Existing remote sensing detection methods have achieved excellent detection performance, but the current detectors still present some problems: 1. In the process of label assignment and loss calculation, only one negative weight is assigned to calculate the loss of negative samples. 2. The detector only fuses multi-scale features at the Feature Pyramids Networks (FPN), while the output is predicted at the detector head, which makes the detector head lack some of the multi-scale features. 3. A single down sampling method can result in the loss of small and tiny object features. To solve the above problems, in this work we propose an anchor-free detector based on YOLOv8. Our proposed method contains three improvements: An integrated down sampling module (IDS), a Cross-layer Decouple Head (CDH), and a new label assignment method called Dual Negative Weight Task Alignment Learning (DNW-TAL). The IDS integrates three down sampling methods to alleviate the problem of missing features on small and tiny objects. CDH fuses and enhances cross-layer features by encoding and decoding the features. The DNW-TAL first uses CGS to select the actual samples for training. In terms of sample assignment, DNW-TAL divides the negative samples into those inside and outside the ground truth (GT), and obtains the negative sample weights for its loss calculation based on the distribution and performance of the samples. Ablation experiments on the VisDrone and AI-TOD datasets show that our proposed methods improve the APmaxDets=100IOU=0.50:0.95 metrics by 2.2% and 1.9%, which is a performance improvement of 13.75% and 13.97% relative to the baseline. On the VisDrone, AI-TOD, and DOTA v1.5 datasets, the mAP of our proposed method achieved 24.3%, 28.6% and 67.4%, respectively, which are the best results compared to the comparison methods, proving that DCYOLO is comparable to other SOTAs. © 2025 Elsevier Ltd",Anchor-free; Deep learning; Label assignment; Object detection; Remote sensing image; Feature extraction; Object detection; Object recognition; Remote sensing; Anchor-free; Cross layer; Deep learning; Down sampling; Label assignment; Loss calculation; Multi-scale features; Negative samples; Objects detection; Remote sensing images; Proximity sensors
Scopus,"Wang, Z.; Zhu, Y.; Zhang, Y.; Liu, S.",An effective deep learning approach enabling miners’ protective equipment detection and tracking using improved YOLOv7 architecture,,2025,,,,10.1016/j.compeleceng.2025.110173,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217957979&doi=10.1016%2fj.compeleceng.2025.110173&partnerID=40&md5=ce03f3adcea0e1c14269c00da3e00a3c,"In the complex underground mining environment, ensuring the correct wearing of personal protective equipment (PPE) is crucial for coal mine safety production. To overcome the limitations of existing PPE detection and tracking technologies, which often suffer from low precision, slow performance, and complex feature extraction processes, this paper introduces an enhanced, lightweight, and high-precision object detection network model based on YOLOv7. The proposed model incorporates a streamlined backbone feature extraction architecture that combines the Mobile Inverted Bottleneck Convolution module with the GhostBottleneck Lightweight module. This integration significantly improves the detection accuracy of miners’ PPE while simultaneously reducing the number of network parameters. Furthermore, the model adopts adaptive spatial feature fusion to enhance its capability in effectively integrating cross-scale features, thereby further boosting its detection performance. To enable continuous and stable tracking of miners’ PPE usage, this paper integrates the DeepSort tracking algorithm, which is based on OSNet, with the improved YOLOv7 detection model. This combination constructs an efficient video-based multi-object tracking algorithm, providing essential support for enhancing the tracking performance of coal miners’ PPE. Experimental results demonstrate that, compared to other state-of-the-art methods, the proposed model achieves a 2.25% increase in mean Average Precision (mAP), a 2.91% improvement in F1 score, a 0.41% enhancement in precision, and a 5.34% increase in recall for PPE detection. Additionally, it exhibits significant improvements in multi-object tracking metrics, with a 5.9% increase in Multi-Object Tracking Accuracy (MOTA), a 3.5% increase in Multi-Object Tracking Precision (MOTP), and a 6.2% increase in IDF1 score. These results fully validate the model's efficient detection and tracking capabilities for miners’ PPE in complex underground mining environments. © 2025 Elsevier Ltd",DeepSort; Object detection and tracking; PPE; YOLOv7; Miners; Mining equipment; Protective clothing; Underground equipment; Deepsort; Detection and tracking; Features extraction; Learning approach; Mining environments; Multi-object tracking; Object detection and tracking; Personal protective equipment; Underground mining; YOLOv7; Coal mines
Scopus,"Zhang, R.; Ji, X.; Loughney, S.; Wang, J.; Yang, Z.",Visual perception for long-distance and small target detection in autonomous maritime navigation,,2025,,,,10.1016/j.oceaneng.2025.121447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004548784&doi=10.1016%2fj.oceaneng.2025.121447&partnerID=40&md5=3e138e138b8aeb373d6a4527ff0e23d6,"In the pursuit of advancing autonomous maritime navigation, this study aimed to develop a novel architecture designed to enhance the detection accuracy of distant and small targets under the constraints of real-time performance and robustness. Through the innovative integration of the Convolutional Block Attention Module (CBAM) into the detection model's backbone, the study achieved superior feature extraction capabilities tailored for the complexities of maritime environments. Further optimization of the Spatial Pyramid Pooling (SPP) module ensured model compactness and computational efficiency, vital for deployment on edge devices. A key methodological novelty lay in the incorporation of the S-IoU loss function, which offers superior bounding box regression accuracy over the traditional Generalized Intersection over Union, directly contributing to more precise navigation and effective obstacle avoidance. The proposed enhancements collectively yielded a 5.1 % increase in mAP@50 %, accompanied by an 11.2 % reduction in model parameters and a 12.6 % decrease in computational complexity (GFLOPs). These findings underscore the potential of the presented architecture to significantly contribute to maritime safety, presenting an optimized solution for collision avoidance and navigation assistance in congested sea routes and adverse weather conditions. © 2025",Autonomous ships; Computer version; Object detection; Small target; Visual navigation aid; Marine safety; NP-hard; Object detection; Waterway transportation; Autonomous ship; Computer versions; Maritime navigation; Navigation aids; Objects detection; Small target detection; Small targets; Visual Navigation; Visual navigation aid; Visual perception; detection method; maritime transportation; navigation aid; pattern recognition; regression analysis; vessel; Marine navigation
Scopus,"Xue, B.; Zhang, B.; Cheng, Q.",Experiment study on UAV target detection algorithm based on YOLOv8n-ACW,,2025,,,,10.1038/s41598-025-91394-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001738010&doi=10.1038%2fs41598-025-91394-1&partnerID=40&md5=9cef6fb1c8dd0ff5db6bb7324cc4eb94,"To address the challenges associated with dense and occluded targets in small target detection utilizing unmanned aerial vehicle (UAV), we propose an enhanced detection algorithm referred as the YOLOv8n-ACW. Building upon the YOLOv8n baseline network model, we have integrated Adown into the Backbone and developed a CCDHead to further improve the drone’s capability to recognize small targets. Additionally, WIoU-V3 has been introduced as the loss function. Experiment results derived from the Visdrone2019 dataset indicate that, the YOLOv8n- ACW has achieved a 4.2% increase in mAP50(%) compared to the baseline model, while simultaneously reducing the parameter count by 36.7%, exhibiting superior capabilities in detecting small targets. Furthermore, utilizing a self-constructed dataset of G5-Pro drones for target detection experiments, the results indicate that the enhanced model has robust generalization capabilities in real-world environments. The UAV target detection experiment combines experimental simulation with real-world testing, while combining scientific exploration with educational objectives. This experiment has high fidelity, excellent functional scalability, and strong practicality, aiming to cultivate students’ comprehensive practical and innovative abilities. © The Author(s) 2025.",Deep learning; Experiment study; Target detection; Unmanned aerial vehicle; article; controlled study; deep learning; detection algorithm; drone; nonhuman; pharmacology; simulation; unmanned aerial vehicle
Scopus,"Wang, K.; He, G.; Li, X.",A Lightweight Multi-Scale Context Detail Network for Efficient Target Detection in Resource-Constrained Environments,,2025,,,,10.3390/s25123800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009030092&doi=10.3390%2fs25123800&partnerID=40&md5=1bdc14d788001d89fa584bb9febda855,"Target detection in resource-constrained environments faces multiple challenges such as the use of camouflage, diverse target sizes, and harsh environmental conditions. Moreover, the need for solutions suitable for edge computing environments, which have limited computational resources, adds complexity to the task. To meet these challenges, we propose MSCDNet (Multi-Scale Context Detail Network), an innovative and lightweight architecture designed specifically for efficient target detection in such environments. MSCDNet integrates three key components: the Multi-Scale Fusion Module, which improves the representation of features at various target scales; the Context Merge Module, which enables adaptive feature integration across scales to handle a wide range of target conditions; and the Detail Enhance Module, which emphasizes preserving crucial edge and texture details for detecting camouflaged targets. Extensive evaluations highlight the effectiveness of MSCDNet, which achieves 40.1% mAP50-95, 86.1% precision, and 68.1% recall while maintaining a low computational load with only 2.22 M parameters and 6.0 G FLOPs. When compared to other models, MSCDNet outperforms YOLO-family variants by 1.9% in mAP50-95 and uses 14% fewer parameters. Additional generalization tests on VisDrone2019 and BDD100K further validate its robustness, with improvements of 1.1% in mAP50 on VisDrone and 1.2% in mAP50-95 on BDD100K over baseline models. These results affirm that MSCDNet is well suited for tactical deployment in scenarios with limited computational resources, where reliable target detection is paramount. © 2025 by the authors.",context-aware modulation; edge computing; lightweight neural network; multi-scale feature fusion; target detection; Camouflage; Edge detection; Feature extraction; Radar target recognition; Target tracking; Context-Aware; Context-aware modulation; Edge computing; Features fusions; Lightweight neural network; Multi-scale feature fusion; Multi-scale features; Multi-scales; Neural-networks; Targets detection; Edge computing
Scopus,"Wang, J.; Wang, Y.; Zhou, C.; Wang, H.",SOR-YOLO: a strip-shaped object detection network for traffic columns and traffic cones,,2025,,,,10.1007/s11760-025-04017-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003109115&doi=10.1007%2fs11760-025-04017-7&partnerID=40&md5=9945b5d8a0f3b2f5719f8479662c08f8,"Traffic columns and traffic cones, as crucial traffic guidance tools, are often used together to maintain the orderliness of road traffic. However, due to their small size and frequent use in large quantities and high density, it is difficult for autonomous vehicles to accurately capture and locate them. Therefore, we choose the YOLOv8n network as the baseline and propose a novel strip-shaped object recognition model, namely SOR-YOLO. Firstly, we utilize the SGE attention mechanism and GhostConv module to construct a lightweight grouped enhanced C2f module, namely LGEC2f module, which reduces the number of parameters and computations of the network. Secondly, we integrate the MSCA module into the SPPF module to form the MSCSPPF module, thus enhancing the network’s ability to recognize strip-shaped objects. Finally, we replace the loss function of the YOLOv8n network with the WIoU loss function to accelerate the convergence of the model. Experiments show that SOR-YOLO improves the mAP50 and mAP75 by 2.7% and 3.0% respectively, and the mAP50-95 by approximately 1.1% compared to YOLOv8n in the recognition of traffic columns and traffic cones, demonstrating higher recognition accuracy. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",Dense environment; Small object detection; Strip-shaped object; YOLOv8; Air traffic control; Cones; Electronic guidance systems; Highway traffic control; Image segmentation; Magnetic levitation vehicles; Motor transportation; Object detection; Robotics; Street traffic control; Autonomous Vehicles; Dense environment; Detection networks; Loss functions; Objects detection; Road traffic; Small object detection; Strip-shaped object; Traffic cones; YOLOv8
Scopus,"Chiroma, H.",Emerging trends in artificial intelligence: Integrating theories and practice,,2025,,,,10.1088/978-0-7503-6320-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009189858&doi=10.1088%2f978-0-7503-6320-4&partnerID=40&md5=df45ebe6f0dd4cdd2b3108086a17b00f,"Emerging Artificial Intelligence: Integrating theories and practice is specifically tailored for researchers in AI who are eager to delve into the latest developments and explore the integration of theoretical concepts with practical applications. This book is an essential guide for researchers in AI seeking to bridge the gap between theory and practice, embark on a transformative journey through the latest advancements in AI and gain a comprehensive understanding of their real-world applications. After reading the book, the reader will be able to clearly understand the industrial applications of each emerging AI domain and appreciate the operations of AI in industry as well as society in general. Part of IOP Series in Next Generation Computing. Key features • Case studies within the text and summaries at the end of each chapter. • Emerging field of artificial intelligence at a glance. • Industrial applications of the emerging artificial intelligence field. • Shows the connection between theories and practical applications of emerging artificial intelligence. • Presents a concise summary of the different aspect of artificial intelligence from origin to current times in one spot. © IOP Publishing Ltd 2025. All rights reserved.",Artificial intelligence; 'current; Case-studies; Emerging trends; Key feature; Latest development; Real-world; Theory and practice; Computation theory
Scopus,"Chen, Z.; Ma, Y.; Gong, Z.; Cao, M.; Yang, Y.; Wang, Z.; Wang, T.; Li, J.; Liu, Y.",R-AFPN: a residual asymptotic feature pyramid network for UAV aerial photography of small targets,,2025,,,,10.1038/s41598-025-00008-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004686757&doi=10.1038%2fs41598-025-00008-3&partnerID=40&md5=ca5c89a11bda1b0f26a81e4a2d5ccf95,"This study proposes an improved Residual Asymptotic Feature Pyramid Network (R-AFPN) to address challenges in small target detection from the Unmanned Aerial Vehicle (UAV) perspectives, such as scale imbalance, feature extraction difficulty, occlusion, and computational constraints. The R-AFPN integrates three key modules: Residual Asymptotic Feature Fusion (RAFF) for adaptive spatial fusion and cross-scale linking, Shallow Information Extraction (SIE) for capturing detailed shallow features, and Hierarchical Feature Fusion (HFF) for bottom-up incremental fusion to enhance deep feature details. Experimental results demonstrate that R-AFPN-L achieves 50.7% AP50 on the TinyPerson dataset and 48.9% mAP50 on the VisDrone2019 dataset, outperforming the baseline by 3% and 1.2%, respectively, while reducing parameters by 15.1%. This approach offers a lightweight, efficient solution for small target detection in UAV applications. © The Author(s) 2025.",Asymptotic feature fusion; Context learning; Residual connectivity; Small target detection; UAV; adult; aged; article; context learning; controlled study; diagnosis; feature extraction; human; male; photography; therapy; unmanned aerial vehicle
Scopus,"Yang, H.; Fu, G.; Shao, H.; Wang, Y.; Shao, Y.; Chu, H.; Deng, H.",Small Object Detection in Aerial Imagery Using Multi-Scale Hiearchical Feature Fusion Based Approach,,2025,,,,10.3778/j.issn.1002-8331.2408-0105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007328921&doi=10.3778%2fj.issn.1002-8331.2408-0105&partnerID=40&md5=a304f4dc1f754d28f9921f02fe4c6f2e,"Aiming at the problem of low accuracy in detecting small objects due to large field of view, small object size, and dense distribution in aerial images, a multi-scale feature fusion aerial detection algorithm based on improved YOLOv8 is proposed. Firstly, a lightweight L-MobileViT module is constructed to capture effective relationships between features, mitigate information loss, and improve the detection performance of the model. Secondly, a hierarchical multi-scale fusion module HF (hierarchical fusion) is proposed to integrate deep spatial information and shallow semantic information, enhancing the detection capability of small objects in dense scenes. Finally, a tiny detection head is added, and a large detection head is removed based on YOLOv8 to focus on the detection ability of small objects and reduce the missed detection rate of small objects. Experimental results show that the improved model achieves superior performance on the VisDrone2019 and UAV-TrafficTinyDataset datasets, with mAP@50 increased by 17.6% and 15.7%, respectively, compared to the baseline model. The detection effect of small objects is significantly improved, and the overall performance is better than mainstream aerial detection algorithms. This demonstrates that the improved algorithm has better generalization and robustness, making it suitable for detection tasks in aerial scenarios. © 2025 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",L-MobileViT; multi-level feature fusion; small target detection; UAV images; YOLOv8; Aerial detection; Detection algorithm; Features fusions; L-mobilevit; Multi-level feature fusion; Multilevels; Small objects; Small target detection; UAV image; YOLOv8; Target drones
Scopus,"Huang, D.; Zhang, G.; Li, Z.; Liu, K.; Luo, W.",Light-YOLO: A lightweight and high-performance network for detecting small obstacles on roads at night,,2025,,,,10.1016/j.cviu.2025.104428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008508782&doi=10.1016%2fj.cviu.2025.104428&partnerID=40&md5=169a0703af23d6c804b0b8efa5b5c9b8,"To address the challenges of detecting small obstacles and model portability, this study proposes a lightweight, high-precision, and high-speed small obstacle detection network at nighttime road environments referred to as Light-YOLO. First, the SPDConvMobileNetV3 feature extraction network is introduced, which significantly reduces the total number of parameters while enhancing the ability to capture small obstacle details. Next, to make the network more focused on small obstacles at nighttime conditions, a loss function called Wise-IoU is incorporated, which is more suitable to low-quality images. Finally, to improve overall model performance without increasing the total number of parameters, a parameter-free attention mechanism (SimAM) is integrated. By comparing the publicly available data with the self-built dataset, the experimental results show that Light-YOLO achieves a mean average precision (mAP50) of 97.1% while maintaining a high image processing speed. Additionally, compared to other advanced models in the same series, Light-YOLO has fewer parameters, a smaller computational load (GFLOPs), and reduced model weight (Best.pt). Overall, Light-YOLO strikes a balance between lightweight design, accuracy, and speed, making it more suitable for hardware-constrained devices. © 2025 Elsevier Inc.",Lightweight network; Night road; Parameter-free attention mechanism; Small obstacle detection; SPDConvMobileNetV3; Wise-IoU; Computer software portability; Image processing; Obstacle detectors; Attention mechanisms; High performance networks; High-precision; Lightweight network; Night road; Obstacles detection; Parameter-free attention mechanism; Small obstacle detection; Spdconvmobilenetv3; Wise-IoU; Roads and streets
Scopus,"Han, Y.; Wang, C.; Luo, H.; Wang, H.; Chen, Z.; Xia, Y.; Yun, L.",LRDS-YOLO enhances small object detection in UAV aerial images with a lightweight and efficient design,,2025,,,,10.1038/s41598-025-07021-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009535307&doi=10.1038%2fs41598-025-07021-6&partnerID=40&md5=9ce696b8b65e42b35b7b462dc451bec2,"Small object detection in UAV aerial images is challenging due to low contrast, complex backgrounds, and limited computational resources. Traditional methods struggle with high miss detection rates and poor localization accuracy caused by information loss, weak cross-layer feature interaction, and rigid detection heads. To address these issues, we propose LRDS-YOLO, a lightweight and efficient model tailored for UAV applications. The model incorporates a Light Adaptive-weight Downsampling (LAD) module to retain fine-grained small object features and reduce information loss. A Re-Calibration Feature Pyramid Network (Re-Calibration FPN) enhances multi-scale feature fusion using bidirectional interactions and resolution-aware hybrid attention. The SegNext Attention mechanism improves target focus while suppressing background noise, and the dynamic detection head (DyHead) optimizes multi-dimensional feature weighting for robust detection. Experiments show that LRDS-YOLO achieves 43.6% mAP50 on VisDrone2019, 11.4% higher than the baseline, with only 4.17M parameters and 24.1 GFLOPs, striking a balance between accuracy and efficiency. On the HIT-UAV infrared dataset, it reaches 84.5% mAP50, demonstrating strong generalization. With its lightweight design and high precision, LRDS-YOLO offers an effective real-time solution for UAV-based small object detection. © The Author(s) 2025.",Attention mechanisms; Feature pyramid network; Real time; Small object detection; UAV (unmanned aerial vehicle); adult; article; background noise; calibration; controlled study; diagnosis; female; human; male; unmanned aerial vehicle
Scopus,"Xing, J.; Zhan, C.; Ma, J.; Chao, Z.; Liu, Y.",Lightweight detection model for safe wear at worksites using GPD-YOLOv8 algorithm,,2025,,,,10.1038/s41598-024-83391-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214426180&doi=10.1038%2fs41598-024-83391-7&partnerID=40&md5=6566889a78aeb234d0f2976265bd832d,"To address the significantly elevated safety risks associated with construction workers’ improper use of helmets and reflective clothing, we propose an enhanced YOLOv8 model tailored for safety wear detection. Firstly, this study introduces the P2 detection layer within the YOLOv8 architecture, which substantially enriches semantic feature representation. Additionally, a lightweight Ghost module is integrated to replace the original backbone of YOLOv8, thereby reducing the parameter count and computational burden. Moreover, we incorporate a Dynamic Head (Dyhead) that employs an attention mechanism to effectively extract features and spatial location information critical for site safety wear detection. This adaptation significantly enhances the model’s representational power without adding computational overhead. Furthermore, we adopt an Exponential Moving Average (EMA) SlideLoss function, which not only boosts accuracy but also ensures the stability of our safety wear detection model’s performance. Comparative evaluation of the experimental results indicates that our proposed model achieves a 6.2% improvement in mean Average Precision (mAP) compared to the baseline YOLOv8 model, while also increasing the detection speed by 55.88% in terms of frames per second (FPS). © The Author(s) 2024.",Deep learning; Ghost module; Site safety wearable detection; YOLOv8; adaptation; algorithm; article; clothing; construction worker; controlled study; deep learning; female; helmet; human; safety; velocity; wearable device
Scopus,"Zhuo, S.; Shi, J.; Bai, H.; Zhou, X.; Kan, J.; Cai, J.",Improved Threshold-free Automatic Dependent Surveillance-Broadcast preamble detection algorithm based on deep learning framework,,2025,,,,10.1016/j.dsp.2025.105307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004923925&doi=10.1016%2fj.dsp.2025.105307&partnerID=40&md5=f3f88bb461ded53bbe311624cba80c1f,"In the study of Automatic Dependent Surveillance-Broadcast (ADS-B) signal decoding in S-mode, accurate detection of the signal preamble is a critical prerequisite for successful decoding. To address the challenges of low detection accuracy and slow processing speed in low Signal-to-Noise Ratio (SNR) environments, we propose an intelligent ADS-B signal preamble detection algorithm. First, an improved You Only Look Once version 8 (YOLOv8) object detection model is utilized to precisely capture the ADS-B signal Preamble in the frequency domain. Next, a coordinate transformation method is employed to obtain the temporal position of the preamble pulses within the time domain signal. Finally, an enhanced threshold-free cross-correlation preamble detection algorithm is applied to achieve precise preamble detection in the time domain. Experimental results demonstrate that, in both simulated datasets and real-world measurement environments, the proposed algorithm effectively mitigates the issue of preamble detection accuracy degradation caused by threshold fluctuations under low-SNR conditions. Specifically, the proposed algorithm achieves detection accuracies of 58.7% and 99.8% at SNR = -3 dB and 15 dB, respectively, surpassing traditional detection algorithms in accuracy. © 2025 Elsevier Inc.",ADS-B signal; Coordinate transformation; Cross-correlation detection; S mode; YOLOv8; Automatic dependent surveillance broadcasts; Automatic dependent surveillance-broadcast signal; Broadcast signals; Coordinate transformations; Correlation detection; Cross-correlation detection; Cross-correlations; Preamble detection; S mode; You only look once version 8; Cutoff frequency
Scopus,"Bongomin, O.; Mwape, M.C.; Mpofu, N.S.; Bahunde, B.K.; Kidega, R.; Mpungu, I.L.; Tumusiime, G.; Owino, C.A.; Goussongtogue, Y.M.; Yemane, A.; Kyokunzire, P.; Malanda, C.; Komakech, J.; Tigalana, D.; Gumisiriza, O.; Ngulube, G.",Digital twin technology advancing industry 4.0 and industry 5.0 across sectors,,2025,,,,10.1016/j.rineng.2025.105583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007788941&doi=10.1016%2fj.rineng.2025.105583&partnerID=40&md5=3f4d6604e4d3623daa4481e133e76beb,"Digital Twin (DT) technology is transforming industrial systems by integrating physical assets with digital models, enabling real-time monitoring, predictive analytics, and process optimization, particularly within the framework of Industry 4.0 (I4.0). As the global industrial landscape shifts toward Industry 5.0 (I5.0), DTs are increasingly being redefined to support human-centric innovation, sustainability, and system resilience. This review examines the evolving role of DTs in bridging the efficiency-driven goals of I4.0 with the inclusive, sustainable objectives of I5.0. It explores ten enabling technologies such as artificial intelligence (AI), internet of things, blockchain, cloud and edge computing, and extended reality, while discussing both the opportunities and challenges posed by I5.0. The study emphasizes key principles of DTs, including real-time synchronization, feedback mechanisms, and lifecycle integration. A detailed sectorial analysis across manufacturing, infrastructure, energy, transportation, mining, agriculture, and healthcare illustrates how DTs are being applied in diverse contexts to enhance operational efficiency, product quality, and decision-making. The mapping of applications by country, sector, and industrial focus reveals growing trends toward I5.0 in areas such as logistics and infrastructure. Common application domains include monitoring, optimization, prediction, and decision support. Despite their potential, DT adoption faces challenges including high implementation costs, data integration issues, cybersecurity concerns, and lack of standardization. The review discusses these barriers alongside the importance of validation and security for trusted deployment. It concludes by identifying future directions, including cognitive twins, industrial metaverse integration, and ethical AI. DTs are positioned as foundational technologies for advancing sustainable, resilient, and human-centered industrial systems. © 2025 The Authors",Artificial intelligence; Digital twins; Human-centric innovation; Industry 4.0; Industry 5.0; Predictive analytics; Sustainability; Computer control; Decision making; Model predictive control; Network security; Trusted computing; Analytic optimization; Digital modeling; Human-centric; Human-centric innovation; Industrial systems; Industry 5.0; Physical assets; Process optimisation; Real time monitoring; System resiliences; Prediction models
Scopus,"Silva, R.M.; Azevedo, G.F.; Berto, M.V.V.; Rocha, J.R.; Fidelis, E.C.; Nogueira, M.V.; Lisboa, P.H.; Almeida, T.A.",Vulnerable road user detection and safety enhancement: A comprehensive survey,,2025,,,,10.1016/j.eswa.2025.128529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008325759&doi=10.1016%2fj.eswa.2025.128529&partnerID=40&md5=ccabfc1bdcf5ff6ab17ff366aed13424,"Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study investigates the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is mandatory for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development. © 2025 Elsevier Ltd",Collision avoidance; Intention prediction; Machine learning; Object detection; Sensor data processing; Sensor datasets; Simulation environments; Traffic communication ecosystem; Traffic sensors; Vulnerable road user; Accident prevention; Behavioral research; Cutting; Data communication systems; Data handling; Ecosystems; Highway accidents; Learning algorithms; Learning systems; Machine learning; Motor transportation; Object recognition; Roads and streets; Sensor data fusion; Surveying; Telecommunication traffic; Traffic control; Traffic signals; Collisions avoidance; Intention predictions; Machine-learning; Objects detection; Road users; Sensor data processing; Sensor dataset; Simulation environment; Traffic communication; Traffic communication ecosystem; Traffic sensors; Vulnerable road user; Collision avoidance; Object detection
Scopus,"Essien, E.; Frimpong, S.","Enhancing Autonomous Truck Navigation in Underground Mines: A Review of 3D Object Detection Systems, Challenges, and Future Trends",,2025,,,,10.3390/drones9060433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008956975&doi=10.3390%2fdrones9060433&partnerID=40&md5=7d3f3efb5e648d0c4b889b7ef4e9489a,"Integrating autonomous haulage systems into underground mining has revolutionized safety and operational efficiency. However, deploying 3D detection systems for autonomous truck navigation in such an environment faces persistent challenges due to dust, occlusion, complex terrains, and low visibility. This affects their reliability and real-time processing. While existing reviews have discussed object detection techniques and sensor-based systems, providing valuable insights into their applications, only a few have addressed the unique underground challenges that affect 3D detection models. This review synthesizes the current advancements in 3D object detection models for underground autonomous truck navigation. It assesses deep learning algorithms, fusion techniques, multi-modal sensor suites, and limited datasets in an underground detection system. This study uses systematic database searches with selection criteria for relevance to underground perception. The findings of this work show that the mid-level fusion method for combining different sensor suites enhances robust detection. Though YOLO (You Only Look Once)-based detection models provide superior real-time performance, challenges persist in small object detection, computational trade-offs, and data scarcity. This paper concludes by identifying research gaps and proposing future directions for a more scalable and resilient underground perception system. The main novelty is its review of underground 3D detection systems in autonomous trucks. © 2025 by the authors.",3D object detection; autonomous trucks; deep learning; sensor fusion; underground mines; YOLO algorithms; Automobiles; Autonomous vehicles; Data mining; Learning systems; Mine trucks; Navigation; Object detection; Object recognition; Real time systems; Underground mine transportation; 3D object; 3d object detection; Autonomous truck; Deep learning; Detection models; Detection system; Objects detection; Sensor fusion; Underground mine; You only look once algorithm; Deep learning; Economic and social effects
Scopus,"Wang, H.; Liu, Y.",YOLO-LiRa: lightweight detection algorithm for small aerial targets,,2025,,,,10.1088/1361-6501/addbf9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007510172&doi=10.1088%2f1361-6501%2faddbf9&partnerID=40&md5=b930694dc37052e8f7b2fb519a0097cf,"With the rapid development of drone technology, aerial small target detection has become an important research direction in the field of computer vision. This paper proposes a lightweight object detection algorithm based on YOLOv11, YOLO-LiRa (Lightweight Intelligent Remote-sensing Algorithm), to address the issues of small size, complex background, and dense targets faced by small object detection in aerial images. Incorporating Monte Carlo attention mechanism and partial convolution into the C3K2 module enhances the extraction of small target features and reduces computational complexity. Referring to the lightweight design concept of MobileNetV3 to optimize the backbone network, combined with GSConv and VoVGSCSP modules, the multi-scale feature fusion capability of the neck network is enhanced. Moreover, the feature map resolution and detection performance were optimized using the DySample upsampling operator. Experiments on the publicly available AI-TOD datasets have shown that YOLO-LiRa achieves 0.27 on the mAP50-95 evaluation metric, reducing the parameter count by 24.4% and computational complexity by 12.5% compared to the Baseline model, while achieving a good balance between detection accuracy and speed. Compared with other mainstream object detection algorithms, YOLO-LiRa exhibits more competitive performance in small object detection tasks. The method proposed in this article is suitable for applications such as unmanned aerial vehicle monitoring, intelligent transportation, and agricultural monitoring that require high lightweight and real-time performance. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",aerial images; lightweight; small target detection; YOLOv11; Aerial photography; Drones; Aerial images; Aerial targets; Complex background; Detection algorithm; Lightweight; Object detection algorithms; Remote sensing algorithms; Small object detection; Small target detection; YOLOv11; Target drones
Scopus,"Hussain, K.; Moreira, C.; Pereira, J.; Jardim, S.; Jorge, J.",A Comprehensive Literature Review on Modular Approaches to Autonomous Driving: Deep Learning for Road and Racing Scenarios,,2025,,,,10.3390/smartcities8030079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009305631&doi=10.3390%2fsmartcities8030079&partnerID=40&md5=988ae5f31d95a5e04ae003f620960aae,"Highlights: What are the main accomplishments of this study? A comprehensive analysis of deep learning techniques in both on-road and autonomousracing cars, highlighting distinct challenges and requirements for each context. The identification of critical challenges for future research, to ensure safety and performancein autonomous systems. What are the implications of the main findings? The detailed evaluation of planning methods and performance metrics points to opportunitiesto refine existing methodologies and identify emerging research areasthat can guide the development of more efficient, robust, and scalable autonomousdriving technologies. The challenges identified in sensor fusion, environmental robustness, and computationalefficiency imply that addressing these issues is critical to progress inautonomous systems. Autonomous driving technology is advancing rapidly, driven by integrating advanced intelligent systems. Autonomous vehicles typically follow a modular structure, organized into perception, planning, and control components. Unlike previous surveys, which often focus on specific modular system components or single driving environments, our review uniquely compares both settings, highlighting how deep learning and reinforcement learning methods address the challenges specific to each. We present an in-depth analysis of local and global planning methods, including the integration of benchmarks, simulations, and real-time platforms. Additionally, we compare various evaluation metrics and performance outcomes for current methodologies. Finally, we offer insights into emerging research directions based on the latest advancements, providing a roadmap for future innovation in autonomous driving. © 2025 by the authors.",autonomous driving; autonomous racing; control; deep learning; modular system; perception; planning; safety; simulations
Scopus,"Yan, B.; Li, X.; Yan, R.",An AI-Based Horticultural Plant Fruit Visual Detection Algorithm for Apple Fruits,,2025,,,,10.3390/horticulturae11050541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006582024&doi=10.3390%2fhorticulturae11050541&partnerID=40&md5=4d9f0009d4cb7ea5e914ab545d923a4f,"In order to improve the perception accuracy of the apple tree fruit recognition model and to reduce the model size, a lightweight apple target recognition method based on an improved YOLOv5s artificial intelligence algorithm was proposed, and relevant experiments were designed. The Depthwise Separable Convolution (DWConv) module has many advantages: (1) It has high computational efficiency, reducing the number of parameters and calculations in the model; (2) It makes the model lightweight and easy to deploy in hardware; (3) DWConv can be combined with other modules to enhance the multi-scale feature extraction capability of the detection network and improve the ability to capture multi-scale information; (4) It balances the detection accuracy and speed of the model; (5) DWConv can flexibly adapt to different network structures. Because of its efficient computing modes, lightweight design, and flexible structural adaptation, the DWConv module has significant advantages in multi-scale feature extraction, real-time performance improvement, and small-object detection. Therefore, this method improves the original YOLOv5s network architecture by replacing the embedded Depthwise Separable Convolution in its Backbone network, which reduces the size and parameter count of the model while ensuring detection accuracy. The experimental results show that for the test-set images, the proposed improved model has an average recognition accuracy of 92.3% for apple targets, a recognition time of 0.033 s for a single image, and a model volume of 11.1 MB. Compared with the original YOLOv5s model, the average recognition accuracy was increased by 0.8%, the recognition speed was increased by 23.3%, and the model volume was compressed by 20.7%, effectively achieving lightweight improvement of the apple detection model and improving the accuracy and speed of detection. The detection algorithm proposed in the study can be extended to the intelligent measurement of apple biological and physical characteristics, including for size measurement, shape analysis, and color analysis. The proposed method can improve the intelligence level of orchard management and horticultural technology, reduce labor costs, assist precision agriculture technology, and promote the transformation of the horticultural industry toward sustainable development. © 2025 by the authors.",apple fruits; artificial intelligence; deep learning; horticulture; plants
Scopus,"Ye, J.; Li, P.; Zhang, Y.; Guo, Z.; Zeng, S.; Zhan, Y.",MLHI-Net: multi-level hybrid lightweight water body segmentation network for urban shoreline detection,,2025,,,,10.1038/s41598-025-87209-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218238669&doi=10.1038%2fs41598-025-87209-y&partnerID=40&md5=58214d180f5b30364f159dca780d110b,"The capacity to detect shorelines is critical for the autonomous navigation of Unmanned Surface Vehicles (USVs). The majority of extant methods are unable to adapt to the discrimination of high similarity features between the shore and reflections in complex and diverse environments. Moreover, they are also incapable of accurately extracting fuzzy boundaries caused by different scenes and climatic conditions. To address these challenges, this paper proposes a multi-level hybrid lightweight water segmentation network, MLHI-Net. First, we design a convolutional module (ORRD) compatible with over-parameterized and redundancy removal techniques based on lightweight design. The over-parameterized convolutional layers enhance the interactive ability of feature representation and context information. The removal of spatial and channel redundancy, in conjunction with interactive reconstruction, serves to simulate attention and enhance the learning ability of waterscape. Then, we design a multi-branch two-layer attention fusion module (MDA), which achieves diverse attention and global optimisation of edge details by connecting spatial attention, channel attention and pixel attention in parallel. thereby guiding feature fusion and solving the problem of receptive field mismatch. This module guides feature fusion and solves the problem of receptive field mismatch. To validate the proposed methodology, a dataset, CityWater, was constructed, with multiple fields and climatic conditions, and a substantial number of experiments were conducted on this and other public datasets. Experimental results show that MLHI-Net outperforms other advanced segmentation networks in Mean Intersection over Union (MIoU) and Pixel Accuracy (PA) on the CityWater and USVInland datasets, with MIoU of 97.86% and PA of 98.92% on the CityWater dataset, and MIoU of 98.12% and PA of 99.10% on the USVInland dataset. Additionally, the network’s computational GLOPS is 13.45 G, and the number of parameters is 46.92 M, which can meet the requirements for real-time detection. The MLHI-Net has been shown to perform well in a range of environments. In addition, it has good generalisation capabilities, providing reliable support to the autonomous system. © The Author(s) 2025.",MDA; ORRD; Shoreline detection; Unmanned surface vehicles; Water segmentation network; article; controlled study; human; hybrid; receptive field; simulation; spatial attention; water
Scopus,"Li, Y.; Tan, L.; Xu, X.; Zhang, Z.",Room-level localization method in industrial workshops using LiDAR-based point cloud registration and object recognition,,2025,,,,10.1007/s10489-025-06244-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217748150&doi=10.1007%2fs10489-025-06244-4&partnerID=40&md5=1fd16b487d52f204115022f1ed545aed,"In this work, we aim to achieve room-level localization for mobile robots in industrial workshops. It is difficult to obtain precise localization information via common methods because of the complexity of the industrial environment. Our findings show that precise room-level localization can be achieved via LiDAR-based point cloud registration and object recognition. For this purpose, we formulate room-level localization as a classification problem. Registration and object recognition are used to extract features from point clouds. After the data enhancement algorithm, called Stacked Auto Encoder is employed to overcome the issue of limited feature data, the neural network algorithm is leveraged to address the classification problem. To this end, we collected point cloud data from industrial workshops and performed experimental validation. We evaluated the recognition performance of the algorithm in a metallurgical workshop and achieved good accuracy. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",Data enhancement; Mobile robot; Neural network algorithm; Object recognition; Registration; Room-level localization; Industrial robots; Interpolation; Metadata; Object detection; Data enhancement; Localisation; Localization for mobile robot; Localization information; Localization method; Neural networks algorithms; Objects recognition; Point cloud registration; Registration; Room-level localization; Mobile robots
Scopus,"Chen, B.; Ma, Z.; Li, X.",Boosting Detection Accuracy: An Enhanced YOLOv8 for Small Target Detection in Remote Sensing,,2025,,,,10.1007/s44196-025-00895-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008519377&doi=10.1007%2fs44196-025-00895-4&partnerID=40&md5=646059da8ebd9236b9b63c5bf441ad7c,"Object detection in remote sensing images has broad applications in military reconnaissance, urban planning, and disaster management. However, detecting small targets and extracting multi-scale features in complex scenes remain challenging. This paper presents YOLOv8s-Improved, enhancing small-target detection via PPHGNetV2, Progressive Feature Pyramid Network (AFPN-P2), and Diverse Branch Block (DBB) modules. Experiments on the DIOR and VisDrone2019 datasets show that YOLOv8s-Improved achieves mAP scores of 0.824 and 45.3%, respectively, representing improvements of 1.5 and 6.4 percentage points over the baseline YOLOv8s model (0.809 and 38.9%). The improved model demonstrates strong performance in multi-category object detection, particularly in complex scenes. The results suggest that the proposed method addresses the challenges of small target detection in remote sensing and exhibits generalization capabilities across different datasets. © The Author(s) 2025.",DIOR dataset; Diverse branch block (DBB); PPHGNetV2; Remote sensing object detection; Small target detection; VisDrone2019 dataset; Complex networks; Disaster prevention; Disasters; Military applications; Object detection; Object recognition; Radar target recognition; Target tracking; Urban planning; Complex scenes; Detection accuracy; DIOR dataset; Diverse branch block; Objects detection; PPHGNetV2; Remote sensing object detection; Remote-sensing; Small target detection; Visdrone2019 dataset; Remote sensing
Scopus,"Ma, Z.; Luo, P.; Shen, X.",LMSOE-Net: lightweight multi-scale small object enhancement network for UAV aerial images,,2025,,,,10.1007/s40747-025-01971-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007889727&doi=10.1007%2fs40747-025-01971-0&partnerID=40&md5=10292128f9178df893a0a09e9dd90354,"Detecting objects of varying scales, especially small ones, in Unmanned Aerial Vehicle (UAV) aerial images across diverse scenarios and viewpoints using onboard edge devices is a major challenge in computer vision. To tackle this issue, we propose a lightweight multi-scale small object enhancement network (LMSOE-Net) based on the YOLOv8 architecture. To improve both detection performance and model efficiency, we introduce the Efficient Multi Scale Pyramid (EMSP) neck network. This versatile feature fusion network enhances multi-scale feature extraction and integration by using convolutional modules with varying kernel sizes. This design enables more effective feature extraction and facilitates the fusion of local details with channel features. Additionally, we replace the Spatial Pyramid Pooling Fast (SPPF) module in YOLOv8 with the Feature Pyramid Shared Convolution (FPSC) module. This upgrade strengthens the network’s ability to capture fine details and complex patterns, improving multi-scale feature extraction without a significant increase in parameters. We further optimize the model by incorporating shared convolution with detail-enhancement capabilities in the detection head, which improves the detection of small objects across different scales. We evaluate LMSOE-Net through ablation and comparison experiments on the VisDrone2019 and DOTAv1.5 datasets. Compared to three variants of YOLOv8, LMSOE-Net reduces parameters and computational complexity by approximately 30%, while improving mAP@0.5 and mAP@0.5:0.95 by 1–2%. The results demonstrate that our approach significantly boosts detection accuracy and optimizes model efficiency through the integration of our proposed enhancement modules. The source codes are at: https://github.com/ljdl1/LMSOE-Net. © The Author(s) 2025.",Lightweight; Multi-scale; Small object detection; UAV; YOLOv8
Scopus,"Du, Y.; Chen, L.; Hao, X.",RL-Net: a rapid and lightweight network for detecting tiny vehicle targets in remote sensing images,,2025,,,,10.1007/s40747-025-01956-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007522033&doi=10.1007%2fs40747-025-01956-z&partnerID=40&md5=578edcb1b423632018bd32d4db7c149e,"Traffic accidents remain a critical issue that significantly impacts public safety and poses major challenges to intelligent transportation systems. The integration of Unmanned Aerial Vehicles (UAVs) with object detection technology offers a promising solution to this problem. However, existing detection networks often exhibit limitations such as missed detections and inadequate real-time performance, particularly for small vehicle targets in remote sensing images, and fail to meet the efficiency requirements of edge computing devices. To address these challenges, this study proposes RL-Net, a rapid and lightweight network model based on enhancements to the YOLOv9s architecture. First, MobileNetV4 is introduced to optimize initial feature extraction, significantly improving the network’s efficiency. Second, the Lightweight Spatial Pyramid Pooling Fast (LSPPF) structure is designed to enhance multiscale feature extraction while accelerating computational speed. Additionally, the Lightweight Representation Cross Stage Partial with ELAN (LRepCSPELAN) module is proposed to further reduce the model’s memory and computational resource demands. Finally, an enhanced feature fusion network is designed to improve detection performance for tiny vehicle targets. Comprehensive evaluations on the VisDrone2019 and UA-DETRAC datasets demonstrate that, compared to YOLOv9s model, RL-Net achieves a 34.5% reduction in parameters, a 4.4% reduction in computations, a 30.8% increase in Frames Per Second (FPS), and improvements of 2.5% in mean Average Precision (mAP) and 2% in recall, effectively balancing detection efficiency and performance. © The Author(s) 2025.",Edge computing; Lightweight; Object detection; Tiny target; UAVs
Scopus,"Cheng, X.; Fang, Y.; Feng, J.; Yin, H.",EL-YOLOv8: a lightweight algorithm for efficient detection of pipeline welding defects in X-ray images,,2025,,,,10.1007/s11760-025-03877-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218426758&doi=10.1007%2fs11760-025-03877-3&partnerID=40&md5=9044584c12ee63c01bbf725afdefa2e9,"To address the problems of low detection efficiency and false detection of small target defects in the intelligent detection of X-ray image weld defects of industrial pressure pipelines, this paper proposed a more efficient and light EL-YOLOv8 weld defect detection algorithm. Firstly, data augmentation was performed to solve the problem of unclear defects and insufficient data sets. In order to improve the original YOLOv8 model, the FasterNetBlock was combined with the Efficient Multi-Scale Attention (EMA) module to design a lightweight multi-scale feature Faster-EMA module, which was fused with the CSPDarknet53 to Two-stage FPN (C2f) module in the backbone network. The C2f-Faster-EMA module is proposed to realize multi-scale feature object detection and enhance the feature extraction ability. The experimental results show that compared with five mainstream defect detection algorithms such as YOLOv8-Ghost, the proposed model achieves 91.5% mAP@0.5 defect accuracy on the self-developed X-ray welding defect image dataset, which is 2.8% higher than that of the baseline model. Compared with four mainstream lightweight models such as MobileNet V2, the parameter number of the proposed model is 2.5, the FPS reaches 205, and the processing speed is the fastest and the model is the lightest while maintaining a high accuracy, achieving lightweight. At the same time, on the public datasets COCO128 and ImageNet100, the AP@0.50:0.95 of our model is 2.5% higher than that of YOLOv8, which proves that this model also has good generalizability. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",Defect detection; Lightweight; Weld; YOLOv8; X ray detectors; Defect detection; Defect detection algorithm; Efficient detection; Lightweight; Multi-scale features; Multi-scales; Pipe line welding; Welding defects; X-ray image; YOLOv8; Image enhancement
Scopus,"Chen, Y.; Sun, Y.; Qin, Z.; Wang, Z.; Geng, Y.",CSEANet: Cross-Stage Enhanced Aggregation Network for Detecting Surface Bolt Defects in Railway Steel Truss Bridges,,2025,,,,10.3390/s25113500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007818984&doi=10.3390%2fs25113500&partnerID=40&md5=0a33acaddd479c07fabe3430ab12fdf7,"The accurate detection of surface bolt defects in railway steel truss bridges plays a vital role in maintaining structural integrity. Conventional manual inspection techniques require extensive labor and introduce subjective assessments, frequently yielding variable results across inspections. While UAV-based approaches have recently been developed, they still encounter significant technical obstacles, including small target recognition, background complexity, and computational limitations. To overcome these challenges, CSEANet is introduced—an improved YOLOv8-based framework tailored for bolt defect detection. Our approach introduces three innovations: (1) a sliding-window SAF preprocessing method that improves small target representation and reduces background noise, achieving a 0.404 mAP improvement compared with not using it; (2) a refined network architecture with BSBlock and MBConvBlock for efficient feature extraction with reduced redundancy; and (3) a novel BoltFusionFPN module to enhance multi-scale feature fusion. Experiments show that CSEANet achieves an mAP@50:95 of 0.952, confirming its suitability for UAV-based inspections in resource-constrained environments. This framework enables reliable, real-time bolt defect detection, supporting safer railway operations and infrastructure maintenance. © 2025 by the authors.",bolt defect detection; multi-scale feature fusion; railway safety; small object detection; UAV-based inspection; Railroad transportation; Bolt defect detection; Defect detection; Features fusions; Multi-scale feature fusion; Multi-scale features; Railway safety; Small object detection; Small targets; Steel truss bridge; UAV-based inspection; Subways
Scopus,"Peng, S.; Zhang, X.; Zhou, L.; Wang, P.",YOLO-CBD: Classroom Behavior Detection Method Based on Behavior Feature Extraction and Aggregation,,2025,,,,10.3390/s25103073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006657579&doi=10.3390%2fs25103073&partnerID=40&md5=ed80e149c43883dd74cdbcf83423850f,"Classroom behavior can effectively reflect learning states, and thus classroom behavior detection is crucial for improving teaching methods and enhancing teaching quality. To address issues such as severe occlusions and large scale variations in student behavior detection, this paper proposes a classroom behavior detection model, named YOLO-CBD (YOLOv10s Classroom Behavior Detection). Firstly, BiFormer attention is introduced to redesign the Efficientv2 network, leading to a novel backbone network for efficient feature extraction of student classroom behaviors. The proposed attention module enables accurate localization in densely populated student settings. Secondly, a novel feature aggregation module is designed for replacing the basic C2f module in the YOLOv10s neck network and enhancing the capability to detect occluded targets effectively. Additionally, a feature pyramid network with efficient feature fusion is constructed to address inconsistencies among features of different scales. Finally, the Wise-IoU loss function is incorporated to handle sample imbalance issues. Experimental results show that, compared to the baseline model, YOLO-CBD improves precision by 5.7%, recall by 3.7%, and mAP50 by 3.5%, achieving effective classroom behavior detection. © 2025 by the authors.",classroom behavior detection; feature aggregation; feature extraction; multi-scale feature fusion; YOLOv10; Engineering education; Students; Behavior detection; Classroom behavior detection; Detection methods; Feature aggregation; Features extraction; Features fusions; Multi-scale feature fusion; Multi-scale features; Teaching methods; YOLOv10; article; controlled study; diagnosis; feature extraction; human; learning; male; neck; nonhuman; Teaching
Scopus,"Fonod, R.; Cho, H.; Yeo, H.; Geroliminis, N.",Advanced computer vision for extracting georeferenced vehicle trajectories from drone imagery,,2025,,,,10.1016/j.trc.2025.105205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009507433&doi=10.1016%2fj.trc.2025.105205&partnerID=40&md5=761b73429e7592816084f547b14ea42e,"This paper presents a comprehensive framework for extracting georeferenced vehicle trajectories from high-altitude drone imagery, addressing key challenges in urban traffic monitoring and the limitations of traditional ground-based systems. Our approach integrates several novel contributions, including a tailored object detector optimized for high-altitude bird's-eye view perspectives, a unique track stabilization method that uses detected vehicle bounding boxes as exclusion masks during image registration, and an orthophoto and master frame-based georeferencing strategy that enhances consistent alignment across multiple drone viewpoints. Additionally, our framework features robust vehicle dimension estimation and detailed road segmentation, enabling comprehensive traffic dynamics analysis. Conducted in the Songdo International Business District, South Korea, the study utilized a multi-drone experiment covering 20 intersections, capturing approximately 12TB of ultra-high-definition video data over four days. The framework produced two high-quality datasets: the Songdo Traffic dataset, comprising approximately 700,000 unique vehicle trajectories, and the Songdo Vision dataset, containing over 5000 human-annotated images with about 300,000 vehicle instances categorized into four classes. Comparisons with high-precision sensor data from an instrumented probe vehicle highlight the accuracy and consistency of our extraction pipeline in dense urban environments. The public release of the Songdo Traffic and Songdo Vision datasets, along with the complete source code for the extraction pipeline, establishes new benchmarks in data quality, reproducibility, and scalability in traffic research. The results demonstrate the potential of integrating drone technology with advanced computer vision methods for precise and cost-effective urban traffic monitoring, providing valuable resources for developing intelligent transportation systems and enhancing traffic management strategies. © 2025 The Authors",Georeferenced vehicle trajectories; Machine vision; Multi-drone data collection; Traffic monitoring; Urban traffic; Vehicle tracking; Video image processing; Incheon [South Korea]; Songdo; South Korea; Advanced traffic management systems; Advanced traveler information systems; Aircraft detection; Benchmarking; Computer vision; Data handling; Drones; Highway traffic control; Image enhancement; Image registration; Image segmentation; Information management; Motor transportation; Street traffic control; Urban planning; Urban transportation; Vehicle detection; Video recording; Video signal processing; Data collection; Georeferenced vehicle trajectory; Ground-based systems; Machine-vision; Multi-drone data collection; Object detectors; Traffic monitoring; Urban traffic; Vehicle trajectories; Video-image processing; computer vision; imagery; monitoring system; tracking; traffic management; trajectory; transport vehicle; unmanned vehicle; urban transport; Cost effectiveness
Scopus,"Li, M.; Lan, J.; Wang, L.",Road object localization and detection of thermal infrared images using a recursive gated convolution cross-aggregation network,,2025,,,,10.1016/j.measurement.2025.116971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217262407&doi=10.1016%2fj.measurement.2025.116971&partnerID=40&md5=1bf25860b21ca4d2d87fdb34d3ec4a4f,"Achieving accuracy perception is critical for ensuring vehicle safety in autonomous driving. Among sensing technologies, thermal infrared imaging provides the advantage of all-light capability. However, detection and positioning performance declines due to the low contrast of infrared images and the complex traffic backgrounds. To address this challenge, this paper proposes the recursive gated convolution cross-aggregation network (RGC-CAN). Firstly, the recursive gated convolutional network (RGC-Net) is proposed as the feature extraction module. This network employs gated convolution within a recursive-positional attention architecture to capture high-order spatial interactions, enhancing the contrast between the object and the background. Secondly, the cross-aggregation networks (CA-Net) is proposed for feature fusion. By leveraging both top-down and lateral connections, CA-Net fully exploits the semantic abstract features from deeper layers and the pixel features from shallower layers, improving overall feature representation. Finally, the cross-data interaction loss function (CILF) is proposed, which enhances the model's capabilities in sample classification and position regression. Experimental evaluations on the FLIR-ADAS dataset and KAIST dataset show that the proposed network achieves mean average precision of 90.4% and 96.9%, respectively, surpassing current state-of-the-art (SOTA) methods. Additionally, a series of ablation studies and extended experiments validate the effectiveness and broad applicability of our network. © 2025 Elsevier Ltd",Autonomous driving; Gated convolution; Object detection; Thermal infrared image; Object detection; Object recognition; Aggregation network; Autonomous driving; Detection performance; Gated convolution; Object localization; Objects detection; Sensing technology; Thermal infrared images; Thermal infrared imaging; Vehicle safety; Infrared imaging
Scopus,"Zhang, H.; Fu, W.; Wang, X.; Li, D.; Zhu, D.; Su, X.",An improved and lightweight small-scale foreign object debris detection model,,2025,,,,10.1007/s10586-024-05002-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003671787&doi=10.1007%2fs10586-024-05002-4&partnerID=40&md5=e1a35d5a61a055a1f6b6e721a6b042ca,"Foreign object debris (FOD) on runways may cause irreparable damage and detecting FOD in intellectual technology has attracted more and more attention. Due to the limitation of the equipment, the main goal of the FOD detection methods is to obtain as high accuracy as possible by employing the model with as few parameters as possible. In this article, we propose a Lightweight FOD detection model named LF-YOLO, which can improve the detection accuracy of small FOD items and simultaneously decrease the parameters of the implemented model. The proposed LF-YOLO follows the overall framework of YOLOv8 and can be viewed as one of its improved variants. Firstly, to compensate for the loss of information regarding small objects during feature extraction, high-resolution feature maps were incorporated into the detection layer to fuse multiscale features, and the large object detection layer was removed from the model. Secondly, a lightweight backbone with strong feature extraction ability was developed by introducing Lightweight Downsampling Convolution (LDConv) modules. Subsequently, the lightweight Slim-Head was proposed by introducing slim-neck and Group-RepConv with Efficient Channel Attention Mechanism Head (GREHead) modules. Ultimately, to validate the effectiveness of the proposed model, comparative experiments were conducted using the small target FOD dataset. The results demonstrate that the proposed LF-YOLO can achieve better accuracy, over the other state-of-the-art (SOTA) methods with small parameters. (Our code is available at https://github.com/zflyy/LF-YOLO.git). © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",Deep learning; Foreign object debris (FOD); Lightweight design; Object detection; YOLOv8 model; Feature extraction; Object detection; Object recognition; Deep learning; Detection methods; Detection models; Features extraction; Foreign object debris; Lightweight design; Objects detection; Small scale; YOLOv8 model; Deep learning
Scopus,"Yao, J.; Lei, J.; Zhou, J.; Liu, C.",FG-YOLO: an improved YOLOv8 algorithm for real-time fire and smoke detection,,2025,,,,10.1007/s11760-025-03894-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219641237&doi=10.1007%2fs11760-025-03894-2&partnerID=40&md5=a159a4ba2b5ffa37e2019634fd361d21,"To overcome the limitations of conventional fire detection methods in accurately recognizing small-scale, multi-target flames and irregularly shaped, low-quantity smoke due to insufficient feature extraction, this study proposes an enhanced YOLOv8-based algorithm called FG-YOLO. By integrating the FSCNet backbone and the GScELAN efficient layer aggregation module, FG-YOLO effectively mitigates feature detail loss that commonly arises from convolutional iterations. This advancement significantly boosts the representation ability for multi-scale, occluded, and small-object features, thereby improving detection accuracy while reducing computational overhead. The proposed approach was evaluated on the FASD dataset, which comprises 15,778 manually annotated images of flames and smoke labeled with the LabelImg tool. Experimental results show that FG-YOLO achieves a 5.4% increase in accuracy, a 6.8% increase in recall, a 4.5% increase in mAP50, and a 9.9% increase in mAP50-95 compared to the YOLOv8 baseline, alongside an inference speed of 76.51 FPS. With only 2.8 million parameters, FG-YOLO delivers high performance with optimized computational efficiency, offering a robust solution for real-time fire and smoke detection in safety-critical applications. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.",FG-YOLO; Fire and smoke detection; Multi-scale feature fusion; Small object detection; Fires; Inference engines; Premixed flames; Features fusions; FG-YOLO; Fire detection; Fire-detection method; Multi-scale feature fusion; Multi-scale features; Real- time; Small object detection; Small scale; Smoke detection; Smoke detectors
Scopus,"Trinh, M.L.; Nguyen, D.T.; Dinh, L.Q.; Nguyen, M.D.; Setiadi, D.R.I.M.; Nguyen, M.T.","Unmanned Aerial Vehicles (UAV) Networking Algorithms: Communication, Control, and AI-Based Approaches",,2025,,,,10.3390/a18050244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006623990&doi=10.3390%2fa18050244&partnerID=40&md5=d8676793527bbab7e84158af1d9c9086,"This paper focuses on algorithms and technologies for unmanned aerial vehicles (UAVs) networking across different fields of applications. Given the limitations of UAVs in both computations and communications, UAVs usually need algorithms for either low latency or energy efficiency. In addition, coverage problems should be considered to improve UAV deployment in many monitoring or sensing applications. Hence, this work firstly addresses common applications of UAV groups or swarms. Communication routing protocols are then reviewed, as they can make UAVs capable of supporting these applications. Furthermore, control algorithms are examined to ensure UAVs operate in optimal positions for specific purposes. AI-based approaches are considered to enhance UAV performance. We provide either the latest work or evaluations of existing results that can suggest suitable solutions for specific practical applications. This work can be considered as a comprehensive survey for both general and specific problems associated with UAVs in monitoring and sensing fields. © 2025 by the authors.",artificial intelligence (AI); communications; formation control; routing protocols; UAV networking; UAVs; Carrier communication; Drones; Micro air vehicle (MAV); Speech transmission; Target drones; Telegraph; Aerial vehicle; Artificial intelligence; Communication control; Formation control; Lower energies; Routing-protocol; Unmanned aerial vehicle; Unmanned aerial vehicle networking; Vehicle networkings; Signaling
Scopus,"Patel, R.; Chandalia, D.; Nayak, A.; Jeyabose, A.; Jijo, D.",CGI-Based Synthetic Data Generation and Detection Pipeline for Small Objects in Aerial Imagery,,2025,,,,10.1109/ACCESS.2025.3553530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003033354&doi=10.1109%2fACCESS.2025.3553530&partnerID=40&md5=cb7a3ba6e9cbbd3619bc5f07587cfbe9,"Small object detection in drone-captured imagery presents critical challenges across environmental monitoring, urban planning, and disaster management. Current approaches struggle with accurate landmark identification due to varying altitudes, diverse landscapes, and object scale variability. This research proposes an innovative two-stage object detection pipeline specifically designed for aerial imagery analysis. Our approach integrates advanced data generation techniques with a novel detection methodology. Experimental results demonstrate significant performance improvements: precision increased by 29.3% (from 0.616 to 0.796), recall improved by 24% (0.699 to 0.867), and mean Average Precision (mAP@0.5) enhanced by 23.2% (0.677 to 0.834) compared to traditional YOLO models. The pipeline combines Computer-Generated Imagery (CGI) for synthetic data creation with a two-stage detection approach. The first stage employs YOLOv9 for efficient region of interest localization, while the second stage utilizes a ResNet-based model for precise classification. By addressing challenges associated with small object detection, our methodology provides a scalable solution for automated landmark recognition in diverse aerial environments. © 2013 IEEE.",AAV object detection; Aerial imagery; drone-based image recognition; small object detection; synthetic data generation; two-stage object detection; Aerial photography; Aircraft detection; Direct air capture; Network security; Urban planning; Aerial imagery; Computer generated imagery; Data-detection; Drone-based image recognition; Objects detection; Small object detection; Synthetic data generations; Two-stage object detection; UAV object detection; Drones
Scopus,"Lin, F.; Wang, B.; Chen, Z.; Zhang, X.; Song, C.; Yang, L.; Cheng, J.C.P.",Efficient visual inspection of fire safety equipment in buildings,,2025,,,,10.1016/j.autcon.2025.105970,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215955960&doi=10.1016%2fj.autcon.2025.105970&partnerID=40&md5=ee4d05c93d9177dabb1185dad317975d,"Fire safety equipment (FSE) in buildings is critical in ensuring occupant safety and mitigating losses during emergencies. However, its effectiveness is frequently compromised by inadequate maintenance. As buildings increase size and complexity, traditional manual inspection methods become impractical due to scalability and data management challenges. To address these issues, this paper proposes an advanced FSE detection framework with improvement strategies. The process commences with the developed YOLO-FSE algorithm, which is capable of identifying objects of varying sizes. This is complemented by precise localization of these objects through an enhanced tracking algorithm and visual simultaneous localization and mapping (vSLAM). The experiments demonstrate that this approach can effectively detect various fire safety equipment with the potential to replace labor-intensive manual methods. Notably, the YOLO-FSE network achieves a 7.9 % improvement in mean Average Precision (mAP) at a threshold of 0.5 (mAP@0.5), and a 9.4 % increase in mAP@0.95, indicating significant enhancements in detection accuracy. © 2025 Elsevier B.V.",Equipment detection; FSE; Object tracking; Visual positioning; Fire extinguishers; Equipment detection; Fire safety; Fire safety equipment; In-buildings; Inadequate maintenance; Object Tracking; Occupant safety; Safety equipments; Visual inspection; Visual positioning; Inspection equipment
Scopus,"Jiang, H.; Ma, Y.; Hong, T.; Gong, T.",Enhanced and lightweight design of small object detector based on YOLOv5s model,,2025,,,,10.1007/s13042-024-02383-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204434062&doi=10.1007%2fs13042-024-02383-1&partnerID=40&md5=34d64ad120bf2a7908ba5d39baeaf1c2,"Owing to the challenges of dense target distribution and complex backgrounds in small detection targets, existing small object detection algorithms suffer from poor performance and high model complexity, which is extremely difficult to deploy on embedded platforms. To address above issues, we optimized the YOLOv5s model structure to enhance detection accuracy. To avoid incurring extra computational expenses, we introduced a local pruning strategy to reduce redundancy, which enables the detection model more suitable for embedded systems. Considering pruning may cause accuracy degradation, we employ knowledge distillation techniques combining feature distillation and output distillation. Specifically, we transfer the knowledge from a high-precision teacher model to a student model, enabling exceptional real-time performance. The experimental results on the VisDrone2019 dataset show that compared to the original algorithm, our model has reduced the parameter count by 50.38%, computation by 51.81%, and model size by 52.94%, totaling just 8 M. The average precision (mAP@0.5) improved to 42.2%. Our proposed model outperforms the current state-of-the-art methods for small object detection in terms of both accuracy and computational efficiency. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Local pruning; Model optimization; Small object detection; Two-stage distillation; Complex background; Detection targets; Lightweight design; Local pruning; Model optimization; Object detection algorithms; Object detectors; Small object detection; Small objects; Two-stage distillation; Teaching
Scopus,"Zhao, Z.; Bo, K.; Hsu, C.-Y.; Liao, L.",Lightweight unmanned aerial vehicle object detection algorithm based on improved YOLOv8,,2025,,,,10.3233/IDA-230929,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005407150&doi=10.3233%2fIDA-230929&partnerID=40&md5=6e32a27524995bc75afcb2f98d73b27a,"With the rapid development of unmanned aerial vehicle (UAV) technology and computer vision, real-time object detection in UAV aerial images has become a current research hotspot. However, the detection tasks in UAV aerial images face challenges such as disparate object scales, numerous small objects, and mutual occlusion. To address these issues, this paper proposes the ASM-YOLO model, which enhances the original model by replacing the Neck part of YOLOv8 with an efficient bidirectional cross-scale connections and adaptive feature fusion (ABiFPN). Additionally, a Structural Feature Enhancement Module (SFE) is introduced to inject features extracted by the backbone network into the Neck part, enhancing inter-network information exchange. Furthermore, the MPDIoU bounding box loss function is employed to replace the original CIoU bounding box loss function. A series of experiments was conducted on the VisDrone-DET dataset, and comparisons were made with the baseline network YOLOv8s. The experimental results demonstrate that the proposed model in this study achieved reductions of 26.1% and 24.7% in terms of parameter count and model size, respectively. Additionally, during testing on the evaluation set, the proposed model exhibited improvements of 7.4% and 4.6% in the AP50 and mAP metrics, respectively, compared to the YOLOv8s baseline model, thereby validating the practicality and effectiveness of the proposed model. Subsequently, the generalizability of the algorithm was validated on the DOTA and DIOR datasets, which share similarities with aerial images captured by drones. The experimental results indicate significant enhancements on both datasets. © 2024 – IOS Press. All rights reserved.",computer vision; drone aerial images; feature fusion; multi-scale object detection; real-time object detection; Aerial photography; Feature extraction; Micro air vehicle (MAV); Object detection; Object recognition; Target drones; Aerial images; Aerial vehicle; Drone aerial image; Features fusions; Multi-scale object detection; Multi-scales; Objects detection; Real- time; Real-time object detection; Drones
Scopus,"Liu, R.; Huang, M.; Wang, L.; Hu, Y.; Tao, Y.",Roadside Object Detection Algorithm Based on Multiscale Sequence Fusion,,2025,,,,10.3788/LOP241187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218980485&doi=10.3788%2fLOP241187&partnerID=40&md5=760781b6cf7a96841f8e89c061d63f77,"This study develops a lightweight roadside object detection algorithm called MQ-YOLO. The algorithm is based on multiscale sequence fusion. It addresses the challenges of low detection accuracy for small and occluded targets and the large number of model parameters in urban traffic roadside object detection tasks. We design a D-C2f module based on multi-branch feature extraction to enhance feature representation while maintaining speed. To strengthen the integration of information from multiscale sequences and enhance feature extraction for small targets, the plural-scale sequence fusion (PSF) module is designed to reconstruct the feature fusion layer. Multiple attention mechanisms are incorporated into the detection head for greater focus on the salient semantic information of occluded targets. To enhance the detection performance of the model, a loss function based on the normalized Wasserstein distance is introduced. Experimental results on the DAIR-V2X-I dataset demonstrate that MQ-YOLO achieves improved mAP@50 and mAP@ (50‒95) by 3. 9 percentage point and 6. 0 percentage point compared to the valuses obtained with baseline YOLOv8n with 3. 96 Mbit parameters. Experiments on the DAIR-V2X-SPD-I dataset show that the model has good generalizability. During roadside deployment, the model reaches detection speeds of 62. 5 frame/s, meeting current roadside object detection requirement for edge deployment in urban traffic. © 2025 Universitat zu Koln. All rights reserved.",feature fusion; lightweight; plural-scale sequence; roadside target detection; YOLOv8n
Scopus,"Chhetri, T.R.",Improving decision making using semantic web technologies,,2025,,,,10.1007/978-3-658-45877-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005031077&doi=10.1007%2f978-3-658-45877-5&partnerID=40&md5=1eaa39d133e102e6fe123b32c38d3a82,"As technology becomes integral to our lives, its influence on decision making in smart cities, healthcare, and manufacturing is undeniable. However, challenges such as limited contextual awareness, domain knowledge, explainability of machine learning (ML), and issues of interoperability, data quality, and GDPR (General Data Protection Regulation) compliance in data sharing hinder effective decision making. This book addresses these critical challenges by exploring how the synergy of semantic technologies (SW), like ontologies and knowledge graphs, with or without ML, can overcome these challenges to improve decision making. Through real-world case studies in data sharing, manufacturing, and agriculture, it offers theoretical and practical insights and guidelines of how SW can enhance prediction accuracy, integrate domain knowledge, support ML explainability, and tackle interoperability, data quality, and GDPR challenges. © Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2025. All rights reserved.",Artificial intelligence (AI); Data privacy; Data sharing; Decision making; Edge intelligence; Explainable AI (XAI); General data protection regulation (GDPR) compliance; Internet of things (IoT); Knowledge graphs; Domain Knowledge; Knowledge graph; Artificial intelligence; Data Sharing; Decisions makings; Edge intelligence; Explainable artificial intelligence (XAI); General data protection regulation  compliance; General data protection regulations; Internet of thing; Knowledge graphs; Regulation compliance; Decision making
Scopus,"Ismael, M.; Cornil, M.",Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment,,2025,,,,10.5220/0013132600003912,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001839253&doi=10.5220%2f0013132600003912&partnerID=40&md5=6fefa73aa5dd847534c32133f3d536a6,"This paper presents an outdoor tracking system using Real-Time Kinematic (RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s)) in urban areas where the accurate tracking of objects is critical and where displaying occluded information is important for safety reasons. The approach presented here replaces 2D screens/tablets and offers distinct advantages, particularly in scenarios demanding hands-free operation. The integration of RTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD represents a promising solution for outdoor applications. This paper provides valuable insights into leveraging the combined potential of RTK and OST-HMD for outdoor tracking tasks from the perspectives of systems integration, performance optimization, and usability. The main contributions of this paper are: 1) a system for seamlessly merging RTK systems with OST-HMD to enable relatively precise and intuitive outdoor tracking, 2) an approach to determine a global location to achieve the position relative to the world, 3) an approach referred to as ’semi-dynamic’ for system assessment. © 2025 by SCITEPRESS - Science and Technology Publications, Lda.",Augmented Reality; OST-HMD; RTK Systems; Tracking and Visual Navigation
Scopus,"Zhou, P.; Wang, P.; Zhu, D.; Zhou, B.; Lv, J.; Ye, Z.",SFANet: Efficient Detection of Vehicle Targets in SAR Images Based on SAR-Specialized Feature Aggregation,,2025,,,,10.1109/TIM.2025.3573019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005980872&doi=10.1109%2fTIM.2025.3573019&partnerID=40&md5=499935ec131e9681ab7d2aa67f42987c,"With the swift progress of deep learning technologies, the detection of ground vehicles in synthetic aperture radar (SAR) images has emerged as a prominent research focus. In this article, research is conducted on the detection of ground vehicles using MiniSAR mounted on unmanned aerial vehicles. However, the substantial background interference and high similarity between ground vehicles make it challenging for object detection. To address these issues, an efficient object detection network based on SAR-specialized feature attention (SFA) and lightweight global detector (LGD) is proposed in this article. First, the SFA is proposed, which introduces proportional attention to different detection layers. By analyzing and focusing on spatial and channel information, it alleviates the classification challenges when dealing with highly similar ground vehicles and limited information. Second, a novel detector LGD is proposed, which combines Swin Transformer and lightweight convolutions. LGD captures deeper semantic information through global attention, enabling the network to detect vehicles in complex scenes. Finally, transfer learning is utilized to load pre-classification weights of the backbone. To verify the effectiveness of the proposed algorithm, comprehensive experiments were performed across multiple datasets. The results indicate that the proposed method offers substantial performance enhancements. © 2025 IEEE. All rights reserved.",Feature attention; global attention; lightweight; MiniSAR; object detection; Automobiles; Bicycles; Buses; Cabs (truck); Drones; Magnetic levitation vehicles; Motorcycles; Efficient detection; Feature aggregation; Feature attention; Global attention; Image-based; Lightweight; MiniSAR; Objects detection; Synthetic aperture radar images; Vehicle targets; Vehicle detection
Scopus,"Xu, S.; Zhang, W.; Tao, S.; Chai, Y.; Bahri, P.A.; Wang, H.",A Dynamic Pig Face Detection Method Based on Spatial Channel Weight Optimization Attention Mechanism and Adaptive Anchor Point Selection,,2025,,,,10.1109/TIM.2025.3573344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006625080&doi=10.1109%2fTIM.2025.3573344&partnerID=40&md5=5c060b0883b2c0655442e1a9d633ff08,"Pig face detection plays a crucial role in the field of agricultural farming, especially in precise feeding and disease monitoring. This study proposes a method called adaptive region-based convolutional neural network (A-RCNN) for multiangled dirty pig face detection in outdoor environments. First, in order to address the interference caused by dirty cleaning surfaces, a feature enhancement module (FEM) is designed to improve the network’s classification ability. Second, in order to ensure that the anchor points are more in line with the shape of the pig surface, an anchor point selection module (APSM) is introduced to generate high-quality area recommendations. Finally, in response to the interference problem of complex outdoor backgrounds, this article adopts a dynamic training strategy (DTS) to optimize the final detection results using these high-quality region suggestions. This study conducted an in-depth exploration of the publicly available JD dataset, and the experimental results showed that compared with existing methods, this method demonstrated excellent performance, achieving 56.3% mean average precision (mAP) and an improvement of 6.02% compared to the baseline Faster RCNN. In addition, to verify the practical application effect of the model, this article also deployed it on the edge device Raspberry Pi 4B, further confirming the effectiveness and practicality of the model. © 1963-2012 IEEE.",Anchor point selection; attention mechanism; dynamic training; pig face detection; Face recognition; Surface cleaning; Anchor point; Anchor point selection; Attention mechanisms; Dynamic training; Face detection methods; Faces detection; High quality; Pig face detection; Point selection; Spatial channels; Convolutional neural networks
Scopus,"Alkandary, K.; Yildiz, A.S.; Meng, H.",A Comparative Study of YOLO Series (v3–v10) with DeepSORT and StrongSORT: A Real-Time Tracking Performance Study,,2025,,,,10.3390/electronics14050876,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000520215&doi=10.3390%2felectronics14050876&partnerID=40&md5=26515be2a3df3740cf57dde144426ad5,"Many previous studies have explored the integration of a specific You Only Look Once (YOLO) model with real-time trackers like Deep Simple Online and Realtime Tracker (DeepSORT) and Strong Simple Online and Realtime Tracker (StrongSORT). However, few have conducted a comprehensive and in-depth analysis of integrating the family of YOLO models with these real-time trackers to study the performance of the resulting pipeline and draw critical conclusions. This work aims to fill this gap, with the primary objective of investigating the effectiveness of integrating the YOLO series, in light-sized versions, with the real-time DeepSORT and StrongSORT tracking algorithms for real-time object tracking in a computationally limited environment. This work will systematically compare various lightweight YOLO versions, from YOLO version 3 (YOLOv3) to YOLO version 10 (YOLOv10), combined with both tracking algorithms. It will evaluate their performance using detailed metrics across diverse and challenging real-world datasets: the Multiple Object Tracking 2017 (MOT17) and Multiple Object Tracking 2020 (MOT20) datasets. The goal of this work is to assess the robustness and accuracy of these light models in multiple complex real-world environments in scenarios with limited computational resources. Our findings reveal that YOLO version 5 (YOLOv5), when combined with either tracker (DeepSORT or StrongSORT), offers not only a solid baseline in terms of the model’s size (enabling real-time performance on edge devices) but also competitive overall performance (in terms of Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP)). The results suggest a strong correlation between the choice regarding the YOLO version and the tracker’s overall performance. © 2025 by the authors.",autonomous driving; DeepSORT; detection; StrongSORT; tracking; YOLO
Scopus,"Tian, S.; Zhao, K.; Song, L.",Research on Small Target Detection Algorithm for Autonomous Vehicle Scenarios,,2025,,,,10.1155/atr/8452511,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002470346&doi=10.1155%2fatr%2f8452511&partnerID=40&md5=30a332076810faa9e817297c161ef75a,"In recent years, road traffic object detection has gained prominence in areas such as traffic monitoring, autonomous driving, and road safety. Nonetheless, existing algorithms offer room for improvement, particularly when detecting distant or inherently small targets, such as vehicles and pedestrians, from camera perspectives. By addressing the detection accuracy issues associated with small targets, this study introduces the YOLOv5s-LGC detection algorithm. This model incorporates a multiscale feature fusion network and leverages the lightweight GhostNet module to reduce model parameters. Furthermore, the GC attention module is employed to mitigate background interference, thereby enhancing the average detection accuracy across all categories. Through data analysis, target detection at different scales and sampling rates is determined. Experiments indicate that the YOLOv5s-LGC model surpasses the baseline YOLOv5s in detection accuracy on the Partial_BDD100K and KITTI datasets by 3.3% and 1.6%, respectively. This improvement in locating and classifying small targets presents a novel approach for applying object detection algorithms in road traffic scenarios. Copyright © 2025 Sheng Tian et al. Journal of Advanced Transportation published by John Wiley & Sons Ltd.",autonomous driving; global context attention module; road traffic; target detection; Motor transportation; Object detection; Vehicle detection; Autonomous driving; Autonomous Vehicles; Detection accuracy; Global context; Global context attention module; Road traffic; Small target detection; Small targets; Target detection algorithm; Targets detection; Autonomous vehicles
Scopus,"Porto, J.V.d.A.; Szemes, P.T.; Pistori, H.; Menyhárt, J.","Trending Machine Learning Methods for Vehicle, Pedestrian, and Traffic for Detection and Tracking Task in the Post-Covid Era: A Literature Review",,2025,,,,10.1109/ACCESS.2025.3565901,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003977666&doi=10.1109%2fACCESS.2025.3565901&partnerID=40&md5=376a57c7006030a8b6a401693cc49c5a,"This study, aimed at professionals in research and development in the fields of computer vision, artificial intelligence, and intelligent transportation, presents a systematic literature review on recent machine learning methodologies applied to the detection and tracking of vehicles, pedestrians, and traffic flow. The analysis of articles published between 2022 and 2025 (early access) in the post-COVID era explored the integration of machine learning and deep learning to address traffic challenges, allowing for the comparison of different approaches and the formulation of hypotheses based on the 46 articles that comprised the review corpus. Furthermore, the evaluation of the reported metrics revealed inconsistencies in the methodologies employed, attributed to the lack of standardization across the studies. In light of this, this work proposes alternatives for future experiments, emphasizing the emerging potential of the field through the adoption of new standardization systems and the exploration of experimental combinations. © 2013 IEEE.",Deep learning; detection; machine learning; tracking; urban mobility; Calibration; Contrastive Learning; Navigation; Supervised learning; Traffic control; Urban transportation; Vehicles; Deep learning; Detection; Detection and tracking; Literature reviews; Machine learning methods; Machine-learning; Research and development; Tracking; Urban mobility; Vehicle traffic; Deep learning
Scopus,"Ma, Y.; Zhang, P.; Tan, F.",Improved YOLOv8s Model for Smoke and Flame Detection in Complex Backgrounds,,2025,,,,10.3778/j.issn.1002-8331.2406-0348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007359684&doi=10.3778%2fj.issn.1002-8331.2406-0348&partnerID=40&md5=2bf317561cf1751b3fc55f1763e8f7c6,"Aiming to address issues such as confusion between smoke flame targets and background within complex backgrounds, which often result in low accuracy of smoke flame detection, an enhanced model based on YOLOv8s for detecting smoke flames within complex backgrounds is proposed. Firstly, the feature channels are highly similar to each other, and in order to effectively utilize the redundancy across different channels and improve the ability of model to differentiate between smoke and flame targets and backgrounds, the C2fFR (C2f with partial rep conv) lightweight feature extraction module is introduced. Secondly, the MCFM (multi-scale context fusion module) is designed to capture and utilize contextual information for enhancing feature representation. Lastly, the Inner-SIoU loss function is employed to address bounding box mismatches and the regression ability of the model is improved for high IoU samples. Experimental results demonstrate that compared to the baseline YOLOv8s model, the enhanced YOLOv8s smoke flame detection model achieves improvements of 4.6 percentage points in mAP@50 and 2.3 percentage points in mAP@50:95. Moreover, it reduces the number of model parameters by 18.9% and computation by 8.1%. while maintaining an FPS (frame per second) of 93. Additionally, it exhibits superior detection performance when compared to other mainstream detection algorithms. © 2025 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",C2fFR; Inner-SIoU; multi-scale context fusion module; YOLOv8s; Flame research; Premixed flames; Regression analysis; Smoke detectors; C2fFR; Complex background; Flame detection; Fusion modules; Inner-SIoU; Multi-scale context fusion module; Multi-scales; Percentage points; Target and background; YOLOv8; Feature extraction
Scopus,"Khorsand, H.; Arezoomandan, S.; Han, D.K.",Enhanced Long-Range UAV Detection: Leveraging Slicing Aided Hyper Inference with YOLOv8,,2025,,,,10.1109/ICCE63647.2025.10930186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006535293&doi=10.1109%2fICCE63647.2025.10930186&partnerID=40&md5=2ac52e3535d3bd666db3bd8d3d3880ec,"The increasing use of Unmanned Aerial Vehicles (UAVs) in commercial applications has highlighted the urgent need for advanced detection systems that can reliably identify small drones from long distances. Detecting small drones at extended ranges remains challenging due to their minimal size within the image frame. While higher-resolution cameras can capture more details, traditional object detection methods struggle with resolution constraints, leading to significant degradation in detection accuracy after downscaling. To address these limitations, this research explores the application of the Slicing Aided Hyper Inference (SAHI) method, which enhances object detection by dividing high-resolution images into smaller, overlapping patches that align with the input resolution requirements of detectors like YOLO. By preserving critical pixel information through this approach, SAHI significantly improves the detection accuracy of small, distant drones. To evaluate this, we conducted a series of experiments using various drone datasets, including Long Range Drone Detection (LRDD), Drone vs. Birds, DetFly, and GAN-translated synthetic images. Results show a significant improvement in detecting small drones with SAHI compared to the baseline YOLOv8 model. © 2025 IEEE.",Slicing aided hyper inference; Small object detection; UAV detection; YOLO; Micro air vehicle (MAV); Photointerpretation; Target drones; Advanced detections; Aerial vehicle; Commercial applications; Detection accuracy; Detection system; Slicing aided hyper inference; Small object detection; Unmanned aerial vehicle detection; Vehicles detection; YOLO; Drones
Scopus,"Guo, D.; Xu, P.; Cai, M.; Liu, E.; Wang, M.; Shan, Z.; Jiang, F.",DBG-YOLO: Efficient detection of hidden dangers of manhole covers based on deep learning YOLO network,,2025,,,,10.1007/s11042-025-20982-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008458870&doi=10.1007%2fs11042-025-20982-0&partnerID=40&md5=9a5fc551819fbd33588bf33fd4fcc374,"Manhole covers are a critical component of urban infrastructure, and their damage poses significant threats to road safety and structural integrity. Real-time detection and tagging of manhole covers enable timely maintenance, reducing traffic risks and improving infrastructure reliability. To address this issue, we propose an improved manhole cover hazard detection algorithm, DBG-YOLO, based on the YOLOv8n framework. The proposed DBG-YOLO model integrates a Dilated Reparam Block (DRB) into the C2f module of the backbone network, enhancing both the receptive field and feature representation capabilities. This optimization significantly improves detection accuracy for small objects and complex scenarios. When integrated with comprehensive data augmentation strategies, the framework demonstrates exceptional adaptability to low-light conditions, achieving robust detection even under insufficient illumination. For feature fusion, the neck network incorporates a Bidirectional Feature Pyramid Network (BiFPN) and a Global Attention Mechanism (GAM), forming an advanced multi-scale architecture that effectively addresses partial occlusion challenges. By replacing the conventional loss function with SlideLoss, the model further refines bounding box regression accuracy, ensuring precise localization in demanding environments. Collectively, these innovations, synergized with adaptive data augmentation, provide a holistic solution to detection limitations in low-light and occluded scenarios. Experimental results demonstrate that DBG-YOLO achieves superior performance with 93.6% mAP@50 at 167.5 FPS, outperforming both Faster R-CNN (81.4%/13.4 FPS) and YOLOv5s (91.3%/142.7 FPS) in terms of accuracy and inference speed. This lightweight architecture establishes a new state-of-the-art balance between detection accuracy, computational efficiency, and model complexity. This innovative approach provides robust technical support for real-time detection tasks, enhancing urban infrastructure monitoring, improving road safety, and contributing to the development of smart cities. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",Bidirectional feature pyramid network; Dilated reparam block; Global attention mechanism; Manhole cover hazard detection; Accident prevention; Complex networks; Critical infrastructures; Deep learning; Hazards; Motor transportation; Network architecture; Object detection; Roads and streets; Smart city; Attention mechanisms; Bidirectional feature pyramid network; Dilated reparam block; Feature pyramid; Global attention mechanism; Hazard detection; Manhole cover; Manhole cover hazard detection; Pyramid network; Urban infrastructure; Computational efficiency
Scopus,"Wu, J.; Zhao, F.; Jin, Z.",LEN-YOLO: a lightweight remote sensing small aircraft object detection model for satellite on-orbit detection,,2025,,,,10.1007/s11554-024-01601-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212458255&doi=10.1007%2fs11554-024-01601-x&partnerID=40&md5=cfaec125d577ad801049e399c453af58,"The performance of conventional detection algorithms in small aircraft target detection is often unsatisfactory due to the intricate backgrounds of remote sensing images and the diminutive size of aircraft targets. Furthermore, prevalent deep learning algorithms typically prove overly complex for integration into resource-constrained satellite platforms. In response to these challenges, an enhanced algorithm named LEN-YOLO (Lite backbone - Enhanced Neck - YOLO) has been devised to enhance detection accuracy while preserving model simplicity for the detection of small aircraft in satellite on-orbit scenarios. First, the EIoU Loss is adopted for target localization, enabling the network to effectively focus on small aircraft targets. Second, a Lite backbone is designed by discarding high semantic information, using low-semantic feature maps to detect small targets. Finally, a Bidirectional Weighted FPN based on SimAM and GSConv (BSG-FPN) is proposed to fuse feature maps of different scales to increase detailed information. Experimental results on RSOD and DIOR datasets demonstrate compared to the baseline YOLOv5, LEN-YOLO achieves an increase of 5.1% and 4.2% in APs respectively. Notably, parameters are reduced by 78.3% and floating-point operations by 33.2%. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Lightweight; Object detection; Remote sensing; Small aircraft detect; YOLO; Aircraft detection; Deep learning; Object tracking; Remote sensing; Small satellites; Aircraft targets; Detection models; Feature map; Lightweight; Objects detection; On orbit; Remote-sensing; Small aircraft; Small aircraft detect; YOLO; Proximity sensors
Scopus,"Gerdan Koc, D.; Vatandas, M.",Development and Performance Analysis of an Autonomous Agricultural Vehicle for Fruit Transportation,,2025,,,,10.1002/rob.22573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004214031&doi=10.1002%2frob.22573&partnerID=40&md5=26f1962c9c3afa17f40a591f84d253a3,"Reducing product damage, preserving quality, and enhancing efficiency from harvest to consumption are crucial for sustainable agriculture. The integration of advanced information and communication technologies into agricultural practices plays a vital role in meeting these goals. This study introduces an autonomous transport vehicle designed for the efficient logistics of fruit transportation in agricultural settings. The vehicle's software framework is constructed on the Robot Operating System (ROS) and incorporates an enhanced hybrid navigation system that merges the Extended Kalman Filter (EKF) with Simultaneous Localization and Mapping (SLAM) for precise localization. The A* algorithm facilitates global path planning, whereas the Dynamic Window Approach (DWA) guarantees real-time obstacle avoidance. Essential hardware components comprise high-resolution LIDAR for environmental mapping, an Inertial Measurement Unit (IMU) for motion estimation, and wheel encoders for odometry. The performance evaluation was executed across five distinct terrain types: concrete, fine-tilled soil, coarse-tilled soil, asphalt, and grass. The vehicle attained optimal path-following precision on concrete, exhibiting a deviation of 5.39 cm at a speed of 0.3 m/s with a 200 kg payload, whereas tracking errors escalated on uneven terrains like grass and coarse-tilled soil. Maneuverability assessments verified a turning radius of 60.0 cm for 90° turns and 125.0 cm for 180° turns, ensuring suitability in restricted agricultural environments. Finite element analysis (FEA) evaluated structural durability under diverse loads (2000–4000 N), indicating a minimum safety factor of 1.23, thereby affirming structural stability under static conditions. This study demonstrates the potential of autonomous transport vehicles to revolutionize agricultural logistics by reducing labor dependency, improving operational efficiency, and supporting sustainable farming. © 2025 The Author(s). Journal of Field Robotics published by Wiley Periodicals LLC.",Autonomous agricultural vehicle; Combined positioning; Motion planning; Path planning; Agricultural robots; Fruits; Maneuverability; Navigation; Tracked vehicles; Tractors (agricultural); Vehicle performance; Advanced informations; Autonomous agricultural vehicles; Autonomous transport vehicles; Combined positioning; Information and Communication Technologies; Motion-planning; Performances analysis; Product damage; Sustainable agriculture; Tilled soils; Autonomous vehicles
Scopus,"Eom, T.-H.; Kim, H.-J.",A Novel Approach to Object Detection Under Diverse Illumination Conditions Using Row-Wise Exposure Images,,2025,,,,10.1109/JSEN.2025.3554806,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002021042&doi=10.1109%2fJSEN.2025.3554806&partnerID=40&md5=aa7a6a5960e43a94f862a22c00d7d5e7,"In this study, we present a novel approach to object detection utilizing row-wise exposure (RWE) images to substantially improve object detection performance in low- and high-illumination conditions. Unlike previous RWE imaging techniques that require row-wise merging for high dynamic range (HDR) synthesis, our system directly utilizes raw RWE images without postprocessing. It enables the proposed object detection algorithm to effectively overcome the limitations of conventional systems in dynamic and variable lighting scenarios. Additionally, we introduce a tailored data augmentation strategy optimized for the unique characteristics of RWE images, enhancing model training and robustness without relying on dedicated RWE datasets. We developed a prototype CMOS image sensor (CIS) with RWE functionality to demonstrate the practical viability of our approach. This prototype was instrumental in validating the system’s effectiveness in real-world conditions. The proposed data augmentation method, designed specifically for raw RWE images, enriches the training dataset, enabling models to adapt to various lighting situations and improve their generalization abilities. Our experiments employed widely adopted object detection models, such as YOLOv7 and YOLOv9, along with standard datasets, such as MS COCO and HDR4RTT, to evaluate model performance under varying illumination coefficients and motion blur intensities. The results showed significant performance improvements over existing object detection methods, especially in challenging illumination conditions and dynamic environments. © 2001-2012 IEEE.",Challenging illumination conditions; CMOS imager sensor (CIS); high dynamic range (HDR); machine vision; object detection; row-wise exposure (RWE) images; Image coding; Image enhancement; Laser beams; Machine vision; Remote sensing; Challenging illumination condition; CMOS imager sensor; CMOS imagers; Data augmentation; High dynamic range; Illumination conditions; Machine-vision; Objects detection; Row-wise exposure image; CMOS integrated circuits
Scopus,"Li, J.; Yang, K.; Qiu, C.; Wang, L.; Cai, Y.; Wei, H.; Yu, Q.; Huang, P.",HYFF-CB: Hybrid Feature Fusion Visual Model for Cargo Boxes,,2025,,,,10.3390/s25061865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001001187&doi=10.3390%2fs25061865&partnerID=40&md5=380e4ba094408a5dd599297173ae1ef7,"In automatic loading and unloading systems, it is crucial to accurately detect the locations of boxes inside trucks in real time. However, the existing methods for box detection have multiple shortcomings, and can hardly meet the strict requirements of actual production. When the truck environment is complex, the currently common models based on convolutional neural networks show certain limitations in the practical application of box detection. For example, these models fail to effectively handle the size inconsistency and occlusion of boxes, resulting in a decrease in detection accuracy. These problems seriously restrict the performance and reliability of automatic loading and unloading systems, making it impossible to achieve ideal detection accuracy, speed, and adaptability. Therefore, there is an urgent need for a new and more effective box detection method. To this end, this paper proposes a new model, HYFF-CB, which incorporates key technologies such as a location attention mechanism, a fusion-enhanced pyramid structure, and a synergistic weighted loss system. After real-time images of a truck were obtained by an industrial camera, the HYFF-CB model was used to detect the boxes in the truck, having the capability to accurately detect the stacking locations and quantity of the boxes. After rigorous testing, the HYFF-CB model was compared with other existing models. The results show that the HYFF-CB model has apparent advantages in detection rate. With its detection performance and effect fully meeting the actual application requirements of automatic loading and unloading systems, the HYFF-CB model can excellently adapt to various complex and changing scenarios for the application of automatic loading and unloading. © 2025 by the authors.",automatic loading and unloading; box detection; machine learning; synergistic weighted loss; Automobile testing; Automobiles; Convolutional neural networks; Image enhancement; Automatic loading; Automatic loading and unloading; Box detection; Detection accuracy; Hybrid features; Loading and unloading; Loading system; Machine-learning; Synergistic weighted loss; Unloading systems; article; camera; controlled study; convolutional neural network; diagnosis; hybrid; machine learning; male; reliability; velocity; Trucks
Scopus,"Zhou, E.; Leather, O.; Zhuang, A.; Budhwani, A.; Dempster, R.; Li, Q.; Al-Sharman, M.; Rayside, D.; Melek, W.",RALACs: Action Recognition in Autonomous Vehicles Using Interaction Encoding and Optical Flow,,2025,,,,10.1109/TCYB.2024.3515104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213558463&doi=10.1109%2fTCYB.2024.3515104&partnerID=40&md5=6ea71a1011f2b4ad2816309a1d566064,"When applied to autonomous vehicle (AV) settings, action recognition can enhance an environment model's situational awareness. This is especially prevalent in scenarios where traditional geometric descriptions and heuristics in AVs are insufficient. However, action recognition has traditionally been studied for humans, and its limited adaptability to noisy, un-clipped, un-pampered, raw RGB data has limited its application in other fields. To push for the advancement and adoption of action recognition into AVs, this work proposes a novel two-stage action recognition system, termed RALACs. RALACs formulates the problem of action recognition for road scenes, and bridges the gap between it and the established field of human action recognition. This work shows how attention layers can be useful for encoding the relations across agents, and stresses how such a scheme can be class-agnostic. Furthermore, to address the dynamic nature of agents on the road, RALACs constructs a novel approach to adapting Region of Interest (ROI) alignment to agent tracks for downstream action classification. Finally, our scheme also considers the problem of active agent detection, and utilizes a novel application of fusing optical flow maps to discern relevant agents in a road scene. We show that our proposed scheme can outperform the baseline on the ICCV2021 Road Challenge dataset (Singh et al., 2023) algorithm and by deploying it on a real vehicle platform, we provide preliminary insight to the usefulness of action recognition in decision making.  © 2013 IEEE.",Action recognition; autonomous vehicles; ICCV2021 Road Challenge; interaction encoding; motion prediction; optical flow; Autonomous vehicles; Image coding; Magnetic levitation vehicles; Motor transportation; Optical character recognition; Action recognition; Autonomous Vehicles; Encodings; Environment models; Iccv2021 road challenge; Interaction encoding; Motion prediction; Optical-; Setting action; Situational awareness; Optical flows
Scopus,"Wang, Z.; Dang, C.; Wang, L.",Personnel Search and Rescue Detector Based on Reconfigurable FPGA Accelerator: A Lightweight Multi-Scale Parallel Drone-Mounted Detector,,2025,,,,10.1109/LGRS.2025.3550349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000049139&doi=10.1109%2fLGRS.2025.3550349&partnerID=40&md5=a91c6b58d39809e8e80ee01b2677d11e,"Natural disasters cause significant human and economic losses every year, making timely search and rescue crucial. In remote, hard-to-reach areas and during network outages after disasters, rescue missions become even more challenging. Traditional edge device solutions rely on extensive hardware resources and high power consumption to support precision and speed, which cannot meet the numerous constraints of post-disaster scenarios. Moreover, existing state-of-the-art detectors (e.g., YOLO series, RetinaNet) have chain-structured networks with narrow receptive fields and single-scale features, and cannot achieve high-speed inference through parallel computing. This letter proposes a custom convolutional neural network (CNN) solution for search and rescue, equipped with field-programmable gate array (FPGA) accelerators on drones. This method is autonomous, low-cost, highly accurate, and efficient for drone rescue missions. Tested on the Lacmus Drone Dataset (LADD), it achieved an accuracy of 87.5%, inference speed of 24.7 ms, and an efficiency ratio of 19.1. Additionally, the search and rescue NET (SRNET) incorporates a finite element method (FEM) structure, which expands the receptive field through a multi-scale parallel architecture, well-suited for the parallel processing capabilities of FPGAs. A reconfigurable FPGA accelerator was also developed, reusing different modules based on the instruction set. It innovatively employs kernel partitioning, significantly reducing redundant access to input feature maps and kernels, balancing parallel computation capabilities while consuming less system power. In large-scale disaster search and rescue, this approach outperforms other systems. © 2004-2012 IEEE.",Accelerator; convolutional neural network (CNN); finite element method (FEM) field-programmable gate array (FPGA); optical remote sensing image; reconfigurable; ship detection; Aircraft accidents; Analog storage; Balancing; Convolutional neural networks; Digital storage; Network security; Parallel architectures; Reconfigurable architectures; Reconfigurable hardware; Risk management; Accelerato r; Convolutional neural network; FEM FPGA; Multi-scales; Optical remote sen ing image; Optical-; Reconfigurable; Reconfigurable FPGA; Search and rescue; Ship detection; array; artificial neural network; detection method; finite element method; hardware; instrumentation; satellite imagery; search and rescue; Drones
Scopus,"Richter, Y.; Zach, S.; Blum, M.Y.; Pinhasi, G.A.; Pinhasi, Y.",Tracking of Low Radar Cross-Section Super-Sonic Objects Using Millimeter Wavelength Doppler Radar and Adaptive Digital Signal Processing,,2025,,,,10.3390/rs17040650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219178732&doi=10.3390%2frs17040650&partnerID=40&md5=0551a4c31a800c9388d617b94550c523,"Small targets with low radar cross-section (RCS) and high velocities are very hard to track by radar as long as the frequent variations in speed and location demand shorten the integration temporal window. In this paper, we propose a technique for tracking evasive targets using a continuous wave (CW) radar array of multiple transmitters operating in the millimeter wavelength (MMW). The scheme is demonstrated to detect supersonic moving objects, such as rifle projectiles, with extremely short integration times while utilizing an adaptive processing algorithm of the received signal. Operation at extremely high frequencies qualifies spatial discrimination, leading to resolution improvement over radars operating in commonly used lower frequencies. CW transmissions result in efficient average power utilization and consumption of narrow bandwidths. It is shown that although CW radars are not naturally designed to estimate distances, the array arrangement can track the instantaneous location and velocity of even supersonic targets. Since a CW radar measures the target velocity via the Doppler frequency shift, it is resistant to the detection of undesired immovable objects in multi-scattering scenarios; thus, the tracking ability is not impaired in a stationary, cluttered environment. Using the presented radar scheme is shown to enable the processing of extremely weak signals that are reflected from objects with a low RCS. In the presented approach, the significant improvement in resolution is beneficial for the reduction in the required detection time. In addition, in relation to reducing the target recording time for processing, the presented scheme stimulates the detection and tracking of objects that make frequent changes in their velocity and position. © 2025 by the authors.",adaptive detection; adaptive DSP radar; bi-static radar; doppler radar; millimeter wave radar; MIMO radar; stealth object detection; target tracking; Image analysis; Image coding; Image segmentation; Image thinning; MIMO radar; Radar clutter; Radar cross section; Radar signal processing; Radar target recognition; Radar transmitters; Textile classing; Adaptive detection; Adaptive DSP radar; Bistatic radars; Doppler; Millimeter-wave radar; Millimetre-wave radar; Objects detection; Radar cross-sections; Stealth object detection; Targets tracking; Doppler radar
Scopus,"Samarin, A.; Savelev, A.; Toropov, A.; Dzestelova, A.; Motyko, A.; Kotenko, E.; Mikhailova, E.; Malykh, V.",Modified Attention Block for Detecting Cars at a Great Distance,,2025,,,,10.1007/978-3-031-80463-2_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000283930&doi=10.1007%2f978-3-031-80463-2_13&partnerID=40&md5=fa60f7f814df2ea4c8bb06cf267be89c,"This study aims to enhance the detection of small objects within intricate visual environments by harnessing the latest deep-learning advancements. We propose a novel method that features a dual-stream self-attention mechanism integrated within a multi-head framework, along with an innovative output reweighting technique to further refine detection accuracy. The core of our approach is specifically designed to address the difficulties posed by small objects that often overlap multiple tokens in feature maps, a common limitation of traditional detection models. By dynamically adjusting the attention scale across various heads, our method facilitates detailed feature capture at multiple levels of granularity, significantly improving the model’s capability to detect and describe small objects. Additionally, we introduce a softmax-based reweighting function that selectively emphasizes crucial features for object recognition, reducing noise and irrelevant information. Our model, named SSD-MSDSSA-ORT, not only exceeds the accuracy of existing state-of-the-art solutions but also showcases superior processing efficiency and scalability. These contributions advance the theoretical understanding of attention mechanisms in deep neural networks and offer practical enhancements for real-world applications requiring precise small object detection. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Attention block; Car Detection; Small Object Recognition
Scopus,"Li, J.; Hou, X.",Global induced local network for infrared: dim small target detection,,2025,,,,10.1088/1361-6501/ad86da,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215119552&doi=10.1088%2f1361-6501%2fad86da&partnerID=40&md5=05ce6cc1abb5aa5da63628c461d5c075,"It is challenging to detect infrared dim targets submerged in complicated backgrounds due to their small size and faint intensity. The previous attention-based detection networks frequently require global long-range dependence. Significant calculations are required to determine the target’s sparse but meaningful position. To prevent wasting calculations on the background, this paper offers a detection network guided by global context for local feature learning, named global induced local network (GILNet). It designs a global location module (GLM) and a local feature interaction module (LFIM) to capture the global position and features of targets, respectively. More specifically, using global context interaction, the GLM finds the region that might contain dim small targets, that is, the coarse location. In the coarsely located regions, the LFIM further acquires feature information about targets. Next, we also design an eight-directional attention operation to obtain the contour information of targets in the low feature map. It is fused with the high feature map in the multi-directional feature fusion module, which retains more semantic and spatial information about targets. Finally, quantitative and qualitative analysis show that the GILNet performs better than eight comparison methods on two public datasets. © 2024 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",context guidance; eight-directional attention; feature interaction; global to local; infrared dim small target detection; Infrared devices; Text mining; Context guidance; Detection networks; Dim small target detection; Eight-directional attention; Feature interactions; Global to local; Infrared dim small target detection; Infrared dim small targets; Local feature; Local networks; Feature extraction
Scopus,"Ying, X.; Xiao, C.; An, W.; Li, R.; He, X.; Li, B.; Cao, X.; Li, Z.; Wang, Y.; Hu, M.; Xu, Q.; Lin, Z.; Li, M.; Zhou, S.; Liu, L.; Sheng, W.",Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines,,2025,,,,10.1109/TPAMI.2025.3544621,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000462112&doi=10.1109%2fTPAMI.2025.3544621&partnerID=40&md5=9b41a7fa1c2f5c693d9864871f40c668,"Visible-thermal small object detection (RGBT SOD) is a significant yet challenging task with a wide range of applications, including video surveillance, traffic monitoring, search and rescue. However, existing studies mainly focus on either visible or thermal modality, while RGBT SOD is rarely explored. Although some RGBT datasets have been developed, the insufficient quantity, limited diversity, unitary application, misaligned images and large target size cannot provide an impartial benchmark to evaluate RGBT SOD algorithms. In this paper, we build the first large-scale benchmark with high diversity for RGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93 K frames and 1.2 M manual annotations. RGBT-Tiny contains abundant objects (7 categories) and high-diversity scenes (8 types that cover different illumination and density variations). Note that, over 81% of objects are smaller than 16×16, and we provide paired bounding box annotations with tracking ID to offer an extremely challenging benchmark with wide-range applications, such as RGBT image fusion, object detection and tracking. In addition, we propose a scale adaptive fitness (SAFit) measure that exhibits high robustness on both small and large objects. The proposed SAFit can provide reasonable performance evaluation and promote detection performance. Based on the proposed RGBT-Tiny dataset, extensive evaluations have been conducted with IoU and SAFit metrics, including 30 recent state-of-the-art algorithms that cover four different types (i.e., visible generic object detection, visible SOD, thermal SOD and RGBT object detection). © 1979-2012 IEEE.",benchmark dataset; tiny object detection; Visible-thermal; Image annotation; Object detection; Object recognition; RGB color model; Adaptive fitness; Benchmark datasets; Objects detection; Search and rescue; Small object detection; Thermal; Tiny object detection; Traffic monitoring; Video surveillance; Visible-thermal; algorithm; article; benchmarking; controlled study; diagnosis; human; illumination; male; video surveillance; Image fusion
Scopus,"Shi, D.; Zhao, C.; Shao, J.; Feng, M.; Luo, L.; Ouyang, B.; Huang, J.",Context-Aware Enhanced Feature Refinement for small object detection with Deformable DETR,,2025,,,,10.3389/fnbot.2025.1588565,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008868278&doi=10.3389%2ffnbot.2025.1588565&partnerID=40&md5=1c83f2b9db8174a57729dabf540ce61a,"Small object detection is a critical task in applications like autonomous driving and ship black smoke detection. While Deformable DETR has advanced small object detection, it faces limitations due to its reliance on CNNs for feature extraction, which restricts global context understanding and results in suboptimal feature representation. Additionally, it struggles with detecting small objects that occupy only a few pixels due to significant size disparities. To overcome these challenges, we propose the Context-Aware Enhanced Feature Refinement Deformable DETR, an improved Deformable DETR network. Our approach introduces Mask Attention in the backbone to improve feature extraction while effectively suppressing irrelevant background information. Furthermore, we propose a Context-Aware Enhanced Feature Refinement Encoder to address the issue of small objects with limited pixel representation. Experimental results demonstrate that our method outperforms the baseline, achieving a 2.1% improvement in mAP. Copyright © 2025 Shi, Zhao, Shao, Feng, Luo, Ouyang and Huang.",autonomouts driving; Deformable DETR; feature extraction; mask attention; small object detection; Computer vision; Extraction; Image coding; Object detection; Object recognition; Pixels; Smoke; Autonomous driving; Autonomouts driving; Context-Aware; Critical tasks; Deformable DETR; Feature refinement; Features extraction; Mask attention; Small object detection; Small objects; article; controlled study; diagnosis; feature extraction; nonhuman; ship; Copyrights; Feature extraction
Scopus,"Xu, Y.; Lu, J.; Wang, C.",YOLO-SOD: Improved YOLO Small Object Detection,,2025,,,,10.1007/978-981-96-0125-7_14,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210317772&doi=10.1007%2f978-981-96-0125-7_14&partnerID=40&md5=e981465e19269459be97e102d99236ad,"Small object detection has important application value in the fields of environmental monitoring, resource detection and analysis, etc. However, the current general object detectors are not very ideal for the detection of small objects. To this end, this paper proposes an efficient, low-complexity, anchor-free small object detection framework YOLO-SOD based on YOLOv8. First, a content-aware feature recombination upsampling operator (CARAFE) is integrated in the upsampling operation part of the framework to achieve more accurate and efficient feature reconstruction. Then, the spatial and channel reconstruction convolutional block (SCConv) is widely integrated in BackBone and Neck to reduce the computational cost caused by redundant feature extraction in visual tasks. Finally, the occlusion-aware attention module (SEAM) is introduced at the end of the detection framework to help the model more accurately identify occluded objects. The ablation experiment on the general small object detection dataset VisDrone2021 proves the effectiveness of several modules introduced in this paper for small object detection. On the VisDrone2021 dataset, YOLO-SOD can achieve an accuracy of 31.1% AP50:95 and 52.3% AP50, which are 2.5% and 3.1% higher than the baseline model respectively. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.",Attention Mechanism; CNN; Object Detection; Small Object Detection; Upsample; 'current; Attention mechanisms; Detection framework; Environmental Monitoring; Object detectors; Objects detection; Small object detection; Small objects; Upsample; Upsampling; Image reconstruction
Scopus,"Singhal, N.; Prasad, L.",MIRYO: A Hybrid Model for Detecting Vehicles in Noisy Images,,2025,,,,10.25103/jestr.182.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007309535&doi=10.25103%2fjestr.182.16&partnerID=40&md5=90a27611271ab4dbb8803eaac983db96,"Detection of vehicles is a key task in many smart transportation applications, involving traffic management, road infrastructure, autonomous driving, and other challenges that arise due to the daily growth in vehicle numbers. Several deep learning (DL) based techniques have previously been explored and investigated by the researchers for vehicle detection, but vehicle detection in noisy images is still considered a difficult task. Low-light, low-resolution, and other environmental noise have a substantial impact on images, significantly reducing vehicle detection system performance. In this paper, we implemented MIRYO, a hybrid model for detection of vehicles in challenging images based on MirNet-v2 and modified Yolov3. The task of vehicle detection was completed by creating a two-stage pipeline. MIRNet-v2 was used in the first stage of this pipeline to reduce noise and improve image contrast in low-quality challenging images. The second stage of this pipeline, used modified YOLOv3 to detect and localize vehicles in images. The hybrid model MIRYO is evaluated on two baseline datasets: the challenging MIOTCD and the high-resolution Highway dataset, and its performance is compared to that of the Yolov3, Yolov4, and Yolov5 architectures on the same dataset. MIRYO achieved an overall mAP of 76.9% on the MIOTCD dataset, while YOLOv3, YOLOv4, and YOLOv5 achieved 75.1%, 74%, and 75%, respectively, and a mAP of 94.8% on the Highway dataset, while YOLOv3, YOLOv4, and YOLOv5 achieved 94.5%, 94.9%, and 94.8%, respectively. © 2025 School of Science, DUTH. All rights reserved.",deep learning; MIRNet; MIRYO; Vehicle detection; YOLO; Advanced public transportation systems; Advanced traffic management systems; Advanced traveler information systems; Autonomous vehicles; Highway traffic control; Railroad traffic control; Street traffic control; Vehicle performance; Autonomous driving; Deep learning; Hybrid model; MIRNet; MIRYO; Noisy image; Road infrastructures; Traffic management; Vehicles detection; YOLO; Highway administration
Scopus,"Song, H.; Xie, J.; Wang, Y.; Fu, L.; Zhou, Y.; Zhou, X.",Optimized Data Distribution Learning for Enhancing Vision Transformer-Based Object Detection in Remote Sensing Images,,2025,,,,10.1111/phor.70004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000184533&doi=10.1111%2fphor.70004&partnerID=40&md5=6d4092328f40a2ca9da2da15e68aa021,"Existing Vision Transformer (ViT)-based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the over-reliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster R-CNN architecture, named ODDL-Net. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plug-and-play components—namely, the OA, OMA, and FFR modules—ensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDL-Net, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLH-Bridge datasets demonstrate that both variants of ODDL-Net achieve impressive accuracy, surpassing 23 state-of-the-art methods introduced since 2023. Specifically, ODDL-Net-NViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLH-Bridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster R-CNN-ResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for high-accuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViT-based detectors, offering a practical approach that requires little model modification. © 2025 Remote Sensing and Photogrammetry Society and John Wiley & Sons Ltd.",deep learning; object detection; ODDL-net; optimized data distribution learning; remote sensing images; Distribution transformers; Image coding; Image enhancement; Network security; Remote sensing; Augmentation techniques; Data distribution; Deep learning; Detection framework; Learning strategy; Objects detection; Optimized data distribution learning; Optimized data distribution learning-net; Remote sensing images; algorithm; data set; image processing; machine learning; optimization; remote sensing; Deep learning
Scopus,"Zhao, X.; Du, X.; Ma, C.; Hu, Z.; Yang, W.; Zheng, B.",Atmospheric neutron single event effects for multiple convolutional neural networks based on 28-nm and 16-nm SoC,,2025,,,,10.1088/1674-1056/ad8b38,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215254282&doi=10.1088%2f1674-1056%2fad8b38&partnerID=40&md5=77c95d5e0f4959cdeaa9985a5d6542aa,"The single event effects (SEEs) evaluations caused by atmospheric neutrons were conducted on three different convolutional neural network (CNN) models (Yolov3, MNIST, and ResNet50) in the atmospheric neutron irradiation spectrometer (ANIS) at the China Spallation Neutron Source (CSNS). The Yolov3 and MNIST models were implemented on the XILINX 28-nm system-on-chip (SoC). Meanwhile, the Yolov3 and ResNet50 models were deployed on the XILINX 16-nm FinFET UltraScale+MPSoC. The atmospheric neutron SEEs on the tested CNN systems were comprehensively evaluated from six aspects, including chip type, network architecture, deployment methods, inference time, datasets, and the position of the anchor boxes. The various types of SEE soft errors, SEE cross-sections, and their distribution were analyzed to explore the radiation sensitivities and rules of 28-nm and 16-nm SoC. The current research can provide the technology support of radiation-resistant design of CNN system for developing and applying high-reliability, long-lifespan domestic artificial intelligence chips. © 2025 Chinese Physical Society and IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",atmospheric neutron; convolutional neural network; single event effects; system on chip; FinFET; Forward error correction; Integrated circuit design; Network-on-chip; Neutron irradiation apparatus; Neutron sources; Neutron spectrometers; Neutrons; Radiation hardening; Atmospheric neutrons; Convolutional neural network; Effect evaluation; FinFETs; Network-based; Neural network model; Neural network systems; Single event effects; Spallation neutron sources; Systems-on-Chip; Convolutional neural networks
Scopus,"Guan, R.; Jia, L.; Yao, S.; Yang, F.; Xu, S.; Purwanto, E.; Zhu, X.; Man, K.L.; Lim, E.G.; Smith, J.; Hu, X.; Yue, Y.",WaterVG: Waterway Visual Grounding Based on Text-Guided Vision and mmWave Radar,,2025,,,,10.1109/TITS.2025.3527011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217017429&doi=10.1109%2fTITS.2025.3527011&partnerID=40&md5=88ccf83b94c2070733a23c4baf8d6296,"Waterway perception is critical for the special operations and autonomous navigation of Unmanned Surface Vessels (USVs), but current perception schemes are sensor-based, neglecting the interaction between humans and USVs for embodied perception in various operations. Therefore, inspired by visual grounding, we present WaterVG, the inaugural visual grounding dataset tailored for USV-based waterway perception guided by human prompts. WaterVG contains a wealth of prompts describing multiple targets, with instance-level annotations, including bounding boxes and masks. Specifically, WaterVG comprises 11,568 samples and 34,987 referred targets, integrating both visual and radar characteristics. The text-guided two-sensor pattern provides a fine granularity of text prompts aligned with the visual and radar features of the referent targets, containing both qualitative and numeric descriptions. To enhance the endurance and maintain the normal operations of USVs in open waterways, we propose Potamoi, a low-power visual grounding model. Potamoi is a multi-task model employing a sophisticated Phased Heterogeneous Modality Fusion (PHMF) mechanism, which includes Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). The ARW module utilizes a gating mechanism to adaptively extract essential radar features for fusion with visual inputs, ensuring prompt alignment. MHSCA, characterized by its low parameter count and computational efficiency (FLOPs), effectively integrates contextual information from both sensors with linguistic features, delivering outstanding performance in visual grounding tasks. Comprehensive experiments and evaluations on WaterVG demonstrate that Potamoi achieves state-of-the-art results compared to existing methods. © 2000-2011 IEEE.",interactive perception; multi-modal learning; perception of unmanned surface vessels; Visual grounding; Air navigation; Clutter (information theory); Computer vision; Adaptive radar; Autonomous navigation; Current perception; Interactive perception; Mm waves; Multi-modal learning; Perception of unmanned surface vessel; Special operations; Unmanned surface vessels; Visual grounding; Radar target recognition
Scopus,"Wang, J.; Zhai, Y.; Zhu, L.; Xu, L.; Zhao, Y.; Yuan, H.",Sheep-YOLO: a lightweight daily behavior identification and counting method for housed sheep,,2025,,,,10.1088/1361-6501/ad9f8d,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215391396&doi=10.1088%2f1361-6501%2fad9f8d&partnerID=40&md5=7e0500475e2d41ae8ed97fbecc097bff,"Daily behavior detection and monitoring of sheep is crucial for assessing their health status. In recent years, computer vision has been widely used in livestock behavior detection, but it usually requires large memory and computational resources. In addition, most studies have focused only on the behavior of sheep during the day, while the behavior of sheep during the night is equally important for a comprehensive understanding of their health status and well-being. Therefore, in this study, we developed a lightweight daily behavior detection and counting method for housed sheep to detect lying, feeding, and standing behaviors, and to count the number of each behavior as well as the total number of sheep. First, we propose a new PCBAM module and incorporate it into the neck part of YOLOv8n to enhance the feature information contained in the feature map, second, we use the slim neck design paradigm incorporating GSConv to lighten and improve the model operation efficiency, and finally, we reconstruct the detection head to eliminate the redundant small target detection head, reduce the model computational burden, and improve the detection performance of medium and large targets. The Sheep-YOLO model is validated using the daily behavioral dataset of housed sheep, and the experimental results show that the improved model is effective in detecting sheep behavior in complex environments, and the mAP@0.5 is improved by 5.4% compared to the baseline model, and in particular, the lying and feeding behaviors of sheep are improved by 7.2% and 8.8%, respectively. Comparative experiments with other mainstream target detection algorithms validate the advantages of our proposed model for sheep behavior detection. This study provides an effective solution for behavioral detection and counting of housed sheep. © 2024 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.",attentional mechanisms; daily behavior; object detection; sheep behavior; smart farming; Attentional mechanism; Behavior detection; Behavior identifications; Behaviour monitoring; Daily behaviors; Health status; Memory resources; Objects detection; Sheep behaviour; Smart farming; Livestock
Scopus,"Naga Jyothi Vandavasi, B.; Shakeera, S.; Narayanaswamy, V.; Ramadass Gidugu, A.; Venkataraman, H.",EM and Vision-Aided Multisensor Fusion Homing Guidance System (MSF-HGS) for Intelligent AUVs,,2025,,,,10.1109/JSEN.2024.3494041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210391749&doi=10.1109%2fJSEN.2024.3494041&partnerID=40&md5=141236eba69fceba82306b3656c77ac1,"Autonomous underwater vehicle (AUV) is an efficient technological tool for ocean exploration in the mid-21st century. Submerged docking stations (DSs) serve as charging stations to enhance vehicle endurance and enable long-range operations. In this regard, precise homing guidance techniques are essential for navigating toward homing and DS in unpredictable ocean environments. This article proposes an unsupervised multisensor fusion homing guidance system (MSF-HGS) based on Kalman filter (KF), extended KF (EKF), and advanced filter-like cubature KF (CKF) techniques. The system uses data from K-nearest neighbors (KNNs) techniques for electromagnetic (EM) field and deep learning (DL)-based underwater images. The novelty of this work lies in the fusion of different sampling frequency data (EM+ vision) in time series. Performance was validated by integrating multirate sensor data through field experiments, with EM field measurements at 2 Hz and optical images at ten frames per second collected onboard an AUV, referenced to the DS. Real-time measurements and computed AUV kinematics are fused to determine the optimal guidance for intelligent homing of AUV toward the DS. The performance of unsupervised sensor fusion of KF and EKF is demonstrated in real-time in different water bodies. The DL system is resistant to stray magnetic fields and turbid water conditions within range of 7 m and terminal homing precision within ±0.2 m from the center of dock. The success probability is 99.65% using the proposed mechanism, while it is 90% with conventional KF methods, 95% with EKF technique, and 96% with CKF technique. © 2001-2012 IEEE.",Autonomous underwater vehicle (AUV); deep learning (DL); docking; electromagnetic (EM); homing; sensor fusion; vision; Autonomous underwater vehicles; Image coding; Image enhancement; Image sampling; Magnetic levitation; Magnetic levitation vehicles; Photomapping; Sensor data fusion; Underwater imaging; Underwater photography; Autonomous underwater vehicles]; Deep learning; Docking; Docking station; Electromagnetics; Homing; Homing guidance; Kalman filter technique; Multi-sensor fusion; Sensor fusion; Extended Kalman filters
Scopus,"Trigka, M.; Dritsas, E.",A Comprehensive Survey of Machine Learning Techniques and Models for Object Detection,,2025,,,,10.3390/s25010214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214516774&doi=10.3390%2fs25010214&partnerID=40&md5=9432d943f719252fdeeadcd94ea2123d,"Object detection is a pivotal research domain within computer vision, with applications spanning from autonomous vehicles to medical diagnostics. This comprehensive survey presents an in-depth analysis of the evolution and significant advancements in object detection, emphasizing the critical role of machine learning (ML) and deep learning (DL) techniques. We explore a wide spectrum of methodologies, ranging from traditional approaches to the latest DL models, thoroughly evaluating their performance, strengths, and limitations. Additionally, the survey delves into various metrics for assessing model effectiveness, including precision, recall, and intersection over union (IoU), while addressing ongoing challenges in the field, such as managing occlusions, varying object scales, and improving real-time processing capabilities. Furthermore, we critically examine recent breakthroughs, including advanced architectures like Transformers, and discuss challenges and future research directions aimed at overcoming existing barriers. By synthesizing current advancements, this survey provides valuable insights for enhancing the robustness, accuracy, and efficiency of object detection systems across diverse and challenging applications. © 2025 by the authors.",deep learning; machine learning; models; object detection; techniques; Contrastive Learning; Deep learning; Deep reinforcement learning; Federated learning; Autonomous Vehicles; Deep learning; In-depth analysis; Machine learning models; Machine learning techniques; Machine-learning; Medical diagnostics; Objects detection; Research domains; Technique; autonomous vehicle; benchmarking; computer vision; deep learning; diagnosis; human; machine learning; review; Adversarial machine learning
Scopus,"Zhang, C.; Wu, Z.; Liang, Y.",Traffic sign installation angle measurement method based on the You Only Look Once algorithm and binocular stereo vision,,2025,,,,10.1117/1.JEI.34.2.023014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005416205&doi=10.1117%2f1.JEI.34.2.023014&partnerID=40&md5=ff1d40ff795fff28a2cc545c8e52032f,"We propose a framework for traffic sign installation angle measurement. The framework integrates an enhanced You Only Look Once-Lite (YOLO-Lite) detection model with binocular stereo vision. The enhanced YOLO-Lite detection model incorporates a lightweight attention convolution (LAC) module and adaptive downsampling mechanism (ADM). Both the LAC module and ADM contribute to improving the accuracy of traffic sign detection. The LAC module enhances traffic sign detection accuracy by integrating spatial and channel attention mechanisms, enabling the extraction of discriminative features from small targets in complex backgrounds. Also, the LAC module reduces the number of training parameters through depthwise separable convolutions, contributing to improved computational efficiency without compromising detection accuracy. Simultaneously, the ADM module mitigates information loss during multi-scale processing through adaptive downsampling strategies that preserve critical features. An Iterative Geometry Encoding Volume with MobileNetV2 enhances performance through MobileNetV2-based lightweight encoding and single-iteration convolutional gated recurrent unit optimization. The model maintains the consistency of contour matching and multi-scale depth estimation while reducing the computational burden. The parameters and inference time of the overall framework are only respectively 7.8 M and 1.37 s, with an angle measurement error of 4.80%. This meets the accuracy and real-time requirements for traffic sign installation angle measurement implemented on embedded devices. The proposed framework provides an efficient, cost-effective solution for the standardized installation and maintenance of road traffic infrastructure.  © 2025 SPIE and IS&T.",angular measurement; binocular stereo vision; embedded devices; Iterative Geometry Encoding Volume with MobileNetV2; you only look once-lite; Binocular-stereo visions; Detection accuracy; Detection models; Down sampling; Embedded device; Encodings; Installation angle; Iterative geometry encoding volume with mobilenetv2; Traffic sign detection; You only look once-lite; Angle measurement
Scopus,"Tian, D.; Li, J.; Lei, J.",Multi-sensor information fusion in Internet of Vehicles based on deep learning: A review,,2025,,,,10.1016/j.neucom.2024.128886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208768433&doi=10.1016%2fj.neucom.2024.128886&partnerID=40&md5=5db0d7f8ff949124d2de66c90a95c8b1,"Environmental perception is a crucial component of intelligent driving technology, providing the informational foundation for intelligent decision-making and collaborative control. Due to the limitations of single sensors and the continuous advancements in deep learning and sensor technologies, multi-sensor information fusion in the Internet of Vehicles (IoV) has emerged as a major research hotspot. This approach is also a primary solution for achieving full self-driving. However, given the complexity of the technology, there are still many challenges in achieving accurate and reliable real-time multi-source information perception. Current discussions often focus on specific aspects of multi-sensor fusion in intelligent driving, while detailed discussions on sensor fusion in the context of the IoV are relatively scarce. To provide a comprehensive discussion and analysis of multi-sensor information fusion in IoV, this paper first provides a detailed introduction to its developmental background and the commonly involved sensors. Subsequently, a detailed analysis of the strategies, deep learning architectures, and methods for multi-sensor information fusion in the IoV is presented. Finally, the specific applications and key issues related to multi-sensor information fusion in IoV are discussed from multiple perspectives, along with an analysis of future development trends. This paper aims to serve as a valuable reference for advancing multi-sensor information fusion technology in IoV environments and supporting the realization of full self-driving. © 2024 Elsevier B.V.",Deep learning; Intelligent driving; Internet of Vehicles; Multi-sensor information fusion; Collaborative control; Deep learning; Environmental perceptions; Intelligent decision-making; Intelligent driving; Internet of vehicle; Learning technology; Multi-sensor information fusion; Self drivings; Single sensor; deep learning; human; internet of things; sensor; short survey; surgical technology; Information fusion
Scopus,"Patil, A.N.; Mohanty, S.N.; Khan, T.",SkyViewSentinel: A Deep Learning-Driven Military Object Detection Application for Remote-Sensing Satellite Images,,2025,,,,10.2174/0126662558333174241009112642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005081767&doi=10.2174%2f0126662558333174241009112642&partnerID=40&md5=8299e6545d63501de78582987c061909,"Background: In today’s ever-changing world, military forces face significant challenges in maintaining situational awareness and responding swiftly to emerging threats. Traditional aerial surveillance often fails to give timely and thorough intelligence over large areas. Limited coverage, mistakes, and difficulty noticing small changes on the ground hinder military operations. To address these problems, this paper introduces the development of a deep learning-based web application named “SkyViewSentinel”, a solution tailored specifically for military aerial surveillance. Methods: The application framework i.e., SkyViewSentinel has been developed through multiple stages i.e., (i) pre-process the Xview overhead Satellite imagery dataset using Ground Truth refinement and image partitioning method, (ii) employed a SOTA deep model i.e., YOLOv8 as a baseline architecture for the research problem and assessed the performance on experimental dataset, (iii) a series of rigorous experiments have been conducted using deep model and obtained results are reported. (iv) Finally, the trained model has been seamlessly integrated into the web application and develops a comprehensive web-based object detection application. The developed application detects military-based objects from real-time satellite images. Results: The developed application has shown promising results in identifying military objects from satellite images, outperforming other contemporary methods. The designed framework has achieved an overall mAP score of 0.315 for all nine classes of military-based objects. For certain specific classes, detection accuracy exceeds 70%, demonstrating the robustness of the framework. Conclusion: The designed web application enables users to detect military-based objects in the region provided by the user. By harnessing the power of satellite object recognition technology, SkyViewSentinel provides a new way to monitor and understand activities in operational areas. © 2024 Bentham Science Publishers.",Military objects; Object detection; Satellite imagery; SkyViewSentinel; XView dataset; YOLOv8; army; article; awareness; controlled study; deep learning; human; intelligence; remote sensing; satellite imagery
Scopus,"Wang, M.; Wang, H.; Li, Y.; Chen, L.; Cai, Y.; Shao, Z.",MSAFusion: Object Detection Based on Multisensor Adaptive Fusion Under BEV,,2025,,,,10.1109/TIM.2025.3548792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001539723&doi=10.1109%2fTIM.2025.3548792&partnerID=40&md5=70a3c3096089884aadb805a46a4baa1d,"Object detection is a critical component of autonomous driving perception. To achieve comprehensive environmental perception, mainstream methods commonly rely on multimodal sensor fusion. However, existing solutions often face challenges such as low sensor utilization and suboptimal fusion strategies. To address these issues, this article proposes MSAFusion, a multisensor adaptive fusion framework based on a bird’s eye view (BEV). In our framework, we extract multiview features using Vision Mamba (Vim), generate BEV queries through positional encoding for preliminary fusion with multimodal features, and employ a deep Q-network (DQN) for adaptive fusion based on feature consistency and continuity. This approach enables efficient utilization of multimodal sensors and optimal fusion across diverse environments. Extensive experiments on the nuScenes and Radiate datasets demonstrate that MSAFusion achieves state-of-the-art performance, delivering superior panoramic environmental perception, improved object detection accuracy, and enhanced flexibility compared to existing multisensor fusion methods. © 1963-2012 IEEE.",Autonomous driving; environment perception; multisensor adaptive fusion; Vision Mamba (Vim); Object recognition; Signal encoding; Adaptive fusion; Autonomous driving; Environment perceptions; Environmental perceptions; Multi sensor; Multi-sensor adaptive fusion; Multimodal sensor; Objects detection; Sensor fusion; Vision mamba; Object detection
Scopus,"Liang, H.; Yang, Y.; Hu, J.; Yang, J.; Liu, F.; Yuan, S.",Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds,,2025,,,,10.1109/ICASSP49660.2025.10890359,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009588079&doi=10.1109%2fICASSP49660.2025.10890359&partnerID=40&md5=932f3c099d5e52d9b5c716dd352f8e97,"Compact UAV systems, while advancing delivery and surveillance, pose significant security challenges due to their small size, which hinders detection by traditional methods. This paper presents a cost-effective, unsupervised UAV detection method using spatial-temporal sequence processing to fuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios. Our approach segments point clouds into foreground and background, analyzes spatial-temporal data, and employs a scoring mechanism to enhance detection accuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024 UG2+ Challenge, demonstrating its practical effectiveness. We plan to open-source all designs, code and sample data for the research community @ github.com/lianghanfang/UnLiDAR-UAV-Est. © 2025 IEEE.",Point Clouds; Trajectory Estimation; UAV detection; Unsupervised; Computer vision; Cost effectiveness; Data accuracy; Open systems; Remote sensing; Robotics; Unmanned aerial vehicles (UAV); 3-D trajectory; Cost effective; Detection methods; Point-clouds; Security challenges; Sparse point cloud; Trajectory estimation; UAV detection; UAV systems; Unsupervised; Aircraft detection
Scopus,"Satama-Bermeo, G.; Lopez-Guede, J.M.; Rahebi, J.; Teso-Fz-Betoño, D.; Boyano, A.; Akizu-Gardoki, O.",PRISMA Review: Drones and AI in Inventory Creation of Signage,,2025,,,,10.3390/drones9030221,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001276737&doi=10.3390%2fdrones9030221&partnerID=40&md5=d8465d25a127e4430c6bc574ca30976f,"This systematic review explores the integration of unmanned aerial vehicles (UAVs) and artificial intelligence (AI) in automating road signage inventory creation, employing the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology to analyze recent advancements. The study evaluates cutting-edge technologies, including UAVs equipped with deep learning algorithms and advanced sensors like light detection and ranging (LiDAR) and multispectral cameras, highlighting their roles in enhancing traffic sign detection and classification. Key challenges include detecting minor or partially obscured signs and adapting to diverse environmental conditions. The findings reveal significant progress in automation, with notable improvements in accuracy, efficiency, and real-time processing capabilities. However, limitations such as computational demands and environmental variability persist. By providing a comprehensive synthesis of current methodologies and performance metrics, this review establishes a robust foundation for future research to advance automated road infrastructure management to improve safety and operational efficiency in urban and rural settings. © 2025 by the authors.",AI; data fusion; LiDAR; R-CNN; UAVs; Yolo; Deep learning; Drones; Highway administration; Road and street markings; Sensor data fusion; Advanced sensors; Aerial vehicle; Cutting edge technology; Light detection and ranging; Meta-analysis; R-CNN; Road signage; Systematic Review; Unmanned aerial vehicle; Yolo
Scopus,"Zhang, X.; Feng, Y.; Wang, N.; Lu, G.; Mei, S.",Aerial Person Detection for Search and Rescue: Survey and Benchmarks,,2025,,,,10.34133/remotesensing.0474,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002996073&doi=10.34133%2fremotesensing.0474&partnerID=40&md5=37dc3264acc3dfedd7d6d1c8d937a9de,"Robust person detection in aerial images under all-weather conditions stands as a fundamental technology pivotal to the efficacy of intelligent search and rescue (SaR) tasks. However, the challenges stem from the varied postures, sparsity, diminutiveness, and faintness of personnel objects when viewed from an air-to-ground perspective, leading to issues with insufficient feature representation and suboptimal detection accuracy. This survey commences by underscoring the extensive potential applications and the prevailing limitations associated with aerial person detection (APD) within the scope of drone-assisted SaR scenarios. To meet the requirement of APD applications, we thoroughly investigate advancements and challenges in 4 related methodologies, including object-aware methods for size and perspective variability, sample-oriented methods with sparse distribution, information-fusion methods for the issue of lighting or visibility, and lightweight methods on constrained devices. Furthermore, to foster advancements in APD, we have conducted a comprehensive APD dataset labeled as “VTSaR”, which stands out from the existing publicly accessible APD datasets by offering a greater diversity of scenes, varying personnel behaviors, flexible capture angles, differing capture heights, and an inclusion of aligned visible and infrared samples along with synthetic samples. Finally, we evaluate the performance of mainstream detection methods on VTSaR benchmarks, advocating for APD’s broader application across various domains. Copyright © 2025, Xiangqing Zhang et al. Exclusive licensee Aerospace Information Research Institute, Chinese Academy of Sciences. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License (CC BY 4.0).",Aerial photography; Aircraft detection; Antenna grounds; Benchmarking; Labeled data; Network security; Aerial images; Assisted search; Condition; Detection accuracy; Feature representation; Intelligent search; Person detection; Search and rescue; Search and rescue tasks; Suboptimal detection; Information fusion
Scopus,"Ma, K.; Guo, B.; Liu, S.; Fang, C.; Luo, S.; Zheng, Z.; Yu, Z.",AdaShift: Anti-Collapse and Real-Time Deep Model Evolution for Mobile Vision Applications,,2025,,,,10.1109/TMC.2025.3572215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006775330&doi=10.1109%2fTMC.2025.3572215&partnerID=40&md5=644728abd4f6fe2d7b5cce9f80c27c68,"As computational hardware advance, integrating deep learning (DL) models into mobile devices has become ubiquitous for visual tasks. However, ""data distribution shift""in live sensory data can lead to a degradation in the accuracy of mobile DL models. Conventional domain adaptation methods, constrained by their dependence on pre-compiled static datasets for offline adaptation, exhibit fundamental limitations in real-time practicality. While modern online adaptation methodologies enable incremental model evolution, they remain plagued by two critical shortcomings: computational latency from excessive resource demands on mobile devices that compromise temporal responsiveness, and accuracy collapse stemming from error accumulation through unreliable pseudo-labeling processes. To address these challenges, we introduce AdaShift, an innovative cloud-assisted framework enabling real-time online model adaptation for vision-based mobile systems operating under non-stationary data distributions. Specifically, to ensure real-time performance, the adaptation trigger and plug-and-play adaptation mechanisms are proposed to minimize redundant adaptation requests and reduce per-request costs. To prevent accuracy collapse, AdaShift introduces a novel anti-collapse parameter restoration mechanism that explicitly recovers knowledge, ensuring stable accuracy improvements during model evolution. Through extensive experiments across various vision tasks and model architectures, AdaShift demonstrates superior accuracy and 100ms-level adaptation latency, achieving an optimal balance between accuracy and real-time performance compared to baselines.  © 2025 IEEE.",Cloud-assisted online model adaptation; Real-time adaptation performance; Resource-constrained mobile devices; Cloud-assisted online model adaptation; Learning models; Model Adaptation; Model evolution; On-line modelling; Performance; Real- time; Real-time adaptation; Real-time adaptation performance; Resource-constrained mobile device
Scopus,"Raj, R.; Kos, A.",An Extensive Study of Convolutional Neural Networks: Applications in Computer Vision for Improved Robotics Perceptions,,2025,,,,10.3390/s25041033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219211046&doi=10.3390%2fs25041033&partnerID=40&md5=2dffa75404263c2ac1b8e7953c584c7c,"Convolutional neural networks (CNNs), a type of artificial neural network (ANN) in the deep learning (DL) domain, have gained popularity in several computer vision applications and are attracting research in other fields, including robotic perception. CNNs are developed to autonomously and effectively acquire spatial patterns of characteristics using backpropagation, leveraging an array of elements, including convolutional layers, pooling layers, and fully connected layers. Current reviews predominantly emphasize CNNs’ applications in various contexts, neglecting a comprehensive perspective on CNNs and failing to address certain recently presented new ideas, including robotic perception. This review paper presents an overview of the fundamental principles of CNNs and their applications in diverse computer vision tasks for robotic perception while addressing the corresponding challenges and future prospects for the domain of computer vision in improved robotic perception. This paper addresses the history, basic concepts, working principles, applications, and the most important components of CNNs. Understanding the concepts, benefits, and constraints associated with CNNs is crucial for exploiting their possibilities in robotic perception, with the aim of enhancing robotic performance and intelligence. © 2025 by the authors.",artificial intelligence (AI); computer vision; convolutional neural network (CNN); deep learning (DL); machine learning (ML); mobile robot (MR); perception; Deep neural networks; Mobile robots; Artificial intelligence; Computer vision applications; Convolutional neural network; Deep learning; Machine learning; Machine-learning; Mobile robot; Neural network application; Neural-networks; artificial intelligence; artificial neural network; back propagation; computer; computer vision; convolutional neural network; deep learning; diagnosis; human; intelligence; machine learning; perception; review; robotics; Convolutional neural networks
Scopus,"Al Akbar, R.; Samuel, D.W.; Adinata, M.F.",Mobility Aid for the Visually Impaired Using Machine Learning and Spatial Audio,,2025,,,,10.18196/jrc.v6i2.25245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003201474&doi=10.18196%2fjrc.v6i2.25245&partnerID=40&md5=7b95120e881f6b753a20d90df9f22183,"Assistive technology is crucial in enhancing the quality of life for individuals with disabilities, including the visually impaired. Many mobility aids lack advanced features such as real-time machine learning-based object detection and spatial audio for environmental awareness. This research contributes to developing more intelligent and adaptable assistive technology for visually impaired individuals, promoting improved navigation and environmental awareness. This research presents a head-mounted mobility aid that integrates a time-of-flight camera, a web camera, and a touch sensor with K-Means clustering, Convolutional Neural Networks (CNNs), and concurrent programming on a Raspberry Pi 4B to detect and classify surrounding obstacles and objects. The system converts obstacle data into spatial audio, allowing users to perceive their surroundings through sound direction and intensity. Object recognition is activated via a touch sensor, providing distance and directional information relative to the user using audio description. The concurrent programming implementation improves execution time by 50.22% compared to Infinite Loop Design (ILD), enhancing real-time responsiveness. However, the system has limitations, including object recognition limited to 80 predefined categories, a 4-meter detection range, reduced accuracy under high-intensity sunlight, and potential interference in spatial audio perception due to external noise. Assistive technology to help the mobility of blind people using advanced technology based on machine learning has developed in a form that can be used flexibly for the user's mobility. © 2025 Department of Agribusiness, Universitas Muhammadiyah Yogyakarta. All rights reserved.",Assistive Technology; Blind People; Concurrent Programming; Image Recognition; K-Means; Time-of-Flight Camera
Scopus,"Ding, X.; Gu, J.; Geng, J.; Liu, X.; Li, H.; Luo, J.; Wang, X.",Feature-enhanced small object detection algorithm for surface defects of cigarette pack,,2025,,,,10.1117/12.3069428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005024411&doi=10.1117%2f12.3069428&partnerID=40&md5=a755d148b7e71cbe73236804b10d04fe,"The surface defects of cigarette pack are characterized by varying scales and complex backgrounds. Small target detection algorithms, as a potential solution, have been proven to have the ability to identify such surface defects. However, the existing small target detection algorithms still have many limitations, especially in terms of detection accuracy, model complexity, and computing speed. To address these challenges, we propose a lightweight cigarette pack defect detection algorithm, YOLOv5s-FDE. In the backbone network, we introduce the GSConv structure, which incorporates channel shuffle and depthwise separable convolution to construct the feature enhancement module (FEM) with a cross-stage architecture. It aims to reduce information loss and enhance feature utilization. In the feature fusion stage, we introduce the DySample upsampling module, which effectively preserves target detail features through a dynamic sampling mechanism, and reduces the memory usage. Finally, the E-Triplet cross-dimensional attention module is proposed, which leverages a multi-branch structure to enhance cross-dimensional information interaction and suppress irrelevant background information. Experimental results demonstrate that, compared to the YOLOv5s baseline algorithm, YOLOv5s-FDE achieves an average precision of 95.6%. The number of model parameters and computational cost are reduced by 0.88M and 3.5G, respectively. The proposed method outperforms recent state-of-the-art detection algorithms, whiles providing theoretical support for intelligent defect detection of cigarette pack. © 2025 SPIE.",Attention mechanism; Cigarette pack defect detection; Dynamic upsampling; Feature enhancement; YOLOv5s; Image coding; Image compression; Attention mechanisms; Cigarette pack defect detection; Cigarette packs; Defect detection; Dynamic upsampling; Feature enhancement; Small target detection; Target detection algorithm; Upsampling; YOLOv5; Image segmentation
Scopus,"Partheepan, S.; Sanati, F.; Hassan, J.",Evaluating YOLO Variants With Transfer Learning for Real-Time UAV Obstacle Detection in Simulated Forest Environments,,2025,,,,10.1109/ACCESS.2025.3577251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007620958&doi=10.1109%2fACCESS.2025.3577251&partnerID=40&md5=7a3f1c9e92f6827d6a73b7dc8b55161b,"Autonomous navigation in forest environments poses significant challenges for Uncrewed Aerial Vehicles (UAVs), where reliable obstacle detection is essential for safe operation. This study presents a detailed performance evaluation of recent YOLO models (v8–v11) using a realistic simulation environment built on Unreal Engine and AirSim. To the best of our knowledge, this is the first study to comprehensively benchmark YOLOv8–v11 in a UAV-based obstacle detection context, incorporating both onboard deployment factors and the impact of transfer learning (TL) using EfficientNet backbones. We analyze detection accuracy (mAP@50, precision, recall), inference time, FPS, and memory usage across all YOLO configurations, from lightweight (YOLOv8s) to high-performance models (YOLOv11x). TL significantly improved model performance; for instance, YOLOv8s showed a recall increase from 0.6318 to 0.7804 and mAP@50 from 0.7228 to 0.8447. YOLOv8m and YOLOv10b emerged as optimal models, offering the best trade-off between speed and detection accuracy for real-time UAV deployment. Larger models like YOLOv9c and YOLOv10x delivered superior accuracy but required more computational resources. The results offer practical guidance for selecting YOLO architectures in UAV-based obstacle detection tasks, balancing accuracy, speed, and deployment feasibility in complex environments. © 2013 IEEE.",Obstacle detection; real-time processing; transfer learning; UAV; YOLO; Micro air vehicle (MAV); Target drones; Transfer learning; Aerial vehicle; Autonomous navigation; Detection accuracy; Forest environments; Obstacles detection; Real- time; Realtime processing; Transfer learning; Unmanned aerial vehicle; YOLO; Drones
Scopus,"Wang, S.; Mei, L.; Liu, R.; Jiang, W.; Yin, Z.; Deng, X.; He, T.",Multi-Modal Fusion Sensing: A Comprehensive Review of Millimeter-Wave Radar and Its Integration With Other Modalities,,2025,,,,10.1109/COMST.2024.3398004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192996492&doi=10.1109%2fCOMST.2024.3398004&partnerID=40&md5=47d1249aacf0521e823beafed658867f,"Millimeter-wave (mmWave) radar, with its high resolution, sensitivity to micro-vibrations, and adaptability to various environmental conditions, holds immense potential across multi-modal fusion sensing. Although there exist review papers on mmWave radar, there is a noticeable lack of comprehensive reviews focusing on its multi-modal fusion sensing capabilities. Addressing this gap, our review offers an extensive exploration of mmWave radar multi-modal fusion sensing, emphasizing its integration with other modalities. This review discusses the complex realm of millimeter-wave radar multi-modal fusion sensing, detailing its importance, hardware and software aspects, principles, and current potential and applications. It delves into data characteristics and datasets associated with mmWave radar, focusing on Doppler, point cloud, and multi-modal data formats. The review highlights how these data types enhance multi-modal fusion sensing and discusses methodologies, including signal processing and learning algorithms. Three categories of multi-modal fusion methodologies are proposed to optimally manage and interpret fused data. Various practical applications of mmWave radar multi-modal fusion sensing are illustrated, underlining the unique capabilities it provides when integrated with other sensors. The review concludes by identifying potential future research avenues, underscoring the immense potential of this field for further exploration and advancement. © 1998-2012 IEEE.",millimeter-wave radar; Multi-modal fusion sensing; review; Application programs; Millimeter waves; Modal analysis; Radar antennas; Signal processing; High resolution; Microvibrations; Millimeter-wave radar; Millimeterwave communications; Millimetre-wave radar; Multi-modal fusion; Multi-modal fusion sensing; Radars antennas; Antenna arrays
Scopus,"Huang, L.; Yuan, H.; Chen, S.; Zhou, B.; Guo, Y.",A lightweight deep learning model for real-time rectangle NdFeB surface defect detection with high accuracy on a global scale,,2025,,,,10.1007/s11554-024-01592-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211370700&doi=10.1007%2fs11554-024-01592-9&partnerID=40&md5=e592a8162a87133bc612785e34138bbb,"To solve the problem that it is difficult to detect dynamic tiny square neodymium-iron-boron (NdFeB) surface defects in the case of limited computing resources, this paper proposes a square NdFeB magnet surface defect detection method based on the YOLO (YOLOv8-FCW) lightweight network. Initially, the lightweight global adaptive feature enhancement module (DFNet) network is used as the backbone feature extraction net-work. By customizing the depth of the feature matrix and reducing unnecessary branch structures, the model complexity is reduced while enhancing the network’s ability to extract multi-scale feature information. Subsequently, the deformable convolution module (DCNv3) is utilized to acquire twice downsampling feature maps without information loss, aiming to expand the receptive field for small-sized defects. Finally, to further improve detection accuracy, the Wise-IOU (WIOU) v3 bounding box loss function is introduced to focus on the samples that are difficult to identify and reduce the gradient penalty for low-quality samples. The experimental results show that the YOLOv8-FCW algorithm achieves a mean Average Precision (mAP@0.5) of 78.6% on the rectangle NdFeB magnet dataset, with a model parameter quantity and computational cost reduction of 33.2% and 24.7%, respectively compared with the baseline, and requires less computational resources for higher detection accuracy compared to other mainstream object detection algorithms. Finally, the model was deployed to industrial Automated Optical Inspection (AOI) devices using TensorRT. This deployment reduced the inference time for a single image to 2.7 ms and increased speed by 6.6 times, enabling dynamic micro-detection of surface defects in square NdFeB. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Defect detection; DFNet; Lightweight network; NdFeB magnet; TensoRT; YOLOv8; Accelerator magnets; Electromagnetic induction; Forward error correction; Image enhancement; Light sensitive materials; Neodymium compounds; Particle beams; Photons; Superconducting films; Defect detection; Detection accuracy; DFNet; Learning models; Lightweight network; NdFeB magnet; Real- time; Surface defect detections; Tensort; YOLOv8; Neodymium alloys
Scopus,"Mumba, E.; Sulaiman Gezawa, A.; Liu, C.",Enhanced autonomous driving within Webots simulation for student experiments,,2025,,,,10.1007/s11370-025-00608-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007978282&doi=10.1007%2fs11370-025-00608-y&partnerID=40&md5=75c0ada2d6c0a86bd0dfa54beefaee78,"As artificial intelligence (AI) advances in autonomous driving, autonomous navigation systems should be integrated into educational platforms to provide students practical experience in robotics and machine learning. In this paper, we propose the detailed study of autonomous driving within the Webots simulation platform, specifically designed for student experiments. We incorporate advanced techniques such as obstacle detection, precise lane following, and intelligent traffic light recognition based on an improved YOLOv8 architecture for robust detection of road signs and traffic lights. We utilized the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) which are integral to YOLOv8’s multi-scale feature extraction and enhance their efficiency by replacing standard convolutions with depthwise separable convolution in the neck of network. This modification reduces the computational overhead while preserving feature representation quality, leading to faster inference speeds and improved small-object detection accuracy, thus improving autonomous navigation performance in simulated scenarios. To consider the performance of our proposed model, we compare it against widely used and well-established object detection models on KITTI and RF100 datasets. The experimental results demonstrate that the proposed model exhibits a good advantage in the field of autonomous driving and boosting student engagement and preparing them for future careers in AI and robotics. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.",Autonomous navigation; Object detection; Sensor integration; Student teaching experiments; YOLOv8; Autonomous vehicles; Convolution; Intelligent robots; Learning systems; Navigation; Navigation systems; Object recognition; Simulation platform; Students; Traffic signs; Autonomous driving; Autonomous navigation; Autonomous navigation systems; Educational platforms; Objects detection; Practical experience; Sensor integration; Student experiments; Student teaching experiment; YOLOv8; Object detection
Scopus,"Wu, X.; Zhang, B.; Wan, W.",Adaptive cross-modal fusion for robust multi-modal object detection in infrared–visible imaging,,2025,,,,10.1007/s10043-025-00977-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004839417&doi=10.1007%2fs10043-025-00977-w&partnerID=40&md5=d3949840622048a911868a6794cf45e8,"Given the challenges faced by object detection methods that rely on visible light in complex environments, many researchers have begun to explore the combination of infrared and visible imaging for multi-modal detection. Existing results show that multi-modal fusion has proven effective for improving object detection outcomes. However, most current multi-modal detection methods rely on fixed-parameter feature fusion techniques, failing to account for the imaging differences across diverse environments and the complementary information between different modalities. In this paper, we propose a multi-modal object detection method based on adaptive weight fusion, utilizing the dual-stream framework to extract features from both modalities separately. We design a Cross-Modal Feature Interaction (CMFI) module to integrate global information across modalities and capture long-range dependencies. In addition, we introduce an Adaptive Modal Weight Calculation (AMWC) module, which fully accounts for the characteristics of different modalities in various environments and the complementarity among the modalities. This module dynamically adjusts the fusion weights within the CMFI module based on the input from different modalities. Moreover, a novel loss function is introduced to regulate the internal parameter adjustments of the AMWC module. We conduct extensive experiments on three representative datasets, using mAP@0.5 and mAP@0.5:0.95 as evaluation metrics. Our model achieved 79.1% and 40.6% on the FLIR dataset, 81.9% and 52.1% on M3FD, and 73.5% and 32.5% on KAIST. © The Optical Society of Japan 2025.",Deep learning; Infrared images; Modal weight; Multi-modal feature fusion; Object detection
Scopus,"Xu, H.; Yu, Y.; He, J.; Pang, C.; Zhang, X.",Multistage Fusion Object Detection With Marine Radar and Camera in Complex Maritime Context,,2025,,,,10.1109/TIM.2025.3571112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005793143&doi=10.1109%2fTIM.2025.3571112&partnerID=40&md5=1768489ed44c78f117bd7ec20c0c1dce,"The marine object detection is an essential component for maritime autonomous surface ship (MASS) systems, enabling them to perceive obstacle objects and avoid them. Existing detection algorithms primarily rely on visual sensors and face serious limitations in complex maritime environments. In this article, we propose a novel joint object detection method based on the fusion of marine radar and camera data for robust maritime object detection. By employing a multistage hybrid-level fusion architecture, the proposed method efficiently fuses marine radar data with camera images, thereby enhancing the accuracy and robustness of object detection in complex maritime contexts. In addition, inspired by vision transformer detectors, a new transformer-based fusion module (TFM) is proposed to mitigate semantic disparity between the two modalities and enhance radar information guidance through extracted object queries. Extensive experiments on public maritime perception datasets demonstrate the significant advantages of the proposed method in various maritime environments. The framework also exhibits strong generalization capability, making it applicable to diverse maritime settings, including challenging conditions like low visibility and high-background noise. In public maritime perception datasets, the proposed method achieves state-of-the-art accuracy compared with other single-modality baselines and radar-camera fusion baselines, outperforming the excellent visual object detection model YOLOv8-x in terms of mAP0.35 increased by 13.72% in common scenarios and 24.85% in complex scenarios. © 1963-2012 IEEE.",Marine radar; maritime autonomous surface ship (MASS) perception; multisensor fusion; multistage fusion; object detection; Detection algorithm; Marine objects; Maritime autonomous surface ship perception; Maritime environment; Multi stage fusion; Multi-sensor fusion; Objects detection; Ship systems; Surface ship; Visual sensor
Scopus,"Character, L.; Moline, M.; Breece, M.W.; White, E.; Davis, D.; Colbourn, C.",Deep Learning for Detection of Underwater Aircraft Wrecks from US Conflicts,,2025,,,,10.5334/jcaa.179,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005485410&doi=10.5334%2fjcaa.179&partnerID=40&md5=45826253972350e4c579af9fa476d618,"There are more than 72,000 missing in action service members from WWII, many of whom were lost at sea in aircraft. Using high-resolution sidescan sonar and a YOLOv7 model, we present a deep learning approach to expedite the search for these individuals. Our training dataset is the largest aircraft wreck-focused dataset that has been published to date, with 19 unique aircraft wrecks, composed of 290 individual fragments and located across six countries. Our trained model produces an F1 score of 0.74. We tested the model on newly collected data and the model correctly identified three out of four previously unknown aircraft. As the model becomes more accurate and trust in the model is built, we envision a human-in-the-loop approach in which newly collected data are input into the model in the field, the model is run with a confidence threshold of 0.5, and predictions are then checked by the human field team. As trust builds, the confidence threshold can be made higher so that fewer predictions are generated and human review time is minimized. © 2025 The Author(s).",aircraft wreck; autonomous underwater vehicle; Deep learning; marine archaeology; sidescan sonar; World War II
Scopus,"Angeljulie, J.; Pradish Pranam, S.; Yosuva, B.; Shuaib Khan, S.",Reliable Roadside Assistance and Fast Solutions for Emergency Breakdown States by EF-YOLOv8m,,2025,,,,10.1109/ICSADL65848.2025.10933303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002456072&doi=10.1109%2fICSADL65848.2025.10933303&partnerID=40&md5=3a60fc42d2ac9ee71bcf57f7e0bb1768,"In the technological development era and fast-paced world, Artificial Intelligence techniques and Computer Vision algorithms are transfiguring the automotive industrial services, especially in reliable roadside assistance and emergency services for vehicle breakdowns. Significant AI-oriented algorithms including predictive maintenance, route optimization, resource allocation, and reinforcement learning techniques are used to address crucial problems like optimized resource distribution, rapid response, and proactive fault detection. This article incorporates AI-oriented algorithms for enhancing the accuracy, responsiveness, and efficiency of reliable roadside assistance and fast solutions for emergency breakdown states using the EF-YOLOv8m (Efficient - You Only Look Once version 8) algorithm. This proposed algorithm facilitates the visual prediction of breakdown situations and provides a fast solution for real-time prediction, analysis, and response in emergencies. The deployment of the EF - YOLOv8m algorithm leads to accurate prediction of vehicle breakdown situations, estimates road conditions, and detects critical faults to ensure the optimization of resource allocation and fast decision-making. Using its greater speed, accuracy, and lightweight model architecture, EE-YOLOv8 may process real-time image/video inputs and sensor data from vehicle-attached cameras, IoT devices, and surveillance drones in real time. Based on performance metrics such as confidence scores, detection accuracy, and frames per second (FPS), this model is used to analyse the reliability of EF-YOLOv8m in dynamic roadside environments. Additionally, the incorporation of AI -oriented and predictive maintenance algorithms improves the effectiveness of dispatch functions, reducing delay and guaranteeing timely assistance. Experimental results show the potential of YOLOv8m to transfigure the services of roadside assistance and provide an ascendable, high-performance solution that can address the difficulties of emergency breakdown situations with reliability and precision.  © 2025 IEEE.",Artificial Intelligence; Computer Vision; Efficient YOLOv8; Emergency Breakdown States; Internet of Things (IoT); Reliable Roadside Assistance; Aircraft detection; Intelligent systems; Reinforcement learning; Risk assessment; Satellites; Artificial intelligence techniques; Automotives; Computer vision algorithms; Efficient YOLOv8; Emergency breakdown state; Fast solutions; Internet of thing; Predictive maintenance; Reliable roadside assistance; Technological development; Resource allocation
Scopus,"Sharma, M.; Kaswan, K.S.; Yadav, D.K.",Performance comparison of optical flow and background subtraction and discrete wavelet transform methods for moving objects,,2025,,,,10.11591/ijra.v14i1.pp93-102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219016941&doi=10.11591%2fijra.v14i1.pp93-102&partnerID=40&md5=f9c3d1c39428eb6cf71084f2c9b326d8,"Self-driving cars and other autonomous vehicles rely on systems that can recognize and follow objects. The ways help people make safe decisions and navigate by showing things like people, cars, obstacles, and traffic lights. Computer vision algorithms encompass both object detection and tracking. Different methods are specifically developed for picture or video analysis not only to identify items within the visual content but also to accurately determine their precise locations. This can operate independently as an algorithm or as a constituent of an item-tracking system. Object tracking algorithms can be used to follow objects over video frames, providing a contrasting approach. The research article focuses on the mathematical model simulation of optical flow, background subtraction, and discrete wavelet transform (DWT) methods for moving objects. The performance evaluation of the methods is done based on simulation response time, accuracy, sensitivity, and specificity doe several images in different environments. The DWT has shown optimal behavior in terms of the response time of 0.27 seconds, accuracy of 95.34 %, selectivity of 95.96 %, and specificity of 94.68 %. © 2025, Intelektual Pustaka Media Utama. All rights reserved.",Discrete wavelet transform; Image subtraction; Moving objects; Optical modeling; Signal detection
Scopus,"In, H.; Kweon, J.; Moon, C.",Squeeze-EnGAN: Memory Efficient and Unsupervised Low-Light Image Enhancement for Intelligent Vehicles,,2025,,,,10.3390/s25061825,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000966064&doi=10.3390%2fs25061825&partnerID=40&md5=306ffac4d298cd54a0243b3f6307de3e,"Intelligent vehicles, such as autonomous cars, drones, and robots, rely on sensors to gather environmental information and respond accordingly. RGB cameras are commonly used due to their low cost and high resolution but are limited in low-light conditions. While employing LiDAR or specialized cameras can address this issue, these solutions often incur high costs. Deep learning-based low-light image enhancement (LLIE) methods offer an alternative, but existing models struggle to adapt to road scenes. Furthermore, most LLIE models rely on supervised training but are heavily constrained by the lack of low-light and normal-light paired datasets. In particular, obtaining paired datasets for driving scenes is extremely challenging. To address these issues, this paper proposes Squeeze-EnGAN, a memory-efficient, GAN-based LLIE method capable of unsupervised learning without paired image datasets. Squeeze-EnGAN incorporates a fire module into a U-net architecture, substantially reducing the number of parameters and Multiply-Accumulate Operations (MACs) compared to its base model, EnlightenGAN. Additionally, Squeeze-EnGAN achieves real-time performance on devices like Jetson Xavier (0.061 s). Significantly, enhanced images improve object detection performance over original images, demonstrating the model’s potential to aid high-level vision tasks in intelligent vehicles. © 2025 by the authors.",autonomous driving; generative adversarial network; low-light image enhancement; unsupervised learning; Health risks; Intelligent robots; Laser beams; Magnetic levitation vehicles; Risk assessment; SLAM robotics; Unsupervised learning; Adversarial networks; Autonomous car; Autonomous driving; Environmental information; Low-costs; Low-high; Low-light image enhancement; Low-light images; Memory efficient; RGB cameras; article; autonomous vehicle; camera; deep learning; diagnosis; drone; generative adversarial network; human; image enhancement; learning; light; memory; sensor; Generative adversarial networks
Scopus,"Yan, C.; Xue, S.; Han, H.; Li, M.",Research on the appearance inspection technology of screw assembly quality based on machine vision,,2025,,,,10.1145/3708394.3708430,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218185910&doi=10.1145%2f3708394.3708430&partnerID=40&md5=7c617dcba050693990b8c4a157b5c297,"Aiming at the demand for screw visual inspection in the appearance inspection of a certain complex product assembly quality, a real-Time multi-scale small object detection and classification model (RMSDC) is studied. RMSDC uses YOLOv8n as the base network structure. SPD-Conv consisting of a space-To-depth (SPD) layer and a non-strided convolution layer is used to improve the performance of the model in low-pixel images and small target detection. A convolution attention mixing (CAMixing) block is introduced in the middle of the backbone network and the neck network, which combines the attention mechanism with the multi-scale convolution to enhance the multi-scale feature extraction capability and improve the model accuracy. Choose an ultra-lightweight and effective dynamic upsampler (DySample), which is lightweight and can preserve the high-level feature semantic information of small targets as much as possible. The deformable convolution v4 (DCNv4) is fused to the dynamic head (Dyhead) to improve the generalization ability of the model in terms of shape transformation. Through comparative experiments, RMSDC in this paper improves the recall (R), mAP0.5 and mAP0.5:0.95 by 3.7%, 1.5%, and 1.8% compared to the baseline model on the experimental dataset, and mAP0.5 reaches 98.5%. The excellent performance of RMSDC has also been demonstrated on the ROSD dataset.  © 2024 Copyright held by the owner/author(s).",CAMixing; DCNv4; Screw Detection; SPD; YOLOv8; Cams; Image enhancement; Inspection equipment; Machine vision; Convolution attention mixing; Deformable convolution v4; Detection models; Multi-scales; Object classification; Real- time; Screw detection; Small object detection; Space-to-depth; YOLOv8; Screws
Scopus,"Himeur, Y.; Aburaed, N.; Elharrouss, O.; Varlamis, I.; Atalla, S.; Mansoor, W.; Al-Ahmad, H.",Applications of knowledge distillation in remote sensing: A survey,,2025,,,,10.1016/j.inffus.2024.102742,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207881950&doi=10.1016%2fj.inffus.2024.102742&partnerID=40&md5=70eb0c6b55ed0474e44620594ec7ed83,"With the ever-growing complexity of models in the field of remote sensing (RS), there is an increasing demand for solutions that balance model accuracy with computational efficiency. Knowledge distillation (KD) has emerged as a powerful tool to meet this need, enabling the transfer of knowledge from large, complex models to smaller, more efficient ones without significant loss in performance. This review article provides an extensive examination of KD and its innovative applications in RS. KD, a technique developed to transfer knowledge from a complex, often cumbersome model (teacher) to a more compact and efficient model (student), has seen significant evolution and application across various domains. Initially, we introduce the fundamental concepts and historical progression of KD methods. The advantages of employing KD are highlighted, particularly in terms of model compression, enhanced computational efficiency, and improved performance, which are pivotal for practical deployments in RS scenarios. The article provides a comprehensive taxonomy of KD techniques, where each category is critically analyzed to demonstrate the breadth and depth of the alternative options, and illustrates specific case studies that showcase the practical implementation of KD methods in RS tasks, such as instance segmentation and object detection. Further, the review discusses the challenges and limitations of KD in RS, including practical constraints and prospective future directions, providing a comprehensive overview for researchers and practitioners in the field of RS. Through this organization, the paper not only elucidates the current state of research in KD but also sets the stage for future research opportunities, thereby contributing significantly to both academic research and real-world applications. © 2024 Elsevier B.V.",Knowledge distillation; Model and data distillation; Model compression; Remote sensing; Urban planning and precision agriculture; Balance model; Distillation method; Knowledge distillation; Model and data distillation; Model compression; Modeling accuracy; Performance; Precision Agriculture; Remote-sensing; Urban planning and precision agriculture
Scopus,"Xiang, X.; Zhang, G.; Huang, L.; Zheng, Y.; Xie, Z.; Sun, S.; Yuan, T.; Chen, X.",Research on infrared small target pedestrian and vehicle detection algorithm based on multi-scale feature fusion,,2025,,,,10.1007/s11554-024-01607-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212828304&doi=10.1007%2fs11554-024-01607-5&partnerID=40&md5=697b12176d51795b2c5d51a0fd6ac72e,"Infrared imaging technology relies on detecting the electromagnetic waves emitted by an object's spontaneous thermal radiation for imaging. It can overcome the adverse effects of complex lighting conditions on the detection of pedestrians and vehicles on the road. To address the issues of low accuracy and missed detection in visual detection under complex traffic conditions, such as during rain, snow, or at night, a pedestrian and vehicle detection model using infrared imaging has been proposed. This model improves the neck network and incorporates an attention mechanism. First, by adding a multi-scale feature fusion small-object detection layer to the model's neck, enhancing the capture of detailed information about small infrared objects and reducing missed detections. Second, a novel dual-layer routing attention mechanism is designed, allowing the model to focus on the most relevant feature areas and improving the detection accuracy of small infrared objects. Next, the CARAFE upsampling method is used for adaptive upsampling and context information fusion, which enhances the model's ability to reorganize features and capture details. Finally, a lightweight CSPPC module is constructed using partial convolutions to replace the C2f module in the neck network, which improves the model's frame rate. Experimental results show that, compared to the baseline model, BCC-YOLOv8n improves precision, recall, mAP@0.5, and mAP@0.5:0.95 by 1.4%, 4.8%, 5.3%, and 4.5%, respectively, while reducing the number of parameters by approximately 7%. Additionally, a frame rate of 70.8 FPS was achieved, satisfying the requirements for real-time detection. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Double layer routing attention; Infrared small target detection; Model lightweight; Pedestrian and vehicle detection; Small target detection layer; Image coding; Image enhancement; Spontaneous emission; Thermography (imaging); Double layer routing attention; Double layers; Infrared small target detection; Infrared small targets; Model lightweight; Pedestrian detection; Routings; Small target detection; Small target detection layer; Vehicles detection; Infrared radiation
Scopus,"Patil, O.C.; Nashimath, S.S.; Nadaf, T.; Patil, P.; Kallimani, R.",Development and Execution of an AI-Driven Adaptive Cruise Control with Collision Detection and Speed Locking Mechanism,,2025,,,,10.1109/ICDCECE65353.2025.11035305,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009595426&doi=10.1109%2fICDCECE65353.2025.11035305&partnerID=40&md5=e321a42107bd81d8ac80e4d2c5b68902,"By reducing the need for constant driver intervention, adaptive cruise control (ACC) systems increase driving comfort and safety. This paper introduces an AI-based ACC system that combines dynamic speed adjustment and real-time obstacle detection. The system, which is based on YOLOv3 object detection and voice recognition technology, allows the car to respond to its surroundings and autonomously maintain a predetermined speed. To prevent collisions, the system applies the brakes or lowers speed when it detects obstacles. For BLDC motor control, the architecture combines mathematical modelling, feature extraction, and image processing. Reinforcement learning with real-time feedback optimizes speed regulation. High detection accuracy 91.4%, low response latency 0.82s, and effective speed locking under various road conditions are demonstrated by the experimental results. The system presented provides an intelligent, safe, and intuitive cruise control system, advancing semi-autonomous vehicle technologies. Future improvements and limitations are discussed to enhance real-world applicability.  © 2025 IEEE.",cruise control; image and voice recognition; ml algorithms; obstacle detection; speed control; YOLO model; Adaptive control systems; Adaptive cruise control; Computer control; Computer control systems; Locks (fasteners); Object detection; Object recognition; Speed control; Speed regulators; Adaptive cruise control systems; Collision detection; Collision speed; Driving comfort; Driving safety; Dynamic speed; Locking mechanism; Ml algorithms; Obstacles detection; YOLO model; Obstacle detectors
Scopus,"Dong, Y.; Hu, C.; Quaye, J.A.; Lu, N.; Zhao, L.",Intelligent identification of carbonate components based on deep learning,,2025,,,,10.1007/s10347-024-00694-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217795712&doi=10.1007%2fs10347-024-00694-x&partnerID=40&md5=ed94a64fb3492ccae122c9afd64c9abe,"Many applications in carbonate facies analysis require a compositional analysis of the grain, cement, and pore types. Image analysis based on deep learning models can help automatically extract features for identifying objects and interpreting carbonate thin sections. However, small objects are detected with a lower average precision than medium and large objects due to the loss of information during the deep convolution operation. Existing object detection algorithms cannot simultaneously achieve a high detection accuracy and detection speed, which hinders the further study of petroliferous carbonate successions. You Only Look Once version 5 (YOLOv5) is an advanced, fast, and accurate detector. In this study, the applicability and performance of YOLOv5-based object detection approaches were assessed by conducting a carbonate compositional analysis. The training data comprised more than 6800 individually labeled objects from 1000 carbonate petrographic images. The dataset was grouped into nine different classes for the object detection tasks. Even with a small amount of training, the YOLOv5 could achieve a precision of 99.0%, a recall rate of 98.4%, and a mean average precision of 91.6% for object detection by combining the scale sequence feature pyramid network. The study not only meets the accuracy requirements of identifying multi-scale objects, particularly small objects, but also meets the detection speed requirement, with a significant application potential in identifying carbonate components. © Springer-Verlag GmbH Germany, part of Springer Nature 2025.",Artificial intelligence; Carbonate rocks; Deep learning; Intelligent identification; Object detection; algorithm; carbonate rock; chemical composition; facies analysis; machine learning
Scopus,"Bai, R.; Song, G.; Wang, Q.",YOLORemote: Advancing Remote Sensing Object Detection by Integrating YOLOv8 With the CE-WA-CS Feature Fusion Approach,,2025,,,,10.1109/JSTARS.2025.3543951,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003183428&doi=10.1109%2fJSTARS.2025.3543951&partnerID=40&md5=77260eea5c1c44649b03cd24aa05d899,"The rapid development iteration of YOLO models has spurred extensive research into specialized adaptations tailored for remote sensing object detection (RSOD). Typically, these adaptations involve modifying specific YOLO versions to excel in dataset-specific benchmarks. Such efforts primarily focus on enhancing model performance on specific datasets, a trend that, while beneficial for achieving high leaderboard standings, frequently overlooks the critical aspects of generalizability, scalability, and transferability. Consequently, there is a significant gap in the research concerning the development of universally applicable modules that are easy to adapt across various baseline YOLO models—an essential factor for practical deployment and widespread adoption in real-world applications. This study addresses these shortcomings by introducing a novel feature fusion method, context enhancement (CE), weight adjustment-based spatial attention (WA), and channel shuffling (CS)—collectively termed CE-WA-CS, specifically designed for RSOD. This research rigorously tests the proposed feature fusion method across various YOLO models, substantiating its versatility and effectiveness in enhancing RSOD. Notably, in comparative performance evaluations involving several YOLO architectures, integrating our feature fusion method with YOLOv8 emerged as the most balanced in terms of accuracy, computational complexity, and inference speed. This optimal combination has been designated as YOLORemote. YOLORemote not only exemplifies a significant improvement in performance metrics but also demonstrates a practical balance that is crucial for real-time applications. © 2008-2012 IEEE.",and channel shuffling (CE-WA-CS); Context enhancement; remote sensing object detection (RSOD); weight adjustment-based spatial attention; YOLORemote; Benchmarking; Cerium alloys; Cesium alloys; Object detection; Remote sensing; Context enhancement-WA-channel shuffling; Excel; Feature fusion method; Features fusions; Modeling performance; Objects detection; Real-world; Remote sensing object detection; Remote-sensing; Yoloremote; benchmarking; comparative study; data set; numerical model; performance assessment; remote sensing; Proximity sensors
Scopus,"Hartmann, N.; Rüter, J.; Jünger, F.",Ensuring Safety of Deep Learning Components Using Improved Image-Level Property Selection for Monitoring,,2025,,,,10.2514/6.2025-2512,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001290345&doi=10.2514%2f6.2025-2512&partnerID=40&md5=beaaee834f7f75e8124f59a370254279,"Environment perception will play an important role for autonomous aircraft, e.g., to be able to prevent mid-air collisions or to find emergency landing spots. Deep Learning (DL) based approaches for computer vision often give state-of-the-art results but are currently not certifiable for aviation because of their data driven training process and their black-box character. Runtime monitoring of the model input could mitigate this problem by ensuring that the model output is only considered when the input is deemed to be suitable. On the one hand, this could be achieved by monitoring operational parameters described by an Operational Design Domain (ODD) as suggested by the European Union Aviation Safety Agency (EASA). On the other hand, unsafe input data might be rejected based on its direct impact on the model performance using Out-of-Model-Scope (OMS) detection. However, performing either ODD monitoring or OMS detection for high-dimensional input data such as camera images is a non-trivial task as it is unclear which properties of an input image should be monitored. In this work, we describe a process to derive a set of suitable low-level image properties that can be used to monitor the input of a DL component. We show that the features selected by the process can be used by a runtime monitor to improve the safety of a DL component by filtering images that violate the ODD boundaries or are OMS. © 2025, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",Aircraft landing; Black-box testing; Civil aviation; Input output programs; Network security; Training aircraft; Autonomous aircraft; Data driven; Design domains; Emergency landing; Environment perceptions; Learning-based approach; Mid-air collisions; Operational design; Property; State of the art; Aircraft accidents
Scopus,"Van Lier, M.; Van Leeuwen, M.; Van Manen, B.; Kampmeijer, L.; Boehrer, N.",Evaluation of Spatio-Temporal Small Object Detection in Real-World Adverse Weather Conditions,,2025,,,,10.1109/WACVW65960.2025.00094,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005027292&doi=10.1109%2fWACVW65960.2025.00094&partnerID=40&md5=73278bf860b1e0f932a7ee34fd839f90,"Deep learning-based object detection methods, such as YOLO, are promising for surveillance applications. However, detecting small objects in large-scale scenes with cluttered backgrounds and adverse weather remains challenging. Recent advancements leverage spatio-temporal information to enhance small object detection, yet the impact of (temporal) adverse weather conditions on such methods remains largely unexplored due to the lack of comprehensive evaluation datasets. This paper evaluates the performance of spatio-temporal YOLOv8 (TYOLOv8) for detecting small objects in real-world adverse weather conditions, comparing it to spatial YOLOv8 and the 3FN moving object detection method. Additionally, we propose haze augmentation to improve object detection performance in challenging hazy weather. Due to the lack of suitable datasets for evaluation, this paper introduces a novel real-world video dataset for small object detection, referred to as Nano-VID-weather, with an average object size of 16.42 pixels, consisting of a Tiny Objects subset and three challenging weather subsets: Wind, Rain and Haze. Our findings reveal that TYOLOv8 is resilient to real-world adversarial weather conditions, like wind, rain, and haze. Notably, on average TYOLOv8 outperformed both 3FN and YOLOv8 with +0.21mAP across all our subsets. These results demonstrate that TYOLOv8 can enhance surveillance capabilities for small object detection under real-world adverse weather conditions.  © 2025 IEEE.",adverse weather; dataset; object detection; small object detection; temporal object detection; yolov8; Deep learning; Object recognition; Rain; Snow; Adverse weather; Condition; Dataset; Objects detection; Real-world; Small object detection; Temporal object detection; Temporal objects; Yolov8; Object detection
Scopus,"Xu, F.; Chen, C.; Shang, Z.; Ma, K.-K.; Wu, Q.; Lin, Z.; Zhan, J.; Shi, Y.",Deep Multi-Modal Ship Detection and Classification Network,,2025,,,,10.1109/TCSVT.2024.3519569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212780154&doi=10.1109%2fTCSVT.2024.3519569&partnerID=40&md5=4904e0d89df95892112b4c5404b8b882,"While a majority of single-modal ship detectors solely rely on RGB images, a novel multi-modal real-time transformer-based ship detection and classification method, called the MM-ShipNet, is proposed in this paper that integrates the data acquired from three modalities—i.e., RGB camera, radar, and automatic identification system (AIS). First, a bounding box is generated based on the position information from radar and ship’s actual size information from AIS. This physical information are fused and projected onto the camera-acquired RGB image frame. Each bounding box is then possibly weighted depending on the ship size presented on the image. The generated weighted ship masks (WSMs) will be exploited for facilitating ship classification task. In the second stage of MM-ShipNet, multi-modal detection transformer (MM-DETR) introduces an multi-modal cross-scale encoder (MCE) for improving ship detection and classification performance. Our MCE exploits a dual-flow structure to fuse the features extracted from the WSMs and the RGB images under different scales. Since our method is the first work entailing three aforementioned modalities, no such dataset with all modalities can be found in the open source. Thus, we construct a multi-modal ship dataset, termed MMShips, as another contribution. Our MMShips dataset comprises 9,513 camera-acquired real-life maritime RGB images and their aligned ship masks generated from radar and AIS. Experimental results clearly demonstrate that our MM-ShipNet significantly outperforms multiple state-of-the-art single-modal and multi-modal ship detectors. © 1991-2012 IEEE.",automatic identification system; multi-modality; radar; Ship detection and classification; transformer; Image coding; Photointerpretation; Radar imaging; Remote sensing; Scales (weighing instruments); Ships; Automatic identification system; Bounding-box; Detection networks; Multi-modal; Multi-modality; RGB images; Ship classification; Ship detection; Single-modal; Transformer; Image enhancement
Scopus,"Yu, X.; Yan, P.; Zheng, S.; Du, Q.; Wang, D.",YOLOv8-WTDD: multi-scale defect detection algorithm for wind turbines,,2025,,,,10.1007/s11227-024-06487-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207046338&doi=10.1007%2fs11227-024-06487-x&partnerID=40&md5=49af27c3103a9f20e92c3f1f4fee074f,"In addressing the challenges of wind turbine defect detection, such as different defect scales in UAV aerial photography, interference from different lighting conditions, and small-sized target defects leading to low detection accuracy and inaccurate localization, a YOLOv8-WTBB model based on YOLOv8 is proposed. Firstly, the Diverse Branch Block is designed to enhance multi-scale feature fusion capabilities. Next, the Receptive-Field Attention Convolution is introduced to focus on the spatial features of the receptive field, increasing the distinction between target features and the surrounding environment. Finally, introducing the Minimum Point Distance Intersection over the Union bounding box regression loss function notably improves localization accuracy in object detection and accelerates model convergence. Experimental results demonstrate that the proposed algorithm significantly outperforms the baseline network, with a 4.3% improvement in mean average precision, achieving 89.1%, and a 7.4% increase in mean average recall, reaching 84.8%. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Deep learning; Renewable energy; Wind power generation; Wind turbine defect detection; YOLO; Aerial photography; Aircraft detection; Deep learning; Object detection; Wind power; Wind turbines; Deep learning; Defect detection; Defect detection algorithm; Lighting conditions; Multi-scales; Receptive fields; Renewable energies; Wind power generation; Wind turbine defect detection; YOLO; Windmill
Scopus,"Wang, H.; Qian, H.",SR-DAYOLOv8: cross-domain adaptive object detection based on super-resolution domain classifier,,2025,,,,10.1007/s00530-024-01594-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212781889&doi=10.1007%2fs00530-024-01594-4&partnerID=40&md5=c1535077cdfe06a158e84e2326651373,"Object detection is a fundamental task of environment perception in traffic road scenarios, and its accurate detection results are of great significance for improving the reliability of autonomous driving, optimizing traffic flow management, and enhancing road safety. However, the problem of domain offset between different traffic road scenarios leads to a poor generalization of the target detector. To address this challenge, we propose a new unsupervised domain adaptation object detection algorithm, SR-DAYOLOv8. Specifically, the algorithm contains three effective components. First, we use the source and target domains to train an unpaired image-to-image translator to generate a target-like domain, using the target-like domain as input to compensate for image-level differences. Second, to correct cross-domain discrepancies, we add a new detector for the target-like domain, enabling it to conduct supervised learning training, just like the source domain. Finally, we design a super-resolution domain classifier to obtain domain adaptive feature maps. Domain-invariant features are extracted through image-level adaptation and instance-level adaptation, and consistency regularization is employed to optimize the overall alignment effect. We conducted experiments on Cityscape, Foggy Cityscape, KITTI, SIM10K, and BDD100K datasets for different domain offset scenarios. Experimental results show that our method can improve target detection performance in different domain offset scenarios and outperform other state-of-the-art algorithms. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Domain offset; Object detection; Super-resolution domain classifier; Unsupervised domain adaptation; Advanced traffic management systems; Highway accidents; Highway administration; Highway traffic control; Image enhancement; Motor transportation; Object detection; Street traffic control; Autonomous driving; Cross-domain; Different domains; Domain adaptation; Domain offset; Environment perceptions; Objects detection; Super-resolution domain classifier; Superresolution; Unsupervised domain adaptation; Supervised learning
Scopus,"Song, L.; Jin, X.; Han, J.; Yao, J.",Pedestrian Re-Identification Algorithm Based on Unmanned Aerial Vehicle Imagery,,2025,,,,10.3390/app15031256,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217526239&doi=10.3390%2fapp15031256&partnerID=40&md5=12a223dcf6ddd7d1785d0f2476dfc102,"Pedestrian re-identification in complex scenarios is often hindered by challenges such as viewpoint diversity, background interference, and behavioral complexity, which traditional methods struggle to address effectively in wide-area surveillance. Unmanned Aerial Vehicles (UAVs) offer a promising solution to this problem due to their flexibility and extensive coverage capabilities. However, UAV aerial images introduce additional challenges, including significant viewpoint variations and the complexity of pedestrian behaviors. To address these issues, this paper proposes a Transformer-based model that integrates a multi-scale graph convolution network (MU-GCN) with a non-local attention mechanism to address these challenges. A MU-GCN enhances feature extraction by employing graph convolutional networks to improve feature representation after extracting detailed features at various scales through multi-scale convolution kernels. This strengthens the model’s focus on local information. Meanwhile, the non-local attention mechanism enhances the model’s capacity to capture global contextual information by modeling dependencies between distant regions in the image. This approach is better suited for the unique characteristics of UAV aerial imagery. Experimental results demonstrate that, compared to the baseline model, the proposed method achieves improvements of 9.5% in mean average precision (mAP) and 4.9% in Rank-1 accuracy, validating the effectiveness of the model. © 2025 by the authors.",aerial imagery; graph convolutional network; pedestrian re-identification; transformer; Image enhancement; Unmanned aerial vehicles (UAV); Aerial imagery; Aerial vehicle; Attention mechanisms; Convolutional networks; Graph convolutional network; Identification algorithms; Multi-scales; Nonlocal; Pedestrian re-identification; Transformer; Aerial photography
Scopus,"Yu, H.; Zhang, R.; Sun, H.; Cao, Z.; Yang, B.; Zhang, J.; Liu, G.",RADCI: A Synchronized Radar-RGBT Object Detecting-Tracking Dataset And A Benchmark,,2025,,,,10.1109/ICASSP49660.2025.10890097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009586674&doi=10.1109%2fICASSP49660.2025.10890097&partnerID=40&md5=393a3c510e4033028d03a5c0c51c8eb7,"High-quality perception is crucial in autonomous driving and monitoring systems, where millimeter-wave radar and infrared cameras play important roles due to their robustness and reliability under harsh conditions. Both technologies can serve as low-cost supplements to optical image detection, improving overall system robustness. However, there is currently a lack of widely applicable feature-level fusion methods and multimodal datasets to effectively integrate visible light with these two heterogeneous data types for multiple tasks. In this work, we collect a new multimodal dataset, RADCI8, which synchronizes data from a camera, an infrared camera, and a radar for target detection and tracking. The dataset includes 2D image annotations, radar RAD tensor data with distance, angle, and Doppler information, as well as target ID annotations in both data formats. In addition, to address the incomplete use of radar data in previous fusion algorithms, we propose a detection method that fuses image and radar features using feature concatenation and an attention mechanism. Our proposed algorithm achieves 51.5% AP with an IOU of 50:95 on 2D bounding box prediction, significantly improving average detection accuracy over vision-based methods and maintaining robustness even when a single sensor degrades. © 2025 IEEE.",Dataset; Multimodal; Radar Processing; Sensor Fusion; Benchmarking; Cameras; Feature extraction; Infrared devices; Infrared radiation; Object detection; Radar imaging; Radar target recognition; Radar tracking; Sensor data fusion; Target tracking; Tracking radar; Dataset; High quality; Infra-red cameras; Infrared cameras; Multi-modal; Multi-modal dataset; Object detecting; Quality perceptions; Radar processing; Sensor fusion; Geometrical optics; Image enhancement; Millimeter waves
Scopus,"Wang, P.; Luo, Y.; Zhu, Z.",FDI-YOLO: Feature disentanglement and interaction network based on YOLO for SAR object detection,,2025,,,,10.1016/j.eswa.2024.125442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205029086&doi=10.1016%2fj.eswa.2024.125442&partnerID=40&md5=d1358dae817be331fc97dea277711a64,"Synthetic Aperture Radar (SAR) object detection is one of the key measures to ensure maritime traffic and safety. However, SAR images contain a large amount of speckle noise, which poses a challenge to traditional deep learning methods for feature extraction and processing. Therefore, we propose a YOLO-based feature disentanglement and interaction network for SAR object detection (FDI-YOLO). First, FDI-YOLO proposes a reversible cross stage partial network (RCSPNet) as the backbone. The RCSPNet uses reversible transformations to retain more complete feature information for feature extraction and decompose it into feature maps of different dimensions. Then, we propose a structure with cross-scale depth feature interaction (CDFI), which captures the local texture and global semantic information of the in-scale features using crossover frequency semantic perception (CFSP), and then strengthens the linking of the cross-scale features through bidirectional information interaction. Finally, we use an adaptive object detection head and a bounding box regression loss with a dynamic focusing mechanism to further improve the detection capability of FDI-YOLO for SAR images. We conducted experiments on three publicly available SAR datasets, SSDD, ISDD, and HRSID. On these datasets, we achieve F1 scores of 98.1%/88.6%/88.5%, AP50 scores of 98.7%/90.3%/90.9%, and AP50-95 scores of 71.0%/42.4%/64.3%, respectively. The experimental results show that FDI-YOLO is able to perform the task of SAR object detection well with less computational resources. © 2024 Elsevier Ltd",Attention mechanism; Object detection; Reversible column network; SAR; YOLOv8; Deep learning; Image enhancement; Object detection; Radar target recognition; Attention mechanisms; Features extraction; Interaction networks; Maritime safety; Network-based; Objects detection; Radar objects; Reversible column network; Synthetic aperture radar images; YOLOv8; Object recognition
Scopus,"Liu, G.; Yao, S.; Zhou, Y.; Liu, D.; Chang, B.",Boundary-Guided Global–Local Feature Fusion Network for Polyp Segmentation,,2025,,,,10.1109/TIM.2025.3551578,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000367740&doi=10.1109%2fTIM.2025.3551578&partnerID=40&md5=972b45641fbd753a1be7c76965fe7f20,"Colon polyp segmentation can assist physicians in screening colonoscopy images, which is crucial in preventing colorectal cancer. Due to the limited area occupied by small object polyp objects in images, there is a high risk of overlooking them, which makes it one of the more challenging aspects to address. Additionally, the current segmentation of small object polyps also faces difficulties such as boundary blurring, diverse lesion shapes, and uneven image brightness. While deep learning methods based on convolutional neural networks (CNNs) have been successfully applied to polyp segmentation tasks, three significant challenges persist: 1) limited ability to extract boundary information; 2) inadequate robustness capture of global context information; and 3) insufficient ability for integrating global and local information. To address the issues mentioned above, we propose the boundary-guided global–local feature fusion network for small polyp segmentation (BGGL-Net), with the following contributions: 1) the local information encoder (LIE) and boundary feature extraction module leverage convolutional blocks and Laplacian operators to mine feature boundaries and fine-grained detail features; 2) we design a global information fusion module to enhance the model’s representational capacity and acquire rich and accurate global information; and 3) boundary-guided module (BGM) using a cross-attention mechanism, capturing the inherent relationship between the feature and establishes long-range dependencies between global- and low-level features. We enhance boundary accuracy by employing local boundary features to guide the global features, facilitating the effective fusion of global and local information. In the experiment, we compared ten state-of-the-art (SOTA) networks with BGGL-Net. The BGGL-Net achieves the highest segmentation accuracy on small object polyp datasets. Concerning generalization performance, the BGGL-Net outperforms CaraNet by up to 12.4% in the mDice metric on the ETIS-LaribPolypDB* dataset. © 2025 IEEE. All rights reserved.",Convolutional neural networks (CNNs); feature fusion; Laplacian operators; small-object polyp segmentation; Transformer; Data fusion; Deep neural networks; Image coding; Image enhancement; Image segmentation; Laplace transforms; Convolutional neural network; Features fusions; Global and local informations; Global-local; Laplacian operator; Local feature; Polyp segmentation; Small object polyp segmentation; Small objects; Transformer; Convolutional neural networks
Scopus,"Kiranmai, R.; Deeptha, R.; Purnachand, K.",A Review on Object Detection in Traffic Scenarios Using Deep Learning Models,,2025,,,,10.1109/ICVADV63329.2025.10961569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004409395&doi=10.1109%2fICVADV63329.2025.10961569&partnerID=40&md5=98ff0c00e33fd1a00a8f3d3827ba73fc,"Intelligent vehicle visual perception technology can assist automated driving systems in traffic scenarios by helping to recognize complicated situations quickly and accurately, which is necessary for safe and collision-free driving. Deep learning (DL)-based object identification has been a major advancement in automated driving. A survey is done in this paper on object detection in traffic scenes. This article reviews 30 research works with varied contributions. This work reviews the elected databases and analyses the varied object detection models in each paper. It further reviews the performance of each work and analyzes its maximal performance as well. In addition, varied datasets adopted in each considered work are examined. Finally, research gaps in object detection in traffic scenes are determined, which should be undertaken in future.  © 2025 IEEE.",Accuracy; Computer vision; Intelligent vehicle; Object Detection; YOLO; Automatic identification; Object detection; Object recognition; Accuracy; Automated driving systems; Collision-free; Learning models; Object identification; Objects detection; Performance; Traffic scene; Visual perception; YOLO; Deep learning
Scopus,"Chen, Z.; Yang, K.; Wu, Y.; Yang, H.; Tang, X.",HCLT-YOLO: A Hybrid CNN and Lightweight Transformer Architecture for Object Detection in Complex Traffic Scenes,,2025,,,,10.1109/TVT.2024.3496513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001061677&doi=10.1109%2fTVT.2024.3496513&partnerID=40&md5=c4305cda1ee05ea791339028e9ec5543,"The swift and accurate detection of traffic signs in traffic scenes is a pivotal aspect of environmental perception technology in autonomous driving systems. Traffic signs provide essential road information and regulatory instructions, which are critical to ensuring road safety. This paper presents the HCLT-YOLO model to address the challenges of false alarms and missed detections in complex traffic environments. Specifically, we propose a novel hybrid CNN-transformer network architecture that efficiently integrates both local and global features, thereby improving traffic sign feature representation. To further enhance the modelâs sensitivity to small traffic signs, we optimize the structure by introducing a dedicated small-object detection layer through upsampling and by leveraging SIoU to improve detection accuracy and computational efficiency. However, the addition of the small object detection layer and the Transformer module increases the overall computational complexity and parameter count, potentially affecting real-time performance. To address this issue, we introduce the DG-C2f module, which employs linear transformations for feature mapping, streamlining the convolution process and enhancing real-time feasibility. Experimental evaluations on the GTSDB and TT100K datasets demonstrate that the proposed model improves detection accuracy by 2.5% and 6.8%, respectively, compared to YOLOv8s models. Notably, the detection accuracy for small traffic signs improved significantly, by 6.9% and 11.7%, respectively. Additionally, processor-in-the-loop experiments on the NVIDIA Jetson AGX Orin show that the model achieves an inference speed of 46 FPS, meeting the real-time requirements for in-vehicle applications. © 2024 IEEE.",Autonomous driving; deep learning; lightweight transformer; traffic sign detection; Image coding; Image compression; Image segmentation; Linear transformations; Autonomous driving; Deep learning; Detection accuracy; Driving systems; Environmental perceptions; Lightweight transformer; Objects detection; Small object detection; Traffic scene; Traffic sign detection; Traffic signs
Scopus,"Mao, G.; Wang, K.; Du, H.; Huang, B.; Ren, X.; Fu, T.; Zhang, Z.",SRS-YOLO: Improved YOLOv8-Based Smart Road Stud Detection,,2025,,,,10.1109/TITS.2025.3545942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000442475&doi=10.1109%2fTITS.2025.3545942&partnerID=40&md5=9a3615a7baff831cfcd146685ccca2bc,"Smart road studs have been extensively deployed as road safety and data collection devices. Accurate and reliable detection of smart road studs and its further integration into the perception and control modules of connected and autonomous vehicles (CAVs) undoubtedly benefit road boundary detection, localization of CAVs and augument the safety of CAVs’ driving. This work investigates real-time, accurate and reliable detection of smart road studs, which is a challenging task for CAVs because existing methods fail to achieve accurate and real-time smart road stud detection, especially in harsh road environment. To address these challenges, we first build a real-world smart road stud dataset, and then propose and validate a lightweight and efficient smart road stud detection model based on the you only look once 8th version (YOLOv8), called SRS-YOLO. First, a Squeezeand-Excitation (SE) attention module is used to improve the coarse-to-fine (C2F) module to differentiate the channel importance of feature maps and improve the detection accuracy of smart road studs. Second, a novel downsampling module (DownS) that integrates the average pooling and the max pooling is designed to reduce the number of parameters and minimize information loss during the downsampling process. Third, the loss function is replaced with the Normalized Wasserstein Distance (NWD) loss to alleviate the sensitivity to location deviations when computing the loss for small targets. The experimental results demonstrate that the proposed SRS-YOLO outperforms other state-of-the-art methods, and achieves a 87.92% mean average precision at a real-time speed of 78 frames. © 2000-2011 IEEE.",attention mechanism; connected and autonomous vehicle; real-time detection system; smart road stud detection; SRS-YOLO; Image coding; Studs (structural members); Attention mechanisms; Autonomous Vehicles; Connected and autonomous vehicle; Detection system; Real- time; Real-time detection; Real-time detection system; Reliable detection; Smart road stud detection; SRS-YOLO; Studs (fasteners)
Scopus,"Kaixuan, L.; Xiaofeng, L.; Qiang, C.; Zejiang, Z.",YOLOv8-GAIS: improved object detection algorithm for UAV aerial photography,,2025,,,,10.12086/oee.2025.240295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004051009&doi=10.12086%2foee.2025.240295&partnerID=40&md5=2a6d108edf68c604405d4b5a33f198f3,"To address the issue of complex backgrounds in dim scenes, which cause object edge blurring and obscure small objects, leading to misdetection and omission, an improved YOLOv8-GAIS algorithm is proposed. The FAMFF (four-head adaptive multi-dimensional feature fusion) strategy is designed to achieve spatial filtering of conflicting information. A small object detection head is incorporated to address the issue of large object scale variation in aerial views. The SEAM (spatially enhanced attention mechanism) is introduced to enhance the network's attention and capture ability for occluded parts in low illumination situations. The InnerSIoU loss function is adopted to emphasize the core regions, thereby improving the detection performance of occluded objects. Field scenes are collected to expand the VisDrone2021 dataset, and the Gamma and SAHI (slicing aided hyper inference) algorithms are applied for preprocessing. This helps balance the distribution of different object types in low-illumination scenarios, optimizing the model's generalization ability and detection accuracy. Comparative experiments show that the improved model reduces the number of parameters by 1.53 MB, and increases mAP50 by 6.9%, mAP50-95 by 5.6%, and model computation by 7.2 GFLOPs compared to the baseline model. In addition, field experiments were conducted in Dagu South Road, Jinnan District, Tianjin City, China, to determine the optimal altitude for image acquisition by UAVs. The results show that, at a flight altitude of 60 m, the model achieves the detection accuracy of 77.8% mAP50. © 2025 Chinese Academy of Sciences. All rights reserved.",four-head adaptive multi-dimensional feature fusion; low-brightness image; object detection; unmanned aerial vehicle; YOLOv8
Scopus,"Gong, P.; Zheng, K.; Jiang, Y.; Zhao, H.; Liang, X.; Feng, Z.; Huang, W.",Spatial Orientation Relation Recognition for Water Surface Targets,,2025,,,,10.3390/jmse13030482,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001112842&doi=10.3390%2fjmse13030482&partnerID=40&md5=3e762ab13b6fefec73847a240c2728fb,"Recently, extensive research efforts have concentrated on comprehending the semantic features of images in the field of computer vision. In order to address the spatial orientation relations among water surface targets (WSTs) in an image, which is a fundamental semantic feature, this paper focused on the recognition of spatial orientation relations. We first developed the water surface target spatial orientation vector field (WST-SOVF) algorithm, a novel end-to-end methodology, to recognize these spatial orientation relations among WSTs in an image. The WST-SOVF algorithm encodes the spatial orientation relation into the learning framework of a new deep convolutional neural network model, which comprises two distinct branches: the T-branch and the S-branch, both designed for the spatial feature extraction. The T-branch employs keypoint estimation to identify central points and classify the WST categories, while the S-branch constructs a spatial orientation vector field between WSTs, where each pixel in the field encodes the spatial orientation angle between two separated WSTs and collectively determines the category of spatial orientation. A fusion module was also designed to integrate the spatial feature obtained from both branches, thereby generating a comprehensive triple list that provides not only all the WSTs and their spatial orientation relations, but also their associated confidence levels. We performed a comparative evaluation of our WST-SOVF algorithm based on Huawei’s “Typical Surface/Underwater Target Recognition” dataset and the results demonstrated the outstanding performance of WST-SOVF algorithm. © 2025 by the authors.",deep convolutional neural network; recognition; spatial orientation relation; spatial orientation vector field; water surface target; Deep neural networks; Image coding; Vectors; Water content; Convolutional neural network; Orientation relations; Orientation vector fields; Recognition; Spatial orientation relation; Spatial orientation vector field; Spatial orientations; Surface target; Water surface; Water surface target; Convolutional neural networks
Scopus,"Xie, B.; Li, H.; Luan, Z.; Lei, Z.; Li, X.; Li, Z.",Lightweight coal miners and manned vehicles detection model based on deep learning and model compression techniques: A case study of coal mines in Guizhou region,,2025,,,,10.13225/j.cnki.jccs.2024.0459,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219428264&doi=10.13225%2fj.cnki.jccs.2024.0459&partnerID=40&md5=db531f7cbf2e68160dade33b7ab46fe4,"Intelligent recognition of coal mine workers and manned vehicles (coal mine pedestrian-vehicles) is an important component of video surveillance systems and a key task in the development of coal mine intelligence. However, the detection scene of coal mine pedestrian-vehicles is complex, and deploying large pedestrian-vehicle detection models on limited computing devices is challenging. Balancing between model detection performance and efficiency poses many challenges. This paper proposes a lightweight coal mine pedestrian detection model based on deep learning and model compression techniques. Taking the coal mine video surveillance dataset in Guizhou region as an example. The model accurately and in real-time completes the task of detecting coal mine pedestrian-vehicles, achieving a balance between model detection performance and efficiency. Specifically, in the network model design phase, a lightweight detection model named FCW-YOLO is proposed based on YOLOv8s as the baseline. Faster-Block and coordinate attention are integrated into the feature extraction module of the network, designing a novel C2f-Faster-CA lightweight architecture to reduce redundant channels of the network while adaptively capturing global key information. Furthermore, the WIOU boundary regression loss function is employed to increase the model's focus on common quality samples, addressing issues such as regression errors caused by imbalanced training samples. In the model compression phase, the proposed FCW-YOLO model undergoes channel-level sparsity through a collaborative pruning algorithm, automatically identifying unimportant channels and reducing them, resulting in the FCWP-YOLO model, achieving secondary lightweight design of the coal mine pedestrian-vehicle detection model. Results on a self-built coal mine pedestrian-vehicle detection dataset show that the proposed model has parameters, computational load, and model size of 2.3 M, 4.0 GFLOPs, and 6.0 MB, respectively, achieving compression ratios of 4.9 times, 4.7 times, and 4.4 times compared to the baseline model. The average detection accuracy is 88.7%, an improvement of 1.1%, with a processing speed of only 5.6ms per image. Compared to various lightweight architectures and advanced detection models, this method demonstrates excellent accuracy, lower computational costs, and better real-time performance, providing a feasible coal mine pedestrian-vehicle detection method for resource-constrained coal mine scenarios, meeting the deployment requirements of coal mine video surveillance and enabling real-time alerts for intelligent inspection of coal mine pedestrian-vehicles. © 2025 China Coal Society. All rights reserved.",coal manned vehicles detection; coal miners detection; deep learning; lightweight architecture; model compression; Chemical sensors; Data compression ratio; Image coding; Image compression; Image enhancement; Image segmentation; Miners; Optical flows; Partial pressure sensors; Photointerpretation; Population dynamics; Pressure sensors; Special effects; Temperature sensors; Coal manned vehicle detection; Coal miner detection; Coal miners; Deep learning; Detection models; Lightweight architecture; Manned vehicles; Model compression; Model-based OPC; Vehicles detection; Coal mines
Scopus,"Huang, X.; Teng, Q.; Yang, H.; He, X.; Qing, L.; Wang, P.; Chen, H.",CRKD-YOLO: Cross-Resolution Knowledge Distillation for Low-Resolution Remote Sensing Image Object Detection,,2025,,,,10.1109/TIM.2025.3559616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003700202&doi=10.1109%2fTIM.2025.3559616&partnerID=40&md5=6c3aca5bd0f68faffbf54ebca068af6e,"The majority of advanced remote sensing object detection technologies excel in accurately detecting objects from high-resolution images. However, in practical scenarios, it is often necessary to detect objects in images of varying resolutions due to differences in imaging equipment. When dealing with lower-resolution images, the limited detailed information and blurry boundaries lead to a noticeable decrease in detection accuracy. To address this problem, we propose an efficient object detection method for low-resolution remote sensing images based on the YOLO detector, named CRKD-YOLO. The method constructs a cross-resolution knowledge distillation (CRKD) framework to resolve the issue of feature mismatch, enabling the model with low-resolution inputs to learn more refined feature representations from high-resolution images. Furthermore, to effectively leverage the limited detailed information in low-resolution images, we propose the backbone augment feature pyramid network (BAFPN). It enhances detection accuracy for low-resolution remote sensing images while making the model more lightweight. Massive experiments on DOTA, DIOR, NWPU VHR-10, DroneVehicle, and VEDAI demonstrate that our CRKD-YOLO achieves significant improvements, even achieving higher accuracy compare to training and testing high-resolution images with baseline. © 1963-2012 IEEE.",Cross-resolution knowledge distillation (CRKD); feature enhancement; object detection; remote sensing images; Distillation equipment; Optical remote sensing; Photographic equipment; Cross-resolution knowledge distillation; Detection accuracy; Feature enhancement; High-resolution images; Image object detection; Low resolution images; Lower resolution; Objects detection; Remote sensing images; Remote-sensing; Proximity sensors
Scopus,"Vincze, Z.; Rovid, A.",Aggregated Time Series Features in a Voxel-Based Network Architecture,,2025,,,,10.1109/ACCESS.2025.3535151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216850264&doi=10.1109%2fACCESS.2025.3535151&partnerID=40&md5=0afa597b1c8e653eb77ed0d90f90c91b,"Using point cloud sequences is a popular way to harness the additional information represented in the time domain in order to enhance the performance of 3D object detector neural networks. However, it is not trivial to decide which abstraction level should the additional information presented to the network, or what is the point in the architecture, where aggregating the additional information is most beneficial. In this article, the authors propose various voxel-based networks and analyze their performance in relation to the abstraction level of the time series data. During the evaluation, the authors examine the object detection performance of a popular voxel-based neural network with its original architecture and several variants where the time domain related features were propagated through the network and aggregated at different stages of processing. Based on the evaluation results, a conclusion is drawn regarding the abstraction level at which the time-series aggregation step is performed in order to improve the performance of the baseline voxel-based detector. © 2025 The Authors.",LiDAR point cloud; Neural networks; object detection; point cloud sequence; time series; Cloud platforms; Abstraction level; LiDAR point cloud; Neural-networks; Objects detection; Performance; Point cloud sequence; Point-clouds; Time domain; Times series; Cloud computing architecture
Scopus,"Rehman, A.U.; Jiao, W.; Jiang, Y.; Wei, J.; Sohaib, M.; Sun, J.; E, S.; Rehman, K.U.; Chi, Y.",Deep learning in industrial machinery: A critical review of bearing fault classification methods,,2025,,,,10.1016/j.asoc.2025.112785,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216766102&doi=10.1016%2fj.asoc.2025.112785&partnerID=40&md5=ae7437e2d684a284ae3bd68d9ac404f7,"The review provides an overview of the state-of-the-art in Deep Learning (DL) algorithms for rolling bearing fault classification which remains vital in industrial sectors including transportation, energy, manufacturing, and so forth. Even though they experience a variety of faults, rolling bearings are very crucial in ensuring machine efficiency. This prompts the review of the DL application, which is continuously growing, for intelligently detecting these faults. It comprehensively analyses DL models including Convolutional Neural Networks (CNNs), Auto-Encoders (AEs), Deep Belief Neural Networks (DBNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), and some advanced networks i.e., Transfer Learning (TL), Transformer Neural Network (TNN), Self-Supervised Learning (SSL), Federated Learning (FL), Meta-learning and Interpreting Neural Networks assessing their effectiveness and limitations in fault classification. Thus, the current review is unique among available literature at present since it bridges this crucial gap by including all forms of advanced networks and gives an insight into the potential and challenges. Besides, it emphasizes the importance of different sensing techniques and key datasets in the field to show the contribution towards advancements of DL applications. Finally, referring to current challenges and recommendations for future research directions encompassing environmental adaption, sensor deployment, data preprocessing, model training enhancements, algorithm selection, classifier development, and systematic documentation frame the conclusive part of the paper. This review will serve as an important source for diligent researchers in legitimacy approaches of machinery reliability improvement by means of DL-based techniques for rolling bearing fault classification. © 2025 Elsevier B.V.",Deep learning algorithms; Fault classification methods; Industrial machinery; Rolling Bearing; Convolutional neural networks; Recurrent neural networks; Roller bearings; 'current; Advanced networks; Bearing fault; Classification methods; Deep learning algorithm; Fault classification; Fault classification method; Industrial machinery; Neural-networks; Rolling bearings; Self-supervised learning
Scopus,"Qian, Y.; Liu, Y.; Zhang, Z.; Dang, J.",Traffic lights detection based on enhanced YOLOv5,,2025,,,,10.1145/3711129.3711192,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001862238&doi=10.1145%2f3711129.3711192&partnerID=40&md5=d0b02de91f9ab75b185e1c2652a1856e,"This study introduces a refined YOLOv5 model aimed at overcoming challenges associated with the detection of diminutive traffic signal lights in intricate environmental settings. The Normalized Wasserstein Distance (NWD) metric is employed to counteract the Intersection over Union (IoU) metric’s susceptibility to positional discrepancies in the detection of small targets. An Efficient Multi-scale Attention (EMA) module is integrated to boost the model’s comprehension of pixel-level interactions and overall contextual information. The implementation of the Dynamic Head (DyHead) enhances the representational capacity of the object detection head. These optimizations enhance the model’s capability to capture complex features across different hierarchical levels and spatial locations. The model is trained on the S2TLD dataset, and the issue of class imbalance is addressed by data augmentation. The experimental findings indicate that the introduced algorithm markedly enhances the model’s capacity to identify small traffic lights within complex environments, resulting in a 1.7% improvement in mAP@0.5 over the baseline algorithm. © 2024 Copyright held by the owner/author(s).",Deep learning; Object detection; Traffic lights; YOLOv5; Deep learning; Object detection; Object recognition; Deep learning; Distance metrics; Light detection; Multi-scales; Objects detection; Signal light; Small targets; Traffic light; Wasserstein distance; YOLOv5; Traffic signals
Scopus,"Raina, S.; Challagundla, J.; Acharya, S.; Singh, M.",Improving Small Object Detection with Area-Scaled and Dynamic Focal Loss,,2025,,,,10.1109/CCWC62904.2025.10903720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001129668&doi=10.1109%2fCCWC62904.2025.10903720&partnerID=40&md5=ce6046136f6ca60ff8eb2fe4e85d7d0f,"Detecting small objects remains a persistent challenge for state-of-the-art object detection models, which excel at identifying larger objects but struggle with smaller ones due to limited pixel representation and susceptibility to noise and occlusion. This paper introduces two innovative loss-scaling strategies to address these challenges: Inverse Quadratic Scaling, which adjusts loss inversely proportional to the square root of object area, and Dynamic Focal Loss, which incorporates object size into the focal loss framework. These methods aim to improve training dynamics for small objects while preserving performance for larger objects. We evaluate our strategies on the CPPES dataset using a ResNet-50-based DETR model, characterized by significant object size variations. Experimental results demonstrate substantial improvements in small object detection, achieving F1 score increases of up to 274% in some categories compared to the baseline. Notably, Inverse Quadratic Scaling excels in enhancing underrepresented objects like goggles, while Dynamic Focal Loss offers balanced performance across sizes. These findings underscore the potential of tailored loss functions to mitigate small object detection challenges and provide a foundation for real-world applications such as medical imaging, autonomous vehicles, and surveillance systems. © 2025 IEEE.",deep learning; DETR; loss scaling; object detection; ResNet; small objects; Goggles; Inverse problems; Deep learning; DETR; Excel; Loss scaling; Object size; Objects detection; Performance; Scalings; Small object detection; Small objects; Medical imaging
Scopus,"Shao, Z.; Wang, H.; Cai, Y.; Chen, L.; Li, Y.",UA-Fusion: Uncertainty-Aware Multimodal Data Fusion Framework for 3-D Object Detection of Autonomous Vehicles,,2025,,,,10.1109/TIM.2025.3548184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001087204&doi=10.1109%2fTIM.2025.3548184&partnerID=40&md5=ea7b2e7a34af25a2889154e7b6c0d0bb,"In dynamic traffic scenarios, uncertainties such as occlusions, small objects, as well as unpredictable adverse weather conditions, prevent current environment perception methods from achieving the necessary levels of accuracy, reliability, and safety. In this article, an uncertainty-aware multimodal data fusion framework named UA-Fusion is proposed for 3-D multiobject detection (3D MOD). This framework aims to improve comprehensive and reliable perception capabilities in uncertainty-aware scenarios. Specifically, an uncertainty-aware fusion (UAF) decoder based on a probabilistic cross-modal attention mechanism (PCAM) is presented. This strategy explicitly models and leverages uncertainties in object prediction. It also allows for adaptive and complementary fusion of multimodal data, thereby addressing both aleatoric and epistemic uncertainty. Furthermore, an uncertainty-reduced object query initialization (UOQI) strategy is proposed. This approach fully utilizes the advantages of 2-D object detection in identifying small objects as priors to generate high-quality 3-D queries. Finally, a robustness optimization strategy for training based on query denoising (QD-ROST) is proposed to improve training robustness and convergence in the presence of uncertainty factors. Extensive experiments are conducted on the real-world dataset nuScenes. Notably, UA-Fusion effectively addresses challenges related to uncertainty. Additionally, experiments on the Argoverse 2 (AV2) and RADIATE datasets further validate the generalizability and effectiveness of the proposed method. © 1963-2012 IEEE.",3-D object detection; autonomous vehicle (AV); camera; deep learning; LiDAR; multimodal data fusion; Deep learning; Unmanned aerial vehicles (UAV); 3D object; 3d object detection; Autonomous Vehicles; Deep learning; Dynamic traffic; LiDAR; Multimodal data fusion; Objects detection; Small objects; Uncertainty; Data fusion
Scopus,"Liu, Y.; Yang, D.; Song, T.; Ye, Y.; Zhang, X.",YOLO-SSP: an object detection model based on pyramid spatial attention and improved downsampling strategy for remote sensing images,,2025,,,,10.1007/s00371-024-03434-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193788810&doi=10.1007%2fs00371-024-03434-y&partnerID=40&md5=8c330c96f199ac557fa77ca71746eb8e,"Object detection is an essential task in remote sensing image processing. However, the remote sensing images are characterized by large range of object sizes and complex object backgrounds, which results in challenges in the object detection task. Moreover, the detection effect of existing object detectors on remote sensing images is still not satisfactory. In order to tackle the above problems, an object detection model named YOLO-SSP for remote sensing images is proposed based on the YOLOv8m model in this paper. To begin with, the original downsampling layers are replaced with the proposed lightweight SPD-Conv module, which performs downsampling without loss of fine-grained information and improves the ability of the network to learn the feature representation. In addition, to adapt the large number of small objects in remote sensing images, a small object detection layer is added and achieves the expected results. Finally, a pyramid spatial attention mechanism is proposed to obtain the weights of different spatial positions through hierarchical pooling operations. It effectively improves the detection performance of small objects and those with complex backgrounds. We conducted ablation experiments on the DIOR dataset and compared the YOLO-SSP model with other state-of-the-art models. YOLO-SSP obtains 64.7% of mAP, which is an improvement of 2.3% relative to the baseline model. To demonstrate the generalizability and robustness of the improved model, the comparison experiments are also performed on the TGRS-HRRSD dataset and SIMD dataset with mAP of 77.2 and 64.9%, respectively. The code will be available at https://github.com/YongliLiu/SSP. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Attention mechanism; Object detection; Remote sensing images; Small object; Complex networks; Image enhancement; Object recognition; Remote sensing; Signal sampling; Attention mechanisms; Detection models; Down sampling; Model-based OPC; Object size; Objects detection; Remote sensing image processing; Remote sensing images; Small objects; Spatial attention; Object detection
Scopus,"Sivanandham, S.; Gunaseelan, D.",Enhanced YOLOv5s Model for Improved Multi-Sized Object Detection in Road Scenes,,2025,,,,10.1109/ACCESS.2025.3582136,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009285398&doi=10.1109%2fACCESS.2025.3582136&partnerID=40&md5=c6047e0c9fd31c2ac1f15a9e3921a83a,"Detecting objects in complex driving environments is crucial for autonomous vehicles to navigate safely. However, this task becomes challenging when addressing scale variations, occlusions and diverse backgrounds. This paper proposes an enhanced YOLOv5s model for handling varying object sizes from small pedestrians and traffic signs to larger vehicles in road scenes. The proposed enhancement begins by refining the default anchor boxes using the percentile-based quantile method on the distribution of the bounding boxes and the adjustments to the convolution layers for enhanced feature extraction. Smaller kernel sizes and fewer channels are employed in the initial layers to capture fine-grained details, while in deeper layers, the number of channels is progressively increased to capture broader information that better represents larger objects. Furthermore, an efficient channel attention (ECA) mechanism is integrated into the backbone to prioritize key feature channels, thereby enhancing the model’s ability to detect overlapping and small objects. To improve the feature fusion process, a Multi-scale BiFPN block is integrated into the neck of the model. This combines fine-grained spatial details from the shallow layers with more abstract semantic information from deeper layers, enabling the detection of objects across varying scales. Experimental evaluations carried out on the IDD dataset reveal that the enhanced YOLOv5s model achieves a significant gain in prediction accuracy when compared with the original YOLOv5s. To mitigate the effect of class imbalance and improve generalization across varying object sizes, CutMix data augmentation is employed during training. It shows a 48% increase in mean average precision (mAP@0.5) and a 44% and 49% rise in precision and recall, respectively, with an inference time of 14.6ms compared to the baseline model. These improvements underscore the effectiveness of the proposed enhancements in addressing the challenges of detecting multi-sized objects in complex road environments. © 2013 IEEE.",Anchor box refinement; attention mechanism; CutMix data augmentation; feature fusion; multi-sized object detection; road scenes; YOLOv5; Automobile drivers; Feature Selection; Object recognition; Road vehicles; Roads and streets; Semantics; Traffic signs; Anchor box refinement; Anchor-box; Attention mechanisms; Cutmix data augmentation; Data augmentation; Features fusions; Multi-sized object detection; Objects detection; Road scene; YOLOv5; Object detection
Scopus,"Guo, H.; He, G.; Fang, M.; Xie, S.; Ge, J.",RDW-YOLOv8n: a deeply focused algorithm for small object traffic sign detection under complex weather conditions,,2025,,,,10.1117/1.JEI.34.2.023030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005305948&doi=10.1117%2f1.JEI.34.2.023030&partnerID=40&md5=472df3a851908adf7393ae5343b310df,"In autonomous driving technology, accurate detection of traffic signs is crucial for vehicles to perceive their surrounding road conditions. However, the detection of small traffic signs is often affected by changes in lighting and obstruction from obstacles under complex weather conditions, resulting in inaccurate localization and missed detections. To address these challenges, we introduce a new optimized algorithm - RDW-YOLOv8n. We utilize an enhanced small object layer integrated with the ResCBAM module to replace the original large object layer. Unlike traditional methods of introducing small object layers, this method improves detection accuracy by enhancing the network's perception of small objects through residual attention while effectively addressing the issue of missed detections present in the original algorithm and significantly reducing the number of model parameters. In addition, lightweight depthwise convolution (DWConv) is introduced to further streamline the model without sacrificing accuracy. Finally, the original loss function is replaced with the WIoU loss function to enable the model to focus on the average quality of anchor boxes, balancing the learning of high- and low-quality samples. Experiments conducted on the CCTSDB 2021 dataset show that the RDW-YOLOv8n algorithm achieves a mean average precision (mAP50) of 84.7% and a recall rate (R) of 75.3%, which represent improvements of 6.9 and 5.2 percentage points over the baseline model, respectively. Meanwhile, the model parameter count is reduced by 68.11%, and the F1 score increases by 3.1%. These results effectively demonstrate the detection performance of this algorithm under complex weather. When compared with recent advanced algorithms, RDW-YOLOv8n achieves the highest mAP50 with the smallest number of parameters, highlighting the strong competitiveness and superiority of this algorithm. © 2025 SPIE and IS&T.",complex weather; enhanced small object layer; small object detection; traffic sign; YOLOv8n; Image segmentation; Variable message signs; Vehicle detection; Complex weather; Condition; Enhanced small object layer; Loss functions; Missed detections; Modeling parameters; Small object detection; Small objects; Traffic sign detection; YOLOv8n; Road and street markings
Scopus,"Huang, Y.; Wang, F.",D-TLDetector: Advancing Traffic Light Detection With a Lightweight Deep Learning Model,,2025,,,,10.1109/TITS.2024.3522195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001060297&doi=10.1109%2fTITS.2024.3522195&partnerID=40&md5=6bf09ab731f8fc6f3350d44faa4f98e3,"Traffic signal light detection poses significant challenges in the intelligent driving sector, with high precision and efficiency being crucial for system safety. Advances in deep learning have led to significant improvements in image object detection. However, existing methods continue to struggle with balancing detection speed and accuracy. We propose a lightweight model for traffic light detection that uses a streamlined backbone network and a Low-GD neck architecture. The model's backbone employs structured reparameterization and lightweight Vision Transformers, using multi-branch and Feed-Forward Network structures to boost informational richness and positional awareness, respectively. The Neck network utilizes the Low-GD structure to enhance the aggregation and integration of multi-scale features, reducing information loss during cross-layer exchanges. We introduce a data augmentation strategy using Stable Diffusion to expand our traffic light dataset in complex weather conditions like fog, rain, and snow, improving model generalization. Our method excels on the YCTL2024 traffic light dataset, achieving a detection speed of 135 FPS and 98.23% accuracy, with only 1.3M model parameters. Testing on the Bosch Small Traffic Lights Dataset confirms the method's strong generalization capabilities. This suggests that our proposed method can effectively provide accurate and real-time traffic light detection. © 2000-2011 IEEE.",deep learning; Intelligent transportation system; lightweight network; stable diffusion; traffic light detection; Infrared absorption; Intelligent systems; Laser beams; Motor transportation; Deep learning; Detection speed; Intelligent transportation systems; Learning models; Light detection; Lightweight network; Signal light; Stable diffusion; Traffic light; Traffic light detection; Traffic signals
Scopus,"Yuan, C.; Liu, J.; Wang, H.; Yang, Q.",Object Detection in Complex Traffic Scenes Based on Environmental Perception Attention and Three-Scale Feature Fusion,,2025,,,,10.3390/app15063163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000878142&doi=10.3390%2fapp15063163&partnerID=40&md5=2b7a27068a3b6cc2ce4330c66d86b378,"With the recent advancements in automated driving technology, object detection algorithms that can effectively respond to complex and diverse road traffic scenarios are especially important for driving safety during real driving. In this context, we conduct an in-depth study on object detection algorithms for diverse scenarios in autonomous driving. For diverse and changing backgrounds and multi-scale targets, we propose environmental perception attention (EPA) and the three-scale fusion module (TSFM) to improve the accuracy of object detection algorithms in complex traffic scenes. Environmental perception attention effectively improves the model’s ability to perceive the object by modeling long-range information and inter-channel relationships to direct the model’s attention to important task-related regions and important features in the image. The three-scale fusion module mixes features from different scales while introducing low-level feature map information, enabling the model to take into account the features of objects at different scales. In our experiments, we apply the proposed method to the YOLOv8 model for validation. The results show that compared to the performance of the baseline model on the BDD100K automated driving domain dataset with diverse and complex backgrounds, the mAP@0.5 metric of the improved model is increased by 1.3%. This makes the YOLOv8 more accurate and effective for the detection of different objects in the scenario, and it can better adapt to the different traffic scenarios and environmental changes. © 2025 by the authors.",environmental perception attention; three-scale fusion module; traffic scene target detection; Automated driving; Environmental perception attention; Environmental perceptions; Fusion modules; Object detection algorithms; Objects detection; Targets detection; Three-scale fusion module; Traffic scene; Traffic scene target detection
Scopus,"Yang, Y.; Cui, S.; Xiang, X.; Bai, Y.; Zang, L.; Ding, H.",Research on Improved YOLOv7 for Traffic Obstacle Detection,,2025,,,,10.3390/wevj16010001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215765210&doi=10.3390%2fwevj16010001&partnerID=40&md5=598024a7cabedc8273e269b7420780a2,"Object detection and recognition algorithms are widely used in applications such as real-time monitoring and autonomous driving. However, there is limited research on traffic obstacle detection in complex scenarios involving road construction and sudden accidents. This gap results in low accuracy and difficulties in recognizing occluded targets, thereby hindering the further development and widespread adoption of intelligent transportation systems. To address these issues, this paper proposes an improved algorithm based on YOLOv7, incorporating a lightweight coordinate attention mechanism to focus on small objects at long distances and capture target location information. The use of a high receptive field enhances the feature hierarchy within the detection network. Additionally, we introduce the focal efficient intersection over union loss function to address sample imbalance, which accelerates the model’s convergence speed, reduces loss values, and improves overall model stability. Our model achieved a detection accuracy of 98.1%, reflecting a 1.4% increase, while also enhancing detection speed and minimizing missed detections. These advancements significantly bolster the model’s performance, demonstrating advantages for real-world applications. © 2024 by the authors.",attention mechanism; intelligent transportation; loss function; object detection; traffic obstacles; YOLOv7; Feature extraction; Highway accidents; Intelligent systems; Motor transportation; Object detection; Object recognition; Attention mechanisms; Intelligent transportation; Loss functions; Object detection algorithms; Object detection and recognition; Object recognition algorithm; Objects detection; Obstacles detection; Traffic obstacle; YOLOv7; Obstacle detectors
Scopus,"Liu, Y.; Shi, Y.",VRU-YOLO: A Small Object Detection Algorithm for Vulnerable Road Users in Complex Scenes,,2025,,,,10.1109/ACCESS.2025.3534321,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216978355&doi=10.1109%2fACCESS.2025.3534321&partnerID=40&md5=c5eff3e28c50aa7788b6df9b71fe8468,"Accurate detection of vulnerable road users (VRUs) is critical for enhancing traffic safety and advancing autonomous driving systems. However, due to their small size and unpredictable movements, existing detection methods struggle to provide stable and accurate results under real-Time conditions. To overcome these challenges, this paper proposes an improved VRU detection algorithm based on YOLOv8, named VRU-YOLO. First, we redesign the neck structure and construct a Detail Enhancement Feature Pyramid Network (DEFPN) to enhance the extraction and fusion capabilities of small target features. Second, the YOLOv8 network's Spatial Pyramid Pooling Fast (SPPF) module is replaced with a novel Feature Pyramid Convolution Fast (FPCF) module based on dilated convolution, effectively mitigating feature loss in small target processing. Additionally, a lightweight Optimized Shared Detection Head (OSDH-Head) is introduced, reducing computational complexity while improving detection efficiency. Finally, to alleviate the deficiencies of traditional loss functions in shape matching and computational efficiency, we propose the Wise-Powerful Intersection over Union (WPIoU) loss function, which further optimizes the regression of target bounding boxes. Experimental results on a custom-built multi-source VRU dataset show that the proposed model enhances precision, recall, mAP50, and mAP50:95 by 1.3%, 3.4%, 3.3%, and 1.8%, respectively, in comparison to the baseline model. Moreover, in a generalization test conducted on the remote sensing small target dataset VisDrone2019, the VRU-YOLO model achieved an mAP50 of 31%. This study demonstrates that the improved model offers more efficient performance in small object detection scenarios, making it well-suited for VRU detection in complex road environments.  © 2013 IEEE.",feature fusion; shared convolution; small object detection; Vulnerable road users; YOLOv8; Motor transportation; Remote sensing; Feature pyramid; Features fusions; Loss functions; Road users; Shared convolution; Small object detection; Small targets; User detection; Vulnerable road user; YOLOv8; Object detection
Scopus,"Li, W.; Chen, M.; Zhang, L.; Tian, J.",SR-NET: a lightweight enhanced feature extraction network for fabric defect detection,,2025,,,,10.1177/00405175241293349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007601583&doi=10.1177%2f00405175241293349&partnerID=40&md5=6bd3f0d45e39e2a52be7bda3209c97c1,"In the textile industry, detecting defects in fabrics featuring intricate patterns and small imperfections poses a persistent challenge. There is an urgent need for defect detection systems characterized by high accuracy and ease of deployment. To address these needs, a fabric defect detection algorithm, SR-NET, has been proposed in this paper, featuring lightweight and enhanced multiple feature extraction capabilities. The Spatial Receptive-field Convolution (SRConv) module was introduced as a convolutional module, effectively optimizing the performance of the convolutional neural network and improving overall model detection performance. The Region-Semantic Residual Module (RSR) module was proposed as an attention mechanism, which was fused with the C2f module to form the C2f_RSR module. This enhancement significantly enhanced the modeling ability to extract features pertinent to small defects. To reduce the sensitivity of the model to deviations in the position of small objects, The Gaussian Wasserstein Distance (GWD) metric was introduced into LGWD as a loss function. This adjustment has demonstrated outstanding performance in small target detection. Experimental results demonstrated that SR-NET enhanced mAP by 5.6% compared with baseline model. Simultaneously, SR-NET kept parameters and FLOPs almost unchanged. SR-NET proved to be well suited for practical production applications, and is able to meet real-time detection requirements. © The Author(s) 2025.",computer vision; deep learning; defect detection; Fabric defect; feature extraction; Computer vision; Convolutional neural networks; Fabrics; Feature Selection; Leak detection; Machine vision; Deep learning; Defect detection; Defect-detection systems; Detecting defects; Fabric defect detection; Fabric defects; Features extraction; Gaussians; Performance; Wasserstein distance; Textile industry
Scopus,"Wu, Y.",Fusion-based modeling of an intelligent algorithm for enhanced object detection using a Deep Learning Approach on radar and camera data,,2025,,,,10.1016/j.inffus.2024.102647,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202055723&doi=10.1016%2fj.inffus.2024.102647&partnerID=40&md5=eb8f028df4e89a75991504872ddebd3f,"Object detection, the process of detecting and classifying objects within a given environment, forms the foundational element. Multisensory fusion incorporates data from diverse sensors, like radar and cameras, to refine the reliability and accuracy of detection. Further, Radar and camera data fusion refine this process by integrating the unique strength of both technologies, which leverage the radar's proficiency in adverse weather conditions and the camera's high-resolution imaging. This incorporation enhances the object detection systems, which enables them to effectively operate across the spectrum of scenarios, from autonomous vehicles navigating challenging weather to surveillance systems monitoring critical infrastructure. Deep learning (DL), a branch of machine learning (ML), empowers this system with the capability to learn complex representations and patterns directly from the data, which enables them to generalize and adapt to new situations. By integrating the advanced methodology, we can develop strong perception system capable of interpreting and detecting objects accurately in dynamic and diverse environments, from autonomous vehicles navigating urban landscapes to surveillance systems monitoring complex environments. This study designs an Intelligent Algorithm for Enhanced Object Detection Using Deep Learning Approach on the Radar and Camera Data Fusion (IAEOD-DLRCDF) technique. The presented IAEOD-DLRCDF technique uses multi-angle joint calibration where the spatial sparse alignment of the heterogeneous data of the camera and Radar is realized with image falsification disregarded. Besides, the IAEOD-DLRCDF technique applies YOLOv8 object detector for radar and camera target detection individually which are then integrated with the image plane. Moreover, the detected objects are then classified via the bidirectional long short-term memory (BiLSTM) model. Furthermore, the Adam optimizer is used for the optimum hyperparameter selection of the BiLSTM network which results in a better recognition rate. The performance assessment of the IAEOD-DLRCDF method is tested under benchmark dataset. The empirical analysis stated that the IAEOD-DLRCDF method gains better performance over other models. © 2024",Adam optimizer; Data fusion; Deep learning; Machine learning; Object detection; Radar; YOLOv8; Aircraft detection; Critical infrastructures; Sensor data fusion; Adam optimizer; Autonomous Vehicles; Data fusion technique; Deep learning; Intelligent Algorithms; Learning approach; Machine-learning; Objects detection; Optimizers; YOLOv8; Deep learning
Scopus,"Aljagoub, D.; Na, R.; Cheng, C.",Delamination detection in concrete decks using numerical simulation and UAV-based infrared thermography with deep learning,,2025,,,,10.1016/j.autcon.2024.105940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212427296&doi=10.1016%2fj.autcon.2024.105940&partnerID=40&md5=bf792a8be9effcd1250bd50135dc6c8b,"The potential of concrete bridge delamination detection using infrared thermography (IRT) has grown with technological advancements. However, most current studies require an external input (subjective threshold), reducing the detection's objectivity and accuracy. Deep learning enables automation and streamlines data processing, potentially enhancing accuracy. Yet, data scarcity poses a challenge to deep learning applications, hindering their performance. This paper aims to develop a deep learning approach using supervised learning object detection models with extended data from real and simulated images. The numerical simulation image supplementation seeks to eliminate the limited data barrier by creating a comprehensive dataset, potentially improving model performance and robustness. Mask R-CNN and YOLOv5 were tested across various training data and model parameter combinations to develop an optimal detection model. Lastly, when tested, the model showed a remarkable ability to detect delamination of varying properties accurately compared to currently employed IRT techniques. © 2024",Augmentation; Deep learning; Delamination; Detection automation; Image processing; Infrared thermography (IRT); Instance segmentation; Mask R-CNN; Unmanned arterial vehicle (UAV); YOLOv5; Digital elevation model; Image enhancement; Image segmentation; Network security; Self-supervised learning; Thermography (imaging); Augmentation; Deep learning; Delaminations detection; Detection automation; Images processing; Infrared thermography; Instance segmentation; Mask R-CNN; Unmanned arterial vehicle; YOLOv5; Unmanned aerial vehicles (UAV)
Scopus,"Kong, H.; Huang, C.; Yu, J.; Shen, X.","A Survey of mmWave Radar-Based Sensing in Autonomous Vehicles, Smart Homes and Industry",,2025,,,,10.1109/COMST.2024.3409556,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196065476&doi=10.1109%2fCOMST.2024.3409556&partnerID=40&md5=c7a557496e16e5999474f9fdf7abab1c,"Sensing technology plays a crucial role in bridging the physical and digital worlds. By transforming a multitude of physical phenomena into digital data, it significantly enhances our understanding of the environment and is instrumental in a wide range of applications. Given the wide bandwidth and short wavelength characteristics, millimeter wave (mmWave) radar sensing is considered one of the most promising sensing techniques beyond mmWave communication. In this paper, we provide a comprehensive survey of mmWave radar-based sensing techniques and applications in autonomous vehicles, smart homes, and industry. Specifically, we first review widely exploited mmWave radar techniques and signal processing techniques from the perspective of dedicated radars and communication integration, which are the basis of mmWave radar sensing. Then, we introduce mainstream machine learning techniques, especially the latest deep learning techniques for designing applications with mmWave signals. Related hardware devices, available public datasets, and evaluation metrics are also presented. Afterward, we provide a taxonomy of emerging mmWave radar sensing applications, and review the developments in object detection, ego-motion estimation, simultaneous localization and mapping, activity recognition, pose estimation, gesture recognition, speech recognition, vital sign monitoring, user authentication, indoor positioning, industrial imaging, industrial measurement, environmental monitoring, etc. We conclude the paper by discussing challenges and potential future research directions. © 1998-2012 IEEE.",autonomous vehicle; deep learning; industry; Millimeter wave radar; radar signal processing; smart home; wireless sensing; Deep learning; Learning algorithms; Metadata; Millimeter waves; Motion estimation; Object detection; Object recognition; Signal detection; Speech recognition; Tracking radar; Autonomous Vehicles; Deep learning; Millimeter-wave radar; Millimeterwave communications; Millimetre-wave radar; Radar detection; Radar sensing; Smart homes; Wireless sensing; Automation
Scopus,"Li, X.; He, H.; Huang, J.; Liu, R.; Qian, T.; Hon, C.",ASFM-AFD: Multi-Modal Fusion of AFD-Optimized LiDAR and Camera Data for Paper Defect Detection,,2025,,,,10.1109/TIM.2025.3580902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008660562&doi=10.1109%2fTIM.2025.3580902&partnerID=40&md5=ab472c6d3c60fdc6c56e5075ecfb3f2f,"Current paper defect detection methods often struggle to satisfy the critical demands for precision and adaptability in complex industrial environments. This study developed a novel multi-sensor platform combining low-precision LiDAR and high-resolution camera, along with an Adaptive Sensor Fusion Module - Adaptive Fourier Decomposition (ASFM-AFD) framework for processing multi-modal data. This integration substantially improves defect detection capabilities. The AFD-based optimization approach effectively enhances the quality of low-precision LiDAR data, achieving performance comparable to high-precision LiDAR while providing cost-effective solution for industrial inspection applications. Comparative studies with Empirical Mode Decomposition and Variational Mode Decomposition demonstrates that AFD exhibits superior noise reduction and resolution enhancement while maintaining stable performance under various challenging conditions. Experimental results show that the proposed framework achieves an average precision of 97.5% for paper defect detection in high-speed production environments, significantly improving detection accuracy and demonstrating robustness across various operating conditions. © 1963-2012 IEEE.",Adaptive Fourier Decomposition; Lidar; Multi-Modal Fusion; Point Cloud Data; Signal Processing; Small Object Detection; Cost effectiveness; Data accuracy; Data handling; Defects; Empirical mode decomposition; Modal analysis; Noise abatement; Object detection; Paper; Variational mode decomposition; 'current; Adaptive fourier decompositions; Adaptive sensor fusion; Fusion modules; Lower precision; Multi-modal fusion; Paper defect detections; Point cloud data; Signal-processing; Small object detection; Optical radar
Scopus,"Wei, J.; As'arry, A.; Anas Md Rezali, K.; Zuhri Mohamed Yusoff, M.; Ma, H.; Zhang, K.",A Review of YOLO Algorithm and Its Applications in Autonomous Driving Object Detection,,2025,,,,10.1109/ACCESS.2025.3573376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006931401&doi=10.1109%2fACCESS.2025.3573376&partnerID=40&md5=681c8ccacee7157ee140b3de7613abef,"Object detection in autonomous driving scenarios represents a significant research direction within artificial intelligence. Real-time and accurate object detection and recognition are crucial in ensuring autonomous vehicles’ safe and stable operation. In recent years, the continuous introduction of the YOLO series of algorithms and their enhanced models has led to remarkable performance in autonomous driving object detection. From YOLOv1 to YOLOv12, detection accuracy has improved significantly, with mAP increasing from approximately 63.4% to over 80% and inference speed exceeding 100 FPS in lightweight versions such as YOLOv8n and YOLOv10. This paper reviews the YOLO algorithm and its application in object detection in autonomous driving scenarios. Firstly, the development and distinctions among the YOLO series of detection algorithms are explained, and their performance is analyzed. Secondly, the strategies for improving YOLO-based models across the input, feature extraction, and prediction stages are summarized. Thirdly, the research status and application of the YOLO algorithm in autonomous driving object detection are elaborated upon from the perspectives of traffic vehicles, pedestrians, traffic signs, traffic lights, and lane lines, with comparisons and analyses of performance metrics such as accuracy and real-time performance. Finally, considering the current challenges in autonomous driving object detection, the development trajectory and prospects of the YOLO algorithm are summarized and discussed. © 2013 IEEE.",applications; Autonomous driving; object detection; YOLO algorithm; Feature extraction; Object recognition; Autonomous driving; Autonomous Vehicles; ITS applications; Object detection and recognition; Objects detection; Performance; Real- time; Safe operation; Stable operation; YOLO algorithm; Object detection
Scopus,"Gallagher, J.E.; Oughton, E.J.","Surveying You Only Look Once (YOLO) Multispectral Object Detection Advancements, Applications, and Challenges",,2025,,,,10.1109/ACCESS.2025.3526458,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214897758&doi=10.1109%2fACCESS.2025.3526458&partnerID=40&md5=8a849b1c9c301494ca74c35f58ab999a,"Multispectral imaging and deep learning have emerged as powerful tools supporting diverse use cases from autonomous vehicles to agriculture, infrastructure monitoring and environmental assessment. The combination of these technologies has led to significant advancements in object detection, classification, and segmentation tasks in the non-visible light spectrum. This paper considers 400 total papers, reviewing 200 in detail to provide an authoritative meta-review of multispectral imaging technologies, deep learning models, and their applications, considering the evolution and adaptation of you only look once (YOLO). Ground-based collection is the most prevalent approach, totaling 63% of the papers reviewed, although uncrewed aerial systems (UAS) for YOLO-multispectral applications have doubled since 2020. The most prevalent sensor fusion is red-green-blue (RGB) with long-wave infrared (LWIR), comprising 39% of the literature. YOLOv5 remains the most used variant for adaption to multispectral applications, consisting of 33% of all modified YOLO models reviewed. Future research needs to focus on: 1) developing adaptive YOLO architectures capable of handling diverse spectral inputs that do not require extensive architectural modifications; 2) exploring methods to generate large synthetic multispectral datasets; 3) advancing multispectral YOLO transfer learning techniques to address dataset scarcity; and 4) innovating fusion research with other sensor types beyond RGB and LWIR.  © 2013 IEEE.",convolutional neural networks (CNN); deep learning; HSI; LWIR; MSI; Multispectral object detection; NIR; RGB; SAR; you only look once (YOLO); Antenna grounds; Deep neural networks; Image segmentation; Sensor data fusion; Thermography (imaging); Convolutional neural network; Deep learning; HSI; Longwave infrared; MSI; Multi-spectral; Multispectral object detection; NIR; Objects detection; Red green blues; You only look once; Convolutional neural networks
Scopus,"Liu, S.",Vehicle detection in different traffic scenarios based on YOLOv5,,2025,,,,10.1117/12.3057990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216424648&doi=10.1117%2f12.3057990&partnerID=40&md5=a2d918174bc6d4316793cdf06d48dfa5,"Urban traffic environments pose significant challenges for automated vehicle detection, including fluctuating lighting, adverse weather, and complex road conditions. Visibility issues from fog, rain, and low light, alongside the prevalence of small vehicles in dense traffic, hinder detection accuracy. This study proposes an enhanced YOLOv5-based model for improved vehicle detection in complex urban traffic scenarios. Key contributions include integrating BiFPN for robust multi-scale feature fusion, adding an FFA module to boost detection under low-visibility conditions, and incorporating Image Adjustment Techniques (IAT) for preprocessing. Additionally, select YOLOv5 modules were upgraded to YOLOv8 components, yielding notable performance gains over the baseline model. © 2025 SPIE.",D2City dataset; Defogging; Image enhancement; Small object detection; Vehicle detection; YOLOv5; Laser beams; Urban transportation; Adverse weather; Automated vehicle detection; D2city dataset; Defogging; Scenario-based; Small object detection; Traffic environment; Urban traffic; Vehicles detection; YOLOv5; Vehicle detection
Scopus,"Wang, X.; Miao, H.; Liang, J.; Li, K.; Tan, J.; Luo, R.; Jiang, Y.",Multi-Dimensional Research and Progress in Parking Space Detection Techniques,,2025,,,,10.3390/electronics14040748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218872793&doi=10.3390%2felectronics14040748&partnerID=40&md5=254be4865c75b9926e4da5e9b3186ad6,"Due to the increase in the number of vehicles and the complexity of parking spaces, parking space detection technology has emerged. It is capable of automatically identifying vacant parking spaces in parking lots or on streets, and delivering this information to drivers or parking management systems in real time, which has a significant impact on improving urban parking efficiency, alleviating traffic congestion, optimizing driving experience, and promoting the development of intelligent transportation systems. This paper firstly describes the research significance of parking space detection technology and its research background, and then systematically reviews different types of parking spaces and detection technologies, covering a variety of technical means such as ultrasonic sensors, infrared sensors, magnetic sensors, other sensors, methods based on traditional computer vision, and methods based on deep learning. At the end of the paper, the article summarizes the current research progress in parking space detection technology, analyzes the existing challenges, and provides an outlook on future research directions. © 2025 by the authors.",computer vision; deep learning; parking space detection; sensors
Scopus,"Guo, F.; Wu, J.; Zhang, Q.",Dual-Domain Feature-Guided Task Alignment for Enhanced Small Object Detection,,2025,,,,10.1109/ICASSP49660.2025.10888564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003877954&doi=10.1109%2fICASSP49660.2025.10888564&partnerID=40&md5=e284205d749f12584d4c50a76effd74b,"Small object detection is a critical challenge in Unmanned Aerial Vehicles (UAVs) due to the limited pixel representation of small objects and the impact of successive pooling operations, which frequently results in the disappearance of small objects within intricate backgrounds. To tackle this issue, we propose the Small Object Enhancement Pyramid (SOEP) module, which first transforms feature representations (i.e., in the spatial domain) into the frequency domain to better capture small objects typically characterized by high-frequency components. These feature representations are then fused in the spatial domain using a frequency-based attention map, enhancing small object representations by integrating information from both complementary domains. Furthermore, we introduce a Task Aligned Head (TAH) that integrates classification and localization tasks interactively, reducing the misalignment that occurs when these tasks are learned independently, particularly in the context of small objects. Experimental results on the Visdrone dataset verify that our proposed method (D2FTA) outperforms the baseline method by 12.7%, 14.19% on mAP0.5 and mAP0.5:0.95. © 2025 IEEE.",frequency domain; small object detection; spatial domain
Scopus,"Tang, P.; Lv, M.; Ding, Z.; Xu, W.; Jiang, M.",Pothole detection-you only look once: Deformable convolution based road pothole detection,,2025,,,,10.1049/ipr2.13300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211079719&doi=10.1049%2fipr2.13300&partnerID=40&md5=3b28abffc61b4c6869434e4feabd817d,"The detection of road potholes plays a crucial role in ensuring passenger comfort and the structural safety of vehicles. To address the challenges of pothole detection in complex road environments, this paper proposes a model focusing on shape features (pothole detection you only look once, PD-YOLO). The model aims to overcome the limitations of multi-scale feature learning caused by the use of fixed convolutional kernels in the baseline model, by constructing a feature extraction module that better adapts to variations in the shape of potholes. Subsequently, a cross-stage partial network was designed using a one-time aggregation method, simplifying the model while enabling the network to fuse information between feature maps at different stages. Additionally, a dynamic sparse attention mechanism is introduced to select relevant features, reducing redundancy and suppressing background noise. Experiments conducted on the VOC2007 and GRDDC2020_Pothole datasets reveal that compared to the baseline model YOLOv8, PD-YOLO achieves improvements of 3.9% and 2.8% in mean average precision, with a frame rate of approximately 290 frames per second, effectively meeting the accuracy and real-time requirements for pothole detection. The code and dataset for this paper are located at: https://github.com/woyijiankou/PD-YOLO. © 2024 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",image capture; image classification; image sampling; Baseline models; Feature learning; Image captures; Images classification; Multi-scale features; Passengers comfort; Road environment; Safety of vehicles; Shape features; Structural safety; Image sampling
Scopus,"Zhang, Y.; Mao, Y.; Wang, H.; Yu, Z.; Guo, S.; Zhang, J.; Wang, L.; Guo, B.",Orchestrating Joint Offloading and Scheduling for Low-Latency Edge SLAM,,2025,,,,10.1109/TMC.2025.3547256,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000458312&doi=10.1109%2fTMC.2025.3547256&partnerID=40&md5=d2561a12f8099b2e50449cfd337a3d6d,"Visual Simultaneous Localization and Mapping (vSLAM) is a prevailing technology for many emerging robotic applications. Achieving real-time SLAM on mobile robotic systems with limited computational resources is challenging because the complexity of SLAM algorithms increases over time. This restriction can be lifted by offloading computations to edge servers, forming the emerging paradigm of edge-assisted SLAM. Nevertheless, the exogenous and stochastic input processes affect the dynamics of the edge-assisted SLAM system. Moreover, the requirements of clients on SLAM metrics change over time, exerting implicit and time-varying effects on the system. In this paper, we aim to push the limit beyond existing edge-assist SLAM by proposing a new architecture that can handle the input-driven processes and also satisfy clients' implicit and time-varying requirements. The key innovations of our work involve a regional feature prediction method for importance-aware local data processing, a configuration adaptation policy that integrates data compression/decompression and task offloading, and an input-dependent learning framework for task scheduling with constraint satisfaction. Extensive experiments prove that our architecture improves pose estimation accuracy and saves up to 47\% of communication costs compared with a popular edge-assisted SLAM system, as well as effectively satisfies the clients' requirements. (Figure presented). © 2025 IEEE. All rights reserved.",and constrained reinforcement learning; mobile edge computing (MEC); Simultaneous localization and mapping (SLAM); task offloading; task scheduling
Scopus,"Bai, T.; Zhao, H.; Huang, L.; Wang, Z.; Kim, D.I.; Nallanathan, A.","A Decade of Video Analytics at Edge: Training, Deployment, Orchestration, and Platforms",,2025,,,,10.1109/COMST.2025.3563377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003595638&doi=10.1109%2fCOMST.2025.3563377&partnerID=40&md5=1f10a7c7a226c907b56695d38913a2de,"Video analytics (VA), capable of autonomously understanding events in video content, has demonstrated significant potential across various applications, from surveillance to self-driving cars and industrial automation. However, traditional VA, relying on either end-device or cloud-based solutions, faces limitations such as restricted on-device computing power and network congestion at cloud centers. Edge computing offers a promising solution, enabling low-latency, high-accuracy, and bandwidth-efficient performance, thus supporting the rapid growth of VA deployment. This article provides a comprehensive review of VA at the edge, examining aspects of model training, deployment, end-edge-cloud orchestration, and VA platforms. Specifically, we explore model training approaches conducted in the cloud, at the edge, and in hybrid cloud-edge configurations. We also discuss various model deployment techniques, including quantization and network pruning. Furthermore, the article surveys end-edge-cloud orchestration strategies, categorized into VA query offloading and query scheduling. We evaluate practical deployments and review the literature on VA platforms. Finally, we outline several promising future research directions for advancing this field.  © 1998-2012 IEEE.",computer vision; deep learning; Edge computing; edge intelligence; video analytics; Mobile edge computing; Deep learning; Edge clouds; Edge computing; Edge intelligence; End-devices; Industrial automation; Model training; Self drivings; Video analytics; Video contents; Computation offloading
Scopus,"Gan, Z.; Li, J.; Wu, P.; Bai, Y.; Xiong, B.; Zeng, N.; Zou, F.; He, D.; Ni, W.",A Novel Equipment Based on Improved YOLOv5s for Automated Electrical Cover Handling,,2025,,,,10.1109/TIM.2024.3500043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000373946&doi=10.1109%2fTIM.2024.3500043&partnerID=40&md5=3b8f5bf0d0d96d9432e572874496b7e2,"With the rapid development of the power industry, the number of power cables has increased significantly, making the improvement of maintenance efficiency a critical issue. Reducing accidents caused by the manual handling of heavy electrical cover plates has become a pressing concern. However, the maintenance of power cables often requires precise positioning and efficient movement of these heavy cover plates in energy-poor environments. Existing object detection models typically not only require high-performance computing but also have with high cost and large volume, rendering them quite unsuitable for deployment in the resource-constrained system. To address these challenges, we propose an automated solution that integrates negative pressure adsorption and computer vision techniques. The core module of this method is Cover Plates Edge Line-YOLO (CPEL-YOLO), a lightweight model based on You Only Look Once (YOLO). In CPEL-YOLO, it leverages MobileNetV3 with efficient multiscale attention (EMA) and C3-Faster with EMA to focus the model on precise edge lines with reduced computational complexity. Additionally, Scylla-IoU (SIoU) is incorporated to optimize the training process. A dataset of 1009 cover plate images was constructed for training and evaluation. Experimental results show that CPEL-YOLO achieves a mAP@0.5 of 88.1%, comparable to YOLOv5s, while reducing model size by 42% and improving precision by 4.4%.  © 1963-2012 IEEE.",Cover plate handling; edge line detection; electrical power maintenance; multiscale attention; YOLOv5s; Cover plate; Cover plate handling; Edge line detection; Edge lines; Electrical power; Electrical power maintenance; Line detection; Multiscale attention; Plate edges; YOLOv5; Network security
Scopus,"Zhao, L.; Fu, L.; Jia, X.; Cui, B.; Zhu, X.; Jin, J.",YOLO-BOS: An Emerging Approach for Vehicle Detection with a Novel BRSA Mechanism,,2024,,,,10.3390/s24248126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213081735&doi=10.3390%2fs24248126&partnerID=40&md5=b030f94a7dacc27707e985df5adae3c1,"In intelligent transportation systems, accurate vehicle target recognition within road scenarios is crucial for achieving intelligent traffic management. Addressing the challenges posed by complex environments and severe vehicle occlusion in such scenarios, this paper proposes a novel vehicle-detection method, YOLO-BOS. First, to bolster the feature-extraction capabilities of the backbone network, we propose a novel Bi-level Routing Spatial Attention (BRSA) mechanism, which selectively filters features based on task requirements and adjusts the importance of spatial locations to more accurately enhance relevant features. Second, we incorporate Omni-directional Dynamic Convolution (ODConv) into the head network, which is capable of simultaneously learning complementary attention across the four dimensions of the kernel space, therefore facilitating the capture of multifaceted features from the input data. Lastly, we introduce Shape-IOU, a new loss function that significantly enhances the accuracy and robustness of detection results for vehicles of varying sizes. Experimental evaluations conducted on the UA-DETRAC dataset demonstrate that our model achieves improvements of 4.7 and 4.4 percentage points in mAP@0.5 and mAP@0.5:0.95, respectively, compared to the baseline model. Furthermore, comparative experiments on the SODA10M dataset corroborate the superiority of our method in terms of precision and accuracy. © 2024 by the authors.",BRSA; ODConv; Shape-IOU; vehicle detection; YOLO-BOS; Advanced traffic management systems; Air traffic control; Highway administration; Highway traffic control; Intelligent systems; Magnetic levitation vehicles; Motor transportation; Street traffic control; Vehicle locating systems; Attention mechanisms; Bi-level routing spatial attention; Directional dynamics; Omni-directional; Omni-directional dynamic convolution; Routings; Shape-IOU; Spatial attention; Vehicles detection; YOLO-BOS; article; bronchiolitis obliterans; controlled study; diagnosis; feature extraction; spatial attention; Vehicle detection
Scopus,"Li, Z.; Xiang, J.; Duan, J.",A low illumination target detection method based on a dynamic gradient gain allocation strategy,,2024,,,,10.1038/s41598-024-80265-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210018417&doi=10.1038%2fs41598-024-80265-w&partnerID=40&md5=96d78d0b625f2c9192aa3f6a63c4e319,"Current target detection methods perform well under normal lighting conditions; however, they encounter challenges in effectively extracting features, leading to false detections and missed detections in low illumination environments. To address these issues, this study introduces an efficient target detection method for low illumination, named DimNet. This method optimizes the model through enhancements in multi-scale feature fusion, feature extraction, detection head, and loss function. Firstly, efficient multi-scale feature fusion is performed by using a new neck structure in the original model so that it can fully exchange high-level semantic information and low-level spatial information. Secondly, by designing a new feature aggregation module, it can simultaneously fuse channel and spatial information as well as local and global information to improve the representation of the network. Subsequently, to achieve more accurate target recognition, a new detection head is designed by replacing the original convolutional layer and utilizing the reparameterization technique, which enhances recognition performance in complex scenes. Additionally, the size of the improved detection head is reduced by adopting a parameter-sharing approach, thereby balancing detection accuracy with computational efficiency. Finally, to solve the fuzzy boundary problem caused by the target boundary being similar to the surrounding background due to insufficient illumination under low illumination conditions, a new loss function is designed in this paper, which pays more attention to the center of the target and weakly considers the aspect ratio of the target prediction frame, and at the same time, the new loss function employs a dynamic gradient gain assignment strategy to reduce the effect of the low-quality anchor frames and to improve the target localization Accuracy. The experimental results show that DimNet achieves a mAP50 of 75.60% on the ExDark dataset, which is an improvement of 3.77% over the baseline model and 2.25% over the state-of-the-art (SOTA) model. DimNet outperforms the previous and current SOTA methods in terms of detection accuracy and other aspects of performance, which is a clear advantage. © The Author(s) 2024.",Feature extraction; Feature fusion; Loss function; Low-light image; Target detection; article; diagnosis; feature extraction; human; illumination; neck; prediction
Scopus,"Yang, D.; Solihin, M.I.; Ardiyanto, I.; Zhao, Y.; Li, W.; Cai, B.; Chen, C.",A streamlined approach for intelligent ship object detection using EL-YOLO algorithm,,2024,,,,10.1038/s41598-024-64225-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197383728&doi=10.1038%2fs41598-024-64225-y&partnerID=40&md5=a5e16c6b7ff61229ea0f091fb4489135,"Maritime objects frequently exhibit low-quality and insufficient feature information, particularly in complex maritime environments characterized by challenges such as small objects, waves, and reflections. This situation poses significant challenges to the development of reliable object detection including the strategies of loss function and the feature understanding capabilities in common YOLOv8 (You Only Look Once) detectors. Furthermore, the widespread adoption and unmanned operation of intelligent ships have generated increasing demands on the computational efficiency and cost of object detection hardware, necessitating the development of more lightweight network architectures. This study proposes the EL-YOLO (Efficient Lightweight You Only Look Once) algorithm based on YOLOv8, designed specifically for intelligent ship object detection. EL-YOLO incorporates novel features, including adequate wise IoU (AWIoU) for improved bounding box regression, shortcut multi-fuse neck (SMFN) for a comprehensive analysis of features, and greedy-driven filter pruning (GDFP) to achieve a streamlined and lightweight network design. The findings of this study demonstrate notable advancements in both detection accuracy and lightweight characteristics across diverse maritime scenarios. EL-YOLO exhibits superior performance in intelligent ship object detection using RGB cameras, showcasing a significant improvement compared to standard YOLOv8 models. © The Author(s) 2024.",Improved YOLOv8; Intelligent ship; Lightweight YOLOv8; Modified bounding box regression; Object detection; algorithm; article; camera; controlled study; diagnosis; nonhuman; ship
Scopus,"Qiu, C.; Tang, H.; Yang, Y.; Wan, X.; Xu, X.; Lin, S.; Lin, Z.; Meng, M.; Zha, C.",Machine vision-based autonomous road hazard avoidance system for self-driving vehicles,,2024,,,,10.1038/s41598-024-62629-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194864716&doi=10.1038%2fs41598-024-62629-4&partnerID=40&md5=b42c1e212551e5c761a12fd8364c95f8,"The resolution of traffic congestion and personal safety issues holds paramount importance for human’s life. The ability of an autonomous driving system to navigate complex road conditions is crucial. Deep learning has greatly facilitated machine vision perception in autonomous driving. Aiming at the problem of small target detection in traditional YOLOv5s, this paper proposes an optimized target detection algorithm. The C3 module on the algorithm’s backbone is upgraded to the CBAMC3 module, introducing a novel GELU activation function and EfficiCIoU loss function, which accelerate convergence on position loss lbox, confidence loss lobj, and classification loss lcls, enhance image learning capabilities and address the issue of inaccurate detection of small targets by improving the algorithm. Testing with a vehicle-mounted camera on a predefined route effectively identifies road vehicles and analyzes depth position information. The avoidance model, combined with Pure Pursuit and MPC control algorithms, exhibits more stable variations in vehicle speed, front-wheel steering angle, lateral acceleration, etc., compared to the non-optimized version. The robustness of the driving system's visual avoidance functionality is enhanced, further ameliorating congestion issues and ensuring personal safety. © The Author(s) 2024.",Control algorithm; Deep learning; Machine vision; Risk avoidance; Self-Driving; YOLOv5s; acceleration; algorithm; article; autonomous vehicle; camera; controlled study; deep learning; detection algorithm; human; learning; risk aversion; velocity; vision
Scopus,"Mo, C.; Hu, Z.; Wang, J.; Xiao, X.",SGT-YOLO: A Lightweight Method for PCB Defect Detection,,2025,,,,10.1109/TIM.2025.3563011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003674825&doi=10.1109%2fTIM.2025.3563011&partnerID=40&md5=5281187f599329bfd29b6629630839ef,"Ensuring the quality of the printed circuit board (PCB) is vital. Most of the current fault detection algorithms perform well in PCB defect detection. However, these methods involve too many parameters or computations, which make them unfriendly to devices with limited computational capability and small memory capacity. Additionally, problems such as missed or false detection may occur due to the complex background environment and small defect size. Thus, this article proposes an improved model based on YOLOv5s named SE-ENv2 GC-Neck TSCODE (SGT)-you only look once (YOLO) to strike a better trade-off between accuracy and model complexity. First, an SE-ENv2 backbone derived from EfficientNetv2(ENv2) is proposed, which retains more detail and position information about tiny defects and emphasizes the critical features while maintaining a small model size. Second, the P4 and P5 detection heads were removed from YOLOv5s to decrease the model’s parameters, allowing the model to focus more on small PCB defects. Moreover, the task-specific context decoupling (TSCODE) head is introduced to extract the location and category information about defects separately, strengthening the model’s learning ability. Finally, a GSConv CAA-neck (GC-Neck) consisting of GSConv and C3-GC is proposed, which enhances the model’s capability to extract tiny defect features while reducing parameters. The experimental results show that SGT-YOLO reduced the baseline parameters and floating-point operations by 79% and 35%, respectively. Furthermore, SGT-YOLO improves the mean average precision (mAP) and mAP0.5 by 2.7% and 6.4% on A challenging dataset for PCB defects detection and classification (HRIPCB) datasets, indicating its lightweight and accuracy in PCB defect detection. © 1963-2012 IEEE.",Deep learning; defect detection; printed circuit board (PCB); YOLOv5; Digital arithmetic; 'current; Circuit boards; Computational capability; Deep learning; Defect detection; Fault detection algorithm; Memory capacity; Missed detections; Printed circuit board; YOLOv5; Leak detection
Scopus,"Cai, W.; Chen, H.; Zhang, M.",A survey on collaborative hunting with robotic swarm: Key technologies and application scenarios,,2024,,,,10.1016/j.neucom.2024.128008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196256992&doi=10.1016%2fj.neucom.2024.128008&partnerID=40&md5=f019d1343d73ebe61e5ff585f5510926,"Compared with a single robot, robotic swarm realizes multi-agent cooperative operations through information interaction, and has such characteristics of wide combat monitoring range, flexible combat organization and strong reconfiguration. As the intelligence of robotic swarm, collaborative hunting tasks have been used in the fields of intrusion countermeasures, target elimination, scientific research, target rescue and etc. A comprehensive overview of current state of researches on collaborative hunting tasks with robotic swarm would be beneficial to researchers. Therefore, we provide an overview of research progress related to collaborative hunting tasks of robotic swarm in this paper. Specifically, we analyze key technologies in the collaborative hunting tasks from the perspectives of target searching, hunting task allocation and tracking path planning. Moreover, we summarize the differences of hunting strategies in different scenarios (i.e., air scenario, sea scenario and ground scenario), so it has important guiding significance for future researches. Finally, we discuss future research directions for collaborative hunting tasks of robotic swarm. © 2024 Elsevier B.V.",Collaborative hunting task; Hunting task allocation; Robotic swarm; Target searching; Tracking path planning; Intelligent robots; Multi agent systems; Robot programming; Swarm intelligence; Target tracking; Application scenario; Collaborative hunting task; Hunting task allocation; Key technologies; Robotic swarms; Target searching; Task allocation; Technologies and applications; Technology scenarios; Tracking path planning; animal hunting; female; human; intelligence; short survey; Motion planning
Scopus,"Chen, O.T.-C.; Chang, Y.-X.; Chung, C.-Y.; Cheng, Y.-Y.; Ha, M.-H.",Hardware-Aware Iterative One-Shot Neural Architecture Search With Adaptable Knowledge Distillation for Efficient Edge Computing,,2025,,,,10.1109/ACCESS.2025.3554185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002269481&doi=10.1109%2fACCESS.2025.3554185&partnerID=40&md5=18ba3c58e76f0b1f9b6e73bde1002798,"The growing demand for edge applications calls for efficient and optimized deep neural network models. Neural Architecture Search (NAS) is instrumental in designing such models, but achieving optimal architectures quickly remains a key challenge. To address this, we propose Hardware-aware Iterative One-shot NAS (HIO-NAS), a highly efficient approach that iteratively explores architectures across predefined search spaces for depth, filter, and width, all while respecting hardware constraints. HIO-NAS operates in four main steps: full training, random search, hardware verification, and retraining with adaptable knowledge distillation, repeated for each search space. The hardware-aware mechanism incorporates a lookup table to evaluate and filter subnetworks sampled during random search, ensuring only the most promising candidates proceed to hardware verification. The top-performing subnetwork(s) are then deployed on the target hardware for further validation. The adaptable knowledge distillation technique dynamically adjusts the teacher model’s influence based on deviations in the cost function during training. By progressively refining search spaces, HIO-NAS reduces computational overhead and avoids convergence issues. Its hypernetwork design emphasizes chain-like structures, where layers maintain connectivity. Applied to MobileNet v2 on CIFAR-100 and YOLOv7 on BDD100K, HIO-NAS delivered significant improvements: a 2.1% accuracy gain and 20.8% reduction in latency for MobileNet v2, and better performance over YOLOv7 and its variants. Interestingly, the findings highlight that unit-sharing topologies excel in depth searches, whereas unit-non-sharing topologies perform better in filter and width searches. Overall, HIO-NAS showcases a robust capability for efficiently discovering high-performance, hardware-optimized architectures, making it ideal for edge applications. © 2013 IEEE.",Edge computing; Hardware awareness; Iterative search; Knowledge distillation; Light-weight model; Neural architecture search; One-shot NAS; Deep neural networks; Personnel training; Table lookup; Teaching; Edge computing; Hardware awareness; Iterative search; Knowledge distillation; Light weight; Light-weight model; Neural architecture search; Neural architectures; One-shot neural architecture search; Cost functions
Scopus,"Liang, K.; Zhao, J.; Zhang, Z.; Guan, W.; Pan, M.; Li, M.",Data-driven AI algorithms for construction machinery,,2024,,,,10.1016/j.autcon.2024.105648,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201695100&doi=10.1016%2fj.autcon.2024.105648&partnerID=40&md5=93ce09843c6782270c3ce72a87f3d0c1,"Based on the transition to Industry 4.0, construction operations are gradually moving towards large-scale and high-efficiency development. However, excessive manual labor is becoming a problem, affecting construction industry progress, and causing significant safety hazards. As the continuous development of artificial intelligence and big date technologies, intelligent construction machinery with data-driven methods is considered the best solution for enhancing construction safety and efficiency, which are mainly reflected in prognostic and health management, environment perception and automation control. Therefore, this paper reviews the widespread research on semi-automatic or even fully automatic construction methods reported in the literature. Firstly, it introduces several widely-used artificial intelligence algorithms and their variations. Secondly, three main topics were covered: prognostic and health management applications in experimental and real-world settings, environmental perception systems, and automation control methods for construction machinery. Finally, several research prospects and challenges were presented. © 2024",Artificial intelligence; Automation control; Construction machinery; Data-driven methods; Environment perception; Prognostic and health management; Diagnosis; AI algorithms; Automation controls; Construction machinery; Construction operations; Data driven; Data-driven methods; Environment perceptions; Higher efficiency; Large-scales; Prognostic and health management; Efficiency
Scopus,"Wu, S.; Lu, X.; Guo, C.",YOLOv5_mamba: unmanned aerial vehicle object detection based on bidirectional dense feedback network and adaptive gate feature fusion,,2024,,,,10.1038/s41598-024-73241-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205276103&doi=10.1038%2fs41598-024-73241-x&partnerID=40&md5=c1ea6697fc9d958c77c54e4fbade7dce,"Addressing the problem that the object size in Unmanned Aerial Vehicles (UAVs) aerial images is too small and contains limited feature information, leading to existing detection algorithms having less than ideal performance in small object detection, we propose a UAV aerial object detection system named YOLv5_mamba based on bidirectional dense feedback network and adaptive gate feature fusion. This paper improves the You Only Look Once Version 5 (YOLOv5) algorithm by firstly introducing the Faster Implementation of CSP Bottleneck with 2 convolutions (C2f) module from YOLOv8 into the backbone network to enhance the feature extraction capability of the backbone network. Furthermore, the mamba module and C2f module are introduced to construct a bidirectional dense feedback network to enhance the transfer of contextual information in the neck part. Thirdly, an adaptive gate feature fusion network is proposed to improve the head part of YOLOv5 and enhance its final detection capability. Experimental results on the public UAV aerial dataset VisDrone2019 demonstrate that the proposed algorithm improves the detection accuracy by 9.3% compared to the original YOLOv5 baseline network, showing better detection performance for small objects. For the UCAS_AOD dataset, the proposed algorithm outperforms YOLOv5-s by 9%. In the case of the DIOR dataset, the proposed algorithm exceeds YOLOv5-s by 12%. © The Author(s) 2024.",Adaptive gate feature fusion; Mamba; Object detection; UAV; YOLOv5; algorithm; article; controlled study; detection algorithm; diagnosis; feature extraction; microcatheter; neck; unmanned aerial vehicle
Scopus,"Ali, M.L.; Zhang, Z.","The YOLO Framework: A Comprehensive Review of Evolution, Applications, and Benchmarks in Object Detection",,2024,,,,10.3390/computers13120336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213275573&doi=10.3390%2fcomputers13120336&partnerID=40&md5=bcf406e52c10aa1fd6af3dc57b4d9f4b,"This paper provides a comprehensive review of the YOLO (You Only Look Once) framework up to its latest version, YOLO 11. As a state-of-the-art model for object detection, YOLO has revolutionized the field by achieving an optimal balance between speed and accuracy. The review traces the evolution of YOLO variants, highlighting key architectural improvements, performance benchmarks, and applications in domains such as healthcare, autonomous vehicles, and robotics. It also evaluates the framework’s strengths and limitations in practical scenarios, addressing challenges like small object detection, environmental variability, and computational constraints. By synthesizing findings from recent research, this work identifies critical gaps in the literature and outlines future directions to enhance YOLO’s adaptability, robustness, and integration into emerging technologies. This review provides researchers and practitioners with valuable insights to drive innovation in object detection and related applications. © 2024 by the authors.",deep neural network; performance evaluation; real-time object detection; single stage detection; YOLO; YOLOv10; YOLOv11
Scopus,"Chi, P.; Wang, Z.; Liao, H.; Li, T.; Wu, X.; Zhang, Q.",Towards new-generation of intelligent welding manufacturing: A systematic review on 3D vision measurement and path planning of humanoid welding robots,,2025,,,,10.1016/j.measurement.2024.116065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207885828&doi=10.1016%2fj.measurement.2024.116065&partnerID=40&md5=ab1ad8eed8f47f9c726d345ae2d22d08,"In recent years, intelligent welding technology has emerged as a prominent focus within the welding domain, amalgamating a diverse array of sophisticated technologies, including robotics, computer vision, artificial intelligence, and sensor systems. This integration heralds unprecedented levels of automation in welding processes, endowing them with heightened efficiency, precision, and cognitive capabilities. Notably, scholarly attention has been dedicated to the realms of intelligent welding manufacturing and welding robotics. However, a discernible lacuna exists in the form of a comprehensive review elucidating the pivotal technologies underpinning welding robots, while research about autonomous mobile welding robots appears to have encountered a developmental impasse. In response, this study undertakes a systematic literature review to scrutinize the core technologies of humanoid welding robots (HWR), positing their elevated research prospects within the milieu of next-generation intelligent welding manufacturing. This study explores the hardware of humanoid welding robots as an emerging technology, drawing on current advancements in humanoid robotics. The key technologies relevant to both humanoid and welding robots are also examined, highlighting their integration and potential applications. Initially, the discourse delves into hand-eye calibration methodologies, delineating a multifaceted approach predicated upon a multi-coordinate system tailored to HWR. Subsequently, the significance of visual-based pose estimation and three-dimensional (3D) reconstruction techniques is underscored, given their instrumental role in furnishing HWR with environmental cognition, a discourse expounded meticulously. Additionally, meticulous scrutiny is accorded to mobile robot path planning and dual-robot trajectory planning methodologies, pivotal for orchestrating welding operation sequences tailored to HWR. To assess the job completion and potential applications of HWR, this paper evaluates welding quality judgment and explores the humanoid robot's utility in non-welding scenes. Conclusively, this paper identifies the exigencies confronting HWR and proffers strategic directives delineating avenues for seminal research and practical application within this burgeoning domain. © 2024 Elsevier Ltd",3D target measurement; Hand-eye calibration; Humanoid welding robot; Trajectory planning; Visual-based 3D reconstruction; Industrial robots; Intelligent robots; Mobile robots; Motion planning; Robot programming; Smart manufacturing; Hand/eye calibration; Humanoid robot; Humanoid welding robot; Intelligent welding; Systematic Review; Three-dimensional reconstruction; Three-dimensional target measurement; Trajectory Planning; Visual-based three-dimensional reconstruction; Welding robots; Anthropomorphic robots
Scopus,"Lu, E.H.-C.; Hsieh, Y.-C.",Cross-Field Road Markings Detection Based on Inverse Perspective Mapping,,2024,,,,10.3390/s24248080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213264177&doi=10.3390%2fs24248080&partnerID=40&md5=7895108278eb8fd9b6bc836c02ef9818,"With the rapid development of the autonomous vehicles industry, there has been a dramatic proliferation of research concerned with related works, where road markings detection is an important issue. When there is no public open data in a field, we must collect road markings data and label them by ourselves manually, which is huge labor work and takes lots of time. Moreover, object detection often encounters the problem of small object detection. The detection accuracy often decreases when the detection distance increases. This is primarily because distant objects on the road take up few pixels in the image and object scales vary depending on different distances and perspectives. For the sake of solving the issues mentioned above, this paper utilizes a virtual dataset and open dataset to train the object detection model and cross-field testing in the field of Taiwan roads. In order to make the model more robust and stable, the data augmentation method is employed to generate more data. Therefore, the data are increased through the data augmentation method and homography transformation of images in the limited dataset. Additionally, Inverse Perspective Mapping is performed on the input images to transform them into the bird’s eye view, which solves the “small objects at far distance” problem and the “perspective distortion of objects” problem so that the model can clearly recognize the objects on the road. The model testing on the front-view images and bird’s eye view images also shows a remarkable improvement of accuracy by 18.62%. © 2024 by the authors.",cross-field; deep learning; inverse perspective mapping; object detection; road markings; Inverse transforms; Road and street markings; Augmentation methods; Autonomous Vehicles; Cross-field; Data augmentation; Deep learning; Inverse perspective mappings; Objects detection; Related works; Road marking; Vehicle industry; adult; article; autonomous vehicle; deep learning; diagnosis; human; sake; Taiwan; Mapping
Scopus,"Ranjbarzadeh, R.; Crane, M.; Bendechache, M.",The impact of backbone selection in YOLOv8 models on brain tumor localization,,2025,,,,10.1007/s42044-025-00258-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001482005&doi=10.1007%2fs42044-025-00258-4&partnerID=40&md5=ef2b933df98768ffcc7f8b88aac3e830,"This study investigates the utilization of the You Only Look Once (YOLOv8) deep learning framework for accurately identifying the location of brain tumors in medical imaging. We investigate the effects of model size and pretraining on the accuracy and computational efficiency of tumor detection by utilizing different setups of the YOLOv8 model. These setups include various configurations, ranging from very small to large, and can be pretrained on the COCO dataset or not. The experimental results, carried out on Google Colaboratory using NVIDIA Tesla T4 GPUs, show that pretrained models often achieve better performance by utilizing the extensive feature representations learned from the COCO dataset, resulting in increased precision in tumor location. For instance, the YOLOv8-XS model pretrained on COCO achieves an IoU of 0.278 and a Dice coefficient of 0.435, whereas its non-pretrained counterpart attains only 0.241 IoU and 0.388 Dice, indicating a 15% improvement in tumor localization accuracy. Similarly, pretrained YOLOv8-L achieves 0.269 IoU, outperforming standard object detection models such as Mask R-CNN (IoU: 0.212) and Faster R-CNN (IoU: 0.228). These results highlight the impact of pretraining on model performance, particularly for lightweight architectures, while also revealing diminishing returns for larger models. The research uncovers a subtle connection between the complexity of the model, pretraining, and the time required for training. It emphasizes the possible advantages and constraints of pretraining for various sizes of models. © The Author(s) 2025.",Brain tumor localization; COCO dataset; Deep learning; Object detection; Transfer learning
Scopus,"Gong, J.; Fu, W.; Liu, N.",Design of SAR image target contour enhancement preprocessing module,,2024,,,,10.12305/j.issn.1001-506X.2024.12.09,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213713971&doi=10.12305%2fj.issn.1001-506X.2024.12.09&partnerID=40&md5=eb3ea5e8e2a8e23fa2ad33d8adfdfec7,"In response to the issue that the every Channel's data in three Channels of synthetic aperture radar (SAR) images is the same which may cause Channel Information redundancy when deep learning-based target detection network detects the targets of SAR, a Channel expansion preprocessing algorithm module based on smoothing and sharpening filtering is proposed, which is then named as ORLM (Original, Roberts, Laplace, Mean) block. The proposed algorithm in this article can be encapsulated, integrated, and applied to the data reading program of the target detection algorithm. It can extend the Channel of SAR images with the same data in each Channel, and ensure that the expanded Channel data fully contains the contour information of the target. Through training, testing and comparative experiments of the target detection network with and without the proposed preprocessing algorithm on different ship target detection datasets, the experimental results show that the preprocessing algorithm proposed can be applied to various target detection algorithms and can improve detection accuracy without significantly reducing of the real-time detection Performance. © 2024 Chinese Institute of Electronics. All rights reserved.",deep learning; image processing; synthetic aperture radar (SAR); target detection; Data encapsulation; Deep learning; Laplace transforms; Radar target recognition; Synthetic aperture radar; Deep learning; Detection networks; Images processing; Pre-processing algorithms; Preprocessing modules; Synthetic aperture radar; Synthetic aperture radar images; Target detection algorithm; Targets detection; Three channel; Ship testing
Scopus,"Rigatos, G.; Abbaszadeh, M.; Siano, P.",Nonlinear optimal and flatness-based control methods and applications for complex dynamical systems,,2025,,,,10.1049/PBCE136E,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009517601&doi=10.1049%2fPBCE136E&partnerID=40&md5=1f44a64eecbc09235144ef33ab8a5831,"Robotics, mechatronics and autonomous systems can exhibit complex nonlinear dynamics which can lead to unsatisfactory transients and deviation from setpoints or even to instability. A standard approach in the control of these systems had been the concept of diffeomorphisms to bring a system into a linear form. However, these methods are not straightforward and result in complicated state-space model transformations. In this monograph, new methods have been investigated which are not constrained by the shortcomings of global linearization-based control schemes. They can be implemented in a computationally simple manner, are followed by global stability proofs, and perform better than previous optimal control approaches for a wider class of nonlinear dynamical systems and applications. In this monograph, the authors present two main proven control methods: the nonlinear optimal (H-infinity) control method, and the flatness-based control approach. These methods have shown to be better suited than previous standard approaches in solving control issues, and can be used in a wide class of dynamical systems. They can have a broad range of applications in mechatronics, industrial robotics, space robotics, robotic cranes and pendulums, autonomous vehicles, aerospace systems and satellites, power electronics, biosystems and financial systems. This very comprehensive monograph is a valuable resource for academic researchers and engineers working on control systems and estimation methods, and university staff and graduate students in the fields of control and automation, robotics and mechatronics, electrical engineering, electric power systems and power electronics, biosystems, computer science, financial systems, and physics. The monograph is also a very useful reference for skilled technical professionals developing real world applications. © The Institution of Engineering and Technology and its licensors 2025. All rights reserved.",Dynamics; Electric machine control; Engineering education; Mechatronics; Nonlinear dynamical systems; Optimal control systems; Power electronics; Robotics; State space methods; Students; Bio-systems; Complex dynamical systems; Complex nonlinear dynamics; Control applications; Control approach; Control methods; Financial system; Flatness-based control; Nonlinear optimal; Setpoints; Dynamical systems
Scopus,"Arsenos, A.; Petrongonas, E.; Filippopoulos, O.; Skliros, C.; Kollias, D.; Kollias, S.",NEFELI: A deep-learning detection and tracking pipeline for enhancing autonomy in advanced air mobility,,2024,,,,10.1016/j.ast.2024.109613,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206639436&doi=10.1016%2fj.ast.2024.109613&partnerID=40&md5=3547d23c6cd4285cf818834eae0be6b4,"Efficient detection and accurate collision estimation for non-cooperative aerial vehicles are crucial for the realization of fully autonomous aircraft and Advanced Air Mobility (AAM). This paper introduces NEFELI, a machine learning software utilizing optical sensors to detect and track non-cooperative aerial vehicles. NEFELI's detector employs an enhanced YOLOv5-large model, strengthened with a sliced inference step to enhance detection capabilities for distant, diminutive objects. Furthermore, NEFELI introduces several innovations in its tracking component. A key advancement lies in the creation and utilization of the first large-scale re-identification (Re-ID) dataset of aerial objects. This dataset is used to train the deep learning appearance (Re-ID) model of the tracking module and integrates appearance information into the detection and tracking pipeline, resulting in more robust and reliable tracking performance. Moreover, the tracking model combines the deep learning appearance model with a Kalman Filter-based motion model to address the challenge of precisely tracking distant aerial objects. Notably, an extensive comparative analysis that was conducted showed that NEFELI outperforms state-of-the-art detection and tracking models in terms of Higher Order Tracking Accuracy (HOTA) metric, ID switches, and Association Accuracy (AssA) by a wide margin. A crucial aspect of this work is NEFELI's software architecture design, which enables efficient implementation on a low SWaP (Size, Weight, and Power) edge Graphic Processing Unit (GPU). To further showcase NEFELI's generalization capabilities and edge implementation performance, real-world flight experiments with small UAVs were carried out. The experimental results demonstrate NEFELI's ability to detect and track small UAVs at distances of up to 145 m in real-time speed of 6.7 fps. © 2024 Elsevier Masson SAS",Advanced air mobility (AAM); Autonomous aircraft; Autonomy; Detect and avoid (DAA); Detection; Edge computing; Non-cooperative traffic management; Onboard computing (OBC); Re-identification; Sense and avoid; Tracking; Unmanned aerial vehicle (UAV); Air navigation; Air traffic control; Air transportation; Aircraft detection; Benchmarking; Control towers; Edge computing; Image coding; Image segmentation; Image texture; Image thinning; Kalman filters; Object tracking; Railroad traffic control; Street traffic control; Time switches; Unmanned aerial vehicles (UAV); Advanced air mobility; Aerial vehicle; Autonomous aircraft; Autonomy; Detect and avoid; Detection; Edge computing; Non-cooperative; Non-cooperative traffic management; Onboard computing; Re identifications; Sense and avoid; Tracking; Traffic management; Unmanned aerial vehicle; Aircraft accidents
Scopus,"Hussein, M.; Zhu, W.-X.",A real-time ghost machine learning model built on YOLOv8 for traffic road signs detection and classification in Germany,,2024,,,,10.1007/s00530-024-01527-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209582294&doi=10.1007%2fs00530-024-01527-1&partnerID=40&md5=30e8a2d47d94c473632f407bd816aaee,"Identifying traffic signs is an essential part of traffic safety and self-driving systems. In real life, the driving environment is changing, making detecting traffic signs wisely and economically vital. The traffic sign detection problem has several small objects and complex ambient interference. The detecting situation also requires a practical and lightweight detection model. This study proposes a new lightweight model, the enhanced Ghost-YOLOv8, based on lightweight modules GhostConv and C3Ghost, based on the YOLOv8 model. It used a light method to extract the features, significantly speeding up inference. In addition to small, medium, and large objects, the head was expanded to include a new multi-scale feature extraction module layer focused on x-small. The experiment results show that when using the German Traffic Sign Detection Benchmark (GTSDB) dataset with three classes, the enhanced Ghost-YOLOv8 has mAP (0.50) of 99.4%and has fewer computations than the YOLOv8 model by 155.2 GFLOPs and has 18.6 Mparameters, which represents only 27.3% from the parameters used in the base model. Also, we suggested a new dataset called the GTSDB-43 dataset, which expanded the number of classes on the GTSDB dataset from three or four main classes to 43 classes and mentioned their main category type simultaneously. Compared with notable algorithms, this method's accuracy and speed are competitive. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",Lightweight; Object detection; Real-time; The enhanced Ghost-YOLOv8; YOLOv8 model; Adversarial machine learning; Inference engines; Machine learning; Motor transportation; Object detection; Benchmark datasets; Lightweight; Machine learning models; Objects detection; Real- time; Road sign classifications; Road sign detection; The enhanced ghost-YOLOv8; Traffic sign detection; YOLOv8 model; Traffic signs
Scopus,"Gao, W.; Gu, W.; Yin, Y.; Li, T.; Dong, P.",ODCS-YOLO detection algorithm for rail surface defects based on omni-dimensional dynamic convolution and context augmentation module,,2024,,,,10.1088/1361-6501/ad5dd5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198605669&doi=10.1088%2f1361-6501%2fad5dd5&partnerID=40&md5=3d3f29c77852c3ce5522be9477cbf085,"To solve the problems of easy miss and false detection on rail surface defects caused by small size, dense target, and high similarity between features and background, this paper proposed an improved detection algorithm in complex background. First, the conventional convolution of YOLOv5 backbone network is replaced with omni-dimensional dynamic convolution (ODConv), which improves the feature extraction capability of the network without increasing the computational cost; second, to improve the model’s performance in detecting tiny objects, a two-layer context augmentation module (CAM) is introduced into the path aggregation network (PAN) structure; finally, the traditional non-maximum suppression (NMS) algorithm is replaced by the Soft-NMS algorithm in the network post-processing to reduce the false-alarm and miss-rate. The experimental results on the Railway Track Fault Detection public dataset show that the OD-YOLO (OD stands for ODConv) and C-PAN (CAM module is introduced into PAN) structures could achieve better performance in the same type of improved algorithms; compared with the baseline algorithm YOLOv5, the ODCS-YOLO (OD stands for ODConv, C stands for CAM and S stands for Soft-NMS) algorithm improves the precision by 12.4%, the recall by 3.6%, the map50 by 8.6% and the GFLOPs is reduced by 0.6. Compared with seven classical object detection algorithms, the ODCS-YOLO algorithm achieves the highest detection accuracy, which makes it able to meet the real-time detection requirements of rail surface defects in real working conditions. The ODCS-YOLO model provides certain technical support for the defects detection and a new method for the detection of dense small objects. © 2024 IOP Publishing Ltd.",CAM; dense small object; ODConv; ODCS-YOLO; rail defects detection; Soft-NMS; C (programming language); Fault detection; Feature extraction; Object detection; Signal detection; Surface defects; Context augmentation module; Defect detection; Dense small object; Dimensional dynamics; Non-maximum suppression; ODCS-YOLO; Omni-dimensional dynamic convolution; Rail defect detection; Rail defects; Small objects; Soft-non-maximum suppression; Convolution
Scopus,"Wang, L.; Jiang, F.; Zhu, F.; Ren, L.","Enhanced Multi-Target Detection in Complex Traffic Using an Improved YOLOv8 with SE Attention, DCN_C2f, and SIoU",,2024,,,,10.3390/wevj15120586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213485561&doi=10.3390%2fwevj15120586&partnerID=40&md5=da91324e462fdbb674ee71b53a8ada9b,"This paper presents an enhanced YOLOv8 model designed to address multi-target detection challenges in complex traffic scenarios. The model integrates the Squeeze-and-Excitation attention mechanism, the deformable convolution C2f module, and the smooth IoU loss function, achieving significant improvements in detection accuracy and robustness in various complex environments. Experimental results show that the enhanced YOLOv8 model outperforms existing YOLO solutions across multiple metrics, particularly in precision and recall. Specifically, the enhanced model achieves 83.8% precision and 82.7% recall, improving 1.05 times in precision and 1.1 times in recall compared to the average precision (79.7%) and recall (75.4%) of other YOLO series models. In terms of mAP_0.5, the enhanced model achieves 89%, representing a 1.05-fold improvement over the average mAP_0.5 (84.4%) of YOLO series models. For mAP_0.5:0.95, the enhanced model reaches 76.5%, which is a 1.1-fold improvement over the average mAP_0.5:0.95 (69.7%) of YOLO series models. These improvements demonstrate the superior performance of the proposed model in multi-scale and complex scenarios, providing strong support for intelligent transportation systems and autonomous driving. © 2024 by the authors.",complex traffic environment; deformable convolution C2f module; intelligent transportation; multi-target detection; object detection; SE attention mechanism; smooth IoU loss function; YOLOv8; Object detection; Object recognition; Attention mechanisms; Complex traffic environment; Deformable convolution c2f module; Intelligent transportation; Loss functions; Multi-target detection; Objects detection; SE attention mechanism; Smooth IoU loss function; Traffic environment; YOLOv8; Intelligent systems
Scopus,"Jing, X.; Wang, Y.; Li, D.; Pan, W.",Melon ripeness detection by an improved object detection algorithm for resource constrained environments,,2024,,,,10.1186/s13007-024-01259-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201394021&doi=10.1186%2fs13007-024-01259-3&partnerID=40&md5=f10a0ba5c0f6d1267e0297080467f23a,"Background: Ripeness is a phenotype that significantly impacts the quality of fruits, constituting a crucial factor in the cultivation and harvesting processes. Manual detection methods and experimental analysis, however, are inefficient and costly. Results: In this study, we propose a lightweight and efficient melon ripeness detection method, MRD-YOLO, based on an improved object detection algorithm. The method combines a lightweight backbone network, MobileNetV3, a design paradigm Slim-neck, and a Coordinate Attention mechanism. Additionally, we have created a large-scale melon dataset sourced from a greenhouse based on ripeness. This dataset contains common complexities encountered in the field environment, such as occlusions, overlapping, and varying light intensities. MRD-YOLO achieves a mean Average Precision of 97.4% on this dataset, achieving accurate and reliable melon ripeness detection. Moreover, the method demands only 4.8 G FLOPs and 2.06 M parameters, representing 58.5% and 68.4% of the baseline YOLOv8n model, respectively. It comprehensively outperforms existing methods in terms of balanced accuracy and computational efficiency. Furthermore, it maintains real-time inference capability in GPU environments and demonstrates exceptional inference speed in CPU environments. The lightweight design of MRD-YOLO is anticipated to be deployed in various resource constrained mobile and edge devices, such as picking robots. Particularly noteworthy is its performance when tested on two melon datasets obtained from the Roboflow platform, achieving a mean Average Precision of 85.9%. This underscores its excellent generalization ability on untrained data. Conclusions: This study presents an efficient method for melon ripeness detection, and the dataset utilized in this study, alongside the detection method, will provide a valuable reference for ripeness detection across various types of fruits. © The Author(s) 2024.",Deep learning; Melon; Object detection; Ripeness detection
Scopus,"Li, X.; Liu, J.; Zhao, G.; Liu, L.; Zhang, W.; Hu, X.; Cheng, S.",High precision single-photon object detection via deep neural networks,,2024,,,,10.1364/OE.533032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206007733&doi=10.1364%2fOE.533032&partnerID=40&md5=c4936353b267e98d6cb787e95fbcfea3,"Single-photon imaging is an emerging technology in sensing that is capable of imaging and identifying remote objects under extreme conditions. However, it faces several challenges, such as low resolution and high noise, to do the task of object detection. In this work, we propose an enhanced You Only Look Once network to identify and localize objects within images generated by single-photon sensing. We then experimentally test the proposed network on both the self-built single-photon dataset and the VisDrone2019 public dataset. Our results show that our network achieves a higher detection accuracy than the baseline models. Moreover, it admits a higher average precision in detecting small single-photon objects. Our work is expected to aid significant progress in exploring practical applications of single-photon sensing. © 2024 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.",Atomic beams; Electron beams; Open access; Photons; Emerging technologies; Extreme conditions; High-precision; Low-high; Lower resolution; Neural-networks; Objects detection; Remote object; Single photons; Single-photon imaging; accuracy; article; controlled study; deep neural network; diagnosis; female; noise; photon; Deep neural networks
Scopus,"Kong, L.; Wang, Y.; Chang, D.; Zhao, Y.",Temporal-Enhanced Radar and Camera Fusion for Object Detection,,2024,,,,10.1145/3700442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215566371&doi=10.1145%2f3700442&partnerID=40&md5=7dc28762cbfab52891f45135c787c9f2,"Recently, object detection methods based on multi-modal fusion have gained widespread adoption in autonomous driving, proving to be valuable for detecting objects in dynamic environments. Among them, millimeter wave (mmWave) radar is commonly utilized as an effective complement to cameras, as it is almost unaffected by harsh weather conditions. However, current approaches that fuse mmWave radar and camera often overlook the correlation between the two modalities, failing to fully exploit their complementary features. To address this, we propose a temporal-enhanced radar and camera fusion network to explore the correlation between these two modalities and learn a comprehensive representation for object detection. In our model, a temporal fusion model is introduced to fuse mmWave radar features from different moments, thus mitigating the problem of mmWave radar point-object mismatch due to object movement. Moreover, a new correlation-based fusion strategy using the dedicated mask cross-attention is proposed to fuse mmWave radar and vision features more effectively. Finally, we design a gate feature pyramid network that selects shallow texture information based on deep semantic information to obtain more representative features. The experimental results on the nuScenes benchmark demonstrate the effectiveness of our proposed method. © 2024 Copyright held by the owner/author(s).",Automatic Driving; Cross Attention; Ensemble; Benchmarking; Monolithic microwave integrated circuits; Object recognition; Automatic driving; Autonomous driving; Cross attention; Detecting objects; Ensemble; Millimeter-wave radar; Millimetre-wave radar; Multi-modal fusion; Object detection method; Objects detection; Object detection
Scopus,"Nguyen, V.-T.; Nguyen, P.-T.; Su, S.-F.; Tan, P.X.; Bui, T.-L.",Vision-Based Pick and Place Control System for Industrial Robots Using an Eye-in-Hand Camera,,2025,,,,10.1109/ACCESS.2025.3536496,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217026346&doi=10.1109%2fACCESS.2025.3536496&partnerID=40&md5=3e24fabda40d0dfb5fe74136beffa31e,"In this paper, we present a vision-based pick-and-place control system for industrial robots using an eye-in-hand camera. In industry, using robots with cameras greatly improves efficiency and performance. Previous studies have focused on utilizing robotic arms for the pick-and-place process in simulated environments. The challenge when experimenting with real systems lies in aligning the coordinate systems between the robot and the camera, as well as ensuring high data accuracy during experimentation. To address this issue, our research focuses on utilizing a low-cost 2D camera combined with deep learning algorithms mounted on the end-effector of the robotic arm. This study is evaluated in both simulation and real-world experiments. We propose a novel approach that combines the YOLOv7 (You Only Look Once V7) deep learning network with GAN (Generative Adversarial Networks) to achieve fast and accurate object recognition. This system uses deep learning to process camera data to extract object positions for the robot in real-time. Due to its advantages of fast inference and high accuracy, YOLO is applied as the baseline for research. By training the deep learning model on diverse objects, it effectively recognizes and detects any object in the robot's workspace. Through experimental results, we demonstrate the feasibility and effectiveness of our vision-based pick-and-place system. Our research contributes an important advancement in the field of industrial robots by showcasing the potential of using a 2D camera and an integrated deep learning system for object manipulation.  © 2013 IEEE.",calibration vision; object detection; robot real-time; Robotic arm; vision; Deep learning; End effectors; Generative adversarial networks; Machine vision; Robot learning; Robot vision; Robotic arms; SLAM robotics; Calibration vision; Efficiency and performance; Eye-in-hand; Objects detection; Pick and place; Real systems; Real- time; Robot real-time; Simulated environment; Vision based; Industrial robots
Scopus,"Sun, F.; He, N.; Wang, X.; Liu, H.; Zou, Y.",YOLOv7-P: a lighter and more effective UAV aerial photography object detection algorithm,,2024,,,,10.1007/s11760-024-03476-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201712675&doi=10.1007%2fs11760-024-03476-8&partnerID=40&md5=890950ddde3068b3376b3f5c64ad39f4,"Because of the special way an unmanned aerial vehicle (UAV) acquires aerial photography, UAV images have the characteristics of large coverage area, complex background, and a large proportion of small targets, which exacerbate the difficulty of object detection. Additionally, UAV-based aerial image detection needs to meet lightweight and real-time capabilities. To address these issues, this paper proposes a lightweight model YOLOv7-P that is based on YOLOv7 but has a stronger detection capability for small targets. First, partial convolution (PConv) is used to reduce redundant parameters and computation in YOLOv7. Second, an optimal combination of detection heads is determined that can significantly improve the detection performance of small objects. Third, a novel lightweight convolution called PConv-wide is proposed to replace RepConv in the network, thus simplifying the network without affecting detection accuracy. Finally, the normalized wasserstein distance loss is reasonably combined with the complete intersection over union loss to further improve the sensitivity of the network to small targets. The proposed YOLOv7-P model strikes a delicate balance between precision and parameter count. Compared with the baseline YOLOv7 network, it reduces parameter count by 47.1% without increasing computational complexity and boosts AP50 by 8% and mAP by 5.4% on the VisDrone dataset. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",Lightweight; Small object detection; UAV aerial imaging; YOLOv7; Aircraft detection; Image enhancement; Photographic equipment; Unmanned aerial vehicles (UAV); Aerial imaging; Aerial vehicle; Coverage area; Lightweight; Object detection algorithms; Small object detection; Small targets; Unmanned aerial vehicle aerial imaging; Vehicle images; YOLOv7; Aerial photography
Scopus,"Jing, Y.; Sun, Y.; Wang, Q.",Lightweight Single-Stage Network for Gas Leak Detection Based on Infrared Imaging,,2025,,,,10.1109/TIM.2025.3561424,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002786786&doi=10.1109%2fTIM.2025.3561424&partnerID=40&md5=88b5090d5bf4452fa3cdb1c420b8fb5a,"Gas leak detection is essential for real-time monitoring and safety early warning of industrial production, manufacturing, and transportation processes. For many years, infrared optical gas imaging (IOGI) has been widely used in the field of gas leak monitoring, but the task still faces great challenges due to the limitations of infrared imaging principle and system technology, as well as the characteristics of insubstantial gas objects. First, a dataset containing 66950 infrared images is built, which covers gas leak samples with different scales, shapes, and blurring levels. Second, a single-stage gas leak detection network model named dual layer focus aggregation network (DLFANet) was designed. Specifically, a lightweight feature extraction cross-stage partially efficient two-layer aggregation network cross stage partial-efficient dual layer aggregation network (CSP-EDLAN) module is designed to enhance the transmission of gradient flow information and cross-channel information interaction, where dual convolution (DualConv) is utilized to reduce the computational consumption of feature extraction. A focal modulation module is introduced into the backbone network to realize the focus of the gas target by integrating the characteristic information of different scales. In addition, the wise intersection shape intersection over union (Wise-Shape-IoU) loss function with a dynamic non-monotonic mechanism and shape constraint capability is designed to prevent low-quality samples from generating harmful gradients, which makes the bounding box regression (BBR) of gas targets with greater accuracy. Finally, extensive experimental results on the constructed dataset show that the proposed DLFANet strikes a better balance between detection accuracy (map) and speed frame per second (FPS) while predicting the BBR of gaseous objects more accurately compared to state-of-the-art models. © 2025 IEEE.",Deep learning; industrial gas leakage; infrared imaging; insubstantial gas object detection; Critical path analysis; Information leakage; Network security; Thermography (imaging); Aggregation network; Deep learning; Gas leak detection; Gas leakages; Gas leaks; Industrial gas; Industrial gas leakage; Insubstantial gas object detection; Objects detection; Single stage; Leak detection
Scopus,"Nasir, F.A.; Liaquat, S.; Naqvi, I.H.; Khurshid, K.; Mahyuddin, N.M.",Improved You Only Look Once (YOLOv5)-based Passive Missile Detection using Simulated Solar Blind Ultraviolet Signatures,,2025,,,,10.1109/MAES.2025.3555249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001527622&doi=10.1109%2fMAES.2025.3555249&partnerID=40&md5=d7bfc4606b6e52de9e21802e71bba60a,"Civilian and military aircraft are increasingly vulnerable to passive missile threats, such as short-range or within-visual-range air-to-air missiles (SRAAMs/WVRAAMs) and man-portable air defense systems (MANPADS). These missiles evade detection by an aircraft's radar warning receiver (RWR) due to their lack of radio frequency (RF) emissions. This paper presents a novel deep learning-based passive missile detection algorithm using simulated solar blind ultraviolet (SBUV) signatures, which offer unique advantages over traditional infrared (IR) signatures. The algorithm is built on an improved YOLOv5 (You Only Look Once) framework, capable of detecting and classifying UV signatures in real-time from sequential image data. The architecture of YOLOv5 has been modified for improved SBUV detection by recalculation of anchor box sizes, non-maximum suppression (NMS) threshold re-adjustment, SBUV specific data augmentations and increased resolution in detection heads. To overcome the challenge of limited training data, we employ advanced data synthesis techniques to create realistic training datasets derived from 3D missile and aircraft combat scenario simulations in the SBUV spectrum. These simulations incorporate diverse parameters and conditions to closely replicate real-world scenarios, ensuring high fidelity and robustness. Performance evaluation against real-world scenarios revealed an F1-score of 95% and a mean average precision (mAP) of 95% for synthetic data, compared to 88.36% and 85%, respectively, for real-world data. The algorithm, achieved a detection accuracy of 92% on synthetic data and 86% on real-world data, demonstrating its reliability and robustness. Validation under varying environmental conditions demonstrated optimal performance under clear skies, achieving benchmark results. Minimal performance degradation was observed under low-light conditions, such as dusk or dawn, as well as fog. However, overcast conditions posed the most significant challenge, reducing detection accuracy from 86% to 79% and the F1-score from 88.36% to 80%. The processing speed of the proposed algorithm satisfies the real-time requirements of modern missile warning systems, making it a potential candidate for operational deployment. © 1986-2012 IEEE.",Detection accuracy; F1-score; Infrared (IR) signatures; Passive missile detection; Solar blind ultraviolet (SBUV) signatures; YOLOv5.; Air to surface missiles; Aircraft detection; Benchmarking; Building codes; Chemical sensors; Fighter aircraft; Radar warning systems; Remote sensing; Solar irradiance; Surface to air missiles; Temperature sensors; Tracking radar; Condition; Detection accuracy; F1 scores; Infrared signature; Passive missile detection; Real-world scenario; Solar blind ultraviolet; Solar blind ultraviolet  signature; Synthetic data; YOLOv5.; Air to air missiles
Scopus,"Sánchez Pedroche, D.; Amigo, D.; García, J.; Molina, J.M.; Zubasti, P.",Drone Swarm for Distributed Video Surveillance of Roads and Car Tracking,,2024,,,,10.3390/drones8110695,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210318621&doi=10.3390%2fdrones8110695&partnerID=40&md5=f6a805c14205d31c37545bc4fa566790,"This study proposes a swarm-based Unmanned Aerial Vehicle (UAV) system designed for surveillance tasks, specifically for detecting and tracking ground vehicles. The proposal is to assess how a system consisting of multiple cooperating UAVs can enhance performance by utilizing fast detection algorithms. Within the study, the differences in one-stage and two-stage detection models have been considered, revealing that while two-stage models offer improved accuracy, their increased computation time renders them impractical for real-time applications. Consequently, faster one-stage models, such as the tested YOLOv8 architectures, appear to be a more viable option for real-time operations. Notably, the swarm-based approach enables these faster algorithms to achieve an accuracy level comparable to that of slower models. Overall, the experimentation analysis demonstrates how larger YOLO architectures exhibit longer processing times in exchange for superior tracking success rates. However, the inclusion of additional UAVs introduced in the system outweighed the choice of the tracking algorithm if the mission is correctly configured, thus demonstrating that the swarm-based approach facilitates the use of faster algorithms while maintaining performance levels comparable to slower alternatives. However, the perspectives provided by the included UAVs hold additional significance, as they are essential for achieving enhanced results. © 2024 by the authors.",UAV surveillance; UAV swarm configuration; vehicle detection and tracking; Drones; Security systems; Swarm intelligence; Aerial vehicle; Detection and tracking; Distributed video surveillances; Fast algorithms; Swarm-based approach; Unmanned aerial vehicle surveillance; Unmanned aerial vehicle swarm configuration; Vehicle surveillances; Vehicles detection; Aircraft detection
Scopus,"Yu, X.; Jiang, T.; Zhu, Y.; Li, L.; Fan, F.; Jin, X.",FEL-YoloV8: A New Algorithm for Accurate Monitoring Soybean Seedling Emergence Rates and Growth Uniformity,,2025,,,,10.1109/TGRS.2025.3578800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007896170&doi=10.1109%2fTGRS.2025.3578800&partnerID=40&md5=1ecdb6446a4825355d55878bdcb4a188,"Effective monitoring of soybean emergence rates and growth uniformity is crucial for soybean breeding evaluation and field management. Although Unmanned Aerial Vehicles (UAVs) have improved image acquisition efficiency, detecting soybean seedlings during the vegetative emergence (VE) stage remains challenging due to their small size, low contrast, and insufficient information. Existing studies often neglect emergence rate and growth uniformity quantification. This study proposes a fully automated method for monitoring soybean emergence rate and growth uniformity, applicable to both ground-based and UAV platforms. The FEL-YoloV8 model was constructed by enhancing the feature extraction module, improving the feature fusion module, and incorporating model lightweight modules. Based on the detection results from the FEL-YoloV8 model, the missing seedling locations, counts, and growth uniformity of soybeans were estimated. The study shows that the feature enhancement and fusion modules improved the model’s performance by 2.10%. Under comparable computational complexity, the performance of the FEL-YoloV8 model (AP = 0.979) surpasses current advanced models (e.g., Faster R-CNN, RT-DETR, YoloV8n, s, m, l, x, YoloV9, YoloV10, L-FFCA-Yolo, and TPH-YoloV5). The detection results from the FEL-YoloV8 model enabled the estimation of missing seedling locations and quantities in soybeans. The proposed method enables fully automated monitoring of soybean emergence rates and growth uniformity. This approach lays the foundation for accurate multi-platform evaluation of soybean emergence rates and growth uniformity, guiding soybean breeding evaluation and field management. © 1980-2012 IEEE.",Emergence rates; Feature fusion module; Growth uniformity; Soybean; YoloV8; Aircraft detection; Antennas; Automation; Feature extraction; Nitrogen fixation; Unmanned aerial vehicles (UAV); Aerial vehicle; Emergence rate; Feature fusion module; Features fusions; Field management; Fusion modules; Growth uniformity; Soybean; Soybean seedlings; Yolov8; Image enhancement
Scopus,"Yu, B.; Zhu, Z.; Chen, Y.; Wang, J.; Gao, K.; Qian, X.",A Diffusion model-based intelligent optimization method of rural road environments,,2025,,,,10.1016/j.ijtst.2025.01.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217924747&doi=10.1016%2fj.ijtst.2025.01.014&partnerID=40&md5=8cdcf3c0289cf3243df8d615499ea863,"Well-designed rural road environments can guide drivers to adopt reasonable driving behaviors, thereby significantly improving the driving experience and ensuring road safety. Existing methods for optimizing rural road environments mainly rely on expert knowledge, have low automation degrees, and are limited in efficiency and accuracy. Therefore, this study aims to propose an intelligent optimization method for rural road environments by using image generation technology. Using environment images from a naturalistic driving dataset, the area and location information of semantic components (e.g., lane markings, vegetation, guardrails, traffic signs, etc.) in rural road environments are extracted, and their impacts on driving speed is analyzed based on explainable machine learning (XGBoost and SHAP). These impacts are then utilized to determine how to adjust and optimize the road environment components at appropriate locations (i.e., obtain the optimization scheme). Then, a novel image generation technique, Diffusion model, is employed to establish an intelligent optimization method, which can directly generate optimized images of rural road environments. Compared to traditional manual mapping or other popular image generation algorithms such as CycleGAN, the method proposed in this study has the advantages of high efficiency, labor saving, and better image generation quality. This study can facilitate the design and optimization of rural road environments and enhance rural road safety in a more intelligent way. © 2025 Tongji University and Tongji University Press",Diffusion model; Explainable machine learning; Image generation technology; Intelligent optimization; Rural road environments; Diffusion model; Explainable machine learning; Generation technologies; Image generation technology; Image generations; Intelligent optimization; Intelligent optimization method; Machine-learning; Road environment; Rural road environment; Road and street markings
Scopus,"Li, H.; Peng, T.; Qiao, N.; Guan, Z.; Feng, X.; Guo, P.; Duan, T.; Gong, J.",CrackTinyNet: A novel deep learning model specifically designed for superior performance in tiny road surface crack detection,,2024,,,,10.1049/itr2.12497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186546798&doi=10.1049%2fitr2.12497&partnerID=40&md5=73f51dc4af152782cc70e355d4204095,"With the rapid advancement of highway construction, the maintenance of highway infrastructure has become particularly vital. During highway maintenance, the effective detection of tiny road surface cracks helps to extend the lifespan of roads and enhance traffic efficiency and safety. To elevate the performance of existing road detection models, the CrackTinyNet (CrTNet) algorithm is specifically proposed for detecting tiny road surface cracks. This algorithm utilizes the novel BiFormer general visual transformer, designed expressly for tiny objects, and optimizes the loss function to a normalized Wasserstein distance loss function. It replaces traditional downsampling with Space-to-Depth Conv to prevent the excessive loss of tiny object information in the network structure. To highlight the model's advantage in detecting tiny road cracks, ablation experiments and comparison trials were conducted with mainstream deep learning models for crack detection. The results of the ablation experiments show that, compared to the baseline, CrTNet improved the Mean Average Precision (MAP) by 0.22. When compared to other network models suitable for road detection, these results exhibited an improvement of over 8.9%. In conclusion, the CrTNet proposed in this study enables a more accurate detection of tiny road cracks, playing a significant role in the advancement of intelligent traffic management. © 2024 The Author(s). IET Intelligent Transport Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",crack detection; object detection; road safety; road traffic; Ablation; Crack detection; Deep learning; Highway administration; Highway planning; Highway traffic control; Intelligent systems; Learning systems; Motor transportation; Roads and streets; Surface defects; Learning models; Loss functions; Objects detection; Performance; Road cracks; Road detection; Road safety; Road surfaces; Road traffic; Surface cracks; Object detection
Scopus,"Xing, Z.; Meng, Z.; Zheng, G.; Ma, G.; Yang, L.; Guo, X.; Tan, L.; Jiang, Y.; Wu, H.",Intelligent rehabilitation in an aging population: empowering human-machine interaction for hand function rehabilitation through 3D deep learning and point cloud,,2025,,,,10.3389/fncom.2025.1543643,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005275255&doi=10.3389%2ffncom.2025.1543643&partnerID=40&md5=93a029ba51dfa0ab16d4dba35aadcef4,"Human-machine interaction and computational neuroscience have brought unprecedented application prospects to the field of medical rehabilitation, especially for the elderly population, where the decline and recovery of hand function have become a significant concern. Responding to the special needs under the context of normalized epidemic prevention and control and the aging trend of the population, this research proposes a method based on a 3D deep learning model to process laser sensor point cloud data, aiming to achieve non-contact gesture surface feature analysis for application in the field of intelligent rehabilitation of human-machine interaction hand functions. By integrating key technologies such as the collection of hand surface point clouds, local feature extraction, and abstraction and enhancement of dimensional information, this research has constructed an accurate gesture surface feature analysis system. In terms of experimental results, this research validated the superior performance of the proposed model in recognizing hand surface point clouds, with an average accuracy of 88.72%. The research findings are of significant importance for promoting the development of non-contact intelligent rehabilitation technology for hand functions and enhancing the safe and comfortable interaction methods for the elderly and rehabilitation patients. Copyright © 2025 Xing, Meng, Zheng, Ma, Yang, Guo, Tan, Jiang and Wu.",3D perception; deep learning; human-machine interaction; neural network; non-contact rehabilitation; Assistive technology; Computational neuroscience; Deep neural networks; Functional electric stimulation; Functional neural stimulation; Human rehabilitation equipment; Patient monitoring; 3D perception; Deep learning; Hand function; Human machine interaction; Neural-networks; Non-contact; Non-contact rehabilitation; Point-clouds; Surface feature analysis; Surface points; accuracy; aging; algorithm; Article; artificial intelligence; artificial neural network; cloud; confusion; deep learning; epidemic; feature extraction; gesture; hand function; human; human machine interaction; intelligent rehabilitation; learning algorithm; machine learning; nerve cell network; neuroscience; non contact rehabilitation; perception; rehabilitation patient; theoretical neuroscience; training; Functional assessment
Scopus,"Nguyen, A.V.; Hoang, V.T.; Tran, T.H.",Construction of Robotics and Application of the Optical-Flow Algorithm in Determining Robot Motions,,2024,,,,10.3390/app14209342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207335312&doi=10.3390%2fapp14209342&partnerID=40&md5=9a6568d9b91bc7ccc32f8b63a01de0a9,"This article presents the research results in the application of image processing in determining the position, direction, and moving speed of different objects when they move in the free space in the field of view of measuring cameras. The research includes developing an algorithm to detect, identify, and locate objects and an algorithm to calculate the movement direction and instantaneous velocity of the object. Two robots with measurement systems were designed for testing, one carries the investigating object, and the other the camera. These robots can communicate with a computer system using a tele-wireless system. A program was also built for capturing images and analyzing the state of the model automatically. Experimental results show that the position, angle, and velocity of different objects can be captured well. The average error in determining the direction of movement is an average of 1.25°, and the error of the moving speed is less than 0.5 m/s. The research results provide a potential tool for designing robots for highly effective detection. The algorithm and the measurement system are simple and inexpensive but highly effective, and they can be used in the initial process of determining an object. A detailed description of the algorithm, robot system, and testing will be presented in this study. © 2024 by the authors.",computer vision; controlling robot; image processing; optical flow; robot; Industrial robots; Machine vision; Microrobots; Object detection; Optical data processing; Robot applications; Robot vision; Controling robot; Field of views; Free spaces; Images processing; Measurement system; Moving speed; Optical flow algorithm; Optical-; Research results; Robot motion; Optical flows
Scopus,"Liu, X.; Li, H.",A study on UAV target detection and 3D positioning methods based on the improved deformable DETR model and multi-view geometry,,2025,,,,10.1177/16878132251315505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216127423&doi=10.1177%2f16878132251315505&partnerID=40&md5=ba1cb2360dd200bffcb92be4b3699991,"This paper addresses critical challenges in Unmanned Aerial Vehicle (UAV) target detection and 3D positioning, specifically inaccuracies in localization and lack of robustness in complex environments. The objective of this research is to improve UAV detection and positioning accuracy by proposing an enhanced Deformable DETR (Detection Transformer) model integrated with multi-view geometry theory. To achieve this, the study first preprocesses UAV-collected data, then optimizes the convolutional layers of the original DETR model to better handle object occlusion and scale variations. Furthermore, the research incorporates multi-view geometric modeling and multimodal fusion strategies to enhance detection accuracy during the target recognition process. Experimental results demonstrate that the proposed approach achieves over 70% detection accuracy, significantly outperforming traditional methods. The findings underscore the effectiveness of combining the improved Deformable DETR model with multi-view geometry for high-precision detection and 3D localization in complex environments. This research has significant implications for UAV-based applications, such as autonomous navigation, surveillance, and search-and-rescue missions, where precise target detection and 3D positioning are critical for successful operation. © The Author(s) 2025.",image denoising; improved deformable DETR model; Multiple unmanned aerial vehicle object detection; multiple view geometry; three-dimensional positioning
Scopus,"Lin, T.; Ren, Z.; Zhu, L.; Zhu, Y.; Feng, K.; Ding, W.; Yan, K.; Beer, M.",A Systematic Review of Multi-Sensor Information Fusion for Equipment Fault Diagnosis,,2025,,,,10.1109/TIM.2025.3529577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215662454&doi=10.1109%2fTIM.2025.3529577&partnerID=40&md5=55588458aebc2fafe5989cd27d81f5ed,"In contrast to fault diagnosis relying solely on a single sensor, the method of multi-sensor information fusion for fault diagnosis (MSIFFD) broadens the spectrum of available information sources. It is renowned for its high accuracy and reliability, traits that have attracted growing attention within the research community and led to the generation of a substantial body of publications. However, there is currently a lack of a comprehensive and systematic review in this domain. Thereby, this review aims to thoroughly explore all research achievements in the field of MSIFFD. At the outset, an analysis is undertaken to delineate the fusion level of multi-sensor information, with a specific focus on its location and types of inputs and outputs within the information flow. Subsequently, an examination of the six primary fundamental operations for amalgamating multi-sensor information is undertaken to clarify the motivations and processes involved in information fusion across various operations. Following this, factors influencing fusion diagnostics and strategies aimed at improving their performance are examined to explore approaches for enhancing fusion accuracy. The subsequent sections delve into the analysis of sensor types and application scenarios, providing a reference guide for practical applications. Finally, this review outlines potential future challenges in the field of MSIFFD and provides a range of recommendations and possible solutions for consideration.  © 1963-2012 IEEE.",Equipment Fault Diagnosis; Fusion Level; Fusion Operation; Fusion Strategy; Information Fusion; Multi-Sensor; Signal Type
Scopus,"Butler, J.; Leung, H.",A Heatmap-Supplemented R-CNN Trained Using an Inflated IoU for Small Object Detection,,2024,,,,10.3390/rs16214065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208600801&doi=10.3390%2frs16214065&partnerID=40&md5=0f9190d99ef6ef061fa049ecac550c27,"Object detection architectures struggle to detect small objects across applications including remote sensing and autonomous vehicles. Specifically, for unmanned aerial vehicles, poor detection of small objects directly limits this technology’s applicability. Objects both appear smaller than they are in large-scale images captured in aerial imagery and are represented by reduced information in high-altitude imagery. This paper presents a new architecture, CR-CNN, which predicts independent regions of interest from two unique prediction branches within the first stage of the network: a conventional R-CNN convolutional backbone and an hourglass backbone. Utilizing two independent sources within the first stage, our approach leads to an increase in successful predictions of regions that contain smaller objects. Anchor-based methods such as R-CNNs also utilize less than half the number of small objects compared to larger ones during training due to the poor intersection over union (IoU) scores between the generated anchors and the groundtruth—further reducing their performance on small objects. Therefore, we also propose artificially inflating the IoU of smaller objects during training using a simple, size-based Gaussian multiplier—leading to an increase in the quantity of small objects seen per training cycle based on an increase in the number of anchor–object pairs during training. This architecture and training strategy led to improved detection overall on two challenging aerial-based datasets heavily composed of small objects while predicting fewer false positives compared to Mask R-CNN. These results suggest that while new and unique architectures will continue to play a part in advancing the field of object detection, the training methodologies and strategies used will also play a valuable role. © 2024 by the authors.",convolutional neural network; Mask R-CNN; object detection; UAV; Aerial photography; Aircraft detection; Antenna grounds; Convolutional neural networks; Unmanned aerial vehicles (UAV); Aerial vehicle; Autonomous Vehicles; Convolutional neural network; Heatmaps; Large-scales; Mask R-CNN; Objects detection; Remote-sensing; Small object detection; Small objects; Remote sensing
Scopus,"Chen, W.; Zheng, R.; Jiang, J.; Tian, Z.; Zhang, F.; Liu, Y.",EDSD: efficient driving scenes detection based on Swin Transformer,,2024,,,,10.1007/s11042-024-19622-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198952366&doi=10.1007%2fs11042-024-19622-w&partnerID=40&md5=30588cffbe7f98d7aad829e95e6f638c,"In the field of autonomous driving, the detection of targets such as vehicles, bicycles, and pedestrians in complex road conditions is of great importance. Through extensive experimentation, we have found that various vehicle targets generally occupy large sizes in the image but are easily occluded, while small targets such as pedestrians usually appear densely. The detection of targets of different sizes is an important challenge for the performance of current detectors. To address this issue, we proposed a novel hierarchical feature pyramid network structure. This structure comprises a series of CNN-Transformer variant layers, each of which is a superposition of CST neural network modules and Swin Transformer modules. In addition, considering that the huge computation of the global self-attention mechanism is difficult to be applied in the field of autonomous driving, we adopted the shifted window method in SwinFM, which effectively accelerates the inference process by replacing the traditional method by using the self-attention mechanism within the window. This study uses the Swin Transformer as a baseline. Compared to the baseline, our EDSD model improves the average accuracy by 1.8% and 3.1% on the BDD100K dataset and the KITTI dataset, respectively. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Autonomous driving; Convolutional neural network; Object detection; Small object; Swin transformer; Autonomous vehicles; Convolutional neural networks; Multilayer neural networks; Pedestrian safety; Attention mechanisms; Autonomous driving; Convolutional neural network; Objects detection; Road condition; Scene detection; Small objects; Small targets; Swin transformer; Vehicle targets; Object detection
Scopus,"Lin, N.; Zhang, L.; Wu, T.; Hawbani, A.; Zhou, H.; Zhao, L.",Surface Multiple Object Tracking: An Accurate HAT-YOLOv8-ADT Tracking Model,,2025,,,,10.1109/JIOT.2025.3539852,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217537826&doi=10.1109%2fJIOT.2025.3539852&partnerID=40&md5=27ea0b4d816f334cd507aa71146397a2,"With the development of artificial intelligence technology, Autonomous aerial vehicles (AAV) have the ability to sense the environment. multiple object tracking (MOT) in AAV video is a very important vision task with a wide variety of applications. However, there are still many challenges in MOT in AAV video. First, the movement of the onboard camera in the three-dimensional (3-D) direction during the tracking process, as well as the unpredictable measurement noise characteristics of AAVs flying at high speeds, can lead to significant deviations in the prediction of the object’s position. Second, the applicability of the traditional detection algorithm decreases when the object is small and dense in the AAV viewpoint during detection. Finally, the traditional intersection over union (IoU) matching approach does not take into account the effects of the height and width of the box, and the matching results are inaccurate for the prediction and detection box. In order to address these challenges, we recommend the adaptive DeepSort (ADT) algorithm to reduce the prediction bias due to camera movement and difficulty in predetermining measurement noise characteristics, the hybrid attention transformer-YOLOv8 (HAT-YOLOv8) algorithm to enhance the detection capability of tiny objects, and the IoU of height and width (HWIoU) matching algorithm, which improves the matching accuracy and thus the tracking accuracy. Experimental results show that our proposed solution outperforms the baseline solution. It outperforms the current mainstream StrongSort in MOTA, HOTA and IDF1 by 2.86%, 0.9%, and 9.36%. © 2014 IEEE.",Autonomous aerial vehicles (AAV); deep simple online and realtime tracking (DeepSort); multiple object tracking (MOT); small object detection; YOLOv8; Aircraft detection; Time difference of arrival; Aerial vehicle; Deepsort; Matchings; Measurement Noise; Multiple object tracking; Noise characteristic; Small object detection; Surface multiples; Tracking models; YOLOv8; Object tracking
Scopus,"Li, D.; Yu, S.; Yang, H.",Progress on Environmental Perception Technology of Foreign Unmanned Surface Vehicles,,2024,,,,10.12382/bgxb.2024.0860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212582490&doi=10.12382%2fbgxb.2024.0860&partnerID=40&md5=f3e851a42603670b5e833d21935800a8,"Unmanned surface vehicles (USVs) have high mobility, strong concealment, and extensive operational range, making them highly suitable for performing a wide array of tasks such as reconnaissance,anti-submarine warfare,search and rescue. Environmental perception technology,crucial for the operation of USVs, has attracted considerable attention. This paper conducts a survey on the development status of environmental perception technology for USVs at abroad,and define and analyzed the challenges in USV environmental perception through specific case studies. The current state of research on USV environmental perception technology is analyzed from the perspectives of both unimodal and multimodal perception,considering the sensory equipment utilized by USVs. Finally,the unresolved challenges in USV environmental perception technology are summarized,and its future development is prospected. © 2024 China Ordnance Industry Corporation. All rights reserved.",multi-modal perception; perception technology; radar; unmanned surface vehicle; Unmanned underwater vehicles; Antisubmarine warfare; Case-studies; Development status; Environmental perceptions; High mobility; Multi modal perceptions; Operational range; Perception technology; Search and rescue; Surface vehicles; Unmanned surface vehicles
Scopus,"Hakani, R.; Rawat, A.",Edge Computing-Driven Real-Time Drone Detection Using YOLOv9 and NVIDIA Jetson Nano,,2024,,,,10.3390/drones8110680,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210319320&doi=10.3390%2fdrones8110680&partnerID=40&md5=bd3d68f95a8b7591ef4bfa2cbe91cb80,"Drones, with their ability to vertically take off and land with their stable hovering performance, are becoming favorable in both civilian and military domains. However, this introduces risks of its misuse, which may include security threats to airports, institutes of national importance, VIP security, drug trafficking, privacy breaches, etc. To address these issues, automated drone detection systems are essential for preventing unauthorized drone activities. Real-time detection requires high-performance devices such as GPUs. For our experiments, we utilized the NVIDIA Jetson Nano to support YOLOv9-based drone detection. The performance evaluation of YOLOv9 to detect drones is based on metrics like mean average precision (mAP), frames per second (FPS), precision, recall, and F1-score. Experimental data revealed significant improvements over previous models, with a mAP of 95.7%, a precision of 0.946, a recall of 0.864, and an F1-score of 0.903, marking a 4.6% enhancement over YOLOv8. This paper utilizes YOLOv9, optimized with pre-trained weights and transfer learning, achieving significant accuracy in real-time drone detection. Integrated with the NVIDIA Jetson Nano, the system effectively identifies drones at altitudes ranging from 15 feet to 110 feet while adapting to various environmental conditions. The model’s precision and adaptability make it particularly suitable for deployment in security-sensitive areas, where quick and accurate detection is crucial. This research establishes a solid foundation for future counter-drone applications and shows great promise for enhancing situational awareness in critical, high-risk environments. © 2024 by the authors.",computer vision; deep learning; drone detection; NVIDIA Jetson Nano; YOLOv9; You Only Look Once (YOLO); Aircraft detection; Deep learning; Drones; Edge computing; Military airports; Nanorobots; Network security; Target drones; Deep learning; Drone detection; Edge computing; F1 scores; Hovering performance; NVIDIA jetson nano; Real- time; Take off; YOLOv9; You only look once; Airport security
Scopus,"Bai, L.; Song, H.; Feng, T.; Fu, T.; Yu, Q.; Yang, J.",Revisiting class-incremental object detection: An efficient approach via intrinsic characteristics alignment and task decoupling,,2024,,,,10.1016/j.eswa.2024.125057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201508312&doi=10.1016%2fj.eswa.2024.125057&partnerID=40&md5=901bb2324a9ecdf0fbb21b3aa204e6bb,"In real-world settings, object detectors frequently encounter continuously emerging object instances from new classes. Incremental Object Detection (IOD) addresses this challenge by incrementally training an object detector with instances from new classes while retaining knowledge acquired from previously learned classes. Despite recent advancements, existing studies reveal a critical gap: they diverge from the inherent characteristics of dense detectors, leaving considerable room for improving incremental learning efficiency. To address this challenge, we propose a novel and efficient IOD approach that aligns more closely with the intrinsic properties of dense detectors. Specifically, our approach introduces a learning-aligned mechanism, comprising tailored knowledge distillation and task alignment learning, to achieve more efficient incremental learning. Additionally, we propose expanding the classification network through task decoupling to alleviate performance limitations stemming from different optimization goals in the incremental learning process of the classification branch. Extensive experiments conducted on the MS COCO and PASCAL VOC datasets demonstrate the effectiveness of our method, achieving state-of-the-art performance across various one-step and multi-step incremental scenarios. In multi-step incremental scenarios, our approach demonstrates a significant improvement of up to 12.9% in Average Precision (AP) compared to the previous method ERD. 1 © 2024 Elsevier Ltd",Incremental learning; Incremental object detection; Intrinsic characteristics alignment; Knowledge distillation; Task decoupling; Adversarial machine learning; Federated learning; Multi-task learning; Decouplings; Incremental learning; Incremental object detection; Intrinsic characteristic alignment; Intrinsic characteristics; Knowledge distillation; Multisteps; Object detectors; Objects detection; Task decoupling; Contrastive Learning
Scopus,"Dai, W.; Li, Z.; Xu, X.; Chen, X.; Zeng, H.; Hu, R.",Enhanced Cross Layer Refinement Network for robust lane detection across diverse lighting and road conditions,,2025,,,,10.1016/j.engappai.2024.109473,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207018329&doi=10.1016%2fj.engappai.2024.109473&partnerID=40&md5=ac2c792238bf0419cbc50b92c2f581be,"With the rapid development of autonomous driving technology, lane detection, a key component of intelligent vehicle systems, is crucial for ensuring road safety and efficient vehicle navigation. In this paper, a new lane detection method is proposed to address the problem of degraded performance of existing lane detection methods when dealing with complex road environments. The proposed method evolves from the original Cross Layer Refinement Network (CLRNet) by incorporating two of our carefully designed core components: the Global Feature Optimizer (GFO) and the Adaptive Lane Geometry Aggregator (ALGA). The GFO is a multi-scale attention mechanism that mimics the human visual focusing ability, effectively filtering out unimportant information and focusing on the image regions most relevant to the task. The ALGA is a shape feature-aware aggregation module that utilizes the shape prior of lanes to enhance the correlation of anchor points in an image, better fusing global and local information. By integrating both components into CLRNet, an enhanced version called Enhanced CLRNet (E-CLRNet) is presented, which exhibits higher performance stability in complex roadway scenarios. Experiments on the CULane dataset reveal that E-CLRNet demonstrates superior performance stability over the original CLRNet in complex scenarios, including curves, shadows, missing lines, and dazzling light conditions. In particular, in the curves, the F1 score of E-CLRNet is improved by almost 3% over the original CLRNet. This study not only improves the accuracy and performance stability of lane detection but also provides a new solution for the application of autonomous driving technology in complex environments, which promotes the development of intelligent vehicle systems. © 2024 Elsevier Ltd",Autonomous driving; Intelligent vehicle systems; Lane detection; Multi-scale attention mechanism; Road safety; Autonomous vehicles; Vehicle detection; Attention mechanisms; Autonomous driving; Cross layer; Detection methods; Intelligent vehicle systems; Lane detection; Multi-scale attention mechanism; Multi-scales; Performance stability; Road safety; Vehicle safety
Scopus,"Tian, D.; Yan, X.; Zhou, D.; Wang, C.; Zhang, W.",IV-YOLO: A Lightweight Dual-Branch Object Detection Network,,2024,,,,10.3390/s24196181,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206282940&doi=10.3390%2fs24196181&partnerID=40&md5=92f0a2b738d36c34c5453a015ffbb5f7,"With the rapid growth in demand for security surveillance, assisted driving, and remote sensing, object detection networks with robust environmental perception and high detection accuracy have become a research focus. However, single-modality image detection technologies face limitations in environmental adaptability, often affected by factors such as lighting conditions, fog, rain, and obstacles like vegetation, leading to information loss and reduced detection accuracy. We propose an object detection network that integrates features from visible light and infrared images—IV-YOLO—to address these challenges. This network is based on YOLOv8 (You Only Look Once v8) and employs a dual-branch fusion structure that leverages the complementary features of infrared and visible light images for target detection. We designed a Bidirectional Pyramid Feature Fusion structure (Bi-Fusion) to effectively integrate multimodal features, reducing errors from feature redundancy and extracting fine-grained features for small object detection. Additionally, we developed a Shuffle-SPP structure that combines channel and spatial attention to enhance the focus on deep features and extract richer information through upsampling. Regarding model optimization, we designed a loss function tailored for multi-scale object detection, accelerating the convergence speed of the network during training. Compared with the current state-of-the-art Dual-YOLO model, IV-YOLO achieves mAP improvements of 2.8%, 1.1%, and 2.2% on the Drone Vehicle, FLIR, and KAIST datasets, respectively. On the Drone Vehicle and FLIR datasets, IV-YOLO has a parameter count of 4.31 M and achieves a frame rate of 203.2 fps, significantly outperforming YOLOv8n (5.92 M parameters, 188.6 fps on the Drone Vehicle dataset) and YOLO-FIR (7.1 M parameters, 83.3 fps on the FLIR dataset), which had previously achieved the best performance on these datasets. This demonstrates that IV-YOLO achieves higher real-time detection performance while maintaining lower parameter complexity, making it highly promising for applications in autonomous driving, public safety, and beyond. © 2024 by the authors.",attention mechanism; bi-directional pyramid feature fusion; dual-branch image object detection; IV-YOLO; small target detection; Aircraft detection; Image coding; Image enhancement; Image fusion; Laser beams; Photointerpretation; Proximity sensors; Remote sensing; Thermography (imaging); Attention mechanisms; Bi-directional; Bi-directional pyramid feature fusion; Detection networks; Dual-branch image object detection; Features fusions; Image object detection; IV-YOLO; Objects detection; Small target detection; article; controlled study; diagnosis; human; illumination; male; rain; remote sensing; spatial attention; unmanned aerial vehicle; vegetation; velocity; Drones
Scopus,"Zhang, S.; Li, J.; Shi, L.; Ding, M.; Nguyen, D.C.; Chen, W.; Han, Z.","Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends",,2025,,,,10.1109/COMST.2025.3563919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003588117&doi=10.1109%2fCOMST.2025.3563919&partnerID=40&md5=52c0ad10ac4b5534d496aa6a99f867ea,"As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. The Industrial Metaverse is used for product design, production operations, quality inspection, and testing. However, there is limited understanding of the enabling technologies associated with it, including the specific industrial scenarios targeted by each technology and the potential migration of technologies from other domains to the industrial sector. This paper provides a comprehensive survey of the latest literature on the Industrial Metaverse. We first analyze its advantages for industrial production, then review key enabling technologies such as blockchain (BC), privacy-preserving computing (PPC), digital twin (DT), fifth/sixth generation mobile communication technology (5G/6G), XR, and artificial intelligence (AI), and explore how these technologies support different aspects of industrial production. We also present major challenges in the Industrial Metaverse, including privacy and security concerns, resource limitations, and interoperability constraints, along with existing solutions. Finally, we outline several open issues and future research directions. © 1998-2012 IEEE.",artificial intelligence; blockchain; digital twin; extended reality; fifth/sixth generation mobile communication technology; industrial; Metaverse; privacy-preserving computing; Block-chain; Enabling technologies; Extended reality; Fifth/sixth generation mobile communication technology; Industrial; Industrial production; Metaverses; Mobile communication technology; Privacy preserving; Privacy-preserving computing
Scopus,"Yan, H.; Gao, F.; Zhao, J.; Zhang, X.",MRT-YOLO: A Fine-Grained Feature-Based Method for Object Detection,,2024,,,,10.3390/electronics13234687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211925263&doi=10.3390%2felectronics13234687&partnerID=40&md5=943c46c0af385d8f324314b353bf8f50,"Object detection is an essential component of autonomous driving, unmanned aerial vehicle (UAV) reconnaissance, and other domains. It equips drones and vehicles with the capability to perceive and comprehend their surrounding environment, making it a crucial technology for achieving safe and reliable autonomous driving as well as UAV spot reconnaissance. This paper proposes an end-to-end, high-precision, multi-scale real-time detection algorithm called MRT-YOLO based on YOLOv8. Firstly, in the feature downsampling process of the backbone network, we extend the channel depth to enhance the model’s learning capability for fine-grained features and thereby improve its performance in retaining feature information. Secondly, we enhance the cross-stage partial layer version 2 (C2f) module in YOLOv8 by incorporating a channel self-attention mechanism within it, which optimizes performance through effective feature interaction and integration. Simultaneously, we also employ an improved bidirectional feature pyramid network (BiFPN) and introduce the proposed multi-scale feature learning (MFL) module to further enhance the model’s feature extraction ability. In this study, we fuse the feature maps (C2, C3, C4, and C5) from the backbone network to generate a new feature map C6, thus increasing cross-connections between low-level and high-level features. Lastly, a multi-scale small object detection structure is designed to enhance recognition sensitivity toward densely distributed small objects. The proposed algorithm’s effectiveness and superiority are demonstrated through experiments conducted on two datasets: VisDrone (UAV vision dataset) and BDD100K (automatic driving dataset). © 2024 by the authors.",channel attention; feature extraction; MRT-YOLO; multi-scale feature fusion; object detection
Scopus,"Xie, Y.; Du, D.; Bi, M.",YOLO-ACE: A Vehicle and Pedestrian Detection Algorithm for Autonomous Driving Scenarios Based on Knowledge Distillation of YOLOv10,,2025,,,,10.1109/JIOT.2025.3569735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005375362&doi=10.1109%2fJIOT.2025.3569735&partnerID=40&md5=cab5bc4c9ff412f41a859398ecd21b60,"Vehicle and pedestrian detection are critical tasks in autonomous driving, and fast and accurate detection algorithms are of great significance for improving the safety and reliability of autonomous driving systems. This paper proposes an improved YOLOv10 algorithm, YOLO-ACE, based on knowledge distillation for vehicle and pedestrian detection in autonomous driving scenarios. First, a new Add-CGLU (Additive-Convolutional Gated Linear Unit) architecture is developed to replace the original C2f module in the backbone part. Then, a new FPSC (Feature Pyramid Shared Conv) module is proposed to optimize the original SPPF module. After that, the neck part is redesigned to propose a new EMBS (Efficient Multi-Branch Scale) pyramid network. Finally, a new DD (Double Distillation) strategy is customized to perform knowledge distillation on the overall model. Experimental results on the public dataset BDD100K show that the computational parameters of YOLO-ACE are reduced by 21.6%, FLOPs are reduced by 20.0%, and the model size is reduced by 19.5%. At the same time, the F1 Score increased by 4.9%, the mAP increased by 4.5%, and the running speed reached 70.9 FPS. YOLO-ACE provides a more efficient vehicle and pedestrian detection solution in autonomous driving scenarios, promoting further development of autonomous driving systems.  © 2014 IEEE.",Additive-Convolutional Gated Linear Unit module; Efficient Multi-Branch Scale pyramid network; Feature Pyramid Shared Conv module; knowledge distillation; vehicle and pedestrian detection; YOLOv10; Autonomous vehicles; Boolean functions; Additive-convolutional gated linear unit module; Efficient multi-branch scale pyramid network; Feature pyramid; Feature pyramid shared conv module; Knowledge distillation; Linear units; Pedestrian detection; Pyramid network; Vehicles detection; YOLOv10; Pedestrian safety
Scopus,"Xing, Z.; Ma, G.; Wang, L.; Yang, L.; Guo, X.; Chen, S.",Toward Visual Interaction: Hand Segmentation by Combining 3-D Graph Deep Learning and Laser Point Cloud for Intelligent Rehabilitation,,2025,,,,10.1109/JIOT.2025.3546874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219537245&doi=10.1109%2fJIOT.2025.3546874&partnerID=40&md5=9854f6c2ac4196e72e9c55b1e9e3c2d9,"Against the backdrop of the increasing trend of aging population in China and even globally, the demand for hand function rehabilitation is growing day by day, and human-machine interaction virtual rehabilitation systems have become a research hotspot. Currently, 3-D vision has shown great potential in morphological analysis, but the complexity and irregularity of hand surfaces pose challenges for accurate segmentation. This study has proposed a hand surface segmentation network (HSSN) for intelligent hand function rehabilitation in the virtual reality, which combines 3-D graph deep learning and laser point cloud. HSSN integrates a series of methods, with edge convolution layers effectively addressing the complex morphology of hand surfaces, multiscale edge convolution solving the problem of missing or redundant local features, multidensity processing enhancing the robustness of the model to point cloud density, and normal vector feature enhancement solving the problem of insufficient geometric features of actual hand surface point clouds. Through the comprehensive application of these methods, HSSN has demonstrated excellent performance in comparative experiments. This study is of great significance for promoting the personalized and precise development of intelligent rehabilitation in the virtual reality environment. More importantly, this achievement has provided a new perspective for interdisciplinary research in fields, such as rehabilitation engineering and human-machine interaction. © 2014 IEEE.",3-D vision; graph deep learning; human-machine interaction; intelligent and precise rehabilitation; point cloud processing; Deep learning; 3-D vision; 3d graphs; Cloud processing; Graph deep learning; Human machine interaction; Intelligent and precise rehabilitation; Laser point; Point cloud processing; Point-clouds; Surface segmentation; Virtual environments
Scopus,"Yan, J.; Cheng, Y.; Zhang, F.; Zhou, N.; Wang, H.; Jin, B.; Wang, M.; Zhang, W.",Multimodal Imitation Learning for Arc Detection in Complex Railway Environments,,2025,,,,10.1109/TIM.2025.3556896,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002985474&doi=10.1109%2fTIM.2025.3556896&partnerID=40&md5=08fbfd7abfdb314ea07ffef5d365e06d,"The pantograph-catenary system (PCS) is a critical component of railway vehicles, and its performance directly affects current collection quality. The arc rate serves as an essential measurement indicator for monitoring the PCS state. However, in complex railway environments—where arc sizes and shapes can vary significantly and are further influenced by factors such as reflected light, glare, and adverse weather—the traditional arc detection methods are easily affected by unstable current collection and power fluctuations, resulting in increased false detection rates and reduced measurement accuracy. Deep learning methods, while promising, also face limitations when dealing with such diverse arc morphologies and strong external interference. To address these challenges, this article proposes a multimodal imitation learning-based arc detection network (MILADNet). First, the measurement system fuses infrared and visible-light image features to enhance arc feature extraction in scenarios with strong glare or reflective interference, thereby mitigating false alarms caused by relying on a single sensor. Second, to overcome the lack of information on small arcs, an online imitation learning framework is introduced to improve the system’s detection sensitivity for small arcs. Finally, to address data bias arising from uneven arc distributions, an unsupervised transferable representation learning method is employed to reduce dependence on labeled data and enhance model generalization. Experimental results show that MILADNet exhibits outstanding detection performance for arcs of various sizes and in complex environments, demonstrating both high efficiency and accuracy during measurement and data processing. Beyond improving the precision and reliability of arc detection, this method offers a novel solution for the instrumentation and measurement field and shows significant potential for condition monitoring and anomaly detection in railway systems. © 1963-2012 IEEE.",Arc detection; deep learning; optimal transport theory; pantograph-catenary system (PCS); transfer learning; Anomaly detection; Electric current collection; Glare effects; Health risks; Image enhancement; Labeled data; Luminescent devices; Network security; Arc detection; Current collection; Deep learning; Imitation learning; Multi-modal; Optimal transport; Optimal transport theory; Pantograph catenary system; Transfer learning; Transport theory; Railroads
Scopus,"Zhong, M.; Jiang, B.",Enhancing Target Detection and Recognition in Advanced Driver Assistance Systems Using Infrared Thermal Imaging and the YOLOv5 Algorithm,,2024,,,,10.18280/ijht.420530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209232697&doi=10.18280%2fijht.420530&partnerID=40&md5=365acfe594d9e49504d088deba5ca13c,"The potential of Advanced Driving Assistance Systems (ADAS) to enhance road safety is considerable; however, the reliability of ADAS in detecting and classifying road entities under varied environmental conditions remains a critical challenge. Conventional ADAS sensors often encounter limitations in adverse weather and low-visibility conditions, such as nighttime, rain, snow, and haze, reducing their capacity to detect vehicles and pedestrians effectively. To address these limitations, this study explores the integration of infrared thermal imaging technology into standard automotive sensor kits to enhance target detection capabilities. The YOLOv5 deep learning algorithm is applied to infrared thermal imaging data, aiming to improve the detection and classification of road targets, including pedestrians and motor vehicles, across diverse driving scenarios. Experimental results demonstrate that the proposed approach significantly enhances target detection, maintaining a balance between detection accuracy and real-time performance, particularly under challenging visibility conditions. These findings indicate that the integration of infrared thermal imaging with YOLOv5 in ADAS could reduce accident risks and improve road safety by providing more reliable scene analysis under adverse conditions. ©2024 The authors.",Advanced Driving Assistance System (ADAS); automotive sensor; deep learning; infrared thermal imaging; low-visibility condition; target detection; YOLOv5 algorithm; Highway accidents; Motor transportation; Pedestrian safety; Road vehicles; Satellites; Thermography (imaging); Vehicle detection; Vehicle safety; Advanced driving assistance system; Automotive sensors; Deep learning; Driving assistance systems; Infrared thermal imaging; Low visibility conditions; Road safety; Target detection and recognition; Targets detection; YOLOv5 algorithm; Advanced driver assistance systems
Scopus,"Singh, S.; Lamba, N.; Khosla, A.",A closer look at single object tracking under variable haze,,2024,,,,10.1007/s11042-024-19997-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202618322&doi=10.1007%2fs11042-024-19997-w&partnerID=40&md5=ba3543b7da6b1754fdb93edf09626e67,"The task of monitoring the object's path as it travels within a scene has consistently been challenging. When a specific level of haze is introduced to the environment, the endeavor becomes more difficult. The most recent tracking algorithms claim to be capable of accurately monitoring objects in typical visual conditions. However, it is imperative to conduct a comprehensive analysis of their functionality in hazy conditions, as haze is a meteorological adversity that is frequently encountered and has the potential to result in severe consequences. The primary objective of this investigation is to evaluate the efficacy of prominent tracking algorithms in the presence or absence of haze. Additionally, the performance was assessed by examining it in a variety of hazy conditions that were generated using the monocular depth information of the original image. The comparison between the authentic hazy photographs and the artificially created hazy photos has also been demonstrated. Furthermore, several novel relative parameters have been developed for object tracking under obscured vision conditions. These parameters can be employed to maintain the relative tracking performances under both normal and varying hazy conditions.To emphasize the effects of haze, the results have been obtained by using the help of state-of-the-art tracking algorithms. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Monocular depth estimation; Object Tracking; Robustness; SORT; Synthetic haze generation; Tracking Transformer; Haze pollution; Object detection; Object recognition; Object tracking; Target tracking; Condition; Depth Estimation; Monocular depth estimation; Object Tracking; Robustness; Single object; SORT; Synthetic haze generation; Tracking algorithm; Tracking transformer
Scopus,"Ning, Y.; Chen, J.",Target Detection Algorithm for UAV Aerial Images Based on BMGS-YOLOv8,,2025,,,,10.1109/ICAACE65325.2025.11019851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009121845&doi=10.1109%2fICAACE65325.2025.11019851&partnerID=40&md5=2a7d243350cfc55aad404262d0fbef91,"Aiming at the problem of detection difficulties caused by the small size of targets in UAV aerial images, which are easy to be occluded, as well as the densely populated targets such as pedestrians and vehicles, an improved BMGS-YOLOv8 algorithm is proposed in this paper. The algorithm first introduces the Biformer attention mechanism in the backbone network, which captures both global and local features by filtering key key-value pairs, and improves the efficiency of small target detection while reducing the amount of computation. Secondly, GSConv convolution is used to replace the standard convolution in YOLOv8, which effectively reduces the number of model parameters. Experimental results on the VisDrone2019 dataset show that compared with the benchmark model YOLOv8n, BMGS-YOLOv8 improves the mAP50 and mAP50-95 metrics by 0.8% and 0.2%, respectively, while the amount of model parameters is reduced by 7%. © 2025 IEEE.",aerial imagery; attention mechanisms; drones; small targets; YOLOv8; Aerial photography; Aircraft detection; Antennas; Big data; Convolution; Image enhancement; Radar target recognition; Remote sensing; Robotics; Signal detection; Target drones; Aerial imagery; Aerial images; Attention mechanisms; Back-bone network; Global feature; Image-based; Modeling parameters; Small targets; Target detection algorithm; YOLOv8; Drones
Scopus,"Chen, H.; Min, B.-W.; Zhang, H.",A study on a target detection model for autonomous driving tasks,,2024,,,,10.1049/ipr2.13185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198381038&doi=10.1049%2fipr2.13185&partnerID=40&md5=7e034e941533aac6906105e0705070e1,"Target detection in autonomous driving tasks presents a complex and critical challenge due to the diversity of targets and the intricacy of the environment. To address this issue, this paper proposes an enhanced YOLOv8 model. Firstly, the original large target detection head is removed and replaced with a detection head tailored for small targets and high-level semantic details. Secondly, an adaptive feature fusion method is proposed, where input feature maps are processed using dilated convolutions with different dilation rates, followed by adaptive feature fusion to generate adaptive weights. Finally, an improved attention mechanism is incorporated to enhance the model's focus on target regions. Additionally, the impact of Group Shuffle Convolution (GSConv) on the model's detection speed is investigated. Validated on two public datasets, the model achieves a mean Average Precision (mAP) of 53.7% and 53.5%. Although introducing GSConv results in a slight decrease in mAP, it significantly improves frames per second. These findings underscore the effectiveness of the proposed model in autonomous driving tasks. © 2024 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",image classification; image processing; learning (artificial intelligence); Autonomous vehicles; Convolution; Semantics; Adaptive features; Autonomous driving; Critical challenges; Detection models; Driving tasks; Images classification; Images processing; Learning (artificial intelligence); Small targets; Targets detection; Image classification
Scopus,"Park, J.; Lee, J.; Park, Y.; Lim, Y.",Deep Learning-Based Stopped Vehicle Detection Method Utilizing In-Vehicle Dashcams,,2024,,,,10.3390/electronics13204097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207677839&doi=10.3390%2felectronics13204097&partnerID=40&md5=669fab42e14d57c4cfeed66ec7ed8859,"In complex urban road conditions, stationary or illegally parked vehicles present a considerable risk to the overall traffic system. In safety-critical applications like autonomous driving, the detection of stopped vehicles is of utmost importance. Previous methods for detecting stopped vehicles have been designed for stationary viewpoints, such as security cameras, which consistently monitor fixed locations. However, these methods for detecting stopped vehicles based on stationary views cannot address blind spots and are not applicable from driving vehicles. To address these limitations, we propose a novel deep learning-based framework for detecting stopped vehicles in dynamic environments, particularly those recorded by dashcams. The proposed framework integrates a deep learning-based object detector and tracker, along with movement estimation using the dense optical flow method. We also introduced additional centerline detection and inter-vehicle distance measurement. The experimental results demonstrate that the proposed framework can effectively identify stopped vehicles under real-world road conditions. © 2024 by the authors.",multiple object tracking; object detection; optical flow; stopped vehicle detection
Scopus,"Li, Y.; Huang, Y.; Tao, Q.",Improving real-time object detection in Internet-of-Things smart city traffic with YOLOv8-DSAF method,,2024,,,,10.1038/s41598-024-68115-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199810002&doi=10.1038%2fs41598-024-68115-1&partnerID=40&md5=aafccb3cc980ada0a8ba9af775709420,"With the rise of global smart city construction, target detection technology plays a crucial role in optimizing urban functions and improving the quality of life. However, existing target detection technologies still have shortcomings in terms of accuracy, real-time performance, and adaptability. To address this challenge, this study proposes an innovative target detection model. Our model adopts the structure of YOLOv8-DSAF, comprising three key modules: depthwise separable convolution (DSConv), dual-path attention gate module (DPAG), and feature enhancement module (FEM). Firstly, DSConv technology optimizes computational complexity, enabling real-time target detection within limited hardware resources. Secondly, the DPAG module introduces a dual-channel attention mechanism, allowing the model to selectively focus on crucial areas, thereby improving detection accuracy in high-dynamic traffic scenarios. Finally, the FEM module highlights crucial features to prevent their loss, further enhancing detection accuracy. Additionally, we propose an Internet of Things smart city framework consisting of four main layers: the application domain, the Internet of Things infrastructure layer, the edge layer, and the cloud layer. The proposed algorithm utilizes the Internet of Things infrastructure layer, edge layer, and cloud layer to collect and process data in real-time, achieving faster response times. Experimental results on the KITTI V and Cityscapes datasets indicate that our model outperforms the YOLOv8 model. This suggests that in complex urban traffic scenarios, our model exhibits superior performance with higher detection accuracy and adaptability. We believe that this innovative model will significantly propel the development of smart cities and advance target detection technology. © The Author(s) 2024.",DPAG; FEM; Internet of Things; Smart city construction; Target detection technology; YOLOv8; algorithm; article; controlled study; diagnosis; internet of things; quality of life; reaction time; traffic
Scopus,"Babu, H.; Velmurugan, J.",An effective framework to detect the vehicle with improved accuracy using you only look once over Haar cascade,,2024,,,,10.1063/5.0228868,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205354752&doi=10.1063%2f5.0228868&partnerID=40&md5=bf7654c3b8c3797c6de34cf4a33432f2,"So that you may find out how well You Only Look Once works with OpenCV for speed-based car detection. This investigation involves two groups; one of them is the YOLO over OpenCV group. In each group, there are 10 participants, and the study settings for Glower are (α=0.05) and (power=0.85), both set simultaneously. When it comes to car detection, YOLO produces results that are 91% better than OpenCV's 84% accuracy. The accuracy of the two methods differs by a statistically significant amount of p=0.639 when assessed with two tails. When it comes to identifying new Vehicle Detection, the SVM performs far better than the You Only Look Once model. One may argue that it's the top choice for vehicle detection as well. © 2024 Author(s).",Haar cascade approach; Intelligent Transportation System; Linear Regression; OpenCV; YOLO
Scopus,"Pawłowski, P.; Piniarski, K.",Efficient Lossy Compression of Video Sequences of Automotive High-Dynamic Range Image Sensors for Advanced Driver-Assistance Systems and Autonomous Vehicles,,2024,,,,10.3390/electronics13183651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205054052&doi=10.3390%2felectronics13183651&partnerID=40&md5=ecfbe03fc4df92da10927932b328c4a1,"In this paper, we introduce an efficient lossy coding procedure specifically tailored for handling video sequences of automotive high-dynamic range (HDR) image sensors in advanced driver-assistance systems (ADASs) for autonomous vehicles. Nowadays, mainly for security reasons, lossless compression is used in the automotive industry. However, it offers very low compression rates. To obtain higher compression rates, we suggest using lossy codecs, especially when testing image processing algorithms in software in-the-loop (SiL) or hardware-in-the-loop (HiL) conditions. Our approach leverages the high-quality VP9 codec, operating in two distinct modes: grayscale image compression for automatic image analysis and color (in RGB format) image compression for manual analysis. In both modes, images are acquired from the automotive-specific RCCC (red, clear, clear, clear) image sensor. The codec is designed to achieve a controlled image quality and state-of-the-art compression ratios while maintaining real-time feasibility. In automotive applications, the inherent data loss poses challenges associated with lossy codecs, particularly in rapidly changing scenes with intricate details. To address this, we propose configuring the lossy codecs in variable bitrate (VBR) mode with a constrained quality (CQ) parameter. By adjusting the quantization parameter, users can tailor the codec behavior to their specific application requirements. In this context, a detailed analysis of the quality of lossy compressed images in terms of the structural similarity index metric (SSIM) and the peak signal-to-noise ratio (PSNR) metrics is presented. With this analysis, we extracted some codec parameters, which have an important impact on preservation of video quality and compression ratio. The proposed compression settings are very efficient: the compression ratios vary from 51 to 7765 for grayscale image mode and from 4.51 to 602.6 for RGB image mode, depending on the specified output image quality settings. We reached 129 frames per second (fps) for compression and 315 fps for decompression in grayscale mode and 102 fps for compression and 121 fps for decompression in the RGB mode. These make it possible to achieve a much higher compression ratio compared to lossless compression while maintaining control over image quality. © 2024 by the authors.",ADAS; autonomous vehicles; group of pictures; high-dynamic range imaging; lossy compression
Scopus,"Wu, X.; Duan, J.; Yang, L.; Duan, S.",Intelligent cotter pins defect detection for electrified railway based on improved faster R-CNN and dilated convolution,,2024,,,,10.1016/j.compind.2024.104146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201387987&doi=10.1016%2fj.compind.2024.104146&partnerID=40&md5=1fcebc3b1c1817b0334990adc5d42d5f,"The cotter pin (CP) is a vital fastener for the catenary support components (CSCs) of high-speed electrified railways. Due to the vibration and excitation caused by the passing of railway vehicles, some CPs may be broken or fallen off over time, which poses a significant safety hazard to the railway systems. Currently, the CP defect detection is primarily conducted by humans, which is inefficient and inconsistent. Therefore, there is an urgent need for automatic CP defect detection to ensure railway safety. However, this task is very challenging as it requires covering hundreds or thousands of miles in limited times when the railway stops running. To this end, we first design a traffic track intelligent imaging device to capture catenary images at various angles at high speed. Then, inspired by the success of deep learning-based object detection, we develop a CP detection model based on an improved Faster R-CNN with a multi-scale region proposal network (MS-RPN) and propose the positive sample adaptive loss function (PSALF) to enhance detection accuracy. Finally, we propose a module to recognize the CP defect based on dilated convolution. The experimental results show that our method can effectively detect the CP defect in the catenary image, achieving 99.05 % precision and 98.40 % recall rate on CP defect detection. Furthermore, CP detection method and CP defect detection are significantly faster than baseline method, with FPS improvements of 2.76 and 24.67, respectively, thus making it more suitable for real-time applications in railway systems. © 2024 Elsevier B.V.",Catenary Support Components (CSCs); Cotter Pin (CP); Defect detection; Dilated Convolution; Faster R-CNN; High-speed electrified railway; Electric railroads; Image enhancement; Image segmentation; Locks (fasteners); Railroad accidents; Railroad tracks; Railroad yards and terminals; Thermography (imaging); Catenary support component; Cotte pin; Defect detection; Dilated convolution; Electrified railways; Fast R-CNN; High Speed; High-speed electrified railway; Railway system; Railway vehicles; Railroads
Scopus,"Xiao, Y.; Di, N.",SOD-YOLO: A lightweight small object detection framework,,2024,,,,10.1038/s41598-024-77513-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208082125&doi=10.1038%2fs41598-024-77513-4&partnerID=40&md5=0eab8809e29f18d6de93bc1a205b325e,"Currently, lightweight small object detection algorithms for unmanned aerial vehicles (UAVs) often employ group convolutions, resulting in high Memory Access Cost (MAC) and rendering them unsuitable for edge devices that rely on parallel computing. To address this issue, we propose the SOD-YOLO model based on YOLOv7, which incorporates a DSDM-LFIM backbone network and includes a small object detection branch. The DSDM-LFIM backbone network, which combines Deep-Shallow Downsampling Modules (DSD Modules) and Lightweight Feature Integration Modules (LFI Modules), avoids excessive use of group convolutions and element-wise operations. The DSD Module focuses on extracting both deep and shallow features from feature maps using fewer parameters to obtain richer feature representations. The LFI Module, is a dual-branch feature integration module designed to consolidate feature information. Experimental results demonstrate that the SOD-YOLO model achieves an AP50 of 50.7% and a FPS of 72.5 on the VisDrone validation set. Compared to YOLOv7, our model reduces computational costs by 20.25% and decreases the number of parameters by 17.89%. After scaling the number of channels in the model, it achieves an AP50 of 33.4% with an inference time of 27.3ms on the Atlas 200I DK A2. These experimental results indicate that the SOD-YOLO model can effectively perform small object detection tasks in a large number of aerial images captured by UAVs. © The Author(s) 2024.",Lightweight; Object Detection; SOD-YOLO; UAV Image; article; detection algorithm; diagnosis; unmanned aerial vehicle
Scopus,"Khalili, B.; Smyth, A.W.",SOD-YOLOv8—Enhancing YOLOv8 for Small Object Detection in Aerial Imagery and Traffic Scenes,,2024,,,,10.3390/s24196209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206476633&doi=10.3390%2fs24196209&partnerID=40&md5=043e9c8a97d69204b3921bec785580ee,"Object detection, as a crucial aspect of computer vision, plays a vital role in traffic management, emergency response, autonomous vehicles, and smart cities. Despite the significant advancements in object detection, detecting small objects in images captured by high-altitude cameras remains challenging, due to factors such as object size, distance from the camera, varied shapes, and cluttered backgrounds. To address these challenges, we propose small object detection YOLOv8 (SOD-YOLOv8), a novel model specifically designed for scenarios involving numerous small objects. Inspired by efficient generalized feature pyramid networks (GFPNs), we enhance multi-path fusion within YOLOv8 to integrate features across different levels, preserving details from shallower layers and improving small object detection accuracy. Additionally, we introduce a fourth detection layer to effectively utilize high-resolution spatial information. The efficient multi-scale attention module (EMA) in the C2f-EMA module further enhances feature extraction by redistributing weights and prioritizing relevant features. We introduce powerful-IoU (PIoU) as a replacement for CIoU, focusing on moderate quality anchor boxes and adding a penalty based on differences between predicted and ground truth bounding box corners. This approach simplifies calculations, speeds up convergence, and enhances detection accuracy. SOD-YOLOv8 significantly improves small object detection, surpassing widely used models across various metrics, without substantially increasing the computational cost or latency compared to YOLOv8s. Specifically, it increased recall from 40.1% to 43.9%, precision from 51.2% to 53.9%, mAP0.5 from 40.6% to 45.1%, and mAP0.5:0.95 from 24% to 26.6%. Furthermore, experiments conducted in dynamic real-world traffic scenes illustrated SOD-YOLOv8’s significant enhancements across diverse environmental conditions, highlighting its reliability and effective object detection capabilities in challenging scenarios. © 2024 by the authors.",attention mechanism; bounding box regression; feature pyramid network; small object detection; YOLOv8; Antenna grounds; Emergency traffic control; Image enhancement; Risk management; Attention mechanisms; Bounding box regression; Bounding-box; Feature pyramid; Feature pyramid network; Objects detection; Pyramid network; Small object detection; Traffic scene; YOLOv8; altitude; article; autonomous vehicle; benchmarking; camera; computer vision; controlled study; diagnosis; feature extraction; female; human; imagery; male; punishment; reliability; traffic; Aerial photography
Scopus,"Wang, S.; Xu, Y.",MI-YOLO: An Improved Traffic Sign Detection Algorithm Based on YOLOv8,,2024,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211641922&partnerID=40&md5=d7db3a866721da10627da09f7af63079,"Traffic sign detection plays an essential role in the technology of self-driving vehicles. Recently, deep learning methods have significantly advanced the field of traffic sign recognition. Nevertheless, faced with increasingly complex traffic scenarios, practical applications of traffic sign detection still encounter challenges, including false detections, missed detections, and reduced accuracy. To tackle these challenges, we introduce an enhanced algorithm for traffic sign detection built on the YOLOv8 model, aimed at improving performance and accuracy. Firstly, a Multi-Scale Convolutional Attention (MSCA) module is embedded into the backbone architecture to improve the model’s feature extraction capabilities at multiple scales, enhancing its focus on target areas. Furthermore, a small object detection layer is added during the detection phase, effectively reducing the false positive and missed detection rates for small objects. Finally, we present the Inner-WIoU loss function for bounding boxes, which integrates a dynamic non-monotonic focusing mechanism with auxiliary boxes. This boosts the model’s capability to identify objects and enhances overall detection performance. The findings from the experiments demonstrate that the enhanced algorithm obtains an mAP0.5 value of 83.8% on the TT100K dataset, indicating a 7.8% increase compared to the baseline YOLOv8 algorithm. When compared to existing algorithms, the proposed method demonstrates competitive performance. © 2024, International Association of Engineers. All rights reserved.",Bounding Box Loss; Multi-Scale Attention; Small Object Detection; Traffic Sign Detection; YOLOv8; Adaptive boosting; Deep learning; Object detection; Object recognition; Bounding box loss; Bounding-box; Detection algorithm; Missed detections; Multi-scale attention; Multi-scales; Self drivings; Small object detection; Traffic sign detection; YOLOv8; Traffic signs
Scopus,"Jiang, Z.; Li, X.; Du, C.; Chen, A.; Han, Y.; Li, J.",YOLO⁃v8 with Multidimensional Attention and Upsampling Fusion for Small Air Target Detection in Radar Images,,2024,,,,10.16356/j.1005-1120.2024.06.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003733647&doi=10.16356%2fj.1005-1120.2024.06.004&partnerID=40&md5=67f2b86a202bccaf05f208c54c8c8def,"This study presents an innovative approach to improving the performance of YOLO-v8 model for small object detection in radar images. Initially, a local histogram equalization technique was applied to the original images, resulting in a notable enhancement in both contrast and detail representation. Subsequently, the YOLO-v8 backbone network was augmented by incorporating convolutional kernels based on a multidimensional attention mechanism and a parallel processing strategy, which facilitated more effective feature information fusion. At the model's head, an upsampling layer was added, along with the fusion of outputs from the shallow network, and a detection head specifically tailored for small object detection, thereby further improving accuracy. Additionally, the loss function was modified to incorporate focal-intersection over union (IoU) in conjunction with scaled-IoU, which enhanced the model's performance. A weighting strategy was also introduced, effectively improving detection accuracy for small targets. Experimental results demonstrate that the customized model outperforms traditional approaches across various evaluation metrics, including recall, precision, F1-score, and the receiver operating characteristic (ROC) curve, validating its efficacy and innovation in small object detection within radar imagery. The results indicate a substantial improvement in accuracy compared to conventional methods such as image segmentation and standard convolutional neural networks. © 2024 Nanjing University of Aeronautics an Astronautics. All rights reserved.",machine learning; object detection; radar images; YOLO; Convolutional neural networks; Image enhancement; Object tracking; Photointerpretation; Radar tracking; Air target detections; Innovative approaches; Local histogram equalizations; Machine-learning; Objects detection; Performance; Radar image; Small object detection; Upsampling; YOLO; Image segmentation
Scopus,"Singh, A.; Dass, S.",Advanced Traffic Conflict Detection and Risk Assessment Using Multi-Scale Video Analysis: A YOLOv8 Modified and Attention-Enhanced Safety Metrics Evaluation,,2025,,,,10.1007/s13177-025-00508-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008348739&doi=10.1007%2fs13177-025-00508-6&partnerID=40&md5=a0ad227f5fc7ff55398a43fa58b7d3dd,"This work proposed an Adaptive Temporal-Visual Hybrid Network (ATVHN) suited for precise conflict detection and threat estimation in harsh road scenarios for traffic safety. Conventional traffic systems are sometimes unable to trace the subtle relations between cars and pedestrians, particularly under difficult environments like poor light, occlusion, and anarchic motion. The envisioned framework combines a pre-trained YOLOv8 model with multi-scale video analysis and adds two new attention modules: Adaptive Visual-Attention Embedding Network (AVAENet) and Hybrid Temporal Dynamics Attention Network (HyTeDANet). These modules extract key spatial and temporal features from HWID12 video and time-series data, enriched through preprocessing and fusion techniques. Experimental performance indicates that ATVHN performs better than current models, such as Bi-LSTM and RNN, with high accuracy (0.98), precision (0.97), and sensitivity (0.97). Strong performance notwithstanding, the model is limited in effectiveness by sensitivity to high-quality video input and computational burden. Possible applications include integration within intelligent traffic systems for real-time surveillance, risk prediction alerts, and adaptive city traffic planning. © The Author(s), under exclusive licence to Intelligent Transportation Systems Japan 2025.",Attention mechanisms; Intelligent transportation systems; Multi-scale video analysis; Risk assessment; Traffic conflicts; Traffic safety; YOLOv8; Accident prevention; Advanced traffic management systems; Highway planning; Intelligent systems; Motor transportation; Real time systems; Risk analysis; Risk perception; Security systems; Traffic control; Video analysis; Attention mechanisms; Conflict detection; Intelligent transportation systems; Multi-scale video analyze; Multi-scales; Risks assessments; Traffic conflicts; Traffic safety; Video analysis; YOLOv8; Risk assessment
Scopus,"Zhang, H.; Li, Z.; Wang, C.",YOLO-Dynamic: A Detection Algorithm for Spaceborne Dynamic Objects,,2024,,,,10.3390/s24237684,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211797698&doi=10.3390%2fs24237684&partnerID=40&md5=e6e199792e390dcd79452faf0853d8d7,"Ground-based detection of spaceborne dynamic objects, such as near-Earth asteroids and space debris, is essential for ensuring the safety of space operations. This paper presents YOLO-Dynamic, a novel detection algorithm aimed at addressing the limitations of existing models, particularly in complex environments and small-object detection. The proposed algorithm introduces two newly designed modules: the SC_Block_C2f and the LASF_Neck. SC_Block_C2f, developed in this study, integrates StarNet and Convolutional Gated Linear Unit (CGLU) operations, improving small-object recognition and feature extraction. Meanwhile, LASF_Neck employs a lightweight multi-scale architecture for optimized feature fusion and faster detection. The YOLO-Dynamic algorithm’s performance was validated on real-world images captured at Antarctic observatory sites. Compared to the baseline YOLOv8s model, YOLO-Dynamic achieved a 7% increase in mAP@0.5 and a 10.3% improvement in mAP@0.5:0.95. Additionally, the number of parameters was reduced by 1.48 M, and floating-point operations decreased by 3.8 G. These results confirm that YOLO-Dynamic not only delivers superior detection accuracy but also maintains computational efficiency, making it well suited for real-world applications requiring reliable and efficient spaceborne object detection. © 2024 by the authors.",LASF_Neck; multi-scale feature fusion; SC_Block_C2f; spaceborne dynamic object detection; YOLOv8; Asteroids; Object detection; Object tracking; Space applications; Tropics; Dynamic objects; Features fusions; LASF_neck; Multi-scale feature fusion; Multi-scale features; Objects detection; SC_block_c2f; Space-borne; Spaceborne dynamic object detection; YOLOv8; aged; algorithm; Antarctica; article; controlled study; detection algorithm; diagnosis; diagnostic test accuracy study; feature extraction; space debris; Space debris
Scopus,"Liu, F.; Wang, J.; Jiao, L.; Zhang, J.; Wang, H.; Li, S.; Li, L.; Chen, P.; Liu, X.; Ma, W.; Wang, S.; Yang, S.; Zhang, X.; Du, Y.; Bao, Q.; Sun, L.; Hou, B.","Remote Sensing Video Tracking: Current Status, Challenges, and Future",,2025,,,,10.1109/JSTARS.2025.3573572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006606002&doi=10.1109%2fJSTARS.2025.3573572&partnerID=40&md5=94040ffa95cbd621baab7d7c01de1507,"With the rapid advancement of remote sensing technology, the acquisition and processing of remote sensing video data, including high-resolution satellite, hyperspectral, and synthetic aperture radar video, have become key research areas in remote sensing. As a crucial branch of video analysis, remote sensing video tracking enables continuous monitoring and dynamic analysis of ground targets, with widespread applications in military operations, disaster monitoring, environmental protection, and urban management. This review systematically examines the current state of remote sensing video tracking research, exploring the technological advancements of traditional and modern methods, including model-based approaches, feature-based approaches, and deep-learning-based methods. The work also examines evaluation metrics and benchmark testing methodologies in remote sensing video tracking. Finally, this review highlights ten unresolved challenges in the field and offers insights into the future of next-generation artificial intelligence technologies. Through a comprehensive overview and in-depth analysis of existing technologies, this work aims to provide researchers with a systematic reference, promoting further development and innovation in remote sensing video tracking. © 2008-2012 IEEE.",Evaluation metrics and benchmark; remote sensing video tracking; ten unresolved challenges; Ground penetrating radar systems; Military aviation; Military communications; Military data processing; Military electronic countermeasures; Naval warfare; Space surveillance; Current status; Evaluation metric and benchmark; Evaluation metrics; High resolution satellites; Remote sensing technology; Remote sensing video tracking; Remote-sensing; Ten unresolved challenge; Video data; Video-tracking; artificial intelligence; data acquisition; disaster management; environmental protection; future prospect; image resolution; military application; monitoring system; remote sensing; satellite imagery; synthetic aperture radar; technology adoption; tracking; urban planning; videography; Ground operations
Scopus,"Tang, J.; Ye, C.; Zhou, X.; Xu, L.",YOLO-Fusion and Internet of Things: Advancing object detection in smart transportation,,2024,,,,10.1016/j.aej.2024.09.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204065135&doi=10.1016%2fj.aej.2024.09.012&partnerID=40&md5=a421fc39675f254b3a9dd9b54e5eac44,"In intelligent transportation systems, traditional object detection algorithms struggle to handle complex environments and varying lighting conditions, particularly when detecting small targets and processing multimodal data. Furthermore, existing IoT frameworks are limited in their efficiency for real-time data collection and processing, leading to data transmission delays and increased resource consumption, which constrains the overall performance of intelligent transportation systems. To address these issues, this paper proposes a novel deep learning model, YOLO-Fusion. Based on the YOLOv8 architecture, this model innovatively integrates infrared and visible-light images, utilizing FusionAttention and Dynamic Fusion modules to optimize the fusion of multimodal information. To further enhance detection performance, this paper designs a Fusion-Dynamic Loss, improving the model's performance in complex intelligent transportation scenarios. To support the efficient operation of YOLO-Fusion, this paper also introduces an IoT framework that uses intelligent sensors and edge computing technology to achieve real-time collection, transmission and processing of traffic data, significantly improving data timeliness and accuracy. Experimental results demonstrate that YOLO-Fusion significantly outperforms traditional methods on the DroneVehicle and FLIR datasets, showcasing its broad application potential in intelligent traffic monitoring and management. © 2024 Faculty of Engineering, Alexandria University",Internet of Things; Multimodal data fusion; Smart city; Smart transportation; YOLO-Fusion; Data handling; Data transfer; Deep learning; Multimodal transportation; Complex environments; Environment lighting; Intelligent transportation systems; Lighting conditions; Multimodal data fusion; Object detection algorithms; Objects detection; Smart transportation; Varying lighting; YOLO-fusion; Data fusion
Scopus,"Wang, J.; Yang, H.; Wu, M.; Wang, S.; Cao, Y.; Hu, S.; Shao, J.; Zeng, C.",UR-YOLO: an urban road small object detection algorithm,,2024,,,,10.1007/s10044-024-01324-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204876357&doi=10.1007%2fs10044-024-01324-6&partnerID=40&md5=443def860c5ea67b67986a3b06d0c308,"The autonomous driving system heavily depends on perception algorithms to gather crucial information about the surrounding urban environment. However, detecting small objects on busy urban roads poses a significant challenge. To overcome this obstacle, we present UR-YOLO (Urban Roads-YOLO), a novel small object detection algorithm tailored for urban roads, which builds upon the enhanced YOLOv9 framework. UR-YOLO comprises three key enhancements. Firstly, to mitigate the high background ratio in small object datasets, we employ SCRConv to replace selected standard convolutions in the backbone network. The reduction in spatial redundancy sharpens the perception of vital features. Secondly, to address the sparse distribution of small objects, we incorporate SPPELANBRA, a refined version of SPPELAN, to enhance the model’s sensitivity towards small objects, thereby improving its overall accuracy. Lastly, to address the issue of overlapping small objects, we upgrade the bounding box loss function by substituting the original SIoU loss with the Inner-MPDIoU loss. It not only improves the detection accuracy for small objects but also accelerates the convergence of the training process. To validate the effectiveness of UR-YOLO, we conducted comprehensive ablation and comparative experiments on the 2023 CICVAC dataset. The experimental results reveal that our proposed improvements have boosted the YOLOv9 model’s mAP, precision, and recall by significant margins of 6.02%, 6.63%, and 4.81% respectively. Furthermore, when compared to prior YOLO series and two-stage detection models, UR-YOLO exhibits superior accuracy, higher frames per second, and greater robustness, making it a robust solution for diverse weather conditions on urban roads. Code is available at https://github.com/Ranghao/UR_YOLO. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",Autonomous driving; Feature extraction; Loss function; Small object detection; UR-YOLO
Scopus,"Wang, S.; Yang, X.; Lu, R.; Su, S.; Tang, B.; Zhang, T.; Zhu, Z.",TPDTNet: Two-Phase Distillation Training for Visible-to-Infrared Unsupervised Domain Adaptive Object Detection,,2025,,,,10.1109/JSTARS.2025.3528057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214791468&doi=10.1109%2fJSTARS.2025.3528057&partnerID=40&md5=1cdf838c09a21e35804529c6be4aaaaf,"In remote sensing target detection cases, great challenges are faced when migrating detection models from the visible domain to the infrared domain. Cross-domain migration suffers from problems such as a lack of data annotations in the infrared domain and interdomain feature differences. To improve the detection accuracy attained for infrared images, we propose a novel two-phase distillation training network (TPDTNet). Specifically, in the first phase, we incorporate a contrastive learning framework to maximize the mutual information between the source and target domains. In addition, we construct a generative model that learns only a unidirectional modality conversion mapping, thereby capturing the associations between their visual contents. The source-domain image is converted to an image with the style of the target domain, thereby achieving image-level domain alignment. The generated image is combined with the source-domain image to form an enhanced domain for cross-modal training. Enhanced domain data are fed into the teacher network to initialize the weights and produce pseudolabels. Next, to address small remote sensing target detection tasks, we construct a multidimensional progressive feature fusion detection framework, which initially fuses two adjacent low-level feature maps and then progressively incorporates high-level features to enhance the quality of fusing nonadjacent layer features. Subsequently, a spatial-dimension convolution is integrated into the backbone network. This convolutional operation is embedded following standard convolution to mitigate the loss of detailed features. Finally, a distillation training strategy that utilizes pseudodetection labels to calculate target information. By minimizing the Kullback-Leibler divergence between the probability maps of the teacher and student networks, the channel activations are transformed into probability distributions, thereby achieving knowledge distillation. The training weights are transferred from the teacher network to the student network to maximize the detection accuracy. Extensive experiments are conducted on three optical-to-infrared datasets, and the experimental results show that our TPDTNet method achieves state-of-the-art results relative to those of the baseline model.  © 2008-2012 IEEE.",Distillation training; object detection; remote sensing; unsupervised domain adaptation (UDA); Health risks; Image annotation; Image fusion; Infrared imaging; Optical remote sensing; Personnel training; Photomapping; Proximity sensors; Students; Distillation training; Domain adaptation; Infrared domains; Objects detection; Remote-sensing; Targets detection; Teachers'; Training network; Two phase; Unsupervised domain adaptation; detection method; image analysis; remote sensing; training; Teaching
Scopus,"Yang, N.; Li, G.; Wang, S.; Wei, Z.; Ren, H.; Zhang, X.; Pei, Y.",SS-YOLO: A Lightweight Deep Learning Model Focused on Side-Scan Sonar Target Detection,,2025,,,,10.3390/jmse13010066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216219965&doi=10.3390%2fjmse13010066&partnerID=40&md5=9a58c9a3607a9d4b2cc05e4ff2997082,"As seabed exploration activities increase, side-scan sonar (SSS) is being used more widely. However, distortion and noise during the acoustic pulse’s travel through water can blur target details and cause feature loss in images, making target recognition more challenging. In this paper, we improve the YOLO model in two aspects: lightweight design and accuracy enhancement. The lightweight design is essential for reducing computational complexity and resource consumption, allowing the model to be more efficient on edge devices with limited processing power and storage. Thus, meeting our need to deploy SSS target detection algorithms on unmanned surface vessel (USV) for real-time target detection. Firstly, we replace the original complex convolutional method in the C2f module with a combination of partial convolution (PConv) and pointwise convolution (PWConv), reducing redundant computations and memory access while maintaining high accuracy. In addition, we add an adaptive scale spatial fusion (ASSF) module using 3D convolution to combine feature maps of different sizes, maximizing the extraction of invariant features across various scales. Finally, we use an improved multi-head self-attention (MHSA) mechanism in the detection head, replacing the original complex convolution structure, to enhance the model’s ability to focus on important features with low computational load. To validate the detection performance of the model, we conducted experiments on the combined side-scan sonar dataset (SSSD). The results show that our proposed SS-YOLO model achieves average accuracies of 92.4% (mAP 0.5) and 64.7% (mAP 0.5:0.95), outperforming the original YOLOv8 model by 4.4% and 3%, respectively. In terms of model complexity, the improved SS-YOLO model has 2.55 M of parameters and 6.4 G of FLOPs, significantly lower than those of the original YOLOv8 model and similar detection models. © 2025 by the authors.",feature fusion; lightweight design; multi-head self-attention; partial convolution; side-scan sonar (SSS); YOLOv8
Scopus,"Breckner, K.; Neumayr, T.; Mara, M.; Streit, M.; Augstein, M.",The Changing Nature of Human-AI Relations: A Scoping Review on Terminology and Evolvement in the Scientific Literature,,2025,,,,10.1080/10447318.2025.2482742,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009551578&doi=10.1080%2f10447318.2025.2482742&partnerID=40&md5=52aad23dc05187e7457a7f9aec21cf6e,"Recent years have brought immense progress in the development of AI technology. This broadened its application fields but also led to a surge of interest in many research domains and increasing significance of human-AI relations for the development of AI technology. This rapid growth and evolvement is reflected by the establishment of a great variety of terms, potentially leading to what is known as jingle and jangle fallacies. With our scoping review of the terminology used in scientific literature to describe human-AI relations and its evolvement over time (with 803 records screened, 658 finally included), we capture the variety and development of human-AI terminology in accordance with the shift from interaction to collaboration between humans and AI. We aim to raise awareness of these developments spanning over different research communities and provide a solid basis for future researchers and practitioners conducting complementary, cross-domain research. Our review comprises terminological, bibliometric and thematic analyses, e.g., reporting on the historical development of terms and term composition patterns, but also identifying key authors and publications, geographic distribution of relevant research, and elaborating on term conception and usage, and co-occurrences throughout the literature. © 2025 The Author(s). Published with license by Taylor & Francis Group, LLC.",artificial intelligence; human-ai relations; human-centered ai; Scoping review; Terminology; AI Technologies; Application fields; Human-ai relation; Human-centered ai; ITS applications; Rapid growth; Research communities; Research domains; Scientific literature; Scoping review; Artificial intelligence
Scopus,"Pal, O.K.; Shovon, M.D.S.H.; Mridha, M.F.; Shin, J.","In-depth review of AI-enabled unmanned aerial vehicles: trends, vision, and challenges",,2024,,,,10.1007/s44163-024-00209-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211111138&doi=10.1007%2fs44163-024-00209-1&partnerID=40&md5=290ac0b7767144557c29aee7d1e74f2f,"In recent times, AI and UAV have progressed significantly in several applications. This article analyzes applications of UAV with modern green computing in various sectors. It addresses cutting-edge technologies such as green computing, generative AI, future scope, and related concerns in UAV. The research investigates the role of green computing and generative AI in combination with UAVs for navigation, object recognition and tracking, wildlife monitoring, precision agriculture, rescue operations, surveillance, and UAV communication. This study examines how modern computing technologies and UAVs are being applied in agriculture, surveillance, disaster management, and other areas. The ethics of UAV and AI applications, including safety, legal frameworks, and other issues, are thoroughly investigated. This research examines AI-based UAV applications across different disciplines, using open-source data and current advancements for future growth in this domain. This investigation will aid future researchers in their exploration of UAVs using cutting-edge computing technologies. © The Author(s) 2024.",Aerial vehicles; Agriculture surveillance; CNN; Generative AI; Green computing; Traffic monitoring; YOLO; Aircraft detection; Fertilizers; Aerial vehicle; Agriculture surveillance; Computing technology; Cutting edge technology; Generative AI; Object Tracking; Objects recognition; Traffic monitoring; Wildlife monitoring; YOLO; Unmanned aerial vehicles (UAV)
Scopus,"Deng, F.; Qiao, B.; Li, K.; Zhao, L.; Chen, X.; Li, J.; Liu, J.; Sun, Y.",Wind turbine detection based on high spatial resolution four-band reflectance images,,2025,,,,10.1117/12.3057549,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217421906&doi=10.1117%2f12.3057549&partnerID=40&md5=1263264b7e8165a282fcdec9b4b844b3,"Wind turbine detection is essential for the power industry and regulatory agencies to efficiently and accurately determine their number and spatial distribution. Under complex underlying surface conditions, the detection accuracy of conventional methods is low, especially when dealing with multi-scale wind turbine targets. In addition, the underutilization of the near-infrared (NIR) band limits the capability of optical remote sensing sensors in wind turbine detection. To address the above problems, this paper proposes a wind turbine detection algorithm YOLOv8m-BD based on high spatial resolution four-band reflectance images using YOLOv8m as the baseline model. Firstly, the four-band image is input through channel adaptation modification. Next, the CSPDarknet53 to 2-stage FPN (C2f) module is enhanced by introducing deformable convolution (DCN) to expand the receptive field. Finally, the effectiveness of our design was validated through extensive experiments on a self-built wind turbine dataset. Compared to the YOLOv8m baseline model that only inputs RGB three bands, the improved YOLOv8m-BD achieves an accuracy of 96.4% and a false alarm rate of 2.0%, an increase of 3.4% in accuracy and a decrease of 2.5% in false alarm rate. © 2025 SPIE.",RGB+NIR four band; Wind turbine detection; YOLOv8; Geological surveys; Image resolution; Optical remote sensing; Thermography (imaging); Unattended sensors; Wind turbines; Baseline models; False alarm rate; High spatial resolution; Near Infrared; Near-infrared; Power industry; Reflectance images; RGB+near-infrared four band; Wind turbine detection; YOLOv8; Proximity sensors
Scopus,"Choudhary, A.; Kumar Mishra, R.; Fatima, S.; Panigrahi, B.K.",Multimodal Fusion-Based Fault Diagnosis of Electric Vehicle Motor for Sustainable Transportation,,2025,,,,10.1109/TTE.2024.3502466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001573355&doi=10.1109%2fTTE.2024.3502466&partnerID=40&md5=a1c7489966b0ef30890af83d002d96ad,"Electric vehicles (EVs) are essential for sustainable transportation, and various ecofriendly vehicles are being manufactured. In EVs, the traction motor is a crucial prime mover for propelling the vehicle forward. However, traction motors are susceptible to faults like any other motors which can compromise their performance, safety, and longevity. This study proposes a reliable fault diagnosis strategy by using information fusion of vibration and current sensor data. Initially, vibration and current signals fusion-based diagnostic methods have been developed in the laboratory environment for induction motors (IMs) having seven fault conditions. This developed method involved wavelet synchrosqueezing transform (WSST) for the decomposition of the acquired vibration and current signature and further converted into a time-frequency spectrum. Thereafter, a multi-input fusion network (MiFN) has been designed for the fusion of vibration and current information. Finally, the developed fault diagnosis method has been extended and validated on an electric two-wheeler for diagnosing the faults in the brushless direct current motor (BLDC) hub motor. The suggested approach demonstrated significantly better classification accuracy than the signature of each sensor across a range of different speed situations. The achieved accuracies are in the range of 97.50%–98.35% in the laboratory environment and 90%–95% in the electric two-wheeler. The experimental results demonstrate that the suggested diagnosis methodology is highly accurate and remarkably reliable for pragmatic working conditions of EVs. © 2015 IEEE.",Electric vehicles (EVs); fault diagnosis; information fusion; multi-input fusion network (MiFN); sensor fusion; wavelet synchrosqueezing transform (WSST); Data fusion; Electric fault location; Traction motors; Faults diagnosis; Laboratory environment; Multi-input fusion network; Multi-modal fusion; Multiinput; Sensor fusion; Sustainable transportation; Synchrosqueezing; Two wheelers; Wavelet synchrosqueezing transform; Wavelet decomposition
Scopus,"Huang, Y.; Han, D.; Han, B.; Wu, Z.",ADV-YOLO: improved SAR ship detection model based on YOLOv8,,2025,,,,10.1007/s11227-024-06527-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207086360&doi=10.1007%2fs11227-024-06527-6&partnerID=40&md5=7e665d732085eb12893fd6be497bec77,"Synthetic aperture radar (SAR) ship detection plays a crucial role in supporting maritime traffic control, sea rescue, and marine environmental protection. Despite its importance, SAR ship detection confronts several challenges, including the small size of ship targets, unclear contours, complex background noise, and variable scales of ships. To address these challenges, this paper introduces an enhanced SAR ship detection model, termed ADV-YOLO, which builds upon the YOLOv8 framework. The proposed model incorporates space-to-depth building blocks to improve detection accuracy for low-resolution images and small objects. Additionally, a dilation-wise residual module replaces the C2f module in the network’s neck, augmenting the model’s capability to discern multi-scale targets and enrich feature representation. Furthermore, the WIoU loss function is adopted to replace the conventional CIoU loss, enhancing model accuracy, particularly for low-quality sample bounding boxes. Extensive experiments conducted on the HRSID and SSDD datasets demonstrate the robustness and reliability of ADV-YOLO. Compared to YOLOv8n, there is a significant performance improvement: the proposed method achieves an AP50-95 of 70% on the HRSID dataset, with an improvement of 4.5%. Additionally, it improves by 3.1% for AP50 and 5.7% for AP75. On the SSDD dataset, the AP50-75 improves by 0.9%, AP50 by 1.1%, and AP75 by 0.9%. This advancement underscores the potential of ADV-YOLO in enhancing real-time maritime surveillance and safety applications. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Dilation-wise residual; Space-to-depth; Synthetic aperture radar (SAR) ship detection; YOLOv8; Air traffic control; Marine radar; Radar target recognition; Ships; Waterway transportation; Complex background; Detection models; Dilation-wise residual; Maritime traffic; Model-based OPC; Ship detection; Ship targets; Space-to-depth; Synthetic aperture radar  ship detection; YOLOv8; Synthetic aperture radar
Scopus,"Li, Z.; Zhu, C.; Tao, H.; Zhang, Q.; Pu, C.; Xiao, J.",Robot map construction based on topology and hierarchical fusion in unknown environment,,2025,,,,10.1088/1742-6596/2999/1/012022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004199712&doi=10.1088%2f1742-6596%2f2999%2f1%2f012022&partnerID=40&md5=f971317d1d9bcf455713238faec0f3e5,"Map construction has always been crucial in exploring enclosed unknown environments. Traditional mapping methods have problems such as low accuracy or long time consumption. Due to the indoor and enclosed nature of unknown environments, inspection robots often encounter challenges in mapping and path planning. To address similar issues, we proposed an optimized indoor mapping solution based on topology technology and hierarchical fusion, and conducted experiments using robots in a library simulation scenario in an unknown environment. Figure 1 shows the map that we have obtained by layering and blending.By utilizing topology techniques and hierarchical fusion mapping methods, we have effectively simplified the mapping process, reducing computational complexity and cost while obtaining spatial information and this method allows for the consideration of the geometric characteristics of robots when planning safe and accurate passable paths. © 2025 Institute of Physics Publishing. All rights reserved.",Geometry; Maps; Mathematical morphology; Photomapping; Hierarchical fusions; Inspection robots; Map constructions; Mapping method; Mapping process; Robot maps; Spatial informations; Technology fusion; Time consumption; Unknown environments; Topology
Scopus,"Aldoğan, C.F.; Aksu, K.; Demirel, H.",Enhancement of Sentinel-2A Images for Ship Detection via Real-ESRGAN Model,,2024,,,,10.3390/app142411988,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213221644&doi=10.3390%2fapp142411988&partnerID=40&md5=17c007838f705c15730cb9f2de5e141f,"Ship detection holds great value regarding port management, logistics operations, ship security, and other crucial issues concerning surveillance and safety. Recently, ship detection from optical satellite imagery has gained popularity among the research community because optical images are easily accessible with little or no cost. However, these images’ quality and quantity of feature details are bound to their spatial resolution, which often comes in medium-low spatial resolution. Accurately detecting ships requires images with richer texture and resolution. Super-resolution is used to recover features in medium-low resolution images, which can help leverage accuracy in ship detection. In this regard, this paper quantitatively and visually investigates the effectiveness of super-resolution in enabling more accurate ship detection in medium spatial resolution images by comparing Sentinel-2A images and enhanced Sentinel-2A images. A collection of Sentinel-2A images was enhanced four times with a Real-ESRGAN model that trained PlanetScope images with high spatial resolution. Separate ship detections with YOLOv10 were implemented for Sentinel-2A images and enhanced Sentinel-2A images. The visual and metric results of both detections were compared to demonstrate the contributory effect of enhancement on the ships’ detection accuracy. Ship detection on enhanced Sentinel-2A images has a mAP50 and mAP50-95 value of 87.5% and 68.5%. These results outperformed the training process on Sentinel-2A images with a mAP value increase of 2.6% for both mAP50 and mAP50-95, demonstrating the positive contribution of super-resolution. © 2024 by the authors.",object detection; PlanetScope; Real-ESRGAN; satellite images; Sentinel-2A; ship detection; super-resolution; YOLO; Image enhancement; Image texture; Satellite imagery; Objects detection; Planetscope; Port management; Real-ESRGAN; Satellite images; Sentinel-2a; Ship detection; Spatial resolution; Superresolution; YOLO; Image resolution
Scopus,"Wang, R.; Chen, L.; Huang, Z.; Zhang, W.; Wu, S.",A Review on the High-Efficiency Detection and Precision Positioning Technology Application of Agricultural Robots,,2024,,,,10.3390/pr12091833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205318004&doi=10.3390%2fpr12091833&partnerID=40&md5=9fc89c10e17734a0e21586f7ce1099ac,"The advancement of agricultural technology has increasingly positioned robotic detection and localization techniques at the forefront, ensuring critical support for agricultural development through their accuracy and reliability. This paper provides an in-depth analysis of various methods used in detection and localization, including UWB, deep learning, SLAM, and multi-sensor fusion. In the domain of detection, the application of deep algorithms in assessing crop maturity and pest analysis is discussed. For localization, the accuracy of different methods in target positioning is examined. Additionally, the integration of convolutional neural networks and multi-sensor fusion with deep algorithms in agriculture is reviewed. The current methodologies effectively mitigate environmental interference, significantly enhancing the accuracy and reliability of agricultural robots. This study offers directional insights into the development of robotic detection and localization in agriculture, clarifying the future trajectory of this field and promoting the advancement of related technologies. © 2024 by the authors.",convolutional neural networks (CNN); deep learning; detection and localization; multi-sensor fusion; simultaneous localization and mapping (SLAM); ultra-wideband (UWB) technology; Agricultural robots; Deep neural networks; Precision agriculture; SLAM robotics; Agricultural robot; Convolutional neural network; Deep learning; Detection and localization; Higher efficiency; Multi-sensor fusion; Simultaneous localization and mapping; Ultra-wideband technology; Convolutional neural networks
Scopus,"Li, J.; Zheng, H.; Cui, Z.; Huang, Z.; Liang, Y.; Li, P.; Liu, P.",Intelligent detection method with 3D ranging for external force damage monitoring of power transmission lines,,2024,,,,10.1016/j.apenergy.2024.123983,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199888118&doi=10.1016%2fj.apenergy.2024.123983&partnerID=40&md5=f051514f9f3c9e3b665806bfb6adedc5,"The rapid development of smart grid requires more and more reliable power supply. The external force damage to transmission lines can significantly impact the safety and stability of the power grid, potentially causing electric shock accidents. Existing methods for monitoring external force damage in transmission corridors lack the ability to provide effective warnings based on the real-time distance between potential hazards and power lines. Additionally, limited computational power and storage capacity at edge terminals restrict the efficient deployment of high-precision (high complexity) visual algorithms. This study presents, for the first time, a lightweight intelligent detection method integrating detection and three-dimensional (3D) ranging. A regression loss optimized for small objects is introduced to compensate for the shortcomings of lightweight networks in detection accuracy. Simultaneously, based on the influence of convolutions in various modules of the baseline model on performance, lightweight improvements are made to the detector architecture using Omni-Dimensional Dynamic Convolution and Distribution Shifting Convolution. Finally, a 3D ranging module is integrated into the detector, involving operations such as 2D–3D information matching and back-projection transformation. This method innovatively achieves automated ranging and hierarchical warning. Its effectiveness is validated in transmission corridor scenarios under various weather conditions and surveillance video. The results demonstrate that our method outperforms other algorithms in hazard detection accuracy and lightweight performance. Moreover, the distance prediction error rate is below 1.6%. The hierarchical warning solutions can be applied to more scenarios. © 2024 Elsevier Ltd",Coordinate transformation; Lightweight deep learning; Monocular vision; Prevention of external force damage; Smart power transmission; Computational complexity; Computational efficiency; Damage detection; Deep neural networks; Electric lines; Electric power distribution; Electric power transmission; Electric power transmission networks; Hazards; Power transmission; Security systems; Smart power grids; 3-D ranging; Coordinate transformations; External force; Intelligent detection methods; Lightweight deep learning; Monocular vision; Power-transmission; Prevention of external force damage; Smart power; Smart power transmission; damage mechanics; detection method; electricity supply; energy storage; machine learning; power line; prediction; smart grid; Convolution
Scopus,"Li, J.; Jia, M.; Li, B.; Meng, L.; Zhu, L.",Multi-Grade Road Distress Detection Strategy Based on Enhanced YOLOv8 Model,,2024,,,,10.3390/buildings14123832,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213260692&doi=10.3390%2fbuildings14123832&partnerID=40&md5=3671e38cca3f0bf9bb8baf56d5a7e20c,"The total mileage of the road network in China has been growing rapidly during the last twenty years. With the development of deep learning, the automatic road distr ess detection method is more accurate and effective than manual detection. However, the road are classified into five grades according to the Chinese road standard and each grade has its own characteristics. A single model cannot effectively identify multi-grade roads with different materials and levels of road distress. This study proposes a YOLOv8-based road distress detection strategy adapted for multiple road grades. The improved URetinex-Net network is used to enhance the spatial features and scenario diversity of the road distress datasets. Compared to the base YOLOv8 model, the enhancements have led to a 12% increase in accuracy for cement roads, a 22.3% improvement in detection speed, a 5.5% increase in accuracy for ordinary asphalt roads, a 7.5% increase in recognition accuracy for highways, and a 9.3% improvement in detection speed, with significant effects. This study refines the classification of roads based on their grades and matches them with corresponding artificial intelligence training strategies, providing guidance for road inspection and maintenance. © 2024 by the authors.",data enhancement; deep learning; distress detection; YOLOv8; Classifieds; Data enhancement; Deep learning; Detection methods; Detection speed; Distress detection; Road grades; Road network; Single models; YOLOv8; Deep learning
Scopus,"Tang, D.; Tang, S.; Fan, Z.",LCFF-Net: A lightweight cross-scale feature fusion network for tiny target detection in UAV aerial imagery,,2024,,,,10.1371/journal.pone.0315267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213336340&doi=10.1371%2fjournal.pone.0315267&partnerID=40&md5=36c9f9c135bf7ac6fd0dfa7160fe34ef,"In the field of UAV aerial image processing, ensuring accurate detection of tiny targets is essential. Current UAV aerial image target detection algorithms face challenges such as low computational demands, high accuracy, and fast detection speeds. To address these issues, we propose an improved, lightweight algorithm: LCFF-Net. First, we propose the LFERELAN module, designed to enhance the extraction of tiny target features and optimize the use of computational resources. Second, a lightweight cross-scale feature pyramid network (LC-FPN) is employed to further enrich feature information, integrate multi-level feature maps, and provide more comprehensive semantic information. Finally, to increase model training speed and achieve greater efficiency, we propose a lightweight, detail-enhanced, shared convolution detection head (LDSCD-Head) to optimize the original detection head. Moreover, we present different scale versions of the LCFF-Net algorithm to suit various deployment environments. Empirical assessments conducted on the VisDrone dataset validate the efficacy of the algorithm proposed. Compared to the baseline-s model, the LCFF-Net-n model outperforms baseline-s by achieving a 2.8% increase in the mAP50 metric and a 3.9% improvement in the mAP50–95 metric, while reducing parameters by 89.7%, FLOPs by 50.5%, and computation delay by 24.7%. Thus, LCFF-Net offers high accuracy and fast detection speeds for tiny target detection in UAV aerial images, providing an effective lightweight solution. © 2024 Tang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Algorithms; Image Processing, Computer-Assisted; Neural Networks, Computer; Unmanned Aerial Devices; algorithm; article; controlled study; detection algorithm; diagnosis; human; image processing; imagery; algorithm; artificial neural network; image processing; procedures; unmanned aerial vehicle"
Scopus,"Zhuang, J.; Wang, N.; Zhuang, Y.; Hao, Y.",Frame Extraction Person Retrieval Framework Based on Improved YOLOv8s and the Stage-Wise Clustering Person Re-Identification,,2025,,,,10.1049/ipr2.70046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000439590&doi=10.1049%2fipr2.70046&partnerID=40&md5=6f53bfc07c7ae7b2e5796aee24d4cd2d,"Person re-identification (Re-ID), a crucial research area in smart city security, faces challenges due to person posture changes, object occlusion and other factors, making it difficult for existing methods to accurately retrieving target person in video surveillance. To resolve this problem, we propose a person retrieval framework that integrates YOLOv8s and person Re-ID. Improved YOLOv8s is employed to extract person categories from the video on a frame-by-frame basis, and when combined with the stage-wise clustering person Re-ID network (SCPN), it enables collaborative person retrieval across multiple cameras. Notably, a feature precision (FP) module is added in the YOLOv8s network to form FP-YOLOv8s, and SCPN incorporates innovative enhancements including the stage-wise learning rate scheduler, centralized clustering loss and adaptive representation joint attention module into the person Re-ID baseline model. Comprehensive experiments on COCO, Market-1501 and DukeMTMC-ReID datasets demonstrate that our proposed framework outperforms several other leading methods. Given the scarcity of image-video person Re-ID datasets, we also provide an extended image-video person (EIVP) dataset, which contains 102 videos and 814 bounding boxes of 57 identities captured by 8 cameras. The video reasoning detection score of this framework reaches 78.8% on this dataset, indicating a 3.2% increase compared to conventional models. © 2025 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",feature representation; frame extraction; person re-identification; person retrieval; YOLOv8s; Image retrieval; City securities; Clusterings; Feature representation; Frame extraction; Object occlusion; Person re identifications; Person retrieval; Research areas; Retrieval frameworks; YOLOv8; Image enhancement
Scopus,"Xu, X.; Li, X.",Research on surface defect detection algorithm of pipeline weld based on YOLOv7,,2024,,,,10.1038/s41598-024-52451-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182849727&doi=10.1038%2fs41598-024-52451-3&partnerID=40&md5=788e800d173f40afc444941779df116c,"Aiming at the problems of low target detection accuracy and high leakage rate of the current traditional weld surface defect detection methods and existing detection models, an improved YOLOv7 pipeline weld surface defect detection model is proposed to improve detection results. In the improved model, a Le-HorBlock module is designed, and it is introduced into the back of fourth CBS module of the backbone network, which preserves the characteristics of high-order information by realizing second-order spatial interaction, thus enhancing the ability of the network to extract features in weld defect images. The coordinate attention (CoordAtt) block is introduced to enhance the representation ability of target features, suppress interference. The CIoU loss function in YOLOv7 network model is replaced by the SIoU, so as to optimize the loss function, reduce the freedom of the loss function, and accelerate convergence. And a new large-scale pipeline weld surface defect dataset containing 2000 images of pipeline welds with weld defects is used in the proposed model. In the experimental comparison, the improved YOLOv7 network model has greatly improved the missed detection rate compared with the original network. The experimental results show that the improved YOLOv7 network model mAP@80.5 can reach 78.6%, which is 15.9% higher than the original model, and the detection effect is better than the original network and other classical target detection networks. © 2024, The Author(s).",article; controlled study; detection algorithm; human; pipeline
Scopus,"An, R.; Zhang, X.; Sun, M.; Wang, G.",GC-YOLOv9: Innovative smart city traffic monitoring solution,,2024,,,,10.1016/j.aej.2024.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198272213&doi=10.1016%2fj.aej.2024.07.004&partnerID=40&md5=dc1be1f8c003f0ccb5a05354fa14fb61,"In urban smart city environments, traffic hazards can lead to catastrophic outcomes, including significant property losses and severe threats to public safety. Conventional traffic monitoring systems are limited in terms of accuracy and speed, presenting significant challenges for real-time traffic surveillance. To tackle these challenges, this paper introduces the GC-YOLOv9 algorithm. Specifically, we have enhanced the YOLOv9 model by incorporating Ghost Convolution, markedly improving the model's perceptual abilities and detection accuracy. Furthermore, this study designed an integrated smart city framework that includes layers for service applications, the Internet of Things, edge processing, and data centers. By deploying the enhanced YOLOv9 model within this framework, our method achieved mAP@0.5 scores of 77.15 and 74.95 on the BDD100K and Cityscapes datasets, respectively, surpassing existing technologies. Additionally, the potential applications of this method in public area fire safety management, forest fire monitoring, and intelligent security systems further underscore its significant value in improving the safety and efficiency of smart cities. © 2024 Faculty of Engineering, Alexandria University",Ghost convolution; Internet of Things; Real-time data processing; Smart cities; Traffic monitoring; YOLOv9; Convolution; Deforestation; Network security; Real time systems; Smart city; City traffic; Ghost convolution; Property loss; Public safety; Real-time data processing; Real-time traffic surveillances; Traffic hazards; Traffic monitoring; Traffic monitoring systems; YOLOv9; Internet of things
Scopus,"Chen, Y.; Yan, J.; Liu, Y.; Gao, Z.",LRS²-DM: Small Ship Target Detection in Low-Resolution Remote Sensing Images Based on Diffusion Models,,2025,,,,10.1109/TGRS.2025.3580609,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008648975&doi=10.1109%2fTGRS.2025.3580609&partnerID=40&md5=d6f21df4a6916fdbec84a2d2703f9677,"With advancements in remote sensing technology, ship detection has emerged as a pivotal component in marine environmental protection and maritime traffic management. However, the significant distance of satellite imaging results in ship targets appearing as small-scale objects in the images. Current detection algorithms face challenges in accurately identifying the features of small ship targets in low-resolution settings. Therefore, this article proposes a small ship target detection model for low-resolution remote sensing images based on diffusion models (DMs). In the first stage, cognitive conditions are used as inputs. A low-level super-resolution (L2SR) module enhances image clarity and facilitates the extraction of richer ship target features. The second stage employs a spatial refinement module (SRM) to effectively enhance textures, edges, and other fine-grained features of small targets. Finally, an optimized loss function is designed to mitigate uncertainties arising from noise in the DM for remote sensing images. The experimental results demonstrate that the proposed method achieves superior performance on the DOTA-v2.0-Ship and S-Ship datasets, attaining average precision (AP) values of 95.34% and 96.12%, respectively. Moreover, it sustains a high frame-per-second (FPS) rate, striking an optimal balance between detection accuracy and computational efficiency. © 1980-2012 IEEE.",Diffusion models (DMs); low resolution; ship inspection; small goals; Diffusion; Image enhancement; Remote sensing; Satellite imagery; Ships; Small satellites; Textures; Waterway transportation; Diffusion model; Image-based; Lower resolution; Remote sensing images; Remote sensing technology; Ship detection; Ship inspection; Ship targets; Small goal; Targets detection; detection method; environmental protection; image resolution; remote sensing; satellite imagery; traffic management; Computational efficiency
Scopus,"Wu, F.; Shen, T.; Bäck, T.; Chen, J.; Huang, G.; Jin, Y.; Kuang, K.; Li, M.; Lu, C.; Miao, J.; Wang, Y.; Wei, Y.; Wu, F.; Yan, J.; Yang, H.; Yang, Y.; Zhang, S.; Zhao, Z.; Zhuang, Y.; Pan, Y.","Knowledge-Empowered, Collaborative, and Co-Evolving AI Models: The Post-LLM Roadmap",,2025,,,,10.1016/j.eng.2024.12.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215941211&doi=10.1016%2fj.eng.2024.12.008&partnerID=40&md5=4b22ff775e65fbde0ff1cd79d3e74e4d,"Large language models (LLMs) have significantly advanced artificial intelligence (AI) by excelling in tasks such as understanding, generation, and reasoning across multiple modalities. Despite these achievements, LLMs have inherent limitations including outdated information, hallucinations, inefficiency, lack of interpretability, and challenges in domain-specific accuracy. To address these issues, this survey explores three promising directions in the post-LLM era: knowledge empowerment, model collaboration, and model co-evolution. First, we examine methods of integrating external knowledge into LLMs to enhance factual accuracy, reasoning capabilities, and interpretability, including incorporating knowledge into training objectives, instruction tuning, retrieval-augmented inference, and knowledge prompting. Second, we discuss model collaboration strategies that leverage the complementary strengths of LLMs and smaller models to improve efficiency and domain-specific performance through techniques such as model merging, functional model collaboration, and knowledge injection. Third, we delve into model co-evolution, in which multiple models collaboratively evolve by sharing knowledge, parameters, and learning strategies to adapt to dynamic environments and tasks, thereby enhancing their adaptability and continual learning. We illustrate how the integration of these techniques advances AI capabilities in science, engineering, and society—particularly in hypothesis development, problem formulation, problem-solving, and interpretability across various domains. We conclude by outlining future pathways for further advancement and applications. © 2024 THE AUTHORS",Artificial intelligence; Knowledge empowerment; Large language models; Model co-evolution; Model collaboration; Adversarial machine learning; Co-evolution; Domain specific; Intelligence models; Interpretability; Knowledge empowerment; Language model; Large language model; Model co-evolution; Model collaboration; Roadmap; Collaborative learning
Scopus,"Zhao, R.; Tang, S.H.; Shen, J.; Supeni, E.E.B.; Rahim, S.A.",Enhancing autonomous driving safety: A robust traffic sign detection and recognition model TSD-YOLO,,2024,,,,10.1016/j.sigpro.2024.109619,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199342177&doi=10.1016%2fj.sigpro.2024.109619&partnerID=40&md5=31a273fa41ffffb4f250478847fb7397,"As autonomous driving technology rapidly advances, Traffic Sign Detection and Recognition (TSDR) has become pivotal in ensuring the safety and regulatory compliance of autonomous vehicles. Despite progress, existing technologies struggle under challenging conditions such as adverse weather and complex roadway environments. To overcome these obstacles, we introduce a novel model, TSD-YOLO, which leverages Mamba and YOLO technologies to enhance the accuracy and robustness of traffic sign detection. Our innovative YOLO-MAM dual-branch module merges convolutional layer-based local feature extraction with the long-distance dependency capabilities of the State Space Models (SSMs). We conducted experimental validations using the Tsinghua-Tencent 100K (TT-100K) dataset and the Mapillary Traffic Sign Detection (MTSD) dataset, demonstrating our model's efficacy across various datasets. Furthermore, cross-dataset validations affirm the model's exceptional generalization and robustness across diverse environments. This study not only bolsters traffic sign detection and recognition in autonomous driving systems but also paves the way for future advancements in autonomous driving technology. © 2024 Elsevier B.V.",Autonomous driving; Mamba; Traffic sign detection; TSD-YOLO; YOLOv8; Regulatory compliance; State space methods; Traffic signs; Autonomous driving; Autonomous Vehicles; Detection models; Driving safety; Mamba; Recognition models; Traffic sign detection; Traffic sign detection and recognition; TSD-YOLO; YOLOv8; Autonomous vehicles
Scopus,"Wu, Z.; Zhang, Y.; Wang, X.; Li, H.; Sun, Y.; Wang, G.",Algorithm for detecting surface defects in wind turbines based on a lightweight YOLO model,,2024,,,,10.1038/s41598-024-74798-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206873013&doi=10.1038%2fs41598-024-74798-3&partnerID=40&md5=4fe1a1c1c32cc525379a1d7d057a5961,"Improving wind power generation efficiency and lowering maintenance and operational costs are possible through the early and efficient diagnosis and repair of surface defects in wind turbines. To solve the lightweight deployment difficulty and insufficient accuracy issues of the traditional detection methods, this paper proposes a high-precision PC-EMA block based on YOLOv8 using partial convolution (PConv) combined with an efficient multiscale attention (EMA) channel attention mechanism, which replaces the bottleneck layer of the YOLOv8 backbone network to improve the extraction of target feature information from each layer of the network. In the feature fusion phase, GSConv, which can retain more channel information, is introduced to balance the model’s complexity and accuracy. Finally, by merging two branches and designing the PConv head with a low-latency PConv rather than a regular convolution, we are able to effectively reduce the complexity of the model while maintaining accuracy in the detection head. We use the WIoUv3 as the regression loss for the improved model, which improves the average accuracy by 5.07% and compresses the model size by 32.5% compared to the original YOLOv8 model. Deployed on Jetson Nano, the FPS increased by 11 frames/s after a TensorRT acceleration. © The Author(s) 2024.",Channel attention mechanism; GSConv; Partial convolution; Wind turbines; YOLOv8; acceleration; algorithm; article; controlled study; diagnosis; human; latent period; wind; wind power
Scopus,"Bao, T.; Lin, D.; Zhang, X.; Zhou, Z.; Wang, K.",Pedestrian safety alarm system based on binocular distance measurement for trucks using recognition feature analysis,,2024,,,,10.1007/s43684-024-00080-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209215767&doi=10.1007%2fs43684-024-00080-y&partnerID=40&md5=3e81d3beccf939222506692bad79ad12,"As an essential part of modern smart manufacturing, road transport with large and heavy trucks has in-creased dramatically. Due to the inside wheel difference in the process of turning, there is a considerable safety hazard in the blind area of the inside wheel difference. In this paper, multiple cameras combined with deep learning algorithms are introduced to detect pedestrians in the blind area of wheel error. A scheme of vehicle-pedestrian safety alarm detection system is developed via the integration of YOLOv5 and an improved binocular distance measurement method. The system accurately measures the distance between the truck and nearby pedestrians by utilizing multiple cameras and PP Human recognition, providing real-time safety alerts. The experimental results show that this method significantly reduces distance measurement errors, improves the reliability of pedestrian detection, achieves high accuracy and real-time performance, and thus enhances the safety of trucks in complex traffic environments. © The Author(s) 2024.",Feature recognition; Human distance measurement; PP-human attribute identification; Security alarm; Alarm systems; Automobiles; Binoculars; Deep learning; Magnetic levitation vehicles; Pedestrian safety; Smart manufacturing; Truck transportation; Trucks; Feature analysis; Features recognition; Human attributes; Human distance measurement; Multiple cameras; PP-human attribute identification; Recognition features; Safety alarm system; Security alarm; Smart manufacturing; Wheels
Scopus,"Nie, H.; Zhang, G.; Li, D.; He, Y.",Environment Perception and Motion Planning for Multi-rotors: A Review,,2025,,,,10.13976/j.cnki.xk.2024.4751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009060795&doi=10.13976%2fj.cnki.xk.2024.4751&partnerID=40&md5=711396f92f72837f9aaad85db245f597,"Recently, there has been growing interest among researchers in developing multi-rotors with advanced autonomous flight capabilities. Multi-rotors are capable of performing complex and hazardous tasks in place of humans, such as search and rescue, inspection, and exploration missions. However, they face numerous challenges when operating in the diverse and dynamic real-world environments. To effectively accomplish these tasks, multi-rotors require a robust, safe, and efficient autonomous flight system. As we know, autonomous flight systems involve multiple complex technical aspects, including mapping, state estimation, and motion planning. We offer an in-depth discussion, comparison, and comprehensive review of the strengths and limitations of these submodules. Furthermore, we highlight the current limitations and unresolved challenges of autonomous flight in various scenarios, providing valuable insights for researchers seeking to bridge the gap between theory and practical applications. Finally, we summarize the future challenges and emerging trends in the development of autonomous flight systems. © 2025 Science Press. All rights reserved.",autonomous flight; motion planning; state estimation; trajectory optimization
Scopus,"Zhang, L.; Huang, Z.A.; Shi, C.; Ma, H.; Li, X.; Wu, X.",MFPIDet: improved YOLOV7 architecture based on multi-scale feature fusion for prohibited item detection in complex environment,,2024,,,,10.1007/s40747-024-01580-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201219749&doi=10.1007%2fs40747-024-01580-3&partnerID=40&md5=e392ce19cee1c34ad08b1069e5d44338,"Prohibited item detection is crucial for the safety of public places. Deep learning, one of the mainstream methods in prohibited item detection tasks, has shown superior performance far beyond traditional prohibited item detection methods. However, most neural network architectures in deep learning still lack sufficient local feature representation ability for overlapping and small targets, and ignore the problem of semantic conflicts caused by direct feature fusion. In this paper, we propose MFPIDet, a novel prohibited item detection neural network architecture based on improved YOLOV7 to achieve reliable prohibited item detection in complex environments. Specifically, a multi-scale attention module (MAM) backbone is proposed to filter the redundant information of target regions and further applied to enhance the local feature representation ability of overlapping objects. Here, to reduce the redundant information of target regions, a squeeze-excitation (SE) block is used to filter the background. Then, aiming at enhancing the feature expression ability of overlapping objects, a multi-scale feature extraction module (MFEM) is designed for local feature representation. In addition, to obtain richer context information, We design an adaptive fusion feature pyramid network (AF-FPN) to combine the adaptive context information fusion module (ACIFM) with the feature fusion module (FFM) to improve the neck structure of YOLOV7. The proposed method is validated on the PIDray dataset, and the tested results showed that our method obtained the highest mAP (68.7%), which is improved by 3.5% than YOLOV7 methods. Our approach provides a new design pattern for prohibited item detection in complex environments and shows the development potential of deep learning in related fields. © The Author(s) 2024.",Adaptive context information fusion; Attention mechanism; MFPIDet; Multi-scale feature extraction; Prohibited item detection
Scopus,"Xu, X.; Hou, W.; Li, X.",Detection method of small size defects on pipeline weld surface based on improved YOLOv7,,2024,,,,10.1371/journal.pone.0313348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212048266&doi=10.1371%2fjournal.pone.0313348&partnerID=40&md5=ac55c2ea6d0a7e65790b95402f5d2556,"The background of pipeline weld surface defect image is complex, and the defect size is small. Aiming at the small defect size in the weld image, which is easy to cause missed detection and false detection, a lightweight target detection algorithm based on improved YOLOv7 is proposed. Firstly, in the feature fusion network of YOLOv7, the detection ability of the algorithm to detect small and medium-sized targets in defect images is enhanced by adding a 160*160 small target detection head. Then, the convolution module in the backbone network and the feature fusion network is replaced by the depthwise separable convolution with less computational overhead, so as to effectively reduce the network calculation, parameter quantity and model volume. Finally, the loss function CIoU of YOLOv7 is optimized to EIoU loss function to accelerate the convergence speed of the model. The experimental results show that the defect detection mAP@0.5 based on the improved YOLOv7 algorithm can reach 72.2%, which is 11% higher than that of YOLOv7, and the model calculation amount and parameter amount are reduced by 75.6% and 60.3%, respectively. It can completely detect the small size defects and has a high degree of confidence, which can be effectively applied to the detection of small size defects on the surface of pipeline weld. Copyright: © 2024 Xu et al.","Algorithms; Image Processing, Computer-Assisted; Surface Properties; adult; algorithm; article; controlled study; detection algorithm; diagnosis; human; pipeline; velocity; algorithm; image processing; procedures; surface property"
Scopus,"Qian, R.; Ding, Y.",An Efficient UAV Image Object Detection Algorithm Based on Global Attention and Multi-Scale Feature Fusion,,2024,,,,10.3390/electronics13203989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207677243&doi=10.3390%2felectronics13203989&partnerID=40&md5=9007bf74c99499f0e552980c23b8cffb,"Object detection technology holds significant promise in unmanned aerial vehicle (UAV) applications. However, traditional methods face challenges in detecting denser, smaller, and more complex targets within UAV aerial images. To address issues such as target occlusion and dense small objects, this paper proposes a multi-scale object detection algorithm based on YOLOv5s. A novel feature extraction module, DCNCSPELAN4, which combines CSPNet and ELAN, is introduced to enhance the receptive field of feature extraction while maintaining network efficiency. Additionally, a lightweight Vision Transformer module, the CloFormer Block, is integrated to provide the network with a global receptive field. Moreover, the algorithm incorporates a three-scale feature fusion (TFE) module and a scale sequence feature fusion (SSFF) module in the neck network to effectively leverage multi-scale spatial information across different feature maps. To address dense small objects, an additional small object detection head was added to the detection layer. The original large object detection head was removed to reduce computational load. The proposed algorithm has been evaluated through ablation experiments and compared with other state-of-the-art methods on the VisDrone2019 and AU-AIR datasets. The results demonstrate that our algorithm outperforms other baseline methods in terms of both accuracy and speed. Compared to the YOLOv5s baseline model, the enhanced algorithm achieves improvements of 12.4% and 8.4% in AP50 and AP metrics, respectively, with only a marginal parameter increase of 0.3 M. These experiments validate the effectiveness of our algorithm for object detection in drone imagery. © 2024 by the authors.",feature fusion; global attention; object detection; UAV
Scopus,"Wang, J.; Wang, G.; Li, H.; Han, S.; Zhang, J.",Intelligent Construction Activity Identification for All-Weather Site Monitoring Using 4D Millimeter-Wave Technology,,2024,,,,10.1061/JCEMD4.COENG-14875,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202340123&doi=10.1061%2fJCEMD4.COENG-14875&partnerID=40&md5=72a35f58eef889bfec1e5a6a0e499c04,"Site monitoring is indispensable for modern construction management. Contact approaches, represented by wearable devices, have problems such as privacy leaks and hindering working. Vision-based noncontact methods depend highly on light and environmental conditions, and have poor three-dimensional perception ability. To propose an all-weather noncontact activity identification approach on construction sites, four-dimensional (4D) millimeter-wave (MMW) radar is adopted in this study for the first time because of its excellent abilities of motion sensing, spatial sensing, and penetration. First, a feature processing method is proposed to convert the MMW signal to a seven-dimensional point cloud, which consists of the shape information (x, y, and z) and four attributes (Doppler′, SNR′, H, and V), representing the information of velocity, signal-to-noise ratio, height, and volume, respectively. Second, a novel deep learning framework is developed, which contains (1) one shape subnetwork, driven by the PointNet++ model, to capture the shape information of objects; (2) four attribute subnetworks to fully utilize the additional attribute features; and (3) a two-layer fusion module to combine all the outputs of the subnetworks. With precision of 0.963, recall of 0.961, and an F1 score of 0.962, the results show that the proposed method can accurately identify construction activities under different environmental conditions. It also can facilitate further development of MMW radar-based solutions for construction site analysis. © 2024 American Society of Civil Engineers.",Activity identification; All-weather; Construction monitoring; Four-dimensional (4D) millimeter-wave radar; Gluing; Image coding; Image segmentation; Intelligent buildings; Motion sensors; Structural health monitoring; Time and motion study; Activity identification; All-weather; Construction activities; Construction monitoring; Environmental conditions; Four-dimensional  millimeter-wave radar; Millimeter-wave radar; Millimetre-wave radar; Site monitoring; Subnetworks; Project management
Scopus,"Katariya, V.; Jannat, F.-E.; Pazho, A.D.; Noghre, G.A.; Tabkhi, H.",VegaEdge: Edge AI confluence for real-time IoT-applications in highway safety,,2024,,,,10.1016/j.iot.2024.101268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197759726&doi=10.1016%2fj.iot.2024.101268&partnerID=40&md5=4cb3dfa8451e59a0d41c7c519cb046b4,"Traditional highway safety and monitoring solutions, reliant on surveillance cameras, face limitations due to their dependence on high-speed internet connectivity and the remote processing of Artificial Intelligence (AI) algorithms. This reliance introduces latency, undermining the real-time detection and analysis crucial for highway applications. The fusion of AI with the Internet of Things (IoT) opens new avenues for highway safety and surveillance innovation. Yet, most existing solutions are confined to vehicle detection and tracking, hindered by edge-IoT platforms’ limited power and processing capabilities. Addressing these limitations, this paper presents VegaEdge, an AI framework optimized for edge-IoT devices capable of real-time vehicle detection and tracking, trajectory forecasting, and identifying anomalous driving behaviors, such as road departures, sudden stops, and hazardous merges. A novel lightweight anomaly detection algorithm based on trajectory prediction is used for identifying hazardous driving on highways. VegaEdge demonstrates its versatility and efficiency across various traffic conditions and roadway configurations and has been evaluated on platforms like the Nvidia Jetson Orin and Xavier NX. The Nvidia Jetson Orin processes up to 738 trajectories per second and detects up to 140 vehicles in a single frame. Additionally, the Carolinas Anomaly Dataset (CAD) an extension of the Carolinas Highway Dataset (CHD) is introduced. While CHD consists of standard highway vehicle videos and trajectories, CAD includes video data of anomalous driving behaviors, providing a crucial resource for enhancing anomaly detection algorithms. CAD is available at https://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set. © 2024 Elsevier B.V.",Dataset; Deep learning; Edge; Embedded; Highway safety; IoT; Pipeline
Scopus,"Wang, Y.; Liu, Z.; Liu, J.; Shi, Y.; Ren, W.; Yan, X.; Fan, J.; Li, F.",GIFF-AlgaeDet: An effective and lightweight deep learning method based on Global Information and Feature Fusion for microalgae detection,,2024,,,,10.1016/j.algal.2024.103815,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210413904&doi=10.1016%2fj.algal.2024.103815&partnerID=40&md5=8da15a9797a809d572075ddf7666aaf5,"The identification and detection of microalgae are essential prerequisites for the development and utilization of microalgal resources. Traditional methods for the identification and detection of microalgae face the challenges of poor accuracy and time-consuming labor. Here is a method for microalgae identification and detection proposed in this paper, which utilizes Global Information and Feature Fusion (GIFF). Initially, to address the issue of low accuracy, the Coordinate Attention Group Shuffle Convolution (CAGS) is incorporated into the method to enhance the feature extraction capability. Furthermore, to address the issue of time-consuming labor, two small object detection heads for microalgae detection have been designed to effectively improve the training and detection speed. Additionally, the SCYLLA-IoU (SIoU) algorithm is employed to address the issue of unstable model convergence. To assess the efficacy of the method employed in this study, a dataset was intentionally created for the purpose of detecting microalgae. The experimental results indicate that, under the same experimental conditions, the proposed method has achieved significant improvements in terms of average precision, mAP@50, and mAP@95. Compared to the original method, it has increased by 3.1 %, 2 %, and 9.8 %, respectively. Moreover, this algorithm obtains a great improvement in detection speed and lightness, with a 29 % reduction in parameters and a single image detection time of 0.0219 s, which is significantly less than baseline. Location of the dataset and code: https://github.com/DjtuResearch/Microalgae_detection. © 2024",Deep learning; Microalgal detection; Microalgal resources; Object detection
Scopus,"Kozłowski, M.; Racewicz, S.; Wierzbicki, S.",Image Analysis in Autonomous Vehicles: A Review of the Latest AI Solutions and Their Comparison,,2024,,,,10.3390/app14188150,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205265639&doi=10.3390%2fapp14188150&partnerID=40&md5=3d0850dcf24d3abc98742a4bba055dcd,"The integration of advanced image analysis using artificial intelligence (AI) is pivotal for the evolution of autonomous vehicles (AVs). This article provides a thorough review of the most significant datasets and latest state-of-the-art AI solutions employed in image analysis for AVs. Datasets such as Cityscapes, NuScenes, CARLA, and Talk2Car form the benchmarks for training and evaluating different AI models, with unique characteristics catering to various aspects of autonomous driving. Key AI methodologies, including Convolutional Neural Networks (CNNs), Transformer models, Generative Adversarial Networks (GANs), and Vision Language Models (VLMs), are discussed. The article also presents a comparative analysis of various AI techniques in real-world scenarios, focusing on semantic image segmentation, 3D object detection, vehicle control in virtual environments, and vehicle interaction using natural language. Simultaneously, the roles of multisensor datasets and simulation platforms like AirSim, TORCS, and SUMMIT in enriching the training data and testing environments for AVs are highlighted. By synthesizing information on datasets, AI solutions, and comparative performance evaluations, this article serves as a crucial resource for researchers, developers, and industry stakeholders, offering a clear view of the current landscape and future directions in autonomous vehicle image analysis technologies. © 2024 by the authors.",AI solutions; autonomous vehicles; image analysis; safety features; Magnetic levitation vehicles; Semantic Segmentation; Visual languages; Artificial intelligence solution; Autonomous driving; Autonomous Vehicles; Convolutional neural network; Image analyze; Image-analysis; Intelligence models; Safety features; State of the art; Transformer modeling; Convolutional neural networks
Scopus,"Li, L.; Jin, Z.; Yu, X.; Wang, A.",Road Vehicle and Pedestrian Detection Based on YOLOv9 for Haar Wavelet Downsampling,,2024,,,,10.3778/j.issn.1002-8331.2406-0204,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007353484&doi=10.3778%2fj.issn.1002-8331.2406-0204&partnerID=40&md5=5cc16a356d9b53cf5fecb78ceab42105,"In the current background of intelligence and informatization, the YOLOv9 algorithm based on Haar wavelet downsampling (HWD) is proposed for vehicle and pedestrian target detection in complex environments with autonomous driving mode to intelligently collect pedestrian and vehicle targets on the road. The operation of Haar wavelet downsampling reduces the spatial resolution of feature maps and preserves detailed information such as edges and textures as much as possible, effectively reducing the uncertainty of information. By utilizing the sum of cross entropy loss and generalized dice loss as the loss function of the network, the difference between probability distributions can be effectively measured, and dice loss calculations can be performed pixel by pixel, making it easier to optimize the network. The experimental results show that the average accuracy of the proposed model reaches 95.86%, and the detection frame rate reaches 179 FPS on the KITTY dataset. Compared with YOLOv9, the improved algorithm can accurately identify vehicles and pedestrians of different scales on complex roads, which not only improves the redundancy of computational capacity and missed detection of small targets in the original detection algorithm, but also provides visual technology support for intelligent autonomous driving. © 2024 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",deep learning; Haar wavelet downsampling (HWD); small object detection; vehicles and pedestrians; YOLOv9; Image coding; Image segmentation; Autonomous driving; Deep learning; Down sampling; Haar wavelet downsampling; Haar-wavelets; Road vehicles; Small object detection; Vehicle and pedestrian; Vehicles detection; YOLOv9
Scopus,"Zhang, Y.; Deng, J.; Liu, P.; Li, W.; Zhao, S.",Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network,,2025,,,,10.1109/TASE.2024.3370147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187001436&doi=10.1109%2fTASE.2024.3370147&partnerID=40&md5=675e9b11286e7a2bd5fa0729b8b5e8e4,"Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively. © 2004-2012 IEEE.",domain adaptation; MAV dataset; MAV detection; noise suppression; Benchmarking; Cameras; Job analysis; Micro air vehicle (MAV); Noise abatement; Spurious signal noise; Statistical tests; Adaptation models; Benchmark testing; Domain adaptation; Features extraction; Micro air vehicle dataset; Micro air vehicle detection; Micro air-vehicles; Noise suppression; Task analysis; Vehicles detection; Feature extraction
Scopus,"Chaudhry, R.",SD-YOLO-AWDNet: A hybrid approach for smart object detection in challenging weather for self-driving cars,,2024,,,,10.1016/j.eswa.2024.124942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200551348&doi=10.1016%2fj.eswa.2024.124942&partnerID=40&md5=e6ad1040eaa414e08c7dfc8962196371,"Several deep learning algorithms are currently focused on object detection in adverse weather scenarios for autonomous driving systems. However, these algorithms face challenges in real-time scenarios, leading to a reduction in detection accuracy. To tackle these issues, this paper introduces a lightweight object detection model named Self Driving Cars You Only Look Once Adverse Weather Detection Network (SD-YOLO-AWDNet), derived from enhancements to the YOLOv5 algorithm. The model incorporates four progressive improvement levels within the YOLOv5 framework. This includes integrating C3Ghost and GhostConv modules in the backbone to enhance detection speed by reducing computational overhead during feature extraction. To address potential accuracy issues arising from these modules, Depthwise-Separable Dilated Convolutions (DSDC) are introduced, striking a balance between accuracy and parameter reduction. The model further incorporates a Coordinate Attention (CA) module in the GhostBottleneck to enhance feature extraction and eliminate unnecessary features, improving precision in object detection. Additionally, a novel “Focal Distribution Loss” replaces CIoU Loss, accelerating bounding box regression and loss reduction. Test dataset experiments demonstrate that SD-YOLO-AWDNet outperforms YOLOv5 with a 54% decrease in FLOPs, a 52.53% decrease in model parameters, a 2.24% increase in mAP, and a threefold improvement in detection speed. © 2024 Elsevier Ltd",Deep neural network; Object detection; Self driving cars; YOLO; Autonomous vehicles; Extraction; Feature extraction; Object detection; Object recognition; Statistical tests; Adverse weather; Autonomous driving; Detection networks; Detection speed; Driving systems; Features extraction; Hybrid approach; Objects detection; Smart objects; YOLO; Deep neural networks
Scopus,"Pao, W.Y.; Carvalho, M.; Hosseinnouri, F.; Li, L.; Rouaix, C.; Agelin-Chaab, M.; Hangan, H.; Gultepe, I.; Komar, J.",Evaluating weather impact on vehicles: a systematic review of perceived precipitation dynamics and testing methodologies,,2024,,,,10.1088/2631-8695/ad2033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183981658&doi=10.1088%2f2631-8695%2fad2033&partnerID=40&md5=ea67cb73e23daebc92ed2338d31fdf6a,"The performance of road vehicles degrades when driving in adverse weather conditions. Weather testing for vehicles is important to understand the impacts of precipitation on vehicle performance, such as driver visibility, autonomous sensor signal, tire traction, and structural integrity due to corrosion, to ensure safety. This tutorial summarizes the essential elements for performing realistic testing by applying physical and meteorological rationale to vehicle applications. Three major topics are identified as crucial steps for precise quantitative studies, including understanding the natural precipitation characteristics, estimating the perceived precipitation experienced by a moving vehicle, and selecting data collection strategies. The methods used in current practices to investigate the effects of rain and snow on road vehicles at common facilities of outdoor test tracks, drive-through weather chambers, and climatic wind tunnels are summarized. The testing techniques and relevant instrumentations are also discussed, with considerations of factors that influence the measured data, such as particle size distribution, precipitation intensity, wind-induced droplet dynamic events, accumulation behaviour, etc. The goals of this paper are to provide a tutorial with guidelines on designing weather testing experiments for road vehicles and to promote the idea of establishing standardized methodologies for realistic vehicle testing that facilitates accurate prediction of vehicle performance in adverse weather conditions. © 2024 The Author(s). Published by IOP Publishing Ltd",adverse weather; perceived precipitation; rain; snow; vehicle testing; Corrosion; Digital storage; Particle size; Particle size analysis; Rain; Roads and streets; Safety testing; Vehicle performance; Adverse weather; Autonomous sensors; Condition; Perceived precipitation; Performance; Road vehicles; Systematic Review; Testing methodology; Vehicle testing; Weather impact; Snow
Scopus,"Bao, S.; Shi, W.; Yang, D.; Xiang, H.; Yu, Y.",Global principal planes aided LiDAR-based mobile mapping method in artificial environments,,2024,,,,10.1016/j.aei.2024.102472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189004189&doi=10.1016%2fj.aei.2024.102472&partnerID=40&md5=afba8f044381f6823f54e9fccbb590c3,"3-D mapping of buildings is crucial for urban renewal, but traditional LiDAR-based mapping methods are often less effective for buildings with narrow spaces and limited geometric features. Current methods attempt to overcome this by integrating additional sensors, such as cameras, which increases cost and complexity. This paper proposes a novel LiDAR-based mobile mapping framework using global principal planes (GPPs) to address this challenge without additional sensors. GPPs are defined as unlimited planes characterized by principal normal vectors (PNVs). GPPs can provide stronger constraints than traditional small planes extracted from one or certain LiDAR frames because they are little affected by the accumulative error from point cloud matching. A PNV estimation method is also proposed based on an inertial measurement unit and polar histogram, and PNVs are axes of the natural cartesian XYZ coordinate system. Point clouds are transformed into the PNVs coordinate system to extract robust edge and plane feature points and GPPs. The proposed framework is tested in various environments. It achieves about 3 cm accuracy in corridors and similar accuracy in stairwells. Compared to five state-of-the-art mapping methods (Cartographer, etc.), its accuracy improves by over 76%, increasing at least an order of magnitude. In the outdoor KITTI dataset, it shows a reduction in absolute pose errors by 4% to 20%. Extensive experiments demonstrate its accuracy, robustness, and generalizability. Ablation experiments further validate the efficacy of different components in the framework. © 2024",Feature extraction; Global principal plane; LiDAR; Mobile mapping; Principal normal vector; Optical radar; Artificial environments; Co-ordinate system; Features extraction; Global principal plane; LiDAR; Mapping method; Mobile mapping; Normal vector; Principal normal vector; Principal planes; Mapping
Scopus,"Abu-raddaha, A.; El-Shair, Z.A.; Rawashdeh, S.",Leveraging Perspective Transformation for Enhanced Pothole Detection in Autonomous Vehicles,,2024,,,,10.3390/jimaging10090227,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205060414&doi=10.3390%2fjimaging10090227&partnerID=40&md5=8483e5d6adae2cb32ccfdc3ba6baa384,"Road conditions, often degraded by insufficient maintenance or adverse weather, significantly contribute to accidents, exacerbated by the limited human reaction time to sudden hazards like potholes. Early detection of distant potholes is crucial for timely corrective actions, such as reducing speed or avoiding obstacles, to mitigate vehicle damage and accidents. This paper introduces a novel approach that utilizes perspective transformation to enhance pothole detection at different distances, focusing particularly on distant potholes. Perspective transformation improves the visibility and clarity of potholes by virtually bringing them closer and enlarging their features, which is particularly beneficial given the fixed-size input requirement of object detection networks, typically significantly smaller than the raw image resolutions captured by cameras. Our method automatically identifies the region of interest (ROI)—the road area—and calculates the corner points to generate a perspective transformation matrix. This matrix is applied to all images and corresponding bounding box labels, enhancing the representation of potholes in the dataset. This approach significantly boosts detection performance when used with YOLOv5-small, achieving a 43% improvement in the average precision (AP) metric at intersection-over-union thresholds of 0.5 to 0.95 for single class evaluation, and notable improvements of 34%, 63%, and 194% for near, medium, and far potholes, respectively, after categorizing them based on their distance. To the best of our knowledge, this work is the first to employ perspective transformation specifically for enhancing the detection of distant potholes. © 2024 by the authors.",autonomous vehicles; computer vision; deep learning; mobile robotics; perspective transformation; pothole detection; Highway accidents; Mobile robots; Object detection; Adverse weather; Autonomous Vehicles; Avoiding obstacle; Corrective actions; Deep learning; Human reaction; Mobile robotic; Perspective transformation; Pothole detection; Road condition; Human reaction time
Scopus,"Wang, S.; Mei, L.; Yin, Z.; Li, H.; Liu, R.; Jiang, W.; Lu, C.X.",End-to-End Target Liveness Detection via mmWave Radar and Vision Fusion for Autonomous Vehicles,,2024,,,,10.1145/3628453,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199910727&doi=10.1145%2f3628453&partnerID=40&md5=20b618dcdfed4a43468d849bef855779,"The successful operation of autonomous vehicles hinges on their ability to accurately identify objects in their vicinity, particularly living targets such as bikers and pedestrians. However, visual interference inherent in real-world environments, such as omnipresent billboards, poses substantial challenges to extant vision-based detection technologies. These visual interference exhibit similar visual attributes to living targets, leading to erroneous identification. We address this problem by harnessing the capabilities of mmWave radar, a vital sensor in autonomous vehicles, in combination with vision technology, thereby contributing a unique solution for liveness target detection. We propose a methodology that extracts features from the mmWave radar signal to achieve end-to-end liveness target detection by integrating the mmWave radar and vision technology. This proposed methodology is implemented and evaluated on the commodity mmWave radar IWR6843ISK-ODS and vision sensor Logitech camera. Our extensive evaluation reveals that the proposed method accomplishes liveness target detection with a mean average precision of 98.1%, surpassing the performance of existing studies. © 2024 Copyright held by the owner/author(s).",mmWave radar; Target liveness detection; Millimeter waves; Radar interference; Tracking radar; Autonomous Vehicles; End to end; Liveness; Liveness detection; Mm waves; Mmwave radar; Real world environments; Target liveness detection; Targets detection; Vision technology; Autonomous vehicles
Scopus,"Chen, T.; Zhu, S.; Gao, T.; Li, H.; Tu, H.; Li, Z.",Real-time Vehicle Detection Based on Adaptive Fusion,,2024,,,,10.11908/j.issn.0253-374x.23399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191900879&doi=10.11908%2fj.issn.0253-374x.23399&partnerID=40&md5=0f6c0021a6ab69329c444a83564a3bf7,A traffic target detection algorithm，fusion attention adaptive pyramid network （FAAP-Net） ， is proposed to address the issues of slow speed and low accuracy in traditional vehicle detection techniques，significantly reducing the occurrence of traffic accidents. To mitigate computational complexity， a lightweight complementary pooling structure （CPS） is designed，employing two sets of different pooling combinations in width and height，which maintains a high precision while significantly reducing the floating point operations per second （GFLOPs） and the parameter count of the network. Addressing the information loss during intelligent traffic system feature map generation， the adaptive fusion feature pyramid network （AF-FPN）incorporates the adaptive attention module （AAM） and the feature enhancement module （FEM） to integrate shape features for vehicle detection. Lastly，to address the weak representation of vehicle detail features，a channel-wise grouped attention （SA） mechanism is introduced，enhancing the focus of the backbone network on various vehicle detection details and effectively extracting significant features. The experimental results on the BDD100K dataset demonstrate that the FAAP-Net algorithm achieves a notable improvement，increasing the average precision from 30.3 % to 43.7 %. © 2024 Science Press. All rights reserved.,adaptive fusion; complementary pooling; object detection; shuffle attention; vehicle detection; Accidents; Digital arithmetic; Feature extraction; Vehicles; Adaptive fusion; Adaptive pyramid; Algorithm fusion; Complementary pooling; Objects detection; Pyramid network; Real- time; Shuffle attention; Target detection algorithm; Vehicles detection; Object detection
Scopus,"Deng, P.; Zhou, L.; Chen, J.",PVC-SSD: Point-Voxel Dual-Channel Fusion With Cascade Point Estimation for Anchor-Free Single-Stage 3-D Object Detection,,2024,,,,10.1109/JSEN.2024.3380898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189535436&doi=10.1109%2fJSEN.2024.3380898&partnerID=40&md5=6958b810b8dae447467539c496cd0d81,"Existing single-stage 3-D object detection algorithms, whether relying on point or voxel methodologies, face challenges in achieving high-performance detection across diverse object categories simultaneously. Moreover, current algorithms employing point-voxel approaches often fall short in fully leveraging the advantages offered by the two sparse point cloud feature extraction methods. Consequently, these methodologies inadequately capture both the local and global features of the object. In response to these challenges, we introduce a novel single-stage 3-D object detection algorithm called PVC-SSD. This algorithm adopts an anchor-free methodology and employs point-voxel dual-channel fusion (PVCF) encoding to effectively model both local and global features, thereby enhancing the overall performance of object detection. The proposed algorithm comprises three key components. First, the PVCF module is designed to seamlessly integrate both local and global features of the object. Second, the cascade candidate point estimation (CCPE) module focuses on improving the quality of candidate points. At last, the position encoding self-attention (PESA) module is dedicated to establishing pointwise correlations within sparse point clouds. And this module is instrumental in reinforcing foreground features and mitigating geometric differences within the same category induced by factors such as viewpoint and distance. Through extensive experiments conducted on the KITTI and Waymo large scale 3-D object detection datasets, we substantiate the robust competitiveness and efficiency of PVC-SSD in multicategory detection tasks. © 2001-2012 IEEE.",3-D object detection; attention mechanism; autonomous vehicle; light detection and ranging (LiDAR); point cloud; Autonomous vehicles; Encoding (symbols); Extraction; Job analysis; Object detection; Object recognition; Optical radar; Signal detection; Signal encoding; Three dimensional displays; 3D object; 3d object detection; Attention mechanisms; Autonomous Vehicles; Features extraction; Light detection and ranging; Objects detection; Point cloud compression; Point-clouds; Task analysis; Three-dimensional display; Feature extraction
Scopus,"Jiang, C.; Ren, H.; Yang, H.; Huo, H.; Zhu, P.; Yao, Z.; Li, J.; Sun, M.; Yang, S.",M2FNet: Multi-modal fusion network for object detection from visible and thermal infrared images,,2024,,,,10.1016/j.jag.2024.103918,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193715729&doi=10.1016%2fj.jag.2024.103918&partnerID=40&md5=3e5da9fe239bc9bb2270151739c60497,"Fusing multi-modal information from visible (VIS) and thermal infrared (TIR) images is crucial for object detection in fully adapting to varied lighting conditions. However, the existing models usually treat VIS and TIR images as independent information and extract corresponding features from separate networks due to the scarcity of training data with labeled instances from both VIS and TIR registration images. To fill this gap, a novel Multi-Modal Fusion NETwork (M2FNet) based on the Transformer architecture is proposed in this paper, which contains two effective modules: the Union-Modal Attention (UMA) and the Cross-Modal Attention (CMA). The UMA module aggregates multi-spectral features from VIS and TIR images and then extracts multi-modal features via a convolutional neural network (CNN) backbone. The CMA module is designed to learn cross-attention features from VIS and TIR pairwise features by Transformer architecture. Evaluation results by the mean average precision (mAP) metric show that the M2FNet method significantly advances the baseline methods trained using only VIS or TIR images by 10.71 % and 2.97 %, respectively. The increments in mAP are observed in the M2FNet method compared with the existing multi-modal methods on two public datasets. Sensitivity analysis of eight illumination thresholds shows that the M2FNet method presents robustness performance on varied illumination conditions and achieves the maximum increase in accuracy of 25.6 %. Moreover, this method is subsequently applied to a new testing dataset, VI2DA (Visible-Infrared paired Video and Image DAtaset), observed by diverse sensors and platforms for testing the generalization ability of object detectors, which will be publicly available at https://github.com/TIR-OD/Datasets. © 2024 The Author(s)",Low-light condition; Multi-modal fusion network; Object detection; Transformer architecture; Visible and thermal infrared images; accuracy assessment; artificial neural network; comparative study; data set; detection method; infrared imagery; light availability; precision; remote sensing
Scopus,"Xu, S.; Wang, X.; Sun, Q.; Dong, K.",MWIRGas-YOLO: Gas Leakage Detection Based on Mid-Wave Infrared Imaging,,2024,,,,10.3390/s24134345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198393297&doi=10.3390%2fs24134345&partnerID=40&md5=3a104ade3bc1dc5401f65fa455f479ca,"The integration of visual algorithms with infrared imaging technology has become an effective tool for industrial gas leak detection. However, existing research has mostly focused on simple scenarios where a gas plume is clearly visible, with limited studies on detecting gas in complex scenes where target contours are blurred and contrast is low. This paper uses a cooled mid-wave infrared (MWIR) system to provide high sensitivity and fast response imaging and proposes the MWIRGas-YOLO network for detecting gas leaks in mid-wave infrared imaging. This network effectively detects low-contrast gas leakage and segments the gas plume within the scene. In MWIRGas-YOLO, it utilizes the global attention mechanism (GAM) to fully focus on gas plume targets during feature fusion, adds a small target detection layer to enhance information on small-sized targets, and employs transfer learning of similar features from visible light smoke to provide the model with prior knowledge of infrared gas features. Using a cooled mid-wave infrared imager to collect gas leak images, the experimental results show that the proposed algorithm significantly improves the performance over the original model. The segment mean average precision reached 96.1% (mAP50) and 47.6% (mAP50:95), respectively, outperforming the other mainstream algorithms. This can provide an effective reference for research on infrared imaging for gas leak detection. © 2024 by the authors.",gas leak detection; global attention mechanism; mid-wave infrared imaging; small target detection layer; Gases; Infrared radiation; Leak detection; Smoke; Attention mechanisms; Gas leak detection; Gas leakages; Gas leaks; Gas plumes; Global attention mechanism; Leakage detection; Mid-wave infrared imaging; Small target detection; Small target detection layer; adult; algorithm; article; diagnosis; gas; human; infrared radiation; male; plume; smoke; thermography; transfer of learning; Image enhancement
Scopus,"Hsieh, C.-C.; Jia, H.-W.; Huang, W.-H.; Hsih, M.-H.",Deep Learning-Based Road Pavement Inspection by Integrating Visual Information and IMU,,2024,,,,10.3390/info15040239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191662389&doi=10.3390%2finfo15040239&partnerID=40&md5=46762bfa34f8fc09e0a60acf8895a7c8,"This study proposes a deep learning method for pavement defect detection, focusing on identifying potholes and cracks. A dataset comprising 10,828 images is collected, with 8662 allocated for training, 1083 for validation, and 1083 for testing. Vehicle attitude data are categorized based on three-axis acceleration and attitude change, with 6656 (64%) for training, 1664 (16%) for validation, and 2080 (20%) for testing. The Nvidia Jetson Nano serves as the vehicle-embedded system, transmitting IMU-acquired vehicle data and GoPro-captured images over a 5G network to the server. The server recognizes two damage categories, low-risk and high-risk, storing results in MongoDB. Severe damage triggers immediate alerts to maintenance personnel, while less severe issues are recorded for scheduled maintenance. The method selects YOLOv7 among various object detection models for pavement defect detection, achieving a mAP of 93.3%, a recall rate of 87.8%, a precision of 93.2%, and a processing speed of 30–40 FPS. Bi-LSTM is then chosen for vehicle vibration data processing, yielding 77% mAP, 94.9% recall rate, and 89.8% precision. Integration of the visual and vibration results, along with vehicle speed and travel distance, results in a final recall rate of 90.2% and precision of 83.7% after field testing. © 2024 by the authors.",deep learning; image recognition; intelligent inspection; pavement inspection; 5G mobile communication systems; Data handling; Deep learning; Defects; Inspection; Integration testing; Learning systems; Object detection; Pavements; Statistical tests; Vehicles; Vibrations (mechanical); Well testing; Acceleration change; Deep learning; Defect detection; Intelligent inspection; Learning methods; Pavement inspections; Recall rate; Road pavements; Three axes; Visual information; Image recognition
Scopus,"Niu, S.; Nie, Z.; Li, G.; Zhu, W.",Early Drought Detection in Maize Using UAV Images and YOLOv8+,,2024,,,,10.3390/drones8050170,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194050631&doi=10.3390%2fdrones8050170&partnerID=40&md5=64752e390bc22719ec0137b7a628999f,"The escalating global climate change significantly impacts the yield and quality of maize, a vital staple crop worldwide, especially during seedling stage droughts. Traditional detection methods are limited by their single-scenario approach, requiring substantial human labor and time, and lack accuracy in the real-time monitoring and precise assessment of drought severity. In this study, a novel early drought detection method for maize based on unmanned aerial vehicle (UAV) images and Yolov8+ is proposed. In the Backbone section, the C2F-Conv module is adopted to reduce model parameters and deployment costs, while incorporating the CA attention mechanism module to effectively capture tiny feature information in the images. The Neck section utilizes the BiFPN fusion architecture and spatial attention mechanism to enhance the model’s ability to recognize small and occluded targets. The Head section introduces an additional 10 × 10 output, integrates loss functions, and enhances accuracy by 1.46%, reduces training time by 30.2%, and improves robustness. The experimental results demonstrate that the improved Yolov8+ model achieves precision and recall rates of approximately 90.6% and 88.7%, respectively. The mAP@50 and mAP@50:95 reach 89.16% and 71.14%, respectively, representing respective increases of 3.9% and 3.3% compared to the original Yolov8. The UAV image detection speed of the model is up to 24.63 ms, with a model size of 13.76 MB, optimized by 31.6% and 28.8% compared to the original model, respectively. In comparison with the Yolov8, Yolov7, and Yolo5s models, the proposed method exhibits varying degrees of superiority in mAP@50, mAP@50:95, and other metrics, utilizing drone imagery and deep learning techniques to truly propel agricultural modernization. © 2024 by the authors.",maize drought; object detection; small targets; UAV; YOLOv8; Aircraft detection; Deep learning; Image enhancement; Aerial vehicle; Attention mechanisms; Detection methods; Drought detection; Maize drought; Objects detection; Small targets; Unmanned aerial vehicle; Vehicle images; YOLOv8; Unmanned aerial vehicles (UAV)
Scopus,"Molina-Padrón, N.; Cabrera-Almeida, F.; Araña-Pulido, V.; Tovar, B.",Towards a Global Surveillance System for Lost Containers at Sea,,2024,,,,10.3390/jmse12020299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185964094&doi=10.3390%2fjmse12020299&partnerID=40&md5=aa14c836b75eebdf17e7aedd41eaeec8,"Every year, more than 1500 containers are lost around the world. These accidents are increasingly more common due to the boom of the shipping industry, presenting serious consequences for marine ecosystems and maritime navigation. This problem has alerted various international organisms to regulate these catastrophes, incorporating new regulations that will force cargo ships to report the loss of containers during its voyages. However, the lack of technological means that support compliance with this regulation may lead to these accidents continuing to affect the maritime sector. This article analyzes different electronic technologies for the prevention of collisions with floating containers, as well as their monitoring at a global level. The analysis carried out provides a glimpse of the possibility of developing a global monitoring system for containers lost at sea. This analysis compares both the opportunities and limitations of each of the proposed technologies, demonstrating how the current state-of-the-art technology has sufficient means to address this problem. © 2024 by the authors.",container; detection; identification; location; lost; monitoring
Scopus,"Swathi, P.; Tejaswi, D.S.; Khan, M.A.; Saishree, M.; Rachapudi, V.B.; Anguraj, D.K.",Real-Time Vehicle Detection for Traffic Monitoring: A Deep Learning Approach,,2024,,,,10.56294/dm2024295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195962652&doi=10.56294%2fdm2024295&partnerID=40&md5=3b1066a44b2b25cb9f5d8d8b2ca53603,"Vehicle detection is an essential technology for intelligent transportation systems and autonomous vehicles. Reliable real-time detection allows for traffic monitoring, safety enhancements and navigation aids. However, vehicle detection is a challenging computer vision task, especially in complex urban settings. Traditional methods using hand-crafted features like HAAR cascades have limitations. Recent deep learning advances have enabled convolutional neural networks (CNNs) like Faster R-CNN, SSD and YOLO to be applied to vehicle detection with significantly improved accuracy. But each technique has tradeoffs between precision and processing speed. Two-stage detectors like Faster R-CNN are highly accurate but slow at 7 FPS. Single-shot detectors like SSD are faster at 22 FPS but less precise. YOLO is extremely fast at 45 FPS but has lower accuracy. This paper reviews prominent deep learning vehicle detectors. It proposes a new integrated method combining YOLOv3 detection, optical flow tracking and trajectory analysis to enhance both accuracy and speed. Results on highway and urban datasets show improved precision, recall and F1 scores compared to YOLOv3 alone. Optical flow helps filter noise and recover missed detections. Trajectory analysis enables consistent object IDs across frames. Compared to other CNN models, the proposed technique achieves a better balance of real-time performance and accuracy. Occlusion handling and small object detection remain open challenges. In summary, deep learning has enabled major progress but enhancements in model architecture, training data and occlusion handling are needed to realize the full potential for traffic management applications. The integrated method proposed offers improved performance over baseline detectors. We have achieved 99 % accuracy in our project. © 2024; Los autores.",Convolution Neural Network (CNN); Deep Learning; Image Classification; Machine Learning; Multi Detecting Object Tracking; Traffic Detection
Scopus,"Brescia, W.; Gomes, P.; Toni, L.; Mascolo, S.; De Cicco, L.",MilliNoise: a Millimeter-wave Radar Sparse Point Cloud Dataset in Indoor Scenarios,,2024,,,,10.1145/3625468.3652189,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191976218&doi=10.1145%2f3625468.3652189&partnerID=40&md5=251bc5d899bdcedbdc1cfd3000be842f,"Millimeter-wave (mmWave) radar sensors produce Point Clouds (PCs) that are much sparser and noisier than other PC data (e.g., Li-DAR), yet they are more robust in challenging conditions such as in the presence of fog, dust, smoke, or rain. This paper presents MilliNoise, a point cloud dataset captured in indoor scenarios through a mmWave radar sensor installed on a wheeled mobile robot. Each of the 12M points in the MilliNoise dataset is accurately labeled as true/noise point by leveraging known information of the scenes and a motion capture system to obtain the ground truth position of the moving robot. Each frame is carefully pre-processed to produce a fixed number of points for each cloud, enabling classification tools which require data with a fixed shape. Moreover, MilliNoise has been post-processed by labeling each point with the distance to its closest obstacle in the scene, which allows casting the denoising task into the regression framework. Along with the dataset, we provide researchers with the tools to visualize the data and prepare it for statistical and machine learning analysis. MilliNoise is available at: https://github.com/c3lab/MilliNoise  © 2024 Owner/Author.",Microcomputers; Millimeter waves; Radar equipment; Smartphones; Smoke; Condition; Ground truth; Millimeter-wave radar; Millimetre-wave radar; Motion capture system; Moving robots; Point cloud data; Point-clouds; Radar sensors; Sparse point cloud; Mobile robots
Scopus,"Lan, J.; Zheng, M.; Chu, X.; Liu, C.; Ding, S.",A ship high-precision positioning method in the lock chamber based on LiDAR,,2024,,,,10.1016/j.oceaneng.2024.118033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191944686&doi=10.1016%2fj.oceaneng.2024.118033&partnerID=40&md5=78af39af143aac359132b5cdb3b56623,"Accurate perception of ship position can assist ships in safe navigation in the lock. However, due to the limited space in the lock chamber, the satellite positioning signal is unstable, making it difficult to accurately position the ship in real time. Therefore, a novel ship positioning method based on LiDAR is proposed. Firstly, the preprocessing of point cloud is realized through the steps including point cloud reflection intensity filtering, coordinate transformation, and ship point cloud clustering extraction. Then, different segmentation strategies are proposed to extract the point cloud of each ship with respect to the characteristics of point cloud changes during the ship's navigation upstream and downstream through the lock. Finally, real-time tracking of ships is realized by Kalman filter. The experiments of four ships navigating upstream and downstream through the lock were conducted respectively. The results show that the root mean square error (RMSE) of ship position is about 1 m, which is better than the BeiDou positioning accuracy. The overall processing time of each frame is controlled within 0.1 s, which meets the practical requirements. This present study provides a new solution to the ship positioning problem in locks. © 2024",LiDAR; Point cloud segmentation; Ship lock; Ship positioning; Kalman filters; Mean square error; Optical radar; Ships; Down-stream; High precision positioning; LiDAR; Limited space; Point cloud segmentation; Point-clouds; Positioning methods; Safe navigations; Ship lock; Ship positioning; accuracy assessment; BDS; lidar; positioning system; precision; real time; segmentation; ship technology; Locks (fasteners)
Scopus,"Hosseinian, S.M.; Mirzahossein, H.",Efficiency and Safety of Traffic Networks Under the Effect of Autonomous Vehicles,,2024,,,,10.1007/s40996-023-01291-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179343296&doi=10.1007%2fs40996-023-01291-8&partnerID=40&md5=1d58b9d7038d57911c5d31954087cf8a,"Efficiency and safety are two crucial aspects that have garnered significant attention in the context of traffic networks. The investigation of autonomous vehicles (AVs) in terms of efficiency and safety in a traffic network is a relatively new topic, and there have not been many practical developments reported so far. However, despite the availability of information, it is scattered across fragments within multiple resources dealing with efficiency and safety under the influence of AVs. Therefore, this study focused on the effect of efficiency and safety under the influence of AVs in a traffic network and provides an overview of the main research outcomes in this field. The review then delves into the challenges and opportunities associated with integrating AVs into existing traffic systems. The methodologies and approaches used in analyzing the effects of AVs are also explored. This review identifies critical gaps in knowledge and suggests future research directions to further enhance the understanding of how AVs can contribute to the efficiency and safety of traffic networks. Finally, this study offers valuable insights and guidance for policymakers, researchers, and practitioners involved in the planning, design, and management of transportation systems as they navigate the integration of AVs into traffic networks. © The Author(s), under exclusive licence to Shiraz University 2023.",Autonomous vehicles; Efficiency; Safety; Traffic network
Scopus,"Li, Z.; Yuan, T.; Ma, L.; Zhou, Y.; Peng, Y.",Target Detection for USVs by Radar-Vision Fusion with Swag-Robust Distance-Aware Probabilistic Multimodal Data Association,,2024,,,,10.1109/JSEN.2024.3394703,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192181478&doi=10.1109%2fJSEN.2024.3394703&partnerID=40&md5=313a6c0a3eb3b1bb07d9f690b0c6d035,"Unmanned surface vehicles (USVs) have been widely used for a wide range of tasks in the past decades. Accurate perception of the surrounding environment on the water surface under complex conditions is crucial for USVs to conduct effective operations. This article proposes a radar-vision fusion framework for USVs to accurately detect typical targets on the water surface. The modality difference between images and radar measurements, along with their perpendicular coordinates, presents challenges in the fusion process. The swaying of USVs on water and the extensive areas of perception enhance the difficulties of multisensor data association. To address these problems, we propose two modules to enhance multisensor fusion performance: a movement-compensated projection module and a distance-aware probabilistic data association module. The former effectively reduces projection bias during the alignment process of radar and camera signals by compensating for sensor movement using measured roll and pitch angles from the inertial measurement unit (IMU). The latter module models target regions guided by each radar measurement as a bivariate Gaussian distribution, with its covariance matrix adaptively derived based on the distance between targets and the camera. Consequently, the association of radar points and images is robust to projection errors and works well for multiscale objects. Features of radar points and images are subsequently extracted with two parallel backbones and fused at different levels to provide sufficient semantic information for robust object detection. The proposed framework achieves an average precision (AP) of 0.501 on the challenging real-world dataset established by us, outperforming state-of-the-art vision-only and radar-vision fusion methods.  © 2001-2012 IEEE.",Multimodal data association; object detection; radar-vision fusion; unmanned surface vehicles (USVs); Cameras; Covariance matrix; Modal analysis; Object recognition; Radar equipment; Radar measurement; Semantics; Sensor data fusion; Tracking radar; Data association; Multi-modal data; Multi-modal data association; Objects detection; Probabilistics; Radar-vision fusion; Robust distance; Surrounding environment; Targets detection; Water surface; Object detection
Scopus,"Li, M.; Zhang, J.; Li, W.; Yin, T.; Chen, W.; Du, L.; Yan, X.; Liu, H.",Improved Taillight Detection Model for Intelligent Vehicle Lane-Change Decision-Making Based on YOLOv8,,2024,,,,10.3390/wevj15080369,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202352249&doi=10.3390%2fwevj15080369&partnerID=40&md5=91281501c7a3a53fcb80686a00cfaa37,"With the rapid advancement of autonomous driving technology, the recognition of vehicle lane-changing can provide effective environmental parameters for vehicle motion planning, decision-making and control, and has become a key task for intelligent vehicles. In this paper, an improved method for vehicle taillight detection and intent recognition based on YOLOv8 (You Only Look Once version 8) is proposed. Firstly, the CARAFE (Context-Aware Reassembly Operator) module is introduced to address fine perception issues of small targets, enhancing taillight detection accuracy. Secondly, the TriAtt (Triplet Attention Mechanism) module is employed to improve the model’s focus on key features, particularly in the identification of positive samples, thereby increasing model robustness. Finally, by optimizing the EfficientP2Head (a small object auxiliary head based on depth-wise separable convolutions) module, the detection capability for small targets is further strengthened while maintaining the model’s practicality and lightweight characteristics. Upon evaluation, the enhanced algorithm demonstrates impressive results, achieving a precision rate of 93.27%, a recall rate of 79.86%, and a mean average precision (mAP) of 85.48%, which shows that the proposed method could effectively achieve taillight detection. © 2024 by the authors.",deep learning; intelligent vehicle; lane-changing recognition; taillight detection; Deep learning; Autonomous driving; Decisions makings; Deep learning; Detection models; Environmental parameter; Lane change; Lane changing; Lane-changing recognition; Small targets; Taillight detection; Motion planning
Scopus,"Usman, M.; Zaka-Ud-Din, M.; Ling, Q.",Enhanced encoder–decoder architecture for visual perception multitasking of autonomous driving,,2024,,,,10.1016/j.eswa.2024.123249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183467552&doi=10.1016%2fj.eswa.2024.123249&partnerID=40&md5=2e3cace897dfd2a47fb033f80cf4bfc7,"Visual perception plays a vital role in autonomous driving systems, demanding high accuracy and real-time inference speed to ensure safety. In this paper, we propose a multi-task framework that simultaneously performs object detection, drivable area segmentation, and lane line identification, addressing the requirements of accurate and efficient visual perception. Our approach utilizes a shared-encoder architecture with three separate decoders, targeting each specific task. We investigate three configurations for the shared encoder: a Convolutional Neural Network (CNN), a Polyp Vision Transformer (PVT), and a hybrid CNN+PVT model. Through extensive experimentation and comparative analysis on the challenging BD100K dataset, we evaluate the performance of these shared-encoder models and provide valuable insights into their strengths and weaknesses. Our research contributes to the advancement of multi-task visual perception for autonomous driving systems by achieving competitive results in terms of accuracy and efficiency. The source code is publicly available on GitHub to facilitate further research in this domain. © 2024 Elsevier Ltd",Drivable area segmentation; Lane line detection; Multi-task learning; Traffic object detection; Visual perception; Autonomous vehicles; Convolutional neural networks; Decoding; Learning systems; Network architecture; Object recognition; Real time systems; Signal encoding; Vision; Autonomous driving; Drivable area segmentation; Driving systems; Lane line detection; Line detection; Multitask learning; Objects detection; Traffic object detection; Traffic objects; Visual perception; Object detection
Scopus,"Ywet, N.L.; Maw, A.A.; Nguyen, T.A.; Lee, J.-W.",YOLOTransfer-DT: An Operational Digital Twin Framework with Deep and Transfer Learning for Collision Detection and Situation Awareness in Urban Aerial Mobility,,2024,,,,10.3390/aerospace11030179,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188712186&doi=10.3390%2faerospace11030179&partnerID=40&md5=218b9efc7fb723555c652df8caaf1bfc,"Urban Air Mobility (UAM) emerges as a transformative approach to address urban congestion and pollution, offering efficient and sustainable transportation for people and goods. Central to UAM is the Operational Digital Twin (ODT), which plays a crucial role in real-time management of air traffic, enhancing safety and efficiency. This study introduces a YOLOTransfer-DT framework specifically designed for Artificial Intelligence (AI) training in simulated environments, focusing on its utility for experiential learning in realistic scenarios. The framework’s objective is to augment AI training, particularly in developing an object detection system that employs visual tasks for proactive conflict identification and mission support, leveraging deep and transfer learning techniques. The proposed methodology combines real-time detection, transfer learning, and a novel mix-up process for environmental data extraction, tested rigorously in realistic simulations. Findings validate the use of existing deep learning models for real-time object recognition in similar conditions. This research underscores the value of the ODT framework in bridging the gap between virtual and actual environments, highlighting the safety and cost-effectiveness of virtual testing. This adaptable framework facilitates extensive experimentation and training, demonstrating its potential as a foundation for advanced detection techniques in UAM. © 2024 by the authors.",collision detection; deep learning; operational digital twin; situation awareness; transfer learning; urbanair mobility
Scopus,"Saesaria, S.S.; Trilaksono, B.R.; Hidayat, E.M.I.",YOLOv5-GT: A Balanced Improvement in Object Detection Speed and Accuracy for Autonomous Vehicles in Indonesian Mixed Traffic,,2024,,,,10.1109/ICSET63729.2024.10774910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215109594&doi=10.1109%2fICSET63729.2024.10774910&partnerID=40&md5=3241bbcbc1659121a1ef5f2bc851bad2,"Object detection speed and accuracy are critical aspects of the perception system in autonomous vehicles. Speed improvement helps the object detector model achieve performance close to real-time, while accuracy enhancement ensures robust detection across various scenes. Balancing these improvements enhances the safety of autonomous vehicles, particularly in mixed and dense traffic conditions, such as those in Indonesia. This study develops a detection model using local datasets to reflect real-world conditions. It achieved balanced improvements in frames per second (fps) and mean Average Precision (mAP@50-95) through a modified YOLOv5 deep learning model. The key enhancements of the model include the integration of GhostConv and Transformer layers into the YOLOv5 architecture, referred to as YOLOv5s-GT, as well as the application of image augmentations with various scenarios to the training data. Experimental results show that the modified YOLOv5 outperformed the baseline model in both fps and mAP metrics, achieving 82.6 fps and 80.1% mAP@50-95, compared to the baseline performance of 75.8 fps and 77.7% mAP@50-95.  © 2024 IEEE.",autonomous vehicle; deep learning; ghost convolution; Object detection; transformer; YOLOv5; Vehicle detection; Autonomous Vehicles; Deep learning; Detection accuracy; Detection speed; Frames per seconds; Ghost convolution; Mixed traffic; Objects detection; Transformer; YOLOv5; Autonomous vehicles
Scopus,"Zhu, Q.; Fan, L.; Weng, N.",Advancements in point cloud data augmentation for deep learning: A survey,,2024,,,,10.1016/j.patcog.2024.110532,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192682416&doi=10.1016%2fj.patcog.2024.110532&partnerID=40&md5=13da906c107153e1b57ae5e42352b8b1,"Deep learning (DL) has become one of the mainstream and effective methods for point cloud analysis tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys these methods, categorizing them into a taxonomy framework that comprises basic and specialized point cloud data augmentation methods. Through a comprehensive evaluation of these augmentation methods, this article identifies their potentials and limitations, serving as a useful reference for choosing appropriate augmentation methods. In addition, potential directions for future research are recommended. This survey contributes to providing a holistic overview of the current state of point cloud data augmentation, promoting its wider application and development. © 2024 Elsevier Ltd",Augmentation; Classification; Deep learning; Detection; Point cloud; Segmentation; Learning systems; Augmentation; Augmentation methods; Cloud analysis; Data augmentation; Deep learning; Detection; Overfitting; Point cloud data; Point-clouds; Segmentation; Deep learning
Scopus,"Kumar, L.A.; Angalaeswari, S.; Mohana Sundaram, K.; Bansal, R.C.; Patil, A.",Intelligent solutions for sustainable power grids,,2024,,,,10.4018/979-8-3693-3735-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195015694&doi=10.4018%2f979-8-3693-3735-6&partnerID=40&md5=2c2e160aa007f46e7633496ec4b0e98a,"In the environment of energy systems, the effective utilization of both conventional and renewable sources poses a major challenge. The integration of microgrid systems, crucial for harnessing energy from distributed sources, demands intricate solutions due to the inherent intermittency of these sources. Academic scholars engaged in power system research find themselves at the forefront of addressing issues such as energy source estimation, coordination in dynamic environments, and the effective utilization of artificial intelligence (AI) techniques. Power systems grapple with the intermittent nature of renewable energy sources, necessitating advanced forecasting techniques and effective energy management. The coordination among distributed elements, smooth power transfer, and the optimization of power systems remain persistent challenges. Additionally, the competitive nature of distributed networks, coupled with the need for economic considerations, poses hurdles for young researchers entering the field. There is a pressing need for comprehensive insights into these challenges, coupled with practical solutions that leverage emerging technologies. Intelligent Solutions for Sustainable Power Grids focuses on emerging research areas, this book addresses the uncertainty of renewable energy sources, employs state-of-the-art forecasting techniques, and explores the application of AI techniques for enhanced power system operations. From economic aspects to the digitalization of power systems, the book provides a holistic approach. Tailored for undergraduate and postgraduate students as well as seasoned researchers, it offers a roadmap to navigate the intricate landscape of modern power systems. Dive into a wealth of knowledge encompassing smart energy systems, renewable energy integration, stability analysis of microgrids, power quality enhancement, and much more. This book is not just a guide; it is the solution to the pressing challenges in the dynamic field of energy systems. © 2024 by IGI Global. All rights reserved.",
Scopus,"Zhong, J.; Cheng, Q.; Hu, X.; Liu, Z.",YOLO Adaptive Developments in Complex Natural Environments for Tiny Object Detection,,2024,,,,10.3390/electronics13132525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198381331&doi=10.3390%2felectronics13132525&partnerID=40&md5=9434ea902449b1bb646b22c7599198df,"Detection of tiny object in complex environments is a matter of urgency, not only because of the high real-world demand, but also the high deployment and real-time requirements. Although many current single-stage algorithms have good detection performance under low computing power requirements, there are still significant challenges such as distinguishing the background from object features and extracting small-scale target features in complex natural environments. To address this, we first created real datasets based on natural environments and improved dataset diversity using a combination of copy–paste enhancement and multiple image enhancement techniques. As for the choice of network, we chose YOLOV5s due to its nature of fewer parameters and easier deployment in the same class of models. Most improvement strategies to boost detection performance claim to improve the performance of privilege extraction and recognition. However, we prefer to consider the combination of realistic deployment feasibility and detection performance. Therefore, based on the hottest improvement methods of YOLOV5s, we try to make adaptive improvements in three aspects, namely attention mechanism, head network, and backbone network. The experimental results proved that the decoupled head and Slimneck based improvements achieved, respectively, 0.872 and 0.849, 0.538 and 0.479, 87.5% and 89.8% on the mAP0.5, mAP0.5:0.95, and Precision metrics, surpassing the results of the baseline model on these three metrics: 0.705, 0.405 and 83.6%. This result suggests that the adaptively improved model can better meet routine testing needs without significantly increasing the number of parameters. These models perform well on our custom dataset and are also effective on images that are difficult to detect by naked eye. Meanwhile, we find that YOLOV8s, which also has the decoupled head improvement, has the results of 0.743, 0.461, and 87.17% on these three metrics. It proves that under our dataset, it is possible to achieve more advanced results with lower number of model parameters just by adding decoupled head. And according to the results, we also discuss and analyze some improvements that are not adapted to our dataset, which also provides ideas for researchers in similar scenarios: in the booming development of object detection, choosing the suitable model and adapting to combine with other technologies would help to provide solutions to real-world problems. © 2024 by the authors.",backbone network; complex natural environment; copy–paste; head network; object detection; self-attention mechanism; YOLOV5s
Scopus,"Xiong, X.; Meng, A.; Lu, J.; Tan, Y.; Chen, B.; Tang, J.; Zhang, C.; Xiao, S.; Hu, J.",Automatic detection and location of pavement internal distresses from ground penetrating radar images based on deep learning,,2024,,,,10.1016/j.conbuildmat.2023.134483,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180409059&doi=10.1016%2fj.conbuildmat.2023.134483&partnerID=40&md5=80cf341c7510d913e7274f80995d92a9,"Ground penetrating radar (GPR) has been prevailingly applied in nondestructive testing of internal distresses within asphalt pavement. However, the interpretation of abnormal echo signals in massive pavement GPR images is time-consuming and labor-intensive, and prone to misjudgment. To address this issue, a three-step method was proposed for the automatic detection and location of the internal distresses (eg: crack and debonding) echo features from GPR images. The on-site and numerical simulated GPR images of the asphalt pavement together formed the dataset required for the subsequent deep learning models. The first step is that You Only Look Once version 3 (YOLOv3) model predicted rectangular boxes for enclosing the internal distress echo features from GPR images. The second step involves developing the U-net models to segment the internal distress echo feature pixels in the cropped rectangular boxes. The last step is that the median points of the segmentation of the internal distress echo feature were fitted with a theoretical curve equation, to estimate the location of the internal distress. The proposed method has shown that the comprehensive detection accuracy of the internal distress echo features can reach 96.99%, the semantic segmentation accuracies of the internal distress echo feature pixels are not less than 0.856, and the average deviation of the estimated depths of the internal distresses is 3.25 cm. The research method makes further advances in accurately and automatically detecting and locating the internal distresses of asphalt pavement. © 2023 Elsevier Ltd",Asphalt pavement; Debonding; Ground penetrating radar; Internal crack; U-net model; YOLOv3 model; Asphalt; Asphalt pavements; Feature extraction; Geological surveys; Ground penetrating radar systems; Location; Nondestructive examination; Pixels; Radar imaging; Seepage; Semantics; Automatic Detection; Automatic location; Echo features; Ground Penetrating Radar; Image-based; Internal crack; Net model; Rectangular box; U-net model; You only look once version 3 model; Deep learning
Scopus,"Li, J.; Sun, H.; Zhang, Z.",A Multi-Scale-Enhanced YOLO-V5 Model for Detecting Small Objects in Remote Sensing Image Information,,2024,,,,10.3390/s24134347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198347216&doi=10.3390%2fs24134347&partnerID=40&md5=08dcf2cd072fc06eff9f62a4f191cdc8,"As a typical component of remote sensing signals, remote sensing image (RSI) information plays a strong role in showing macro, dynamic and accurate information on the earth’s surface and environment, which is critical to many application fields. One of the core technologies is the object detection (OD) of RSI signals (RSISs). The majority of existing OD algorithms only consider medium and large objects, regardless of small-object detection, resulting in an unsatisfactory performance in detection precision and the miss rate of small objects. To boost the overall OD performance of RSISs, an improved detection framework, I-YOLO-V5, was proposed for OD in high-altitude RSISs. Firstly, the idea of a residual network is employed to construct a new residual unit to achieve the purpose of improving the network feature extraction. Then, to avoid the gradient fading of the network, densely connected networks are integrated into the structure of the algorithm. Meanwhile, a fourth detection layer is employed in the algorithm structure in order to reduce the deficiency of small-object detection in RSISs in complex environments, and its effectiveness is verified. The experimental results confirm that, compared with existing advanced OD algorithms, the average accuracy of the proposed I-YOLO-V5 is improved by 15.4%, and the miss rate is reduced by 46.8% on the RSOD dataset. © 2024 by the authors.",densely connected network; residual network; RSI information; small-object detection; YOLO network; Image enhancement; Object recognition; Remote sensing; Densely connected networks; Image information; Object detection algorithms; Remote sensing image information; Remote sensing images; Residual network; Small object detection; Small objects; YOLO network; algorithm; altitude; article; diagnosis; feature extraction; human; remote sensing; Object detection
Scopus,"Narkhede, M.; Chopade, N.",CycleInSight: An enhanced YOLO approach for vulnerable cyclist detection in urban environments,,2024,,,,10.11591/ijece.v14i4.pp3986-3994,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195064937&doi=10.11591%2fijece.v14i4.pp3986-3994&partnerID=40&md5=7a6db02383feb111c080b55230e2e63e,"As urbanization continues to reshape transportation, the safety of cyclists in complex traffic environments has become a pressing concern. In response to this challenge, our research introduces a CycleInSight framework, which harnesses advanced deep learning and computer vision techniques to enable precise and efficient cyclist detection in diverse urban settings. Utilizing you only look once version 8 (YOLOv8) object detection algorithm, the proposed model aims to detect and localize vulnerable cyclists near vehicles equipped with onboard cameras. Our research presents comprehensive experimental results demonstrating its effectiveness in identifying vulnerable cyclists amidst dynamic and challenging traffic conditions. With an impressive average precision of 90.91%, our approach outperforms existing models while maintaining efficient inference speeds. By effectively identifying and tracking cyclists, this framework holds significant potential to enhance urban traffic safety, inform data-driven infrastructure planning, and support the development of advanced driver assistance systems and autonomous vehicles. © 2024 Institute of Advanced Engineering and Science. All rights reserved.",Advanced driver assistance systems Autonomous vehicles Computer vision Deep learning Object detection Vulnerable cyclist YOLO
Scopus,"Hong, K.; Wu, M.; Gao, B.; Feng, Y.",A Grading Identification Method for Tea Buds Based on Improved YOLOv7-tiny,,2024,,,,10.13305/j.cnki.jts.2024.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192723299&doi=10.13305%2fj.cnki.jts.2024.01.006&partnerID=40&md5=548f22d3b890cbcc80e5adbd6cff96b3,"The intelligent grading and recognition of tea buds in a natural environment are fundamental for the automation of premium tea harvesting. To address the problems of low recognition accuracy and limited robustness caused by complex environmental factors like lighting, obstruction, and dense foliage, we propose an enhanced model based on YOLOv7-tiny. Firstly, a CBAM module was added into the small object detection layer of the YOLOv7-tiny model to enhance the model's ability to focus on small object features and reduce the interference of complex environments on tea bud recognition. We adjusted the spatial pyramid pooling structure to lower computational costs and improve detection speed. Additionally, we utilized a loss function combining IoU and NWD to further enhance the model's robustness in small object detection by addressing the sensitivity of the IoU mechanism to position deviations. Experimental results demonstrate that the proposed model achieves a detection accuracy of 91.15%, a recall rate of 88.54%, and a mean average precision of 92.66%. The model's size is 12.4 MB. Compared to the original model, this represents an improvement of 2.83%, 2.00%, and 1.47% in accuracy, recall rate, and mean average precision, respectively, with a significant increase of 0.1 MB in model size. Comparative experiments with different models show that our model exhibits fewer false negatives and false positives in multiple scenarios, along with higher confidence scores. The improved model can be applied to the bud grading and recognition process of premium tea harvesting robots. © 2024 Editorial Office of Journal of Tea science. All rights reserved.",attention mechanisms; grading identification; NWD loss; tea bud; YOLOv7-tiny
Scopus,"Casas, E.; Ramos, L.; Romero, C.; Rivas-Echeverría, F.",A comparative study of YOLOv5 and YOLOv8 for corrosion segmentation tasks in metal surfaces,,2024,,,,10.1016/j.array.2024.100351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195286396&doi=10.1016%2fj.array.2024.100351&partnerID=40&md5=6af8d8c6e9bf31482dadac6e53d8ac50,"This study delves into the comparative efficacy of YOLOv5 and YOLOv8 in corrosion segmentation tasks. We employed three unique datasets, comprising 4942, 5501, and 6136 images, aiming to thoroughly evaluate the models’ adaptability and robustness in diverse scenarios. The assessment metrics included precision, recall, F1-score, and mean average precision. Furthermore, graphical tests offered a visual perspective on the segmentation capabilities of each architecture. Our results highlight YOLOv8’s superior speed and segmentation accuracy across datasets, further corroborated by graphical evaluations. These visual assessments were instrumental in emphasizing YOLOv8’s proficiency in handling complex corroded surfaces. However, in the largest dataset, both models encountered challenges, particularly with overlapping bounding boxes. YOLOv5 notably lagged, struggling to achieve the performance standards set by YOLOv8, especially with irregular corroded surfaces. In conclusion, our findings underscore YOLOv8’s enhanced capabilities, establishing it as a preferable choice for real-world corrosion detection tasks. This research thus offers invaluable insights, poised to redefine corrosion management strategies and guide future explorations in corrosion identification. © 2024 The Author(s)",Computer vision; Corrosion; Deep learning; Image segmentation; Instance segmentation; YOLO; Computer vision; Corrosion; Deep learning; Assessment metric; Comparatives studies; Corroded surface; Deep learning; F1 scores; Graphical test; Images segmentations; Instance segmentation; Metal surfaces; YOLO; Image segmentation
Scopus,"Duan, Z.; Shao, J.; Zhang, M.; Zhang, J.; Zhai, Z.",A Small-Object-Detection Algorithm Based on LiDAR Point-Cloud Clustering for Autonomous Vehicles,,2024,,,,10.3390/s24165423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202447200&doi=10.3390%2fs24165423&partnerID=40&md5=68704bf423d1953ab1dba42d29cc3f03,"3D object-detection based on LiDAR point clouds can help driverless vehicles detect obstacles. However, the existing point-cloud-based object-detection methods are generally ineffective in detecting small objects such as pedestrians and cyclists. Therefore, a small-object-detection algorithm based on clustering is proposed. Firstly, a new segmented ground-point clouds segmentation algorithm is proposed, which filters out the object point clouds according to the heuristic rules and realizes the ground segmentation by multi-region plane-fitting. Then, the small-object point cloud is clustered using an improved DBSCAN clustering algorithm. The K-means++ algorithm for pre-clustering is used, the neighborhood radius is adaptively adjusted according to the distance, and the core point search method of the original algorithm is improved. Finally, the detection of small objects is completed using the directional wraparound box model. After extensive experiments, it was shown that the precision and recall of our proposed ground-segmentation algorithm reached 91.86% and 92.70%, respectively, and the improved DBSCAN clustering algorithm improved the recall of pedestrians and cyclists by 15.89% and 9.50%, respectively. In addition, visualization experiments confirmed that our proposed small-object-detection algorithm based on the point-cloud clustering method can realize the accurate detection of small objects. © 2024 by the authors.",autonomous driving; ground segmentation; LiDAR; point cloud clustering; small object detection; Heuristic algorithms; Autonomous driving; Clusterings; Ground segmentation; LiDAR; Object detection algorithms; Point cloud clustering; Point-clouds; Segmentation algorithms; Small object detection; Small objects; algorithm; article; autonomous vehicle; cloud computing; clustering algorithm; controlled study; cyclist; detection algorithm; diagnosis; filter; k means clustering; neighborhood; pedestrian; segmentation algorithm; Bicycles
Scopus,"Sha, X.; Guo, Z.; Guan, Z.; Li, W.; Wang, S.; Zhao, Y.",PBTA: Partial Break Triplet Attention Model for Small Pedestrian Detection Based on Vehicle Camera Sensors,,2024,,,,10.1109/JSEN.2024.3398031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193294357&doi=10.1109%2fJSEN.2024.3398031&partnerID=40&md5=1e918e99a16e6adf086af9599e3584e0,"Successfully detecting small pedestrians through vehicle camera sensors would greatly facilitate the development of autonomous driving safety applications. However, the existing pedestrian detection models applied to vehicle camera applications were limited by the scale confusion problem and the weak feature problem of small pedestrian targets. To resolve these issues, this study proposed a partial break triplet attention (PBTA) network composed of two components: the partial break bidirectional feature pyramid network (PBFPN) and the TR-NCSPDarknet53. PBFPN was used to solve the scale confusion problem in shallow feature maps by employing a partial break operation and a branch fusion operation. In TR-NCSPDarknet53, the Ta-conv module was proposed to solve the weak feature problem. The PBTA network provided a new improvement idea for small pedestrian detection. It greatly improved the accuracy while keeping the parameters at a low level, which is vital for safety applications in autonomous driving. Extensive experiments on CityPersons, Crowdhuman, and WiderPerson datasets including various traffic images from camera sensors demonstrate the accuracy of the PBTA network in small pedestrian detection. Compared with the baseline (Yolov8S ) network, the accuracy of small objects (APS) is improved by 50%.  © 2001-2012 IEEE.",Attention mechanism; autonomous vehicle (AV); camera sensors; feature fusion network; small pedestrian detection; Autonomous vehicles; Cameras; Feature extraction; Object detection; Object recognition; Pedestrian safety; Attention mechanisms; Autonomous vehicle; Autonomous Vehicles; Camera sensor; Feature fusion network; Features extraction; Features fusions; Objects detection; Pedestrian; Pedestrian detection; Small pedestrian detection; Semantics
Scopus,"Han, J.; Cao, R.; Brighente, A.; Conti, M.",Light-YOLOv5: A Lightweight Drone Detector for Resource-Constrained Cameras,,2024,,,,10.1109/JIOT.2023.3329221,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181824089&doi=10.1109%2fJIOT.2023.3329221&partnerID=40&md5=142711fbe6a5a800a8b4560654de0673,"Critical infrastructures (CIs), such as military bases and airports, are putting a lot of attention into defending against attacks delivered via drones by deploying drone detection systems. However, the CI area might be very large, with no-fly zones extending to regions where it might not be possible to deploy a power line for resourceful cameras. To this aim, the CI might deploy an Internet of Things (IoT)-based surveillance camera system to capture drone images. However, these IoT cameras are resource-constrained devices that cannot support the currently available detectors. In this article, we propose Light-YOLOv5, a lightweight image-based drone detector for resource-constrained cameras. We make targeted improvements to YOLOv5, including the replacement of the backbone network, the introduction of the transformer module, and the design of a parallel mixed efficient attention module (PEAM). We show that our modifications allow for reduced network size while achieving better classification than other state-of-the-art solutions. To prove these claims, we expanded an already available data set of blurred drone images by adding clear images of aircraft and birds. Since airplanes and birds are easily confused as drones by image classifiers, our addition proves the effectiveness of our solution. Experiments show that Light-YOLOv5 can achieve a very good tradeoff between performance (74.8% mAP) and efficiency (170 FPS). Compared to YOLOv5, Light-YOLOv5 improves mAP by 4.1%, reduces the number of network parameters by 15.7%, can perform detection at 170 frames per second (FPS), and achieves an average accuracy rate of 93.8%.  © 2014 IEEE.",Resource-constrained cameras; unmanned aerial vehicle (UAV) detection; YOLOv5; Aircraft detection; Birds; Cameras; Drones; Economic and social effects; Internet of things; Network security; Security systems; Tracking radar; Detection system; Features extraction; Frames per seconds; Military base; Power lines; Radar detection; Resource constrained camera; Surveillance cameras; UAV detection; YOLOv5; Feature extraction
Scopus,"Sapkota, R.; Ahmed, D.; Karkee, M.",Comparing YOLOv8 and Mask R-CNN for instance segmentation in complex orchard environments,,2024,,,,10.1016/j.aiia.2024.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198959678&doi=10.1016%2fj.aiia.2024.07.001&partnerID=40&md5=c2701883942d79236e41403d5328f36a,"Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively. These findings show YOLOv8's superior accuracy and efficiency in machine learning applications compared to two-stage models, specifically Mask-R-CNN, which suggests its suitability in developing smart and automated orchard operations, particularly when real-time applications are necessary in such cases as robotic harvesting and robotic immature green fruit thinning. © 2024 The Authors",Artificial intelligence; Automation; Deep learning; Machine learning; Machine vision; Mask R-CNN; Robotics; YOLOv8; Agricultural robots; Deep learning; Fruits; Image segmentation; Learning systems; Orchards; Automated tasks; Deep learning; Images processing; Individual objects; Machine-learning; Machine-vision; Mask R-CNN; Processing operations; Segmentation models; YOLOv8; Computer vision
Scopus,"Yao, C.; Liu, X.; Wang, J.; Cheng, Y.",Optimized Design of EdgeBoard Intelligent Vehicle Based on PP-YOLOE+,,2024,,,,10.3390/s24103180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194218234&doi=10.3390%2fs24103180&partnerID=40&md5=dcad72f2e38be7282f0200600cfdf6f3,"Advances in deep learning and computer vision have overcome many challenges inherent in the field of autonomous intelligent vehicles. To improve the detection accuracy and efficiency of EdgeBoard intelligent vehicles, we proposed an optimized design of EdgeBoard based on our PP-YOLOE+ model. This model innovatively introduces a composite backbone network, incorporating deep residual networks, feature pyramid networks, and RepResBlock structures to enrich environmental perception capabilities through the advanced analysis of sensor data. The incorporation of an efficient task-aligned head (ET-head) in the PP-YOLOE+ framework marks a pivotal innovation for precise interpretation of sensor information, addressing the interplay between classification and localization tasks with high effectiveness. Subsequent refinement of target regions by detection head units significantly sharpens the system’s ability to navigate and adapt to diverse driving scenarios. Our innovative hardware design, featuring a custom-designed mainboard and drive board, is specifically tailored to enhance the computational speed and data processing capabilities of intelligent vehicles. Furthermore, the optimization of our Pos-PID control algorithm allows the system to dynamically adjust to complex driving scenarios, significantly enhancing vehicle safety and reliability. Besides, our methodology leverages the latest technologies in edge computing and dynamic label assignment, enhancing intelligent vehicles’ operations through seamless sensor integration. Our custom dataset, specifically designed for this study, includes 4777 images captured by intelligent vehicles under a variety of environmental and lighting conditions. The dataset features diverse scenarios and objects pertinent to autonomous driving, such as pedestrian crossings and traffic signs, ensuring a comprehensive evaluation of the model’s performance. We conducted extensive testing of our model on this dataset to thoroughly assess sensor performance. Evaluated against metrics including accuracy, error rate, precision, recall, mean average precision (mAP), and F1-score, our findings reveal that the model achieves a remarkable accuracy rate of 99.113%, an mAP of 54.9%, and a real-time detection frame rate of 192 FPS, all within a compact parameter footprint of just 81 MB. These results demonstrate the superior capability of our PP-YOLOE+ model to integrate sensor data, achieving an optimal balance between detection accuracy and computational speed compared with existing algorithms. © 2024 by the authors.",autonomous sensing; EdgeBoard intelligent vehicle; optimal route determination; Pos-PID; PP-YOLOE+; target detection; Autonomous vehicles; Classification (of information); Data handling; Deep learning; Intelligent vehicle highway systems; Statistical tests; Traffic signs; Vehicle safety; Autonomous sensing; Detection accuracy; Edgeboard intelligent vehicle; Optimal route determination; Optimal routes; Optimized designs; Pos-PID; PP-YOLOE+; Sensors data; Targets detection; algorithm; article; benchmarking; computer vision; controlled study; data processing; deep learning; human; illumination; pedestrian; proportional integral derivative algorithm; reliability; sensor; vehicle safety; velocity; Digital storage
Scopus,"Peng, C.; He, B.; Xi, W.; Lin, G.",Improved YOLOv7 Algorithm for Floating Waste Detection Based on GFPN and Long-Range Attention Mechanism,,2024,,,,10.1051/wujns/2024294338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005605759&doi=10.1051%2fwujns%2f2024294338&partnerID=40&md5=892deb3935a7be6d6f95f59681fdb97c,"Floating wastes in rivers have specific characteristics such as small scale, low pixel density and complex backgrounds. These characteristics make it prone to false and missed detection during image analysis, thus resulting in a degradation of detection performance. In order to tackle these challenges, a floating waste detection algorithm based on YOLOv7 is proposed, which combines the improved GFPN (Generalized Feature Pyramid Network) and a long-range attention mechanism. Firstly, we import the improved GFPN to replace the Neck of YOLOv7, thus providing more effective information transmission that can scale into deeper networks. Secondly, the convolution-based and hardware-friendly long-range attention mechanism is introduced, allowing the algorithm to rapidly generate an attention map with a global receptive field. Finally, the algorithm adopts the WiseIoU optimization loss function to achieve adaptive gradient gain allocation and alleviate the negative impact of low-quality samples on the gradient. The simulation results reveal that the proposed algorithm has achieved a favorable average accuracy of 86.3% in real-time scene detection tasks. This marks a significant enhancement of approximately 6.3% compared with the baseline, indicating the algorithm’s good performance in floating waste detection. © Wuhan University 2024.",floating waste detection; GFPN (Generalized Feature Pyramid Network); long-range attention; YOLOv7
Scopus,"Wang, C.; Cai, X.; Li, Y.; Zhai, R.; Wu, R.; Zhu, S.; Guan, L.; Luo, Z.; Zhang, S.; Zhang, J.",Research and Application of Panoramic Visual Perception-Assisted Navigation Technology for Ships,,2024,,,,10.3390/jmse12071042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199885394&doi=10.3390%2fjmse12071042&partnerID=40&md5=de6a7d35d3fd3fe8b7c012a3d638e29a,"In response to challenges such as narrow visibility for ship navigators, limited field of view from a single camera, and complex maritime environments, this study proposes panoramic visual perception-assisted navigation technology. The approach includes introducing a region-of-interest search method based on SSIM and an elliptical weighted fusion method, culminating in the development of the ship panoramic visual stitching algorithm SSIM-EW. Additionally, the YOLOv8s model is improved by increasing the size of the detection head, introducing GhostNet, and replacing the regression loss function with the WIoU loss function, and a perception model yolov8-SGW for sea target detection is proposed. The experimental results demonstrate that the SSIM-EW algorithm achieves the highest PSNR indicator of 25.736, which can effectively reduce the stitching traces and significantly improve the stitching quality of panoramic images. Compared to the baseline model, the YOLOv8-SGW model shows improvements in the P, R, and mAP50 of 1.5%, 4.3%, and 2.3%, respectively, its mAP50 is significantly higher than that of other target detection models, and the detection ability of small targets at sea has been significantly improved. Implementing these algorithms in tugboat operations at ports enhances the fields of view of navigators, allowing for the identification of targets missed by AISs and radar systems, thus ensuring operational safety and advancing the level of vessel intelligence. © 2024 by the authors.",intelligent vessels; object detection; panoramic vision; visual perception
Scopus,"Lyu, Z.; An, W.","HDR-YOLO: Adaptive Object Detection in Haze, Dark, and Rain Scenes Based on YOLO",,2024,,,,10.1142/S021800142450006X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194904978&doi=10.1142%2fS021800142450006X&partnerID=40&md5=761487c6deb5d696a9e0a1c1f0a4b66f,"In the context of real-world environments, images acquired through surveillance cameras in such settings are frequently marred by issues including diminished contrast, suboptimal image quality, and color aberrations, rendering conventional object detection models ill-suited for the task. Taking inspiration from the foundational principles of image restoration, this study aims to extract environment-agnostic features across various weather conditions in order to enhance object detection performance in multiple scenarios while maintaining accuracy under typical meteorological conditions. In response to this question, we introduce a detection framework as HDR-YOLO that jointly trains feature extraction and object detection. Meantime, to solve the problem of visual impairments caused by adverse conditions, we propose a Dynamic Extraction of Environment-Agnostic Features (DEAF) module. Additionally, we joint mean squared error (MSE) loss and Log-Cosh loss as optimization techniques, carefully tailored to further elevate detection performance, especially under adverse meteorological conditions. Extensive empirical findings from the AGVS dataset validate the ability of HDR-YOLO to improve object detection performance in airport ground videos within real-world settings while maintaining precision under typical meteorological conditions, which underscores its innovative capabilities and adaptability in complex and diverse environments.  © 2024 World Scientific Publishing Company.",adaptive object detection; convolutional neural networks; Object detection; real-world object detection; Airport security; Convolutional neural networks; Extraction; Feature extraction; Image enhancement; Image reconstruction; Image segmentation; Mean square error; Object recognition; Security systems; Adaptive object detection; Condition; Convolutional neural network; Detection performance; Meteorological condition; Objects detection; Real-world object detection; Real-world objects; Object detection
Scopus,"Zhang, X.; Cao, X.; Zhang, H.; Shen, Y.; Yuan, X.; Cui, Z.; Lu, Z.",An Intelligent Obstacle Detection for Autonomous Mining Transportation With Electric Locomotive via Cellular Vehicle-to-Everything and Vehicular Edge Computing,,2024,,,,10.1109/TITS.2023.3324145,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176374982&doi=10.1109%2fTITS.2023.3324145&partnerID=40&md5=a2c4933ac6ce5da61b51baff0d180a65,"The tremendous revolutionary progress of cellular vehicle-to-everything (C-V2X) and vehicular edge computing (VEC) technologies provide new opportunities to overcome the autonomous transportation issue of the mining electric locomotives (MELs), in which the accurate and fast detection of obstacles is crucial for the safe operation. With the VEC and C-V2X, we proposed a new high-precision obstacle detection strategy for MELs (MEL-YOLO). Firstly, we investigated the convolutional attention mechanism integrated into the path aggregation network of the Neck layer to strengthen the feature extraction capabilities. Secondly, we added a small-object oriented prediction layer in the Head to form the multi-scale feature prediction. Thirdly, we introduced a more efficient loss function to alleviate the gradient explosion problem in the feature transfer. Finally, we utilized the K-means++ optimization to derive the anchor boxes matchable with the dataset, which was collected and created by featuring different scenes to train validate the model. The MEL-YOLO was compressed by BN layer pruning and implemented on the edge device in a 6G/B5G based-V2X environment. Experimental results verify that the MEL-YOLO can effectively detect obstacles and significantly improve detection accuracy for small obstacles, computationally increasing mAP by 3.3% to original model, while maintaining detection speed and model size nearly unchanged. © 2000-2011 IEEE.",6G-vehicle-to-everything; attention mechanism; edge computing; Mining electric locomotive; multi-scale feature prediction; obstacle detection; Edge computing; Engines; Feature extraction; Locomotives; Obstacle detectors; Vehicle to Everything; Vehicles; 6g-vehicle-to-everything; Attention mechanisms; Cellulars; Computing technology; Edge computing; Mining electric locomotive; Mining transportations; Multi-scale feature prediction; Multi-scale features; Obstacles detection; Forecasting
Scopus,"Gao, C.; Zhao, F.; Zhang, Y.; Wan, M.",Research on multitask model of object detection and road segmentation in unstructured road scenes,,2024,,,,10.1088/1361-6501/ad35dd,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189330824&doi=10.1088%2f1361-6501%2fad35dd&partnerID=40&md5=2c1d49b9c3b9f29a0b6ab0ed6c31eba3,"With the rapid development of artificial intelligence and computer vision technology, autonomous driving technology has become a hot area of concern. The driving scenarios of autonomous vehicles can be divided into structured scenarios and unstructured scenarios. Compared with structured scenes, unstructured road scenes lack the constraints of lane lines and traffic rules, and the safety awareness of traffic participants is weaker. Therefore, there are new and higher requirements for the environment perception tasks of autonomous vehicles in unstructured road scenes. The current research rarely integrates the target detection and road segmentation to achieve the simultaneous processing of target detection and road segmentation of autonomous vehicle in unstructured road scenes. Aiming at the above issues, a multitask model for object detection and road segmentation in unstructured road scenes is proposed. Through the sharing and fusion of the object detection model and road segmentation model, multitask model can complete the tasks of multi-object detection and road segmentation in unstructured road scenes while inputting a picture. Firstly, MobileNetV2 is used to replace the backbone network of YOLOv5, and multi-scale feature fusion is used to realize the information exchange layer between different features. Subsequently, a road segmentation model was designed based on the DeepLabV3+ algorithm. Its main feature is that it uses MobileNetV2 as the backbone network and combines the binary classification focus loss function for network optimization. Then, we fused the object detection algorithm and road segmentation algorithm based on the shared MobileNetV2 network to obtain a multitask model and trained it on both the public dataset and the self-built dataset NJFU. The training results demonstrate that the multitask model significantly enhances the algorithm’s execution speed by approximately 10 frames per scond while maintaining the accuracy of object detection and road segmentation. Finally, we conducted validation of the multitask model on an actual vehicle. © 2024 IOP Publishing Ltd.",autonomous vehicles; multitask model; object detection; road segmentation; unstructured road scenes; Autonomous vehicles; Object recognition; Roads and streets; Autonomous driving; Autonomous Vehicles; Back-bone network; Computer vision technology; Multi-task model; Objects detection; Road segmentation; Segmentation models; Targets detection; Unstructured road scene; Object detection
Scopus,"Vellaidurai, A.; Rathinam, M.",A novel OYOLOV5 model for vehicle detection and classification in adverse weather conditions,,2024,,,,10.1007/s11042-023-16450-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168143115&doi=10.1007%2fs11042-023-16450-2&partnerID=40&md5=f552fdb04c680be2e0c0aceac7c29eb2,"An autonomous vehicle must accurately detect its surrounding environment to operate reliably. Adverse weather conditions (ADWC) are snow, rain, sand, and haze, badly affect the quality of vehicle detection (VD) in an autonomous environment. Most existing techniques focused on VD under various weather effects such as signal control, travel pattern, traffic volume variations and collision risk. Only a limited number of works of literature were focused on VD under ADWC at different automation scales. In this paper, a novel deep learning (DL) model, Optimized You Look Only Once Version 5 (OYOLOV5), is proposed for autonomous VD (AVD) in ADWC. The proposed model consists of three phases: data collection, data preprocessing, feature extraction and classification. Initially, the data is collected from the DAWN and COCO dataset to perform VD, which is openly available. The augmentation of the data is carried out on the collected input data by including hue, saturation, blur, brightness, and noise, which helps to get a clear view of vehicles. After data augmentation, feature extraction and classification of the preprocessed images are done using the OYOLOV5 framework, which uses Resnet-50 as the backbone network and Feature Pyramid Network (FPN) for detecting the vehicles at multi-scales. Experiments are conducted, and the outcomes demonstrated the proposed OYOLOV5 model achieves better performance with the state-of-art methods in terms of precision (PRC), recall (RC), f-measure (FMS), accuracy (ACU), average IoU (AI), processing speed (PS), and training time (TTI). Also, the system attains good mean average precision (mAP) than the conventional methods. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",Autonomous vehicle; Detection in adverse weather nature; Feature pyramid network; Fuzzy C-means; Intersection over Union; Residual Network-50; Classification (of information); Data acquisition; Deep learning; Extraction; Feature extraction; Meteorology; Adverse weather; Autonomous Vehicles; C-means; Detection in adverse weather nature; Feature pyramid; Feature pyramid network; Fuzzy C-mean; Intersection over union; Pyramid network; Residual network-50; Autonomous vehicles
Scopus,"Zhang, X.; Lu, X.; Zhang, Z.; Yang, G.; He, Y.; Fang, H.",Simultaneous detection of reference lines in paddy fields using a machine vision-based framework,,2024,,,,10.1016/j.compag.2024.108923,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190734091&doi=10.1016%2fj.compag.2024.108923&partnerID=40&md5=d944cb81f7bdf49f23463e5f6b89def1,"Accurate and robust detection of reference lines in the field is essential for formulating linear tracking and steering strategies for agricultural machinery. The identification of planted and ridge areas poses challenges due to their similar appearance to unplanted areas, along with complex field conditions such as uneven illumination. In this study, we proposed a machine vision-based framework for the simultaneous detection of auxiliary navigation lines and ridge boundary lines. Firstly, we constructed a multi-area Paddy Area Segmentation dataset named PASeg, which contained ridge areas, planted areas, and unplanted areas. Additionally, a deep learning network called G-STDC that integrated the Ghost module into the STDC network was developed for efficient and robust area segmentation. Finally, a multi-line detection method was applied based on a central axis-based point clustering algorithm and random sample consensus (RANSAC) algorithm to extract reference lines. According to the results on PASeg, the proposed G-STDC model obtained a mean intersection over union (mIoU) of 95.23 %, outperforming the baseline model (with the mIoU of 93.18 %). The attitude error and distance error of line extraction on 640 × 512 resolution images were within 0.776° and 4.687 pixels, respectively. The overall detection speed reached 13.9 frames per second (FPS), while the faster G-STDC model (G-STDC-t) achieved 16.7 FPS. The proposed method could provide real-time reference lines for turning path planning and automatic navigation in agro-machinery. © 2024 Elsevier B.V.",Deep learning; Reference line detection; Semantic segmentation; Visual navigation; Agricultural robots; Clustering algorithms; Deep learning; Motion planning; Navigation; Semantic Segmentation; Semantics; Deep learning; Frames per seconds; Line detection; Machine-vision; Reference line detection; Reference lines; Semantic segmentation; Simultaneous detection; Vision based; Visual Navigation; computer vision; detection method; machine learning; machinery; navigation; paddy field; segmentation; tracking; Computer vision
Scopus,"Borts, D.; Liang, E.; Broedermann, T.; Ramazzina, A.; Walz, S.; Palladin, E.; Sun, J.; Brueggemann, D.; Sakaridis, C.; Van Gool, L.; Bijelic, M.; Heide, F.",Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar,,2024,,,,10.1145/3641519.3657510,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199888938&doi=10.1145%2f3641519.3657510&partnerID=40&md5=fdb9e900ffa141e80ed7de6006f732a3,"Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle. While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored. Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques. Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors. We introduce Radar Fields - a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy. The proposed method does not rely on volume rendering. Instead, we learn fields in Fourier frequency space, supervised with raw radar data. We validate our method's effectiveness across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and harsh weather scenarios, where mm-wavelength sensing is favorable.  © 2024 Owner/Author.",neural rendering.; radar
Scopus,"Cai, L.; Zhang, B.; Li, Y.; Chai, H.",IFE-net: improved feature enhancement network for weak feature target recognition in autonomous underwater vehicles,,2024,,,,10.1017/S0263574724000195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184579632&doi=10.1017%2fS0263574724000195&partnerID=40&md5=007369f1affe8a3bd417d6b6e283e30d,"The recognizing underwater targets is a crucial component of autonomous underwater vehicle patrols and detection efforts. In the process of visual image recognition in real underwater environment, the spatial and semantic features of the target often appear to different degrees of loss, and the scarcity of specific types of underwater samples leads to unbalanced data on categories. This kind of problem makes the target features appear weak and seriously affects the accuracy of underwater target recognition. Traditional deep learning methods based on data and feature enhancement cannot achieve ideal recognition effect. Based on the above difficulties, this paper proposes an improved feature enhancement network for weak feature target recognition. Firstly, a multi-scale spatial and semantic feature enhancement module is constructed to extract the feature information of the extraction target accurately. Secondly, this paper solves the influence of target feature distortion on classification through multi-scale feature comparison of positive and negative samples. Finally, the Rank & Sort Loss function was used to train the depth target detection to solve the problem of recognition accuracy under highly unbalanced sample data. Experimental results show that the recognition accuracy of the proposed method is 2.28% and 3.84% higher than that of the existing algorithms in the recognition of underwater fuzzy and distorted target images, which demonstrates the effectiveness and superiority of the proposed method. © The Author(s), 2024. Published by Cambridge University Press.",multi-scale feature comparison; ranking loss; spatial and semantic feature enhancement; unbalanced category data; underwater target recognition; Autonomous vehicles; Deep learning; Image recognition; Learning systems; Semantics; Feature enhancement; Multi-scale feature comparison; Multi-scale features; Ranking loss; Semantic features; Spatial and semantic feature enhancement; Spatial features; Target recognition; Unbalanced category data; Underwater target recognition; Autonomous underwater vehicles
Scopus,"Wang, L.; Lan, J.; Li, M.",PAFNet: Pillar Attention Fusion Network for Vehicle–Infrastructure Cooperative Target Detection Using LiDAR,,2024,,,,10.3390/sym16040401,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191591674&doi=10.3390%2fsym16040401&partnerID=40&md5=a7491e9d798c438d4682d1a2ca08fc10,"With the development of autonomous driving, consensus is gradually forming around vehicle–infrastructure cooperative (VIC) autonomous driving. The VIC environment-sensing system uses roadside sensors in collaboration with automotive sensors to capture traffic target information symmetrically from both the roadside and the vehicle, thus extending the perception capabilities of autonomous driving vehicles. However, the current target detection accuracy for feature fusion based on roadside LiDAR and automotive LiDAR is relatively low, making it difficult to satisfy the sensing requirements of autonomous vehicles. This paper proposes PAFNet, a VIC pillar attention fusion network for target detection, aimed at improving LiDAR target detection accuracy under feature fusion. The proposed spatial and temporal cooperative fusion preprocessing method ensures the accuracy of the fused features through frame matching and coordinate transformation of the point cloud. In addition, this paper introduces the first anchor-free method for 3D target detection for VIC feature fusion, using a centroid-based approach for target detection. In the feature fusion stage, we propose the grid attention feature fusion method. This method uses the spatial feature attention mechanism to fuse the roadside and vehicle-side features. The experiment on the DAIR-V2X-C dataset shows that PAFNet achieved a 6.92% higher detection accuracy in 3D target detection than FFNet in urban scenes. © 2024 by the authors.",feature fusion; LiDAR; target detection; vehicle–infrastructure cooperative
Scopus,"Wei, J.; Che, K.; Gong, J.; Zhou, Y.; Lv, J.; Que, L.; Liu, H.; Len, Y.",Fast and Accurate Detection of Dim and Small Targets for Smart Micro-Light Sight,,2024,,,,10.3390/electronics13163301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202677821&doi=10.3390%2felectronics13163301&partnerID=40&md5=37e1c7d35b58efc791831bf6e91f0736,"To deal with low recognition accuracy and large time-consumption for dim, small targets in a smart micro-light sight, we propose a lightweight model DS_YOLO (dim and small target detection). We introduce the adaptive channel convolution module (ACConv) to reduce computational redundancy while maximizing the utilization of channel features. To address the misalignment problem in multi-task learning, we also design a lightweight dynamic task alignment detection head (LTD_Head), which utilizes GroupNorm to improve the performance of detection head localization and classification, and shares convolutions to make the model lightweight. Additionally, to improve the network’s capacity to detect small-scale targets while maintaining its generalization to multi-scale target detection, we extract high-resolution feature map information to establish a new detection head. Ultimately, the incorporation of the attention pyramid pooling layer (SPPFLska) enhances the model’s regression accuracy. We conduct an evaluation of the proposed algorithm DS_YOLO on four distinct datasets: CityPersons, WiderPerson, DOTA, and TinyPerson, achieving a 66.6% mAP on the CityPersons dataset, a 4.3% improvement over the original model. Meanwhile, our model reduces the parameter count by 33.3% compared to the baseline model. © 2024 by the authors.",dim and small target detection; lightweight; micro-light sight; task alignment
Scopus,"Chen, M.; Liu, Y.; Zhang, Z.; Guo, W.",RCRFNet: Enhancing Object Detection with Self-Supervised Radar–Camera Fusion and Open-Set Recognition,,2024,,,,10.3390/s24154803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200794826&doi=10.3390%2fs24154803&partnerID=40&md5=fb04ab0dae71b137c09d26ebd35e3287,"Robust object detection in complex environments, poor visual conditions, and open scenarios presents significant technical challenges in autonomous driving. These challenges necessitate the development of advanced fusion methods for millimeter-wave (mmWave) radar point cloud data and visual images. To address these issues, this paper proposes a radar–camera robust fusion network (RCRFNet), which leverages self-supervised learning and open-set recognition to effectively utilise the complementary information from both sensors. Specifically, the network uses matched radar–camera data through a frustum association approach to generate self-supervised signals, enhancing network training. The integration of global and local depth consistencies between radar point clouds and visual images, along with image features, helps construct object class confidence levels for detecting unknown targets. Additionally, these techniques are combined with a multi-layer feature extraction backbone and a multimodal feature detection head to achieve robust object detection. Experiments on the nuScenes public dataset demonstrate that RCRFNet outperforms state-of-the-art (SOTA) methods, particularly in conditions of low visual visibility and when detecting unknown class objects. © 2024 by the authors.",autonomous driving; open-set recognition; radar–camera fusion; self-supervised learning; target detection; Cameras; Feature extraction; Millimeter waves; Object detection; Object recognition; Supervised learning; Target tracking; Autonomous driving; Complex environments; Objects detection; Open-set recognition; Radar–camera fusion; Robust fusion; Robust object detection; Self-supervised learning; Targets detection; Visual image; adult; article; camera; diagnosis; feature detection; feature extraction; human; learning; male; retina image; sensor; telecommunication; visibility; visual disorder; Autonomous vehicles
Scopus,"Hussein, M.A.M.; Habib, M.K.",Navigating the Future: Advancing Autonomous Vehicles through Robust Target Recognition and Real-Time Avoidance,,2024,,,,10.1109/ICoCTA64736.2024.00070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002296789&doi=10.1109%2fICoCTA64736.2024.00070&partnerID=40&md5=b90dd5192c4bfbb02a62d2f3c8ff23a6,"This research addresses the critical challenge of enhancing object recognition and real-Time response capabilities in autonomous vehicles (AVs) under varying simulated conditions, which is crucial for ensuring both navigational safety and operational efficiency. Utilizing the Carla 0.9.14 simulator and Unreal Engine 4.26 on Ubuntu 20.04, we focus on improving the detection and classification of key on-road obstacles-vehicles, pedestrians, and cyclists-using the YOLOv7 object detection algorithm. By integrating advanced sensory technologies, specifically stereo vision cameras and LIDAR, we create a dynamic testing environment that simulates diverse urban and rural scenarios. Our methodology enhances the YOLOv7 algorithm's accuracy and speed through extensive training on a meticulously curated dataset of 4,113 images, reflecting a broad spectrum of environmental conditions, including varying lighting and weather conditions. This rigorous approach yielded a significant increase in mean average precision (mAP) from 64.3% to 76.3%, and enhanced the algorithm's reliability, with notable improvements over previous models. The research delineates a clear advancement in AV technology by demonstrating substantial improvements in object detection metrics, contributing foundational insights for future implementations in real-world settings and supporting the further development of real-Time avoidance systems. This study not only progresses the field of AV but also sets a new benchmark for object detection performance, aligning with industry and academic goals to optimize AV systems for complex and unpredictable driving scenarios.  © 2024 IEEE.",autonomous vehicles (AVs); CARLA simulator; dynamic driving scenarios; enhanced object detection; LIDAR; object detection; object recognition accuracy; real-Time processing; simulation environments; stereo cameras; unreal engine; YOLOv7; Automobile driver simulators; Automobile simulators; Pedestrian safety; Stereo vision; Autonomous vehicle; Autonomous Vehicles; CARLA simulator; Dynamic driving scenario; Enhanced object detection; Object recognition accuracy; Objects detection; Objects recognition; Realtime processing; Recognition accuracy; Simulation environment; Stereo cameras; Unreal engine; YOLOv7; Stereo image processing
Scopus,"Cao, J.; Zhang, T.; Hou, L.; Nan, N.",An improved YOLOv8 algorithm for small object detection in autonomous driving,,2024,,,,10.1007/s11554-024-01517-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199417065&doi=10.1007%2fs11554-024-01517-6&partnerID=40&md5=0eccb513c817baef2a4524eb22843c50,"In the task of visual object detection for autonomous driving, several challenges arise, such as detecting densely clustered targets, dealing with significant occlusion, and identifying small-sized targets. To address these challenges, an improved YOLOv8 algorithm for small object detection in autonomous driving (MSD-YOLO) is proposed. This algorithm incorporates several enhancements to improve the performance of detecting small and densely occluded targets. Firstly, the downsampling module is replaced with SPD-CBS (Space-to-Depth) to maintain the integrity of channel feature information. Subsequently, a multi-scale small object detection structure is designed to increase sensitivity for recognizing densely packed small objects. Additionally, DyHead (Dynamic Head) is introduced, equipped with simultaneous scale, spatial, and channel attention to ensure comprehensive perception of feature map information. In the post-processing stage, Soft-NMS (non-maximum suppression) is employed to effectively suppress redundant candidate boxes and reduce the missed detection rate of densely occluded targets. The effectiveness of these enhancements has been verified through various experiments conducted on the BDD100K autonomous driving public dataset. Experimental results indicate a significant improvement in the performance of the enhanced network. Compared to the YOLOv8n baseline model, MSD-YOLO shows a 13.7% increase in mAP50 and a 12.1% increase in mAP50:95, with only a slight increase in the number of parameters. Furthermore, the detection speed can reach 67.6 FPS, achieving a better balance between accuracy and speed. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.",DyHead; Small object detection; Soft-NMS; SPD-CBS; YOLOv8; Electric circuit breakers; Object detection; Object recognition; Autonomous driving; Down sampling; Dynamic head; Objects detection; Performance; Small object detection; Soft-NMS; SPD-CBS; Visual objects; YOLOv8; Autonomous vehicles
Scopus,"Ding, M.; Zhou, W.; Xu, Y.; Xu, Y.",Two-Stage Framework for Specialty Vehicles Detection and Classification: Toward Intelligent Visual Surveillance of Airport Surface,,2024,,,,10.1109/TAES.2023.3342797,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180308678&doi=10.1109%2fTAES.2023.3342797&partnerID=40&md5=cf27d72440e5b5fcdad65f36d2f8760b,"Intelligent visual surveillance (IVS) is being gradually introduced into the field of airport surface surveillance. The first task of IVS is to detect and recognize objects moving on the airport surface. Specialty vehicles play a considerable role in airport ground handling processes and are considerably monitored targets. Because specialty vehicles have diverse appearances and irregular shapes, pixel-level detection would enable them to be targeted more accurately. Specialty vehicles on the surface undertake different jobs in airport ground handling processes, and therefore subcategory classification would more precisely determine the function of these specialty vehicles. Moreover, pixel-level detection and subcategory classification are very useful for detecting key milestone nodes of airport ground handling processes. Thus, in this article, a two-stage framework for specialty vehicle pixel-level detection and subcategory classification for IVS of the airport surface is exploited, which seamlessly integrates state-of-the-art algorithms and techniques, and consists of two segmentation stages (coarse mask generation and refined mask generation). Furthermore, to evaluate related methods, a dataset of airport surface specialty vehicles is established, which contains four types of representative specialty vehicles and corresponding accurate mask labels. All samples in the dataset were captured from surveillance videos of civil airports. Experimental results on the dataset clearly demonstrate that the proposed framework performed favorably compared with the classic instance segmentation methods and achieved pixel-level detection and subcategory classification of specialty vehicles for airport surface surveillance.  © 1965-2011 IEEE.",Aircraft detection; Airport security; Airports; Classification (of information); Object recognition; Pixels; Security systems; Surface treatment; Vehicles; Airport ground handling; Airport surfaces; Category Classification; Handling process; Instance segmentation; Intelligent visual surveillances; Objects detection; Pixel level detection; Surveillance; Task analysis; Object detection
Scopus,"Sheng, X.; Li, S.; Qu, J.; Liu, L.",3D Object Detection Algorithm Based on Improved YOLOv5,,2024,,,,10.3788/LOP240451,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217919454&doi=10.3788%2fLOP240451&partnerID=40&md5=e3f064e67941c87a6dc4361cfbb89d18,"To address the challenge of handling large volumes of point cloud data for three-dimensional (3D) object detection and the limited effectiveness in detecting small objects, in this study, an enhanced 3D target detection method is proposed that improves the YOLOv5 network based on the idea of Complex-YOLO algorithm. The proposed approach first tackles the issue of lengthy processing times due to extensive point cloud data by adopting the Complex-YOLO strategy of converting point cloud data into an RGB-Map format, which is more manageable for the YOLOv5 network. Enhancements to YOLOv5 include an angle prediction branch and a rotation frame regression loss function to accurately position rotating targets within the RGB-Map. Additionally, the YOLOv5 architecture is modified to better detect small objects by incorporating a feature fusion layer and a dedicated prediction head, which heightens the network’s sensitivity to smaller targets. Furthermore, the convolutional block attention module (CBAM) attention mechanism is integrated into the network’s neck to further enhance detection sensitivity. Experimental evaluations on the KITTI dataset confirm the superiority of the modified YOLOv5 method over the original Complex-YOLO, with improvements in mean average precision (mAP): Car type mAP increased by 7. 48 percentage points, Pedestrian type by 12. 54 percentage points, Cyclist type by 1. 2 percentage points, and an overall increase of 7. 08 percentage points across all categories, demonstrating the effectiveness of this algorithm. © 2024 Universitat zu Koln. All rights reserved.",attention mechanism; Complex-YOLO; small target detection; three-dimensional object detection; YOLOv5
Scopus,"Yao, S.; Guan, R.; Huang, X.; Li, Z.; Sha, X.; Yue, Y.; Lim, E.G.; Seo, H.; Man, K.L.; Zhu, X.; Yue, Y.",Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review,,2024,,,,10.1109/TIV.2023.3307157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168687796&doi=10.1109%2fTIV.2023.3307157&partnerID=40&md5=e6bc7030e2fc180c92fd35a1fcd0bcb5,"Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including 'why to fuse', 'what to fuse', 'where to fuse', 'when to fuse', and 'how to fuse', subsequently discussing various challenges and potential research directions within this domain. To ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: https://radar-camera-fusion.github.io.  © 2016 IEEE.",Autonomous driving; object detection; radar-camera fusion; semantic segmentation; Cameras; Cost effectiveness; Data handling; Deep learning; Object detection; Object recognition; Radar antennas; Radar cross section; Semantics; Tracking radar; Autonomous driving; Learning techniques; Object semantic; Objects detection; Perception capability; Radar cross-sections; Radar-camera fusion; Radars antennas; Semantic segmentation; Surrounding environment; Radar imaging
Scopus,"Chen, X.; Wang, C.; Liu, C.; Zhu, X.; Zhang, Y.; Luo, T.; Zhang, J.",Autonomous Crack Detection for Mountainous Roads Using UAV Inspection System,,2024,,,,10.3390/s24144751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199755540&doi=10.3390%2fs24144751&partnerID=40&md5=507d034d0a099b370a2e580547521b6c,"Road cracks significantly affect the serviceability and safety of roadways, especially in mountainous terrain. Traditional inspection methods, such as manual detection, are excessively time-consuming, labor-intensive, and inefficient. Additionally, multi-function detection vehicles equipped with diverse sensors are costly and unsuitable for mountainous roads, primarily because of the challenging terrain conditions characterized by frequent bends in the road. To address these challenges, this study proposes a customized Unmanned Aerial Vehicle (UAV) inspection system designed for automatic crack detection. This system focuses on enhancing autonomous capabilities in mountainous terrains by incorporating embedded algorithms for route planning, autonomous navigation, and automatic crack detection. The slide window method (SWM) is proposed to enhance the autonomous navigation of UAV flights by generating path planning on mountainous roads. This method compensates for GPS/IMU positioning errors, particularly in GPS-denied or GPS-drift scenarios. Moreover, the improved MRC-YOLOv8 algorithm is presented to conduct autonomous crack detection from UAV imagery in an on/offboard module. To validate the performance of our UAV inspection system, we conducted multiple experiments to evaluate its accuracy, robustness, and efficiency. The results of the experiments on automatic navigation demonstrate that our fusion method, in conjunction with SWM, effectively enables real-time route planning in GPS-denied mountainous terrains. The proposed system displays an average localization drift of 2.75% and a per-point local scanning error of 0.33 m over a distance of 1.5 km. Moreover, the experimental results on the road crack detection reveal that the MRC-YOLOv8 algorithm achieves an F1-Score of 87.4% and a mAP of 92.3%, thus surpassing other state-of-the-art models like YOLOv5s, YOLOv8n, and YOLOv9 by 1.2%, 1.3%, and 3.0% in terms of mAP, respectively. Furthermore, the parameters of the MRC-YOLOv8 algorithm indicate a volume reduction of 0.19(×106) compared to the original YOLOv8 model, thus enhancing its lightweight nature. The UAV inspection system proposed in this study serves as a valuable tool and technological guidance for the routine inspection of mountainous roads. © 2024 by the authors.",mountainous road; MRC-YOLOv8; pavement crack detection; SWM; UAV inspection system; Aircraft detection; Antennas; Crack detection; Image enhancement; Motion planning; Navigation; Roads and streets; Unmanned aerial vehicles (UAV); Aerial vehicle; Inspection system; Mountainoi road; MRC-YOLOv8; Pavement crack detection; Slide window method; Slide windows; Unmanned aerial vehicle inspection system; Vehicle inspections; Window methods; algorithm; article; diagnosis; global navigation satellite system; imagery; sensor; unmanned aerial vehicle; Global positioning system
Scopus,"Burgos, D.C.; Iztueta, E.J.; Ormaechea, I.M.; Martínez-Otzeta, J.M.; Mugica, A.C.",Deep Learning-Based Traffic Light Detection in a Custom Embedded Hardware Platform for ADAS Applications,,2024,,,,10.1109/ACCESS.2024.3452608,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203406014&doi=10.1109%2fACCESS.2024.3452608&partnerID=40&md5=c85a32d6d10d8c07785bcdab50c2e4fa,"Automotive Driver Assistance Systems (ADAS) applications are currently an intensive field of study and innovation. The development of an ADAS is a multidisciplinary task involving electronic hardware design, advanced software implementation, safety considerations and many more. Building an ADAS application implies some challenges that are addressed in this paper. Firstly, all ADAS applications run on highly specific hardware devices embedded in the car with limited computation resources. In this work a novel embedded platform, iADASys, is developed and tested. The platform integrates the elements required to implement an artificial vision based ADAS application such as high performance processor with Deep Learning (DL) computation co-processors or multi-channel high resolution video streaming hardware. Secondly, this work implements an artificial vision application for traffic light detection based on deep neural networks. The model selected in this work is SSD_Mobilenet_V1 and it was trained using Bosch Small Traffic Light (BSTL) dataset. To fulfill real time requirement, the model image input resolution was maintained low at 300 × 300 pixel. However, the small object size in the dataset together with low resolution lead to poor detection performance. This situation was addressed by fine tuning the model training hyperparameters related to detection scales and aspect ratios. Lastly, the model is deployed in the hardware platform and its performance is measured. Model inference is executed on a specialized mathematical co-processor obtaining the required real time response. The object detection performance is also measured, obtaining promising results. © 2013 IEEE.",ADAS; deep learning; neural network hardware; object detection; Advanced driver assistance systems; Deep neural networks; Digital storage; Embedded software; Laser beams; Automotive driver assistance systems; Co-processors; Deep learning; Detection performance; Hardware platform; Light detection; Neural network hardware; Objects detection; System applications; Traffic light; Video streaming
Scopus,"Karim, A.; Raza, M.A.; Alharthi, Y.Z.; Abbas, G.; Othmen, S.; Hossain, M.S.; Nahar, A.; Mercorelli, P.",Visual Detection of Traffic Incident through Automatic Monitoring of Vehicle Activities,,2024,,,,10.3390/wevj15090382,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205228215&doi=10.3390%2fwevj15090382&partnerID=40&md5=a3d34d8b472b3894c5c0a644bdd923de,"Intelligent transportation systems (ITSs) derive significant advantages from advanced models like YOLOv8, which excel in predicting traffic incidents in dynamic urban environments. Roboflow plays a crucial role in organizing and preparing image data essential for computer vision models. Initially, a dataset of 1000 images is utilized for training, with an additional 500 images reserved for validation purposes. Subsequently, the Deep Simple Online and Real-time Tracking (Deep-SORT) algorithm enhances scene analyses over time, offering continuous monitoring of vehicle behavior. Following this, the YOLOv8 model is deployed to detect specific traffic incidents effectively. By combining YOLOv8 with Deep SORT, urban traffic patterns are accurately detected and analyzed with high precision. The findings demonstrate that YOLOv8 achieves an accuracy of 98.4%, significantly surpassing alternative methodologies. Moreover, the proposed approach exhibits outstanding performance in the recall (97.2%), precision (98.5%), and F1 score (95.7%), underscoring its superior capability in accurate prediction and analyses of traffic incidents with high precision and efficiency. © 2024 by the authors.",object detection; object tracking; sustainable transportation; traffic incident; Urban transportation; Vehicle detection; Advanced modeling; Automatic monitoring; High-precision; Intelligent transportation systems; Object Tracking; Objects detection; Sustainable transportation; Traffic incidents; Vehicle activity; Visual detection; Intelligent systems
Scopus,"Paredes, J.A.; Hansard, M.; Rajab, K.Z.; Alvarez, F.J.",Spatial Calibration of Millimeter-Wave Radar for Close-Range Object Location,,2024,,,,10.1109/JSEN.2024.3393030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192210881&doi=10.1109%2fJSEN.2024.3393030&partnerID=40&md5=3d920b1b0cf227a7f3bde4adcd1dcb3d,"Accurate object detection and location systems are essential for many robotic applications, including autonomous grasping and manipulation systems. In some cases, the target object may be obscured from view, in clutter, packaging, or debris. Millimeter-wave radar (mmWave) is a potential alternative to visual sensing in such scenarios, owing to its ability to penetrate typical low-density non-metallic materials. However, this approach requires accurate spatial calibration of the radar signal, over the robot workspace. We propose to achieve this with reference to visual data, which provides ground-truth locations for initial training of the system. Specifically, we describe a commodity mmWave radar system for detecting and localizing static metallic objects, over a 2-D workspace. We compare similarity, affine, and thin-plate spline (TPS) models of the spatial transformation from radar estimates to actual locations. Experiments were performed with a frequency modulated continuous wave (FMCW) multiple-input multiple-output (MIMO) device, using a starting frequency of 60 GHz and a bandwidth of 3.4 GHz. It is shown that the spline model performs best, achieving an average spatial error of 7 mm, which is an order of magnitude lower than that of the uncalibrated system.  © 2001-2012 IEEE.",Mapping methods; millimeter-wave (mmWave) radar; RGB camera; spatial calibration; Frequency modulation; Location; Millimeter waves; MIMO radar; MIMO systems; Object detection; Object recognition; Radar imaging; Tracking radar; Close range; Mapping method; Millimeter-wave radar; Millimeterwave communications; Millimetre-wave radar; Mm waves; Mmwave radar; Object location; RGB cameras; Spatial calibration; Calibration
Scopus,"Wang, Q.; Wang, J.; Wang, X.; Wu, L.; Feng, K.; Wang, G.",A YOLOv7-Based Method for Ship Detection in Videos of Drones,,2024,,,,10.3390/jmse12071180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199609922&doi=10.3390%2fjmse12071180&partnerID=40&md5=2652778bfd0280ade6550aad845be8e2,"With the rapid development of the shipping industry, the number of ships is continuously increasing, and maritime accidents happen frequently. In recent years, computer vision and drone flight control technology have continuously developed, making drones widely used in related fields such as maritime target detection. Compared to the cameras fixed on ships, a greater flexibility and a wider field of view is provided by cameras equipped on drones. However, there are still some challenges in high-altitude detection with drones. Firstly, from a top-down view, the shapes of ships are very different from ordinary views. Secondly, it is difficult to achieve faster detection speeds because of limited computing resources. To solve these problems, we propose YOLOv7-DyGSConv, a deep learning-based model for detecting ships in real-time videos captured by drones. The model is built on YOLOv7 with an attention mechanism, which enhances the ability to capture targets. Furthermore, the Conv in the Neck of the YOLOv7 model is replaced with the GSConv, which reduces the complexity of the model and improves the detection speed and detection accuracy. In addition, to compensate for the scarcity of ship datasets in top-down views, a ship detection dataset containing 2842 images taken by drones or with a top-down view is constructed in the research. We conducted experiments on our dataset, and the results showed that the proposed model reduced the parameters by 16.2%, the detection accuracy increased by 3.4%, and the detection speed increased by 13.3% compared with YOLOv7. © 2024 by the authors.",deep learning; drones; object detection; ship detection; YOLOv7
Scopus,"Zhao, R.; Wang, K.; Xiao, Y.; Gao, F.; Gao, Z.",Leveraging Monte Carlo Dropout for Uncertainty Quantification in Real-Time Object Detection of Autonomous Vehicles,,2024,,,,10.1109/ACCESS.2024.3355199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182926594&doi=10.1109%2fACCESS.2024.3355199&partnerID=40&md5=800e400e02689672f57a77d5198414f2,"With the recent advancements in machine learning technology, the accuracy of autonomous driving object detection models has significantly improved. However, due to the complexity and variability of real-world traffic scenarios, such as extreme weather conditions, unconventional lighting, and unknown traffic participants, there is inherent uncertainty in autonomous driving object detection models, which may affect the planning and control in autonomous driving. Thus, the rapid and accurate quantification of this uncertainty is crucial. It contributes to a better understanding of the intentions of autonomous vehicles and strengthens trust in autonomous driving technology. This research pioneers in quantifying uncertainty in the YOLOv5 object detection model, thereby improving the accuracy and speed of probabilistic object detection, and addressing the real-time operational constraints of current models in autonomous driving contexts. Specifically, a novel probabilistic object detection model named M-YOLOv5 is proposed, which employs the MC-drop method to capture discrepancies between detection results and the real world. These discrepancies are then converted into Gaussian parameters for class scores and predicted bounding box coordinates to quantify uncertainty. Moreover, due to the limitations of the Mean Average Precision (MAP) evaluation metric, we introduce a new measure, Probability-based Detection Quality (PDQ), which is incorporated as a component of the loss function. This metric simultaneously assesses the quality of label uncertainty and positional uncertainty. Experiments demonstrate that compared to the original YOLOv5 algorithm, the M-YOLOv5 algorithm shows a 74.7% improvement in PDQ. When compared with the most advanced probabilistic object detection models targeting the MS COCO dataset, M-YOLOv5 achieves a 14% increase in MAP, a 17% increase in PDQ, and a 65% improvement in FPS. Furthermore, against the state-of-the-art probabilistic object detection models for the BDD100K dataset, M-YOLOv5 exhibits a 31.67% enhancement in MAP and a 125.6% increase in FPS.  © 2013 IEEE.",autonomous vehicles; Monte Carlo dropout; object detection; Uncertainty quantification; YOLOv5; Interactive computer systems; Learning systems; Monte Carlo methods; Object detection; Object recognition; Probabilistic logics; Quality control; Uncertainty analysis; Vehicles; Autonomous Vehicles; Computational modelling; Monte carlo dropout; Objects detection; Real - Time system; Transformer; Uncertainty; Uncertainty quantifications; YOLO; YOLOv5; Real time systems
Scopus,"Hua, C.; Luo, K.; Wu, Y.; Shi, R.",YOLO-ABD: A Multi-Scale Detection Model for Pedestrian Anomaly Behavior Detection,,2024,,,,10.3390/sym16081003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202483006&doi=10.3390%2fsym16081003&partnerID=40&md5=1fe254c0c529e6f29874e12650cc2b0c,"Public safety and intelligent surveillance systems rely on anomaly detection for effective monitoring. In real-world pedestrian detection scenarios, Pedestrians often exhibit various symmetrical features such as body contours, facial features, posture, and clothing. However, the accuracy of pedestrian anomaly detection is affected by factors such as complex backgrounds, pedestrian obstruction, and small target sizes. To address these issues, this study introduced YOLO-ABD, a lightweight method for anomaly behavior detection that integrated small object detection and channel shuffling. This approach enhanced the YOLOv8n baseline model by integrating a small-object detection mechanism at the head and employing the symmetric GSConv convolutional module in the backbone network to improve perceptual capabilities. Furthermore, it incorporated the SimAM attention mechanism to mitigate complex background interference and thus enhance target detection performance. Evaluation on the IITB-Corridor dataset showed mAP50 and mAP50-95 scores of 89.3% and 60.6%, respectively. Generalization testing on the street-view-gdogo dataset further underscored the superiority of YOLO-ABD over advanced detection algorithms, demonstrating its effectiveness and generalization capabilities. With relatively fewer parameters, YOLO-ABD provided an excellent lightweight solution for pedestrian anomaly detection. © 2024 by the authors.",lightweight surveillance systems; pedestrian anomaly detection; SimAM attention mechanism; small object detection
Scopus,"Du, Y.; Liu, X.; Yi, Y.; Wei, K.",Incorporating bidirectional feature pyramid network and lightweight network: a YOLOv5-GBC distracted driving behavior detection model,,2024,,,,10.1007/s00521-023-09043-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173937507&doi=10.1007%2fs00521-023-09043-5&partnerID=40&md5=161029716b3aeb080f815540322d75d6,"Distracted driving is one of the leading causes of traffic accidents and has become a bottleneck for improving driver assistance technologies. It is still a challenge to detect distracted driving behavior in real-life scenarios, which have the features of complex backgrounds, different target scales, and resolutions. In this context, a lightweight YOLOv5-GBC model is proposed for real-time distracted driving detection in this work. Firstly, the lightweight network GhostConv is used to perform lightweight operations on the convolutional layers, aiming to reduce a large number of parameters and computations. Secondly, the path aggregation network structure is improved to enhance the model fusion ability for different scale features, and coordinated attention is introduced to enhance the model extraction ability for effective information. The proposed YOLOv5-GBC model can predict different types of distracted driving. Finally, this work conducts extensive experiments; the results show that the proposed model has a mean accuracy (mAP) of 91.8%, which is 3.9% better than the baseline model, with a reduction of 6.5% and 9.1% in the weight file and Floating-point Operations Per Second, respectively. It outperforms the models of Faster-RCNN, SSD, YOLOv3-tiny, and YOLOv4-tiny, which indicates that the proposed model can identify distracted driving behaviors efficiently and rapidly. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023.",Advanced driving assistance system; BiFPN; Distracted driving; GhostConv; Advanced driver assistance systems; Automobile drivers; Behavioral research; Digital arithmetic; Advanced driving assistance system; Behavior detection; Detection models; Distracted driving; Driver assistance; Driving assistance systems; Driving behaviour; Feature pyramid; Ghostconv; Pyramid network; Feature extraction
Scopus,"Zhang, Z.; Yang, Y.; Xu, X.; Liu, L.; Yue, J.; Ding, R.; Lu, Y.; Liu, J.; Qiao, H.",GVC-YOLO: A Lightweight Real-Time Detection Method for Cotton Aphid-Damaged Leaves Based on Edge Computing,,2024,,,,10.3390/rs16163046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202429577&doi=10.3390%2frs16163046&partnerID=40&md5=69d89df8edbc47066d0fa56b01a6d3c3,"Cotton aphids (Aphis gossypii Glover) pose a significant threat to cotton growth, exerting detrimental effects on both yield and quality. Conventional methods for pest and disease surveillance in agricultural settings suffer from a lack of real-time capability. The use of edge computing devices for real-time processing of cotton aphid-damaged leaves captured by field cameras holds significant practical research value for large-scale disease and pest control measures. The mainstream detection models are generally large in size, making it challenging to achieve real-time detection on edge computing devices with limited resources. In response to these challenges, we propose GVC-YOLO, a real-time detection method for cotton aphid-damaged leaves based on edge computing. Building upon YOLOv8n, lightweight GSConv and VoVGSCSP modules are employed to reconstruct the neck and backbone networks, thereby reducing model complexity while enhancing multiscale feature fusion. In the backbone network, we integrate the coordinate attention (CA) mechanism and the SimSPPF network to increase the model’s ability to extract features of cotton aphid-damaged leaves, balancing the accuracy loss of the model after becoming lightweight. The experimental results demonstrate that the size of the GVC-YOLO model is only 5.4 MB, a decrease of 14.3% compared with the baseline network, with a reduction of 16.7% in the number of parameters and 17.1% in floating-point operations (FLOPs). The mAP@0.5 and mAP@0.5:0.95 reach 97.9% and 90.3%, respectively. The GVC-YOLO model is optimized and accelerated by TensorRT and then deployed onto the embedded edge computing device Jetson Xavier NX for detecting cotton aphid damage video captured from the camera. Under FP16 quantization, the detection speed reaches 48 frames per second (FPS). In summary, the proposed GVC-YOLO model demonstrates good detection accuracy and speed, and its performance in detecting cotton aphid damage in edge computing scenarios meets practical application needs. This research provides a convenient and effective intelligent method for the large-scale detection and precise control of pests in cotton fields. © 2024 by the authors.",cotton aphid; edge computing; real-time video detection; YOLOv8; Edge computing; Insect control; Photomapping; Computing devices; Cotton aphid; Detection methods; Edge computing; Large-scales; Real time videos; Real-time detection; Real-time video detection; Video detection; YOLOv8; Cotton
Scopus,"Baek, H.; Yu, S.; Son, S.; Seo, J.; Chung, Y.",Automated Region of Interest-Based Data Augmentation for Fallen Person Detection in Off-Road Autonomous Agricultural Vehicles,,2024,,,,10.3390/s24072371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190233568&doi=10.3390%2fs24072371&partnerID=40&md5=c980107bcfc6ffbacfab8cd2a41724d7,"Due to the global population increase and the recovery of agricultural demand after the COVID-19 pandemic, the importance of agricultural automation and autonomous agricultural vehicles is growing. Fallen person detection is critical to preventing fatal accidents during autonomous agricultural vehicle operations. However, there is a challenge due to the relatively limited dataset for fallen persons in off-road environments compared to on-road pedestrian datasets. To enhance the generalization performance of fallen person detection off-road using object detection technology, data augmentation is necessary. This paper proposes a data augmentation technique called Automated Region of Interest Copy-Paste (ARCP) to address the issue of data scarcity. The technique involves copying real fallen person objects obtained from public source datasets and then pasting the objects onto a background off-road dataset. Segmentation annotations for these objects are generated using YOLOv8x-seg and Grounded-Segment-Anything, respectively. The proposed algorithm is then applied to automatically produce augmented data based on the generated segmentation annotations. The technique encompasses segmentation annotation generation, Intersection over Union-based segment setting, and Region of Interest configuration. When the ARCP technique is applied, significant improvements in detection accuracy are observed for two state-of-the-art object detectors: anchor-based YOLOv7x and anchor-free YOLOv8x, showing an increase of 17.8% (from 77.8% to 95.6%) and 12.4% (from 83.8% to 96.2%), respectively. This suggests high applicability for addressing the challenges of limited datasets in off-road environments and is expected to have a significant impact on the advancement of object detection technology in the agricultural industry. © 2024 by the authors.",automated region of interest; autonomous agricultural vehicles; data augmentation; fallen person detection; Agriculture; Algorithms; Automation; Humans; Pandemics; Technology; Accidents; Agriculture; Autonomous vehicles; COVID-19; Image segmentation; Object recognition; Roads and streets; Automated region of interest; Autonomous agricultural vehicles; Data augmentation; Detection technology; Fallen person detection; Objects detection; Person detection; Region-of-interest; Regions of interest; Road environment; agriculture; algorithm; automation; human; pandemic; technology; Object detection
Scopus,"Çelik, M.T.; Arslankaya, S.; Yildiz, A.",Real-time detection of plastic part surface defects using deep learning- based object detection model,,2024,,,,10.1016/j.measurement.2024.114975,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194278180&doi=10.1016%2fj.measurement.2024.114975&partnerID=40&md5=e103bb4b14945ec4775fba7f0e2fe98e,"In this study, it was aimed to detect defects in plastic parts produced in a company operating in the automotive sub-industry using the YOLOv8 object detection model. The defect types seen in plastic parts were evaluated with the help of Pareto analysis, and scratches, stains and shine were selected as the most common defect types, and data on the three defect types were collected. YOLOv8 models were trained using faulty part images. As a result of the training, the highest mean average precision value of 0.990 was obtained in the YOLOv8s model, and the shortest training time was obtained in the YOLOv8n model. In the YOLOv8s model, which gave the highest mAP value, hyperparameter adjustment was made according to the batch size and learning rate values. The testing phase was carried out with the hyperparameter values that gave the best results and the mAP value was obtained as 0.902. © 2024 Elsevier Ltd",Artificial intelligence; Deep learning; Defect detection; Quality control; You-Only-Look-Once version 8; Deep learning; Object detection; Object recognition; Quality control; Signal detection; Surface defects; Deep learning; Defect detection; Defect type; Detection models; Hyper-parameter; Objects detection; Part surface; Plastics parts; Real-time detection; You-only-look-once version 8; Plastic parts
Scopus,"Xu, Z.; Wang, C.; Huang, K.",BiF-DETR:Remote sensing object detection based on Bidirectional information fusion,,2024,,,,10.1016/j.displa.2024.102802,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199145525&doi=10.1016%2fj.displa.2024.102802&partnerID=40&md5=e9802c6f6776fb381e06158250c121ef,"Remote Sensing Object Detection(RSOD) is a fundamental task in the field of remote sensing image processing. The complexity of the background, the diversity of object scales and the locality limitation of Convolutional Neural Network (CNN) present specific challenges for RSOD. In this paper, an innovative hybrid detector, Bidirectional Information Fusion DEtection TRansformer (BiF-DETR), is proposed to mitigate the above issues. Specifically, BiF-DETR takes anchor-free detection network, CenterNet, as the baseline, designs the feature extraction backbone in parallel, extracts the local feature details using CNNs, and obtains the global information and long-term dependencies using Transformer branch. A Bidirectional Information Fusion (BIF) module is elaborately designed to reduce the semantic differences between different styles of feature maps through multi-level iterative information interactions, fully utilizing the complementary advantages of different detectors. Additionally, Coordination Attention(CA), is introduced to enables the detection network to focus on the saliency information of small objects. To address diversity insufficiency of remote sensing images in the training stage, Cascade Mixture Data Augmentation (CMDA), is designed to improve the robustness and generalization ability of the model. Comparative experiments with other cutting-edge methods are conducted on the publicly available DOTA and NWPU VHR-10 datasets. The experimental results reveal that the performance of proposed method is state-of-the-art, with mAP reaching 77.43% and 94.75%, respectively, far exceeding the other 25 competitive methods. © 2024 The Author(s)",Anchor-free detector; Feature fusion; Remote sensing object detection(RSOD); Visual transformer; Convolutional neural networks; Feature extraction; Image enhancement; Information fusion; Iterative methods; Object recognition; Remote sensing; Semantics; Anchor-free; Anchor-free detector; Convolutional neural network; Detection networks; Features fusions; Objects detection; Remote sensing image processing; Remote sensing object detection; Remote-sensing; Visual transformer; Object detection
Scopus,"Rakhmonov, A.A.U.; Subramanian, B.; Varnousefaderani, B.A.; Kim, J.",Aed-net: Attention-based detection model for disabled signage detection,,2024,,,,10.7840/kics.2024.49.7.976,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203705215&doi=10.7840%2fkics.2024.49.7.976&partnerID=40&md5=e0bf7601cd8de6d34b580e114e92d697,"The aim of having designated parking spaces for individuals with disabilities is to ensure that only vehicles with proper handicapped signage use them, while preventing unauthorized vehicles from occupying those spaces. To achieve this, real-time monitoring is essential. Existing two-stage object detection models suffer from slow image processing and enhanced backbones with feature pyramid networks are also burdened with expanded parameters. While YOLOv5 model is a compelling choice due to its superior speed and performance compared to existing models. Therefore, this study proposes to make certain modifications to a baseline YOLOv5 model. Instead of the original 9 blocks in the backbone and 4 C3 blocks, we propose to replace them with 6 and 4 EfficientNet blocks, accordingly. These EfficientNet blocks have fewer parameters but still offer higher accuracy in detecting disabled signs, among other types of signs on car windshields. To make up for the reduced number of blocks, we have incorporated an attention mechanism into the proposed architecture before the detection phase. This mechanism enables the model to focus on the crucial regions required for the task. Furthermore, we propose utilizing a more advanced optimizer called AdamW to prevent overfitting. With these enhancements, a novel object detector, attention-based efficient detection model (AED-Net) is proposed. To assess the effectiveness of the proposed approach, we will gather and label a dataset comprising images of cars displaying disabled signage on their windshields. Experiments conducted using this dataset demonstrate that the proposed model achieves a superior F1 score of 0.73 compared to that of baseline model, 0.57. The proposed model utilizes 10 percent fewer parameters compared to the baseline model. © 2024, Korean Institute of Communications and Information Sciences. All rights reserved.",Depthwise Separable Convolution; Disabled Signage; Small Object Detection
Scopus,"Soumya, A.; Mohan, C.K.; Cenkeramaddi, L.R.",High Precision Single Shot Object Detection in Automotive Scenarios,,2024,,,,10.5220/0012383100003660,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192136235&doi=10.5220%2f0012383100003660&partnerID=40&md5=85b9eb22de71e38b1adc623633c654ca,"Object detection in low-light scenarios is a challenging task with numerous real-world applications, ranging from surveillance and autonomous vehicles to augmented reality. However, due to reduced visibility and limited information in the image data, carrying out object detection in low-lighting settings brings distinct challenges. This paper introduces a novel object detection model designed to excel in low-light imaging conditions, prioritizing inference speed and accuracy. The model leverages advanced deep-learning techniques and is optimized for efficient inference on resource-constrained devices. The inclusion of cross-stage partial (CSP) connections is key to its effectiveness, which maintains low computational complexity, resulting in minimal training time. This model adapts seamlessly to low-light conditions through specialized feature extraction modules, making it a valuable resource in challenging visual environments. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.",Computer Vision; Convolutional Neural Network; Deep Learning; Multi-Class Classification; Object Detection
Scopus,"Farooq, J.; Muaz, M.; Khan Jadoon, K.; Aafaq, N.; Khan, M.K.A.",An improved YOLOv8 for foreign object debris detection with optimized architecture for small objects,,2024,,,,10.1007/s11042-023-17838-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180648562&doi=10.1007%2fs11042-023-17838-w&partnerID=40&md5=2270328d3529ddfca144b15060c31e40,"Automated Foreign Object Debris (FOD) detection offers significant benefit to the aviation industry by reducing human error and enabling continuous surveillance. This paper focuses on addressing the intricacies of FOD detection, with a specific emphasis on treating FODs as “small” objects, a facet which has received limited attention in prior research. This study provides a pioneering evaluation of state-of-the-art object detectors, including both anchor-based models including SSD, YOLOv5m, Scaled YOLOv4 and anchorless models CenterNet and YOLOv8m, applied to a multiclass FOD dataset, “FOD in Airports (FOD-A)”, as well as meticulously curated subset of FOD-A featuring small FODs. The findings reveal that the anchorless object detector YOLOv8m gives the best time accuracy trade off outperforming all compared anchor-based and anchorless approaches. To address the challenge of detecting small FODs, this study optimizes YOLOv8m model by making architectural modifications and incorporating a dedicated, shallow detection head that is purpose-built for the precise identification of small objects. The proposed model, termed as “Improved YOLOv8”, outperforms YOLOv8m by a margin of 1.02 in Average Precision for small objects (APs), achieving a mean average precision (mAP) of 93.8%. Notably, Improved YOLOv8 also has better mAP than all the considered anchor-based and anchorless object detectors examined, as well as those featured in prior FOD-A dataset research. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",Deep learning; FOD detection; Foreign object debris; Deep learning; Economic and social effects; Object detection; Petroleum reservoir evaluation; Anchorless; Aviation industry; Deep learning; Foreign object debris; Foreign object debris detection; Human errors; Object detectors; Optimized architectures; Small objects; Debris
Scopus,"Zhang, R.; Li, Y.; Wang, J.; Chen, Y.; Wang, Z.; Li, Y.",Multiscale Feature Fusion Approach for Dual-Modal Object Detection,,2024,,,,10.3778/j.issn.1002-8331.2305-0412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007326797&doi=10.3778%2fj.issn.1002-8331.2305-0412&partnerID=40&md5=fdfdb1d24f7511ade69730e550da8af6,"Object detection based on visible images is difficult to adapt to complex lighting conditions such as low light, no light, strong light, etc., while object detection based on infrared images is greatly affected by background noise. Infrared objects lack color information and have weak texture features, which pose a greater challenge. To address these problems, a dual-modal object detection approach that can effectively fuse the features of visible and infrared dual-modal images is proposed. A multiscale feature attention module is proposed, which can extract the multiscale features of the input IR and RGB images separately. Meanwhile, channel attention and spatial pixel attention is introduced to focus the multiscale feature information of dual-modal images from both channel and pixel dimensions. Finally, a dual-modal feature fusion module is proposed to adaptively fuse the feature information of dual-modal images. On the large-scale dual-modal image dataset DroneVehicle, compared with the benchmark algorithm YOLOv5s using visible or infrared single-modal image detection, the proposed algorithm improves the detection accuracy by 13.42 and 2.27 percentage points, and the detection speed reaches 164 frame/s, with ultra-real-time end-to-end detection capability. The proposed algorithm effectively improves the robustness and accuracy of object detection in complex scenes, which has good application prospects. © 2024 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",attention mechanism; dual-modal image; multiscale features fusion; object detection; Benchmarking; Feature extraction; Image enhancement; Image fusion; Image texture; Large datasets; Modal analysis; Object detection; Object recognition; Attention mechanisms; Dual-modal image; Feature information; Features fusions; Lighting conditions; Low light; Multi-scale features; Multiscale feature fusion; Objects detection; Visible image; Infrared imaging
Scopus,"He, B.; Ji, X.; Li, G.; Cheng, B.",Key Technologies and Applications of UAVs in Underground Space: A Review,,2024,,,,10.1109/TCCN.2024.3358545,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184319807&doi=10.1109%2fTCCN.2024.3358545&partnerID=40&md5=134c63d77b78e18f1e71a706d2a9d413,"Robots, particularly unmanned aerial vehicles (UAVs), offer significant advantages in challenging environments. Their application in searching and exploring underground areas has garnered considerable attention. However, subsurface environments present various challenges, such as a lack of localization signals, weak illumination, and severe electromagnetic interference, which make positioning, detecting, and communicating difficult for UAVs. The ability to address these challenges is crucial for successful underground UAV applications. In this article, we highlight high-performance perception and communication as key technologies for underground UAVs. We first summarize the current state of UAV perception and communication, and then analyse the supporting role of multi-source fusion perception technology and joint communication optimization methods for underground UAVs. We also evaluate existing subsurface UAV applications and discuss potential research challenges and future development directions to overcome these challenges in underground UAV application scenarios.  © 2015 IEEE.",joint communication optimization methods; multi-source fusion perception; Subsurface environments; subsurface UAV applications; Aircraft detection; Antennas; Electromagnetic pulse; Electromagnetic wave interference; Signal interference; Aerial vehicle; Autonomous system; Communication optimization; Joint communication optimization method; Multi-source fusion; Multi-source fusion perception; Optimization method; Simultaneous localization and mapping; Subsurface environment; Subsurface unmanned aerial vehicle application; Vehicle applications; Drones
Scopus,"Alaa, R.; Al-Libawy, H.; Hussein, E.A.",Low-Cost Blind Spot Detection System Based on Lite Object Detection Algorithm and Limited Resources Hardware,,2024,,,,10.1109/ICSINTESA62455.2024.10747868,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211568440&doi=10.1109%2fICSINTESA62455.2024.10747868&partnerID=40&md5=de1ed53c9f58610f065a25cd3719c28f,"Blind spot detection systems have become integral components of modern vehicle safety systems, aiming to mitigate the risks associated with lane-changing maneuvers. However, existing blind spot detection solutions often rely on expensive external sensors or complex infrastructure such as Radar and LIDAR, limiting their widespread adoption, particularly in budget-conscious markets. In this work, the combination of computer vision techniques and edge computing to detect vehicles is used in the blind spot region in real time. A lightweight convolutional neural network (CNN) model is deployed on an embedded device, such as a microcontroller, to process video streams from onboard cameras. The CNN model analyzes the captured frames, identifying vehicles near the host vehicle. This study compares the performance of several algorithms, including SSD FPN-Lite, YOLO, and FOMO, using a dataset of automated rickshaws, with images captured at various angles and dimensions. It has been shown that the FOMO algorithm is the best option for working on microcontrollers. An Arduino was used to connect to a camera that captures images in real time, recognizes the object, and identifies it with a center point. The best possible testing accuracy for this work using the collected dataset was 88.24%. © 2024 IEEE.",Depthwise Separable Convolution; Embedded system; MobileNetV2; TensorFlowLite; Tiny machine learning; Analog storage; Convolutional neural networks; Digital storage; Microcontrollers; Blind spot detection system; Convolutional neural network; Depthwise separable convolution; Embedded-system; Machine-learning; Mobilenetv2; Neural network model; Real- time; Tensorflowlite; Tiny machine learning; Budget control
Scopus,"Qiu, Q.; Lau, D.",Assessment of Trees’ Structural Defects via Hybrid Deep Learning Methods Used in Unmanned Aerial Vehicle (UAV) Observations,,2024,,,,10.3390/f15081374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202688898&doi=10.3390%2ff15081374&partnerID=40&md5=7421917a44291bd2786b1d31f48002e6,"Trees’ structural defects are responsible for the reduction in forest product quality and the accident of tree collapse under extreme environmental conditions. Although the manual view inspection for assessing tree health condition is reliable, it is inefficient in discriminating, locating, and quantifying the defects with various features (i.e., crack and hole). There is a general need for investigation of efficient ways to assess these defects to enhance the sustainability of trees. In this study, the deep learning algorithms of lightweight You Only Look Once (YOLO) and encoder-decoder network named DeepLabv3+ are combined in unmanned aerial vehicle (UAV) observations to evaluate trees’ structural defects. Experimentally, we found that the state-of-the-art detector YOLOv7-tiny offers real-time (i.e., 50–60 fps) and long-range sensing (i.e., 5 m) of tree defects but has limited capacity to acquire the patterns of defects at the millimeter scale. To address this limitation, we further utilized DeepLabv3+ cascaded with different network architectures of ResNet18, ResNet50, Xception, and MobileNetv2 to obtain the actual morphology of defects through close-range and pixel-wise image semantic segmentation. Moreover, the proposed hybrid scheme YOLOv7-tiny_DeepLabv3+_UAV assesses tree’s defect size with an averaged accuracy of 92.62% (±6%). © 2024 by the authors.",assessment; deep learning; DeepLabv3+; defect detection; forest; hybrid methods; remote sensing; tree structural defect; unmanned aerial vehicle; YOLO-tiny; Algorithms; Capacity; Defects; Patterns; Reduction; Segmentation; Trees; Aircraft accidents; Deep learning; Hybrid vehicles; Trees (mathematics); Unmanned aerial vehicles (UAV); Aerial vehicle; Assessment; Deep learning; Deeplabv3+; Defect detection; Forest; Hybrid method; Remote-sensing; Structural defect; Tree structural defect; Unmanned aerial vehicle; You only look once-tiny; environmental conditions; image processing; machine learning; pixel; remote sensing; sustainability; tree; unmanned vehicle; Semantic Segmentation
Scopus,"Manakitsa, N.; Maraslidis, G.S.; Moysis, L.; Fragulis, G.F.","A Review of Machine Learning and Deep Learning for Object Detection, Semantic Segmentation, and Human Action Recognition in Machine and Robotic Vision",,2024,,,,10.3390/technologies12020015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185941349&doi=10.3390%2ftechnologies12020015&partnerID=40&md5=c70fc90160e6a4127b015fb6130609ac,"Machine vision, an interdisciplinary field that aims to replicate human visual perception in computers, has experienced rapid progress and significant contributions. This paper traces the origins of machine vision, from early image processing algorithms to its convergence with computer science, mathematics, and robotics, resulting in a distinct branch of artificial intelligence. The integration of machine learning techniques, particularly deep learning, has driven its growth and adoption in everyday devices. This study focuses on the objectives of computer vision systems: replicating human visual capabilities including recognition, comprehension, and interpretation. Notably, image classification, object detection, and image segmentation are crucial tasks requiring robust mathematical foundations. Despite the advancements, challenges persist, such as clarifying terminology related to artificial intelligence, machine learning, and deep learning. Precise definitions and interpretations are vital for establishing a solid research foundation. The evolution of machine vision reflects an ambitious journey to emulate human visual perception. Interdisciplinary collaboration and the integration of deep learning techniques have propelled remarkable advancements in emulating human behavior and perception. Through this research, the field of machine vision continues to shape the future of computer systems and artificial intelligence applications. © 2024 by the authors.",artificial intelligence; computer vision; deep learning; image processing; machine learning; machine vision; mechatronics; object classification; object detection; object segmentation; pattern recognition; robotics
Scopus,"Rahman, M.; Islam, F.; Ball, J.E.; Goodin, C.",Traffic Light Recognition and V2I Communications of an Autonomous Vehicle with the Traffic Light for Effective Intersection Navigation using YOLOv8 and MAVS Simulation,,2024,,,,10.1117/12.3013514,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197523236&doi=10.1117%2f12.3013514&partnerID=40&md5=416fd8ed3b6aab42cec214772d845a8f,"We integrate advanced computer vision and Vehicle-to-Infrastructure (V2I) communication systems for effective intersection navigation. In the first phase, the YOLOv8 deep learning model is employed to accurately detect traffic lights, with specialized training on the S2TLD Dataset for precision. Then we establish seamless V2I communication in MAVS Simulation, allowing vehicles to receive Signal Phase and Timing (SPaT) messages from traffic lights, enabling autonomous adjustment of speed and behavior. Simulating the scenarios in a high-fidelity automotive simulator demonstrates accurate traffic light detection and timely phase information, promising safer and more efficient intersection navigation for autonomous vehicles. © 2024 SPIE.",autonomous driving; intersection navigation; object detection; traffic light recognition; V2X communication; Deep learning; Navigation; Object detection; Vehicle to Everything; Vehicle to vehicle communications; Autonomous driving; Autonomous Vehicles; Communications systems; Intersection navigation; Objects detection; Traffic light; Traffic lights recognition; V2I communications; V2X communication; Vehicle-to-infrastructure; Autonomous vehicles
Scopus,"Hassan, J.; Alsamhi, S.",Applications of machine learning in UAV networks,,2024,,,,10.4018/979-8-3693-0578-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193675656&doi=10.4018%2f979-8-3693-0578-2&partnerID=40&md5=2759b200074b385cb7c8e5d1dd27fac6,"Unmanned aerial vehicles (UAVs) continue to become more advanced and complex as researchers push the boundaries of other supporting technologies. Applications of Machine Learning in UAV Networks presents a pioneering exploration into the symbiotic relationship between machine learning techniques and UAVs. In an age where UAVs are revolutionizing sectors as diverse as agriculture, environmental preservation, security, and disaster response, this meticulously crafted volume offers an analysis of the manifold ways machine learning drives advancements in UAV network efficiency and efficacy. This book navigates through an expansive array of domains, each demarcating a pivotal application of machine learning in UAV networks. From the precision realm of agriculture and its dynamic role in yield prediction to the ecological sensitivity of biodiversity monitoring and habitat restoration, the contours of each domain are vividly etched. These explorations are not limited to the terrestrial sphere; rather, they extend to the pivotal aerial missions of wildlife conservation, forest fire monitoring, and security enhancement, where UAVs adorned with machine learning algorithms wield an instrumental role. Scholars and practitioners from fields as diverse as machine learning, UAV technology, robotics, and IoT networks will find themselves immersed in a confluence of interdisciplinary expertise. The book's pages cater equally to professionals entrenched in agriculture, environmental studies, disaster management, and beyond. Furthermore, the students and researchers finds knowledge that illuminates the convergence of UAVs and machine learning, arguably one of the most riveting frontiers in contemporary research. © 2024 by IGI Global. All rights reserved.",
Scopus,"Arsenos, A.; Karampinis, V.; Petrongonas, E.; Skliros, C.; Kollias, D.; Kollias, S.; Voulodimos, A.",Common Corruptions for Evaluating and Enhancing Robustness in Air-to-Air Visual Object Detection,,2024,,,,10.1109/LRA.2024.3408485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195405957&doi=10.1109%2fLRA.2024.3408485&partnerID=40&md5=a2472f162564ef9ac1999d84da3468f0,"—The main barrier to achieving fully autonomous flights lies in autonomous aircraft navigation. Managing non-cooperative traffic presents the most important challenge in this problem. The most efficient strategy for handling non-cooperative traffic is based on monocular video processing through deep learning models. This letter contributes to the vision-based deep learning aircraft detection and tracking literature by investigating the impact of data corruption arising from environmental and hardware conditions on the effectiveness of these methods. More specifically, we designed 7 types of common corruptions for camera inputs taking into account real-world flight conditions. By applying these corruptions to the Airborne Object Tracking (AOT) dataset we constructed the first robustness benchmark dataset named AOT-C for air-to-air aerial object detection. The corruptions included in this dataset cover a wide range of challenging conditions such as adverse weather and sensor noise. The second main contribution of this letter is to present an extensive experimental evaluation involving 8 diverse object detectors to explore the degradation in the performance under escalating levels of corruptions (domain shifts). Based on the evaluation results, the key observations that emerge are the following: 1) One-stage detectors of the YOLO family demonstrate better robustness, 2) Transformer-based and multi-stage detectors like Faster R-CNN are extremely vulnerable to corruptions, 3) Robustness against corruptions is related to the generalization ability of models. The third main contribution is to present that finetuning on our augmented synthetic data results in improvements in the generalisation ability of the object detector in real-world flight experiments. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",aerial detection; aerial systems; autonomous vehicle navigation; collision avoidance; Common Corruptions; out-of-distribution robustness; perception and autonomy; robot safety; Air navigation; Aircraft accidents; Aircraft detection; Antennas; C (programming language); Cameras; Deep learning; Object recognition; Robots; Unmanned aerial vehicles (UAV); Video signal processing; Aerial detection; Aerial systems; Autonomous vehicle navigation; Benchmark testing; Collisions avoidance; Common corruption; Objects detection; Out-of-distribution robustness; Perception and autonomy; Robot safety; Robustness; Object detection
Scopus,"Gao, W.; Fan, B.; Fang, Y.; Song, N.",Lightweight and multi-lesion segmentation model for diabetic retinopathy based on the fusion of mixed attention and ghost feature mapping,,2024,,,,10.1016/j.compbiomed.2023.107854,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180365112&doi=10.1016%2fj.compbiomed.2023.107854&partnerID=40&md5=cb2314673f16dc1f3442bc58dacdcfae,"Diabetic retinopathy is the main cause of blindness, and lesion segmentation is an important basic work for the diagnosis of this disease. The main lesions include soft and hard exudates, microaneurysms, and hemorrhages. However, the segmentation of these four types of lesions is difficult because of their variability in size and contrast, and high intertype similarity. Currently, many network models have problems, such as a large number of parameters and complex calculations, and most segmentation models for diabetic retinopathy focus only on one type of lesion. In this study, a lightweight algorithm based on BiSeNet V2 was proposed for the segmentation of multiple lesions in diabetic retinopathy fundus. First, a hybrid attention module was embedded in the semantic branch of BiSeNet V2 for 8- and 16-fold downsampling, which helped reassign deep feature-map weights and enhanced the ability to extract local key features. Second, a ghost feature-mapping unit was used to optimize the traditional convolution layers and further reduce the computational cost. Third, a new loss function based on the dynamic threshold loss function was applied to supervise the training by adjusting the training weights of the high-loss difficult samples, which enhanced the model's attention to small goals. In experiments on the IDRiD dataset, we conducted an ablation study to verify the effectiveness of each component and compared the proposed model, BiSeNet V2-Pro, with several state-of-the-art models. In comparison with the baseline BiSeNet V2, the segmentation performance of BiSeNet V2-Pro improved by 12.17 %, 11.44 %, and 8.49 % in terms of Sensitivity (SEN), Intersection over Union (IoU), and Dice coefficient (DICE), respectively. Specifically, IoU of MA reaches 0.5716. Compared with other methods, the segmentation speed was significantly improved while ensuring segmentation accuracy, and the number of model parameters was lower. These results demonstrate the superiority of BiSeNet V2-Pro in the multi-lesion segmentation of diabetic retinopathy. © 2023","Diabetic retinopathy; Fundus image; Lightweight network; Multi-lesion segmentation; Algorithms; Diabetes Mellitus; Diabetic Retinopathy; Fundus Oculi; Humans; Image Processing, Computer-Assisted; Semantics; Diagnosis; Eye protection; Mapping; Semantics; Diabetic retinopathy; Feature mapping; Fundus image; Hard exudates; Lesion segmentations; Lightweight network; Loss functions; Microaneurysms; Multi-lesion segmentation; Segmentation models; Article; BiSeNet V2 method; BiSeNet V2 Pro method; convolutional neural network; deep learning; DeepLabv3+ method; diabetic retinopathy; eye fundus; FC-DenseNet method; feature extraction; FFU-Net method; ghost feature mapping; image artifact; image segmentation; intermethod comparison; L-Seg method; M-Net method; measurement accuracy; mixed attention module; processing speed; segmentation algorithm; algorithm; diabetes mellitus; human; image processing; semantics; Image segmentation"
Scopus,"Zhao, W.; Ren, C.; Tan, A.",Study on Nighttime Pedestrian Trajectory-Tracking from the Perspective of Driving Blind Spots,,2024,,,,10.3390/electronics13173460,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203619800&doi=10.3390%2felectronics13173460&partnerID=40&md5=97b0583304b82a2a7d220fcd820fb64e,"With the acceleration of urbanization and the growing demand for traffic safety, developing intelligent systems capable of accurately recognizing and tracking pedestrian trajectories at night or under low-light conditions has become a research focus in the field of transportation. This study aims to improve the accuracy and real-time performance of nighttime pedestrian-detection and -tracking. A method that integrates the multi-object detection algorithm YOLOP with the multi-object tracking algorithm DeepSORT is proposed. The improved YOLOP algorithm incorporates the C2f-faster structure in the Backbone and Neck sections, enhancing feature extraction capabilities. Additionally, a BiFormer attention mechanism is introduced to focus on the recognition of small-area features, the CARAFE module is added to improve shallow feature fusion, and the DyHead dynamic target-detection head is employed for comprehensive fusion. In terms of tracking, the ShuffleNetV2 lightweight module is integrated to reduce model parameters and network complexity. Experimental results demonstrate that the proposed FBCD-YOLOP model improves lane detection accuracy by 5.1%, increases the IoU metric by 0.8%, and enhances detection speed by 25 FPS compared to the baseline model. The accuracy of nighttime pedestrian-detection reached 89.6%, representing improvements of 1.3%, 0.9%, and 3.8% over the single-task YOLO v5, multi-task TDL-YOLO, and the original YOLOP models, respectively. These enhancements significantly improve the model’s detection performance in complex nighttime environments. The enhanced DeepSORT algorithm achieved an MOTA of 86.3% and an MOTP of 84.9%, with ID switch occurrences reduced to 5. Compared to the ByteTrack and StrongSORT algorithms, MOTA improved by 2.9% and 0.4%, respectively. Additionally, network parameters were reduced by 63.6%, significantly enhancing the real-time performance of nighttime pedestrian-detection and -tracking, making it highly suitable for deployment on intelligent edge computing surveillance platforms. © 2024 by the authors.",deep SORT; lane lines; multi-object tracking; nighttime pedestrian-detection; YOLOP
Scopus,"Shuai, Y.; Lin, Z.; Chen, W.; Shenghuai, W.; Yu, T.",SF-YOLO: An Evolutionary Deep Neural Network for Gear End Surface Defect Detection,,2024,,,,10.1109/JSEN.2024.3403870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194818606&doi=10.1109%2fJSEN.2024.3403870&partnerID=40&md5=2a85a925ba419c8739eb6d7d90c89590,"Metal gears are an essential component of various important mechanical parts, and their quality directly impacts the overall performance and longevity of the automated system. Potential problems can be identified and solved promptly by detecting defects on the gear end face, improving overall product quality. In metal gear end-face defect detection, the inhomogeneity of the gear end-face structure and the multiscale, small-sized defects are the common issues, resulting in current detection methods performing poorly in terms of accuracy. To address the issues mentioned above, we propose a significant region extraction (SF)-YOLO metal gear end-face defect detection method based on evolutionary algorithm optimization to complete automatic detection of gear end-face defects. First, we offer a visual saliency region-based image extraction method that eliminates the interference of invalid features in the nonprocessing regions and reduces image complexity. Then, the neck network feature extraction pyramid is replaced by a weighted bidirectional feature pyramid network to enhance the model's multiscale adaptability and improve its fusion speed and efficiency. Afterward, the convolutional block attention module (CBAM) is combined with the C3 module to constitute the CBAM-C3 attention module and improve the model's attention to small sizes. Finally, an improved sparrow algorithm is proposed to optimize the hyperparameters of the model's neural network and avoid the inadmissible determinism of manual parameter tuning. The experiments showed that the SF-YOLO model achieved an average accuracy of 98.01% on a test set of metal gear end-face defects, with an F1 value of 0.99 and an average detection computation time per image of 0.025 s. Compared to other deep learning models, the proposed SF-YOLO model improves the accuracy and efficiency of gear end-face defect detection, and it can efficiently detect small-size and multiscale metal gear end-face defects to meet enterprises' real-time online inspection needs.  © 2001-2012 IEEE.",BiFPN; convolutional block attention module (CBAM); gear face defect detection; improved sparrow search algorithm (ISSA); multiscale; small size; Automation; Data mining; Deep neural networks; Efficiency; Face recognition; Feature extraction; Surface defects; CBAM; Computational modelling; Convolutional neural network; Defect detection; Features extraction; Gear face defect detection; ISSA; Multi-scales; Optimisations; Small size; Extraction
Scopus,"Yu, X.; Lu, J.; Lin, M.; Zhou, L.; Ou, L.",Shape-adaptive ellipse label assignment for remote sensing image based on FCOS,,2024,,,,10.3772/j.issn.1002-0470.2024.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205924160&doi=10.3772%2fj.issn.1002-0470.2024.08.009&partnerID=40&md5=c0e199bf1c29d2e08bb2aa091c102428,"Anchor-free object detection algorithms have experienced rapid development in object detection in recent years. However,in remote sensing images, the objects with arbitrary angles, dense distribution, and large shape differences make the detection still a challenge. Therefore, an anchor-free method based on improved fully convolu-tional one-stage (FCOS) is proposed. Firstly, to mine more potential high-quality anchor points, a shape-adaptive feature point sampling method based on the ellipse equation is proposed. To further reduce the negative influence of low-quality anchor points, the ellipse centerness is proposed. It can provide more accurate and reasonable weights than the traditional centerness. In addition, to address the inconsistency between classification and regression, a joint intersection over union ( IoU ) guidance strategy is proposed. The proposed ellipse centerness and IoU score are combined as quality scores to guide the training of the classification branch and to make the results of regression more accurate. The mean average precision on the DOTA 1. 0 dataset reaches 79. 17% , which is better than most existing anchor-free detection methods. © 2024 Inst. of Scientific and Technical Information of China. All rights reserved.",deep learning; label assignment; object detection; remote sensing image
Scopus,"Wu, W.; Liu, C.; Zheng, H.",A panoramic driving perception fusion algorithm based on multi-task learning,,2024,,,,10.1371/journal.pone.0304691,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195101332&doi=10.1371%2fjournal.pone.0304691&partnerID=40&md5=43b925fdbf1b7eedaa3c6a8150509b35,"With the rapid development of intelligent connected vehicles, there is an increasing demand for hardware facilities and onboard systems of driver assistance systems. Currently, most vehicles are constrained by the hardware resources of onboard systems, which mainly process single-task and single-sensor data. This poses a significant challenge in achieving complex panoramic driving perception technology. While the panoramic driving perception algorithm YOLOP has achieved outstanding performance in multi-task processing, it suffers from poor adaptability of feature map pooling operations and loss of details during downsampling. To address these issues, this paper proposes a panoramic driving perception fusion algorithm based on multi-task learning. The model training involves the introduction of different loss functions and a series of processing steps for lidar point cloud data. Subsequently, the perception information from lidar and vision sensors is fused to achieve synchronized processing of multi-task and multi-sensor data, thereby effectively improving the performance and reliability of the panoramic driving perception system. To evaluate the performance of the proposed algorithm in multi-task processing, the BDD100K dataset is used. The results demonstrate that, compared to the YOLOP model, the multi-task learning network performs better in lane detection, drivable area detection, and vehicle detection tasks. Specifically, the lane detection accuracy improves by 11.6%, the mean Intersection over Union (mIoU) for drivable area detection increases by 2.1%, and the mean Average Precision at 50% IoU (mAP50) for vehicle detection improves by 3.7%.  © 2024 Wu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Algorithms; Automobile Driving; Humans; Task Performance and Analysis; Article; classification algorithm; computer aided design; image reconstruction; image segmentation; learning algorithm; machine learning; mathematical analysis; motor vehicle; multi task learning; nonhuman; normal distribution; pedestrian; perception; signal noise ratio; spatiotemporal analysis; telecommunication; visual field; visual information; algorithm; car driving; human; psychology; task performance
Scopus,"Li, Y.; Zhou, Z.; Qi, G.; Hu, G.; Zhu, Z.; Huang, X.",Remote Sensing Micro-Object Detection under Global and Local Attention Mechanism,,2024,,,,10.3390/rs16040644,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185711614&doi=10.3390%2frs16040644&partnerID=40&md5=310519fd3919d878a0e18fea02a8af32,"With the rapid advancement of technology, satellite and drone technologies have had significant impacts on various fields, creating both opportunities and challenges. In areas like the military, urban planning, and environmental monitoring, the application of remote sensing technology is paramount. However, due to the unique characteristics of remote sensing images, such as high resolution, large-scale scenes, and small, densely packed targets, remote sensing object detection faces numerous technical challenges. Traditional detection methods are inadequate for effectively detecting small targets, rendering the accurate and efficient detection of objects in complex remote sensing images a pressing issue. Current detection techniques fall short in accurately detecting small targets compared to medium and large ones, primarily due to limited feature information, insufficient contextual data, and poor localization capabilities for small targets. In response, we propose an innovative detection method. Unlike previous approaches that often focused solely on either local or contextual information, we introduce a novel Global and Local Attention Mechanism (GAL), providing an in-depth modeling method for input images. Our method integrates fine-grained local feature analysis with global contextual information processing. The local attention concentrates on details and spatial relationships within local windows, enabling the model to recognize intricate details in complex images. Meanwhile, the global attention addresses the entire image’s global information, capturing overarching patterns and structures, thus enhancing the model’s high-level semantic understanding. Ultimately, a specific mechanism fuses local details with global context, allowing the model to consider both aspects for a more precise and comprehensive interpretation of images. Furthermore, we have developed a multi-head prediction module that leverages semantic information at various scales to capture the multi-scale characteristics of remote sensing targets. Adding decoupled prediction heads aims to improve the accuracy and robustness of target detection. Additionally, we have innovatively designed the Ziou loss function, an advanced loss calculation, to enhance the model’s precision in small target localization, thereby boosting its overall performance in small target detection. Experimental results on the Visdrone2019 and DOTA datasets demonstrate that our method significantly surpasses traditional methods in detecting small targets in remote sensing imagery. © 2024 by the authors.",attention mechanism; loss function; multi scale feature fusion; remote-sensing detection; Environmental technology; Feature extraction; Image enhancement; Military applications; Object detection; Object recognition; Semantics; Attention mechanisms; Features fusions; Loss functions; Multi scale feature fusion; Multi-scale features; Objects detection; Remote sensing images; Remote-sensing; Remote-sensing detection; Small targets; Remote sensing
Scopus,"Cheng, G.; Chao, P.; Yang, J.; Ding, H.",SGST-YOLOv8: An Improved Lightweight YOLOv8 for Real-Time Target Detection for Campus Surveillance,,2024,,,,10.3390/app14125341,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197251266&doi=10.3390%2fapp14125341&partnerID=40&md5=0c581eb21566f61491dd0e45b207a850,"Real-time target detection plays an important role in campus intelligent surveillance systems. This paper introduces Soft-NMS, GSConv, Triplet Attention, and other advanced technologies to propose a lightweight pedestrian and vehicle detection model named SGST-YOLOv8. In this paper, the improved YOLOv8 model is trained on the self-made dataset, and the tracking algorithm is combined to achieve an accurate and efficient real-time pedestrian and vehicle tracking detection system. The improved model achieved an accuracy of 88.6%, which is 1.2% higher than the baseline model YOLOv8. Additionally, the mAP0.5:0.95 increased by 3.2%. The model parameters and GFLOPS reduced by 5.6% and 7.9%, respectively. In addition, this study also employed the improved YOLOv8 model combined with the bot sort tracking algorithm on the website for actual detection. The results showed that the improved model achieves higher FPS than the baseline YOLOv8 model when detecting the same scenes, with an average increase of 3–5 frames per second. The above results verify the effectiveness of the improved model for real-time target detection in complex environments. © 2024 by the authors.",GSConv; Soft-NMS; target tracking; triplet attention; YOLOv8
Scopus,"Tang, H.; Xiong, W.; Dong, K.; Cui, Y.",Radar-optical fusion detection of UAV based on improved YOLOv7-tiny,,2024,,,,10.1088/1361-6501/ad440b,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193000451&doi=10.1088%2f1361-6501%2fad440b&partnerID=40&md5=fdf3ca149356b54aeffcb58dcdf3480f,"This study presents a radar-optical fusion detection method for unmanned aerial vehicles (UAVs) in maritime environments. Radar and camera technologies are integrated to improve the detection capabilities of the platforms. The proposed method involves generating regions of interest (ROI) by projecting radar traces onto optical images through matrix transformation and geometric centroid registration. The generated ROI are matched with YOLO detection boxes using the intersection-over-union (IoU) algorithm, enabling radar-optical fusion detection. A modified algorithm, called SPN-YOLOv7-tiny, is developed to address the challenge of detecting small UAV targets that are easily missed in images. In this algorithm, the convolutional layers in the backbone network are replaced with a space-to-depth convolution, and a small object detection layer is added. In addition, the loss function was replaced with a normalized weighted distance loss function. Experimental results demonstrate that compared to the original YOLOv7-tiny method, SPN-YOLOv7-tiny achieves an improved mAP@0.5 (mean average precision at an IoU threshold of 0.5) from 0.852 to 0.93, while maintaining a high frame rate of 135.1 frames per second. Moreover, the proposed radar-optical fusion detection method achieves an accuracy of 96.98%, surpassing the individual detection results of the radar and camera. The proposed method effectively addresses the detection challenges posed by closely spaced overlapping targets on a radar chart. © 2024 IOP Publishing Ltd.",centroid registration; improved YOLOv7-tiny; radar-optical fusion detection; small target; UAV; Aircraft detection; Antennas; Cameras; Convolution; Drones; Mathematical transformations; Object detection; Radar imaging; Tracking radar; Aerial vehicle; Centroid registration; Detection methods; Improved YOLOv7-tiny; Loss functions; Radar-optical fusion detection; Region-of-interest; Regions of interest; Small targets; Unmanned aerial vehicle; Geometrical optics
Scopus,"Kim, J.; Lee, C.; Chung, D.; Cho, Y.; Kim, J.; Jang, W.; Park, S.",Field experiment of autonomous ship navigation in canal and surrounding nearshore environments,,2024,,,,10.1002/rob.22262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176284813&doi=10.1002%2frob.22262&partnerID=40&md5=1b16550798ebdeac8599c3419d072960,"In this paper, we present the development of autonomous navigation capabilities for small cruise boats, and their verification by field experiments in a canal and its surrounding waters. A cruise boat was converted to an autonomous surface vehicle (ASV) by installing various sensors and actuators to enable autonomous navigation. Navigation and perception sensors, such as global positioning system, attitude and heading reference system, radar, light detection and ranging (LiDAR), and cameras, were mounted on the ASV to estimate its motion and perceive the surrounding environment. Motors and potentiometers were installed for active control of the ASV. Software system components including navigation filters, object-detection, path-planning, and control algorithms were designed and implemented. In the narrow canal region, LiDARs were used to detect the side walls and boundaries of the canal. In open areas outside the canal, obstacles and object features were detected using various combinations of onboard sensors. A model-based path-planning algorithm was designed to avoid the detected obstacles, and the line-of-sight guidance was employed to control the vehicle. The performance of the developed system was verified through a field experiment in a real-world maritime environment. © 2023 Wiley Periodicals LLC.",autonomous navigation; autonomous surface vehicle; field experiment; model-based path planning; multisensor object detection; ship control; system identification; Autonomous vehicles; Boats; Navigation; Object detection; Object recognition; Ships; Tracking radar; Unmanned surface vehicles; Voltage dividers; Autonomous navigation; Autonomous surface vehicles; Field experiment; Model-based OPC; Model-based path planning; Multi sensor; Multisensor object detection; Objects detection; Ship control; System-identification; Motion planning
Scopus,"Zhu, Z.; Li, X.; Zhai, J.; Hu, H.",PODB: A learning-based polarimetric object detection benchmark for road scenes in adverse weather conditions,,2024,,,,10.1016/j.inffus.2024.102385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189001927&doi=10.1016%2fj.inffus.2024.102385&partnerID=40&md5=809f48be58f9fe3fa65029110a1f1b24,"Due to its insensitivity to light intensity and the capability to capture multidimensional information, polarimetric imaging technology has been proven to have advantages over traditional intensity-based imaging techniques for object detection tasks in adverse environmental conditions, particularly in road traffic scenarios. Recently, with the rapid development of artificial intelligence technology, deep learning (DL)-powered object detection techniques can further enhance recognition accuracy and algorithm robustness. This improvement is made possible by the ability of DL technology to leverage large datasets and extract deeper levels of target-specific features. However, constructing large-scale polarimetric datasets poses challenges as obtaining polarimetric information requires multiple captures of intensity images. In other words, the workload is several times higher compared to traditional imaging techniques. To address the current scarcity of polarimetric datasets and evaluate the practical performance of various algorithms on polarimetric datasets, this paper proposes a Polarimetric Object Detection Benchmark (PODB) dataset. The PODB provides an integrated quality evaluation framework for DL-based object detection algorithms in complex road scenes by incorporating polarimetric imaging. Besides, we conducted extensive object detection experiments using the PODB, which allowed for a comprehensive validation and performance evaluation of effective benchmark algorithms. Furthermore, a physics-based multi-scale image fusion cascaded object detection neural network model is proposed. By combining the multidimensional information provided by polarized images with an adaptive learning multi-decision object detection neural network model, the recognition accuracy of complex road scenes in adverse weather conditions has been improved by approximately 10%. Additionally, we anticipate that PODB will serve as an effective platform for evaluating and comparing the performance of object detection algorithms, as well as providing researchers with a baseline for future studies in developing new DL-based methods. © 2024 Elsevier B.V.",Adverse weather conditions; Benchmark; Deep learning; Image fusion; Object detection; Polarimetric imaging; Quality assessment; Benchmarking; Complex networks; Deep learning; Image enhancement; Image fusion; Large datasets; Object recognition; Roads and streets; Adverse weather; Adverse weather condition; Benchmark; Condition; Deep learning; Multidimensional information; Objects detection; Polarimetric imaging; Quality assessment; Recognition accuracy; Object detection
Scopus,"Yin, H.; Tian, D.; Lin, C.; Duan, X.; Zhou, J.; Zhao, D.; Cao, D.",V2VFormer++: Multi-Modal Vehicle-to-Vehicle Cooperative Perception via Global-Local Transformer,,2024,,,,10.1109/TITS.2023.3314919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173289868&doi=10.1109%2fTITS.2023.3314919&partnerID=40&md5=e720a1f86422a2bdde25b27be3fa4fae,"Multi-vehicle cooperative perception has recently emerged for facilitating long-range and large-scale perception ability of connected automated vehicles (CAVs). Nonetheless, enormous efforts formulate collaborative perception as LiDAR-only 3D detection paradigm, neglecting the significance and complementary of dense image. In this work, we construct the first multi-modal vehicle-to-vehicle cooperative perception framework dubbed as V2VFormer++, where individual camera-LiDAR representation is incorporated with dynamic channel fusion (DCF) at bird's-eye-view (BEV) space and ego-centric BEV maps from adjacent vehicles are aggregated by global-local transformer module. Specifically, channel-token mixer (CTM) with MLP design is developed to capture global response among neighboring CAVs, and position-aware fusion (PAF) further investigate the spatial correlation between each ego-networked map in a local perspective. In this manner, we could strategically determine which CAVs are desirable for collaboration and how to aggregate the foremost information from them. Quantitative and qualitative experiments are conducted on both publicly-available OPV2V and V2X-Sim 2.0 benchmarks, and our proposed V2VFormer++ reports the state-of-the-art cooperative perception performance, demonstrating its effectiveness and advancement. Moreover, ablation study and visualization analysis further suggest the strong robustness against diverse disturbances from real-world scenarios.  © 2000-2011 IEEE.",3D object detection; autonomous driving; intelligent transportation systems; multi-modal fused perception; transformer; Vehicle-to-vehicle (V2V) cooperative perception; Benchmarking; Feature extraction; Intelligent systems; Intelligent vehicle highway systems; Object detection; Object recognition; Three dimensional computer graphics; Three dimensional displays; Vehicles; Vehicular ad hoc networks; 3D object; 3d object detection; Autonomous driving; Collaboration; Cooperative perception; Features extraction; Intelligent transportation systems; Multi-modal; Multi-modal fused perception; Objects detection; Three-dimensional display; Transformer; Vehicle to vehicles; Vehicle-to-vehicle (V2V) cooperative perception; Vehicular Adhoc Networks (VANETs); Optical radar
Scopus,"Hekmatnejad, M.; Hoxha, B.; Deshmukh, J.V.; Yang, Y.; Fainekos, G.",Formalizing and evaluating requirements of perception systems for automated vehicles using spatio-temporal perception logic,,2024,,,,10.1177/02783649231223546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183133814&doi=10.1177%2f02783649231223546&partnerID=40&md5=c7e7f692b18c3a6629c8b874690b687f,"Automated vehicles (AV) heavily depend on robust perception systems. Current methods for evaluating vision systems focus mainly on frame-by-frame performance. Such evaluation methods appear to be inadequate in assessing the performance of a perception subsystem when used within an AV. In this paper, we present a logic—referred to as Spatio-Temporal Perception Logic (STPL)—which utilizes both spatial and temporal modalities. STPL enables reasoning over perception data using spatial and temporal operators. One major advantage of STPL is that it facilitates basic sanity checks on the functional performance of the perception system, even without ground truth data in some cases. We identify a fragment of STPL which is efficiently monitorable offline in polynomial time. Finally, we present a range of specifications for AV perception systems to highlight the types of requirements that can be expressed and analyzed through offline monitoring with STPL. © The Author(s) 2024.","Programming environment; recognition; sensing and perception computer vision; simulation, interfaces and virtual reality; visual tracking; Computer circuits; Computer programming; Computer vision; Polynomial approximation; Automated vehicles; Perception systems; Performance; Programming environment; Recognition; Sensing and perception computer visions; Simulation, interface and virtual reality; Spatio-temporal; Temporal perception; Visual Tracking; Virtual reality"
Scopus,"He, Z.; Li, L.; Xu, H.; Zong, L.; Dai, Y.",Collaborative Obstacle Detection for Dual USVs Using MGNN-DANet with Movable Virtual Nodes and Double Attention,,2024,,,,10.3390/drones8090418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205099822&doi=10.3390%2fdrones8090418&partnerID=40&md5=829380749a38cee94ddb1eb269be65e9,"To reduce missed detections in LiDAR-based obstacle detection, this paper proposes a dual unmanned surface vessels (USVs) obstacle detection method using the MGNN-DANet template matching framework. Firstly, point cloud templates for each USV are created, and a clustering algorithm extracts suspected targets from the point clouds captured by a single USV. Secondly, a graph neural network model based on the movable virtual nodes is designed, introducing a neighborhood distribution uniformity metric. This model enhances the local point cloud distribution features of the templates and suspected targets through a local sampling strategy. Furthermore, a feature matching model based on double attention is developed, employing self-attention to aggregate the features of the templates and cross-attention to evaluate the similarity between suspected targets and aggregated templates, thereby identifying and locating another USV within the targets detected by each USV. Finally, the deviation between the measured and true positions of one USV is used to correct the point clouds obtained by the other USV, and obstacle positions are annotated through dual-view point cloud clustering. Experimental results show that, compared to single USV detection methods, the proposed method reduces the missed detection rate of maritime obstacles by 7.88% to 14.69%. © 2024 by the authors.",double attention; dual unmanned surface vessels; movable virtual nodes; obstacle detection; Obstacle detectors; Template matching; Unmanned surface vehicles; Virtual environments; Virtual reality; Clusterings; Detection methods; Double attention; Dual unmanned surface vessel; Missed detections; Movable virtual node; Obstacles detection; Point-clouds; Unmanned surface vessels; Virtual node; Graph neural networks
Scopus,"Sun, Y.; Zhang, Y.; Wang, H.; Guo, J.; Zheng, J.; Ning, H.",SES-YOLOv8n: automatic driving object detection algorithm based on improved YOLOv8,,2024,,,,10.1007/s11760-024-03003-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188325749&doi=10.1007%2fs11760-024-03003-9&partnerID=40&md5=cc37cec82f8ad97276580999b2cb88bf,"The perception system in autonomous driving mainly uses object detection algorithms to obtain the distribution of obstacles for recognition and analysis. Current object detection algorithms have rapidly developed, but it is challenging to balance the requirements of real-time detection and high detection accuracy in actual application scenarios. To solve the above problems, this paper uses YOLOv8n as the baseline model and proposes an object detection network named SES-YOLOv8n. Firstly, the SPPF module in the network was replaced by the SPPCSPC module to enhance further the model’s fusion ability under feature maps of different scales. The efficient multi-scale attention module EMA is introduced into the C2F module of the backbone network, which improves the perception ability in critical areas and the efficiency of feature extraction. Finally, the SPD-Conv module is used to replace part of the convolution modules in the backbone network to replace the downsampling operation, which can more effectively retain the feature information and improve the network’s accuracy and learning ability. Experimental results on the KITTI dataset and BDD100K dataset show that the average accuracy of the improved network model reaches 92.7% and 41.9%, which is 3.4% and 5.0% higher than that of the baseline model and is significantly better than the baseline model. This model can realize real-time image processing in general scenes based on ensuring high detection accuracy. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",Autonomous driving; EMA; Object detection; SPD-Conv; SPPCSPC; YOLOv8n; Automobile drivers; Autonomous vehicles; Object recognition; Signal detection; Autonomous driving; Back-bone network; Baseline models; Detection accuracy; EMA; Object detection algorithms; Objects detection; SPD-conv; SPPCSPC; YOLOv8n; Object detection
Scopus,"Ocholla, I.A.; Pellikka, P.; Karanja, F.; Vuorinne, I.; Väisänen, T.; Boitt, M.; Heiskanen, J.",Livestock Detection and Counting in Kenyan Rangelands Using Aerial Imagery and Deep Learning Techniques,,2024,,,,10.3390/rs16162929,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202453125&doi=10.3390%2frs16162929&partnerID=40&md5=dd9f2500b6c26b67a98ee6985feeed6b,"Accurate livestock counts are essential for effective pastureland management. High spatial resolution remote sensing, coupled with deep learning, has shown promising results in livestock detection. However, challenges persist, particularly when the targets are small and in a heterogeneous environment, such as those in African rangelands. This study evaluated nine state-of-the-art object detection models, four variants each from YOLOv5 and YOLOv8, and Faster R-CNN, for detecting cattle in 10 cm resolution aerial RGB imagery in Kenya. The experiment involved 1039 images with 9641 labels for training from sites with varying land cover characteristics. The trained models were evaluated on 277 images and 2642 labels in the test dataset, and their performance was compared using Precision, Recall, and Average Precision (AP0.5–0.95). The results indicated that reduced spatial resolution, dense shrub cover, and shadows diminish the model’s ability to distinguish cattle from the background. The YOLOv8m architecture achieved the best AP0.5–0.95 accuracy of 39.6% with Precision and Recall of 91.0% and 83.4%, respectively. Despite its superior performance, YOLOv8m had the highest counting error of −8%. By contrast, YOLOv5m with AP0.5–0.95 of 39.3% attained the most accurate cattle count with RMSE of 1.3 and R2 of 0.98 for variable cattle herd densities. These results highlight that a model with high AP0.5–0.95 detection accuracy may struggle with counting cattle accurately. Nevertheless, these findings suggest the potential to upscale aerial-imagery-trained object detection models to satellite imagery for conducting cattle censuses over large areas. In addition, accurate cattle counts will support sustainable pastureland management by ensuring stock numbers do not exceed the forage available for grazing, thereby mitigating overgrazing. © 2024 by the authors.",aerial survey; deep learning; livestock census; object detection; remote sensing; Remote sensing; Aerial imagery; Aerial surveys; Cattles; Deep learning; Detection models; Learning techniques; Livestock census; Objects detection; Performance; Remote-sensing; Aerial photography
Scopus,"Shi, T.; Guo, P.; Wang, R.; Ma, Z.; Zhang, W.; Li, W.; Fu, H.; Hu, H.",A Survey on Multi-Sensor Fusion Perimeter Intrusion Detection in High-Speed Railways,,2024,,,,10.3390/s24175463,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203873290&doi=10.3390%2fs24175463&partnerID=40&md5=7d9d02affc5c7c0591d25703bc1096aa,"In recent years, the safety issues of high-speed railways have remained severe. The intrusion of personnel or obstacles into the perimeter has often occurred in the past, causing derailment or parking, especially in the case of bad weather such as fog, haze, rain, etc. According to previous research, it is difficult for a single sensor to meet the application needs of all scenario, all weather, and all time domains. Due to the complementary advantages of multi-sensor data such as images and point clouds, multi-sensor fusion detection technology for high-speed railway perimeter intrusion is becoming a research hotspot. To the best of our knowledge, there has been no review of research on multi-sensor fusion detection technology for high-speed railway perimeter intrusion. To make up for this deficiency and stimulate future research, this article first analyzes the situation of high-speed railway technical defense measures and summarizes the research status of single sensor detection. Secondly, based on the analysis of typical intrusion scenarios in high-speed railways, we introduce the research status of multi-sensor data fusion detection algorithms and data. Then, we discuss risk assessment of railway safety. Finally, the trends and challenges of multi-sensor fusion detection algorithms in the railway field are discussed. This provides effective theoretical support and technical guidance for high-speed rail perimeter intrusion monitoring. © 2024 by the authors.",high-speed railways; multi-sensor fusion; object detection; perimeter intrusion; Derailments; Information fusion; Intrusion detection; Network intrusion; Railroad cars; Railroad transportation; Railroad yards and terminals; Rails; Risk analysis; Risk assessment; Risk perception; Sensor data fusion; Detection algorithm; Detection technology; High-speed railways; Intrusion-Detection; Multi-sensor fusion; Objects detection; Perimeter intrusion; Research status; Safety issues; Single sensor; detection algorithm; fog; haze; human; railway; rain; review; risk assessment; sensor; velocity; weather; Railroads
Scopus,"Osmani, K.; Schulz, D.",Comprehensive Investigation of Unmanned Aerial Vehicles (UAVs): An In-Depth Analysis of Avionics Systems,,2024,,,,10.3390/s24103064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194218675&doi=10.3390%2fs24103064&partnerID=40&md5=545f6226c631849e0f0bca4fe00df3fa,"The evolving technologies regarding Unmanned Aerial Vehicles (UAVs) have led to their extended applicability in diverse domains, including surveillance, commerce, military, and smart electric grid monitoring. Modern UAV avionics enable precise aircraft operations through autonomous navigation, obstacle identification, and collision prevention. The structures of avionics are generally complex, and thorough hierarchies and intricate connections exist in between. For a comprehensive understanding of a UAV design, this paper aims to assess and critically review the purpose-classified electronics hardware inside UAVs, each with the corresponding performance metrics thoroughly analyzed. This review includes an exploration of different algorithms used for data processing, flight control, surveillance, navigation, protection, and communication. Consequently, this paper enriches the knowledge base of UAVs, offering an informative background on various UAV design processes, particularly those related to electric smart grid applications. As a future work recommendation, an actual relevant project is openly discussed. © 2024 by the authors.",communication modules; control algorithms; embedded sensors; obstacles avoidance; thermal imaging; unmanned aerial vehicles; Air navigation; Aircraft accidents; Antennas; Cyber Physical System; Data handling; Embedded systems; Infrared imaging; Knowledge based systems; Military vehicles; Unmanned aerial vehicles (UAV); Aerial vehicle; Avionic systems; Communication modules; Diverse domains; Embedded sensors; In-depth analysis; Obstacles avoidance; Thermal-imaging; Unmanned aerial vehicle; Vehicle design; aircraft; algorithm; commercial phenomena; data processing; electronics; human; knowledge base; prevention; review; sensor; thermography; unmanned aerial vehicle; Avionics
Scopus,"Zarei, N.; Moallem, P.; Shams, M.",Real-time vehicle detection using segmentation-based detection network and trajectory prediction,,2024,,,,10.1049/cvi2.12236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172790636&doi=10.1049%2fcvi2.12236&partnerID=40&md5=cb92613cf8a1f5fb1d5748edd2fb8f10,"The position of vehicles is determined using an algorithm that includes two stages of detection and prediction. The more the number of frames in which the detection network is used, the more accurate the detector is, and the more the prediction network is used, the algorithm is faster. Therefore, the algorithm is very flexible to achieve the required accuracy and speed. YOLO's base detection network is designed to be robust against vehicle scale changes. Also, feature maps are produced in the detector network, which contribute greatly to increasing the accuracy of the detector. In these maps, using differential images and a u-net-based module, image segmentation has been done into two classes: vehicle and background. To increase the accuracy of the recursive predictive network, vehicle manoeuvres are classified. For this purpose, the spatial and temporal information of the vehicles are considered simultaneously. This classifier is much more effective than classifiers that consider spatial and temporal information separately. The Highway and UA-DETRAC datasets demonstrate the performance of the proposed algorithm in urban traffic monitoring systems. © 2023 The Authors. IET Computer Vision published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",convolutional neural nets; object detection; recurrent neural nets; vehicles; Classification (of information); Computer vision; Convolutional neural networks; Forecasting; Image segmentation; Recurrent neural networks; Vehicles; Convolutional neural net; Detection networks; Network prediction; Objects detection; Real- time; Recurrent neural net; Spatial informations; Temporal information; Trajectory prediction; Vehicles detection; Object detection
Scopus,"Narkhede, M.; Chopade, N.",Real-Time Detection of Vulnerable Road Users Using a Lightweight Object Detection Model,,2024,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185962890&partnerID=40&md5=ad1b569473d29f49c75a56de2f69e715,"Vulnerable Road Users (VRUs), including pedestrians, cyclists, and motorcyclists, face a heightened risk in traffic scenarios. The safety enhancement for VRUs relies heavily on evolving driver assistance systems and autonomous vehicles, anchored by swift VRU detection and localisation. Object detection models in computer vision are pivotal for this. Deploying such models on edge devices, especially the NVIDIA Jetson Nano presents challenges due to computational and power constraints. Our study compares the SSD MobileNetV2 FPN-Lite 320x320 model on the Jetson Nano for VRU detection with models like YOLOv3 and Faster RCNN. Key findings indicate that the SSD MobileNetV2 FPN-Lite 320x320 model achieves a mean average precision (mAP) of 0.45, precision of 0.80, and recall of 0.65 at 25 FPS in baseline evaluations. With optimisation, the FPS improved to 32 with slight changes in other metrics. The insights also touch upon inherent challenges in VRU detection, suggesting future research directions to refine the SSD MobileNetV2 FPN-Lite model's efficiency, ultimately striving for a safer transportation ecosystem. © 2024, Ismail Saritas. All rights reserved.",Deep Learning; Lightweight Model; Object Detection; Real-Time Processing; Road Safety; Vulnerable Road Users
Scopus,"Farhat, W.; Ben Rhaiem, O.; Faiedh, H.; Souani, C.",A novel cooperative collision avoidance system for vehicular communication based on deep learning,,2024,,,,10.1007/s41870-023-01574-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175379965&doi=10.1007%2fs41870-023-01574-3&partnerID=40&md5=3a83a1c4416e7ac73c4a486758865e1f,"Global road traffic injuries represent a significant safety challenge, with the highest fatality rates worldwide stemming from a combination of drivers' reckless behavior and the ever-expanding number of vehicles on the roads. In light of these factors, Autonomous Vehicles (AVs) have received a lot of attention as a technology that has the potential to revolutionize various industries and solve a lot of problems. The rise of the Internet of Vehicles (IoV) and Connected Vehicles (CVs) enables AVs to perceive and react swiftly in complex road scenarios, prioritizing safety. These vehicles prioritize individual safety in all situations, significantly enhancing road safety. Mobile Edge Computing (MEC) plays a crucial role thanks to its capacity for minimal latency and high bandwidth capabilities, enabling critical vehicular applications. In this paper, we introduce a cooperative collision avoidance system based on MEC, named 2CAS-MEC, that proactively identifies and locates road hazards. Our system is based on MEC servers positioned alongside roads, it consistently analyzes traffic and hazard data, issuing targeted alerts to nearby vehicles regarding potential risks. Particularly, our work introduces an advanced deep learning system that utilizes 5G, mobile edge computing, and cloud intelligence to prevent vehicle collisions. The experimental results show that our system outperforms the existing related works and exhibits the anticipated performance, effectively helping drivers in accident avoidance. © The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management 2023.",Cloud; Connected and autonomous vehicles; Deep learning; IOV; MEC; Vehicle to-everything (V2X)
Scopus,"Xue, J.; Chen, H.; Hu, Y.; Chen, M.; Wu, L.-I.; Chang, X.",Reduce Detection Latency of YOLOv5 to Prevent Real-Time Tracking Failures for Lightweight Robots,,2024,,,,10.1145/3671016.3671392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200919229&doi=10.1145%2f3671016.3671392&partnerID=40&md5=f231388dcb0f7c65543477d762a3bd47,"Lightweight robots are frequently engaged in real-time tracking tasks to provide human companionship services. For effective target tracking, the YOLO series is often employed as a lightweight object detection framework in robot systems. However, YOLO still demands substantial resources to train larger-scale models, striking a balance between accuracy and resource efficiency. Deploying YOLO directly on robots with limited computing resources can lead to significant delays in detection, compromising the effectiveness of tracking tasks. A deeper concern arises from the prevalent use of CPUs as the primary computing units in robots, rendering many existing model optimization techniques, which primarily target GPU computing, unsuitable for this context. To tackle this challenge, we propose a novel detection framework called SLCNet-YOLOv5, specifically designed for deployment in CPU-centric computing environments on robots. The core concept of SLCNet-YOLOv5 entails substituting the native YOLOv5 backbone network with SLCNet, which is a simplified version derived from the existing CPU convolutional neural network, PP-LCNet. It is important to note that our aim is not to enhance PP-LCNet to improve inference accuracy but rather to simplify it to enhance inference speed, while tolerating a certain degree of accuracy loss. This is because excessive inference latency may lead to real-time tracking failures. By employing a backbone network optimized for CPU-centric computation and reducing the computational complexity of the detection model, SLCNet markedly reduces latency, expediting the detection process, with only a minor trade-off in accuracy. In comparison to the performance of the state-of-the-art detector YOLOv5, experimental results on publicly available coco-foot-and-leg and PASCAL VOC datasets demonstrate significant enhancements in detection speed per image on CPU-centric terminals, with respective increases of 62.8% and 81.3%, alongside marginal declines in mean Average Precision (mAP) at 0.5 Intersection over Union (IoU) threshold, with losses of 0.077 and 0.165. © 2024 ACM.",Inference Latency; Lightweight Robots; Object Detection; Real-Time Tracking; Chemical detection; Economic and social effects; Image enhancement; Object recognition; Program processors; Robots; Target tracking; Back-bone network; Detection framework; Detection latency; Inference latency; Lightweight robot; Objects detection; Real time tracking; Robots system; Targets tracking; Tracking failure; Object detection
Scopus,"Dai, Y.; Kim, D.; Lee, K.",An Advanced Approach to Object Detection and Tracking in Robotics and Autonomous Vehicles Using YOLOv8 and LiDAR Data Fusion,,2024,,,,10.3390/electronics13122250,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197290154&doi=10.3390%2felectronics13122250&partnerID=40&md5=ab3a6d55dce91bf2a0cbac942c2a39c3,"Accurately and reliably perceiving the environment is a major challenge in autonomous driving and robotics research. Traditional vision-based methods often suffer from varying lighting conditions, occlusions, and complex environments. This paper addresses these challenges by combining a deep learning-based object detection algorithm, YOLOv8, with LiDAR data fusion technology. The principle of this combination is to merge the advantages of these technologies: YOLOv8 excels in real-time object detection and classification through RGB images, while LiDAR provides accurate distance measurement and 3D spatial information, regardless of lighting conditions. The integration aims to apply the high accuracy and robustness of YOLOv8 in identifying and classifying objects, as well as the depth data provided by LiDAR. This combination enhances the overall environmental perception, which is critical for the reliability and safety of autonomous systems. However, this fusion brings some research challenges, including data calibration between different sensors, filtering ground points from LiDAR point clouds, and managing the computational complexity of processing large datasets. This paper presents a comprehensive approach to address these challenges. Firstly, a simple algorithm is introduced to filter out ground points from LiDAR point clouds, which are essential for accurate object detection, by setting different threshold heights based on the terrain. Secondly, YOLOv8, trained on a customized dataset, is utilized for object detection in images, generating 2D bounding boxes around detected objects. Thirdly, a calibration algorithm is developed to transform 3D LiDAR coordinates to image pixel coordinates, which are vital for correlating LiDAR data with image-based object detection results. Fourthly, a method for clustering different objects based on the fused data is proposed, followed by an object tracking algorithm to compute the 3D poses of objects and their relative distances from a robot. The Agilex Scout Mini robot, equipped with Velodyne 16-channel LiDAR and an Intel D435 camera, is employed for data collection and experimentation. Finally, the experimental results validate the effectiveness of the proposed algorithms and methods. © 2024 by the authors.",calibrations; ground threshold; object detection and tracking; onboard sensors
Scopus,"Shyam, P.; Yoo, H.",Lightweight Thermal Super-Resolution and Object Detection for Robust Perception in Adverse Weather Conditions,,2024,,,,10.1109/WACV57701.2024.00730,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192022911&doi=10.1109%2fWACV57701.2024.00730&partnerID=40&md5=3539ae78be6015cadb397a16f7c5dacc,"In this work, we examine the potential application of thermal cameras in improving perception capabilities in adverse weather conditions like snow, night-time driving, and haze, focusing on retaining the performance of Advanced Driver Assistance Systems (ADAS), thus enhancing its functionality and safety characteristics. While thermal sensors offer the advantage of robust information capture in adverse weather conditions, their integration is plagued with issues surrounding poor feature capture in normal conditions, low imaging resolution, and high sensor costs. We address the former by formulating the problem definition as information switching wherein thermal images are selected when visible images are degraded. Furthermore, we consider a single object detector for RGB and thermal images to ensure low latency. We propose utilizing a learnable projection function that translates the thermal image into RGB color space, thus providing minimal modifications to the underlying object detector. We address the issues of low imaging resolution and cost by proposing a novel procedure that combines super-resolution and object detection, enabling the utilization of low-resolution and low-cost uncooled thermal imaging sensors. To ensure the complete pipeline meets the actual deployment requirements of real-time inference on resource-constrained devices, we introduce a lightweight super-resolution algorithm, implementing optimizations within the network structure followed by global pruning. In addition, to improve the feature representations extracted by lightweight encoders, we propose a bidirectional feature pyramid network to enhance the feature representation. We demonstrate the efficacy of the proposed mechanism through extensive simulated evaluations on automotive datasets such as FLIR, KAIST, DENSE, and Freiburg Thermal. © 2024 IEEE.",Applications; Autonomous Driving; Advanced driver assistance systems; Automobile drivers; Computer vision; Constrained optimization; Inference engines; Infrared imaging; Meteorology; Object detection; Optical resolving power; Adverse weather; Autonomous driving; Condition; Imaging resolutions; Object detectors; Objects detection; Resolution detection; Superresolution; Thermal; Thermal images; Object recognition
Scopus,"Hütten, N.; Alves Gomes, M.; Hölken, F.; Andricevic, K.; Meyes, R.; Meisen, T.",Deep Learning for Automated Visual Inspection in Manufacturing and Maintenance: A Survey of Open- Access Papers,,2024,,,,10.3390/asi7010011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185930289&doi=10.3390%2fasi7010011&partnerID=40&md5=10048ad46c43f10e45d6e019bb0c55b4,"Quality assessment in industrial applications is often carried out through visual inspection, usually performed or supported by human domain experts. However, the manual visual inspection of processes and products is error-prone and expensive. It is therefore not surprising that the automation of visual inspection in manufacturing and maintenance is heavily researched and discussed. The use of artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a possible paradigm shift and have the potential to facilitate automated visual inspection, even under complex environmental conditions. For this reason, we explore the question of to what extent deep learning is already being used in the field of automated visual inspection and which potential improvements to the state of the art could be realized utilizing concepts from academic research. By conducting an extensive review of the openly accessible literature, we provide an overview of proposed and in-use deep-learning models presented in recent years. Our survey consists of 196 open-access publications, of which 31.7% are manufacturing use cases and 68.3% are maintenance use cases. Furthermore, the survey also shows that the majority of the models currently in use are based on convolutional neural networks, the current de facto standard for image classification, object recognition, or object segmentation tasks. Nevertheless, we see the emergence of vision transformer models that seem to outperform convolutional neural networks but require more resources, which also opens up new research opportunities for the future. Another finding is that in 97% of the publications, the authors use supervised learning techniques to train their models. However, with the median dataset size consisting of 2500 samples, deep-learning models cannot be trained from scratch, so it would be beneficial to use other training paradigms, such as self-supervised learning. In addition, we identified a gap of approximately three years between approaches from deep-learning-based computer vision being published and their introduction in industrial visual inspection applications. Based on our findings, we additionally discuss potential future developments in the area of automated visual inspection. © 2024 by the authors.",automated visual inspection; computer vision; convolutional neural network; deep learning; industrial applications; vision transformer
Scopus,"Tong, K.; Wu, Y.-Q.",Research Advances on Deep Learning Based Small Object Detection Benchmarks,,2024,,,,10.12263/DZXB.20230624,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193003601&doi=10.12263%2fDZXB.20230624&partnerID=40&md5=93efe83679212f23b93e4d5eb5666d02,"Small object detection is an extremely challenging task in computer vision. It is widely used in remote sensing, intelligent transportation, national defense and military, daily life and other fields. Compared to other visual tasks such as image segmentation, action recognition, object tracking, generic object detection, image classification, video caption and human pose estimation, the research progress of small object detection is relatively slow. We believe that the constraints mainly include two aspects: the intrinsic difficulty of learning small object features and the scarcity of small object detection benchmarks. In particular, the scarcity of small object detection benchmarks can be considered from two aspects: the scarcity of small object detection datasets and the difficulty of establishing evaluation metrics for small object detection. To gain a deeper understanding of small object detection, this article conducts a brand-new and thorough investigation on small object detection benchmarks based on deep learning for the first time. The existing 35 small object detection datasets are introduced from 7 different application scenarios, such as remote sensing images, traffic sign and traffic light detection, pedestrian detection, face detection, synthetic aperture radar images and infrared images, daily life and others. Meanwhile, comprehensively summarize the definition of small objects from both relative scale and absolute scale. For the absolute scale, it mainly includes 3 categories: the width or height of the object bounding box, the product of the width and height of the object bounding box, and the square root of the area of the object bounding box. The focus is on exploring the evaluation metrics of small object detection in detail from 3 aspects: based on IoU (Intersection over Union) and its variants, based on average precision and its variants, and other evaluation metrics. In addition, in-depth analysis and comparison of the performance of some representative small object detection algorithms under typical evaluation metrics are conducted on 6 datasets. These categories of typical evaluation metrics can be further subdivided, including the evaluation metric plus the definition of objects, the evaluation metric plus single object category. More concretely, the evaluation metrics plus the definition of objects can be divided into 4 categories: average precision plus the definition of objects, miss rate plus the definition of objects, DoR-AP-SM (Degree of Reduction in Average Precision between Small objects and Medium objects) and DoR-APSL (Degree of Reduction in Average Precision between Small objects and Large objects). For the evaluation metrics plus single object category, it mainly includes 2 types: average precision plus single object category, OLRP (Optimal Localization Recall Precision) plus single object category. These representative small object detection methods mainly include anchor mechanism, scale-aware and fusion, context information, super-resolution technique and other improvement ideas. Finally, we point out the possible trends in the future from 6 aspects: a new benchmark for small object detection, a unified definition of small objects, a new framework for small object detection, multi-modal small object detection algorithms, rotating small object detection, and high precision and real time small object detection. We hope that this paper could provide a timely and comprehensive review of the research progress of small object detection benchmarks based on deep learning, and inspire relevant researchers to further promote the development of this field. © 2024 Chinese Institute of Electronics. All rights reserved.",deep learning; evaluation metric of small objects; small object dataset; small object detection; small object detection benchmark; the definition of small objects; Deep learning; Face recognition; Image classification; Image segmentation; Infrared imaging; Object recognition; Radar imaging; Remote sensing; Synthetic aperture radar; Tracking radar; Traffic signs; Vision; Deep learning; Evaluation metric of small object; Evaluation metrics; Small object dataset; Small object detection; Small object detection benchmark; Small objects; The definition of small object; Object detection
Scopus,"Zhang, Y.; Gong, Y.; Chen, X.",Research on YOLOv5 Vehicle Detection and Positioning System Based on Binocular Vision,,2024,,,,10.3390/wevj15020062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185551708&doi=10.3390%2fwevj15020062&partnerID=40&md5=78a93cd6a5bf8752459d408f78934419,"Vehicle detection and location is one of the key sensing tasks of automatic driving systems. Traditional detection methods are easily affected by illumination, occlusion and scale changes in complex scenes, which limits the accuracy and robustness of detection. In order to solve these problems, this paper proposes a vehicle detection and location method for YOLOv5(You Only Look Once version 5) based on binocular vision. Binocular vision uses two cameras to obtain images from different angles at the same time. By calculating the difference between the two images, more accurate depth information can be obtained. The YOLOv5 algorithm is improved by adding the CBAM attention mechanism and replacing the loss function to improve target detection. Combining these two techniques can achieve accurate detection and localization of vehicles in 3D space. The method utilizes the depth information of binocular images and the improved YOLOv5 target detection algorithm to achieve accurate detection and localization of vehicles in front. Experimental results show that the method has high accuracy and robustness for vehicle detection and localization tasks. © 2024 by the authors.",binocular vision; positioning; ranging; stereo matching; vehicle detection; YOLOv5 algorithm; Automobile drivers; Image enhancement; Stereo image processing; Stereo vision; Vehicles; Depth information; Detection and localization; Detection methods; Positioning; Stereo-matching; Vehicle detection systems; Vehicle location; Vehicle positioning; Vehicles detection; You only look once version 5 algorithm; Binocular vision
Scopus,"Cui, S.; Liu, F.; Wang, Z.; Zhou, X.; Yang, B.; Li, H.; Yang, J.",DAN-YOLO: A Lightweight and Accurate Object Detector Using Dilated Aggregation Network for Autonomous Driving,,2024,,,,10.3390/electronics13173410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204153465&doi=10.3390%2felectronics13173410&partnerID=40&md5=96ed02400f4b76edda34ed4b4991837b,"Object detection is becoming increasingly critical in autonomous driving. However, the accuracy and effectiveness of object detectors are often constrained by the obscuration of object features and details in adverse weather conditions. Therefore, this paper presented the DAN-YOLO vehicle object detector specifically designed for driving conditions in adverse weather. Building on the YOLOv7-Tiny network, SPP was replaced with SPPF, resulting in the SPPFCSPC structure, which enhances processing speed. The concept of Hybrid Dilated Convolution (HDC) was also introduced to improve the SPPFCSPC and ELAN-T structures, expanding the network’s receptive field (RF) while maintaining a lightweight design. Furthermore, an efficient multi-scale attention (EMA) mechanism was introduced to enhance the effectiveness of feature fusion. Finally, the Wise-IoUv1 loss function was employed as a replacement for CIoU to enhance the localization accuracy of the bounding box (bbox) and the convergence speed of the model. With an input size of 640 × 640, the DAN-YOLO algorithm proposed in this study achieved an increase in mAP0.5 values of 3.4% and 6.3% compared to the YOLOv7-Tiny algorithm in the BDD100K and DAWN benchmark tests, respectively, while achieving real-time detection (142.86 FPS). When compared with other state-of-the-art detectors, it reports better trade-off in terms of detection accuracy and speed under adverse driving conditions, indicating the suitability for autonomous driving applications. © 2024 by the authors.",adverse weather conditions; attention mechanism; autonomous driving; object detection
Scopus,"Lu, A.; Liu, J.; Cui, H.; Ma, L.; Ma, Q.",MLP-YOLOv5: A Lightweight Multi-Scale Identification Model for Lotus Pods with Scale Variation,,2024,,,,10.3390/agriculture14010030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183397240&doi=10.3390%2fagriculture14010030&partnerID=40&md5=1e21eeb234d62eda2bea8161570fa1fa,"Lotus pods in unstructured environments often present multi-scale characteristics in the captured images. As a result, it makes their automatic identification difficult and prone to missed and false detections. This study proposed a lightweight multi-scale lotus pod identification model, MLP-YOLOv5, to deal with this difficulty. The model adjusted the multi-scale detection layer and optimized the anchor box parameters to enhance the small object detection accuracy. The C3 module with transformer encoder (C3-TR) and the shuffle attention (SA) mechanism were introduced to improve the feature extraction ability and detection quality of the model. GSConv and VoVGSCSP modules were adopted to build a lightweight neck, thereby reducing model parameters and size. In addition, SIoU was utilized as the loss function of bounding box regression to achieve better accuracy and faster convergence. The experimental results on the multi-scale lotus pod test set showed that MLP-YOLOv5 achieved a mAP of 94.9%, 3% higher than the baseline. In particular, the model’s precision and recall for small-scale objects were improved by 5.5% and 7.4%, respectively. Compared with other mainstream algorithms, MLP-YOLOv5 showed more significant advantages in detection accuracy, parameters, speed, and model size. The test results verified that MLP-YOLOv5 can quickly and accurately identify multi-scale lotus pod objects in complex environments. It could effectively support the harvesting robot by accurately and automatically picking lotus pods. © 2023 by the authors.",deep learning; lightweight; lotus pod; MLP-YOLOv5; multi-scale object detection
Scopus,"Mori, K.T.; Peters, S.",SHARD: Safety and Human Performance Analysis for Requirements in Detection,,2024,,,,10.1109/TIV.2023.3320395,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173304894&doi=10.1109%2fTIV.2023.3320395&partnerID=40&md5=0a6201b9626493b6a556bc343742a3d2,"Automated driving requires reliable perception of the environment to ensure the safety of the driving task. One common perception task is 3D object detection, which aims at perceiving location and attributes of dynamic objects. This task is typically evaluated on different benchmark datasets, which each propose different metrics. However, these different metrics generally lack consistency and bear no relation to safety. Most notably, there is a lack of consistent definitions of pass/fail criteria for any given detection metric. In this work, the issue is addressed by systematically considering safety and human performance across different aspects of the object detection task. This approach yields interpretable detection metrics as well as thresholds for pass/fail criteria. Furthermore, a validation approach leveraging a prediction network is introduced and successfully applied to the requirements. A comparison of existing detectors shows that current perception algorithms exhibit failures for a majority of objects on the nuScenes dataset. Therefore, the results indicate the necessity of explicit safety consideration in the development of perception algorithms for the automated driving task.  © 2016 IEEE.",Environment perception; object detection; requirements; testing; Benchmarking; Job analysis; Object recognition; Reliability analysis; Safety engineering; Safety testing; Automated driving; Benchmark testing; Environment perceptions; Human performance; Location awareness; Objects detection; Requirement; Safety performance; Task analysis; Object detection
Scopus,"Liu, S.; Wu, J.; Lv, B.; Pan, X.; Wang, X.","Data Fusion of Roadside Camera, LiDAR, and Millimeter-Wave Radar",,2024,,,,10.1109/JSEN.2024.3448428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001209750&doi=10.1109%2fJSEN.2024.3448428&partnerID=40&md5=4e9bb5a1b5c1c372cf065d8ea765a6d3,"Roadside sensor data fusion is an essential component of the vehicle-road cooperation system, thus effectively enhancing the interactive perception level among road targets. However, due to the complex road environment, occlusion, and other problems, the single sensor has low accuracy in the process of target tracking. How to realize the fusion of multisensor trajectory tracking data is the main problem to be solved at present. Therefore, a new multisensor data fusion method for roadside camera, LiDAR, and millimeter wave (mm-Wave) radar is proposed in this study. According to the change in reflection intensity caused by the shift of the LiDAR point cloud with the change in distance and the detection accuracy of mm-Wave radar used in this article, the weight parameters of LiDAR and mm-Wave radar in the fusion process are determined. Finally, the target missed detection rate and trajectory disconnected repair rate were customized, and experimental tests were conducted in five natural environments to verify the robustness of the proposed method.  © 2001-2012 IEEE.",Camera; dynamic weight; LiDAR; millimeter-wave (mm-Wave) radar; object detection; roadside sensor fusion; trajectory tracking; Image compression; Image segmentation; Information fusion; Intelligent systems; Motor transportation; Network security; Radar reflection; Radar target recognition; Radar tracking; Remote sensing; Sensor data fusion; Dynamic weight; LiDAR; Millimeter-wave radar; Millimetre-wave radar; Objects detection; Roadside cameras; Roadside sensor fusion; Sensor fusion; Sensors data fusion; Trajectory-tracking; Cameras
Scopus,"Tanzib Hosain, M.; Zaman, A.; Abir, M.R.; Akter, S.; Mursalin, S.; Khan, S.S.","Synchronizing Object Detection: Applications, Advancements and Existing Challenges",,2024,,,,10.1109/ACCESS.2024.3388889,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190733293&doi=10.1109%2fACCESS.2024.3388889&partnerID=40&md5=485748a70e12d9688561673375c683ab,"From pivotal roles in autonomous vehicles, healthcare diagnostics, and surveillance systems to seamlessly integrating with augmented reality, object detection algorithms stand as the cornerstone in unraveling the complexities of the visual world. Tracing the trajectory from conventional region-based methods to the latest neural network architectures reveals a technological renaissance where algorithms metamorphose into digital artisans. However, this journey is not without hurdles, prompting researchers to grapple with real-time detection, robustness in varied environments, and interpretability amidst the intricacies of deep learning. The allure of addressing issues such as occlusions, scale variations, and fine-grained categorization propels exploration into uncharted territories, beckoning the scholarly community to contribute to an ongoing saga of innovation and discovery. This research offers a comprehensive panorama, encapsulating the applications reshaping our digital reality, the advancements pushing the boundaries of perception, and the open issues extending an invitation to the next generation of visionaries to explore uncharted frontiers within object detection.  © 2013 IEEE.",image classification; image recognition; Object detection; object segmentation; object tracking; semantic detection; Augmented reality; Computer architecture; Deep learning; Image classification; Image recognition; Image segmentation; Interactive computer systems; Network architecture; Neural networks; Object detection; Object recognition; Real time systems; Signal detection; Tracking (position); Computational modelling; Deep learning; Images classification; Object Tracking; Objects detection; Objects segmentation; Real - Time system; Semantic detection; YOLO; Semantics
Scopus,"Liu, L.; Lin, R.; Zhang, F.",Vision and Laser-Based Mobile Robot Following and Mapping,,2024,,,,10.1109/ICMCR60777.2024.10481877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190510154&doi=10.1109%2fICMCR60777.2024.10481877&partnerID=40&md5=4bcf42050c509993d4e480cb7652eb2a,"This paper presents an algorithm called Real-time Vision-based Following Map (RVFM), which enables a mobile robot to perceive and track human body positions in space based on visual perception. Simultaneously, it creates a map of the environment, addressing the autonomous mapping issue for mobile robots in unpredictable environments. The algorithm utilizes the YOLOv5 algorithm for target recognition and optimizes detection results by clustering real bounding box sizes using the DBSCAN algorithm. By optimizing the loss function, detection performance is further improved. For target tracking, a combination of siamese neural networks and RPN network regression is employed, enhancing tracking accuracy and stability. In terms of path planning, the DWA algorithm is introduced, combined with a cost function representing the field of view, to achieve more accurate path planning and keep the target human within the camera's field of view center. For mapping, the algorithm is based on Gmapping and incorporates Kalman filtering and outlier removal techniques to ensure mapping accuracy and reliability. Experimental results demonstrate that the RVFM algorithm provides more accurate human body position information, achieving precise following. It also performs well in indoor path planning and obstacle avoidance, ensuring safety and efficiency during the following process.  © 2024 IEEE.",DWA; human following; mapping; siamese neural networks; YOLOv5; Clustering algorithms; Cost functions; Mobile robots; Motion planning; Robot vision; Target tracking; Body positions; DWA; Field of views; Human bodies; Human following; Neural-networks; Real time vision; Siamese neural network; Vision based; YOLOv5; Mapping
Scopus,"Bouassida, S.; Nouveliere, L.; Neji, N.; Neji, J.",Enhancing Road Safety: A Comparative Study Between UAV-Assisted and Autonomous Vehicles,,2024,,,,10.1109/ICARCV63323.2024.10821584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217365115&doi=10.1109%2fICARCV63323.2024.10821584&partnerID=40&md5=9aa7410164ee6335afe50440b6074379,"The integration of connected autonomous vehicles (CAV) on open roads has gained significant progress in addressing road safety. These vehicles use advanced sensor technology to perceive and react to the road environment, reducing accident risks. Despite these advancements, limitations persist in their perception capabilities. To overcome these limitations, interest is growing in using Unmanned Aerial Vehicles (UAVs) for traffic surveillance, offering extensive coverage and enhanced responsiveness over fixed sensors. In this article, by tackling an optimization problem in road safety using Particle Swarm Optimization (PSO), we particularly focus on a situation where a random flow of vehicles aims to navigate an intersection safely. We compare two scenarios with and without the assistance of an UAV: one where vehicles autonomously manage their speed, and another one where an UAV improve traffic management. Simulation results underscore the pivotal role of drone-assisted vehicles in enhancing road safety, compared to sensors embedded within CAVs. Towards the end of the article, we explore the efficiency of the drone strategy by addressing the issue of delay in the acceptance and implementation of optimal speed instructions, comparing scenarios with and without this delay.  © 2024 IEEE.",Acceptability; CAV; drone to vehicle communication (U2V); Multi-Agent system; optimization; road safety; UAV; Advanced traffic management systems; Air navigation; Air traffic control; Aircraft accidents; Aircraft communication; Autonomous vehicles; Highway accidents; Highway administration; Highway traffic control; Magnetic levitation vehicles; Motor transportation; Road vehicles; Street traffic control; Vehicle safety; Acceptability; Aerial vehicle; Autonomous Vehicles; Connected autonomous vehicle; Drone to vehicle communication (U2V); Multiagent systems (MASs); Optimisations; Road safety; Unmanned aerial vehicle; Vehicle communications; Drones
Scopus,"Chang, C.-Y.; Khanum, A.; Su, S.-G.; Lebaku Moses, M.; Liu, K.-X.",Vertical-Line Mura Defect Detection for TFT-LCDs,,2024,,,,10.1109/ACCESS.2024.3486567,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209184562&doi=10.1109%2fACCESS.2024.3486567&partnerID=40&md5=4dd3cc1e308afdfa844270fdb3376b54,"Mura defects, which manifest as irregularities in brightness and color on display screens, present persistent challenges for Thin Film Transistor Liquid Crystal Display (TFT-LCD) manufacturers. While traditional methods effectively detect significant abnormalities in vertical-line inspections, identifying minor defects, particularly Level-1, remains formidable. In this study, we leverage artificial intelligence to address this challenge, focusing specifically on detecting vertical-line mura defects. Given the difficulty in discerning Level-1 defects directly from single images, where higher-level abnormalities are more conspicuous, our approach introduces You Only Look Once-Ghost Attention (YOLO-GA). This advanced YOLOv8-based algorithm is meticulously designed to swiftly and accurately identify vertical lines mura in liquid crystal displays (LCD) images, even amidst complex backgrounds and minor irregularities. To enhance the model's efficacy, we adopt two pivotal strategies. Firstly, we incorporate the Ghost layer as the backbone and neck network to ensure lightweight deployment while improving feature extraction capabilities, particularly in images with intricate backgrounds. Additionally, we integrate the CBAM (Convolutional Block Attention Module) into the network's architecture, explicitly targeting vertical line mura detection. This augmentation aims to bolster feature extraction and refine detection, especially for defects within liquid crystal displays (LCDs). The dataset utilized in our study is sourced from AUO Corporation and captured using a real single-direction camera. The dataset contains 107,080 images, divided into an 80:20 ratio for training and validation. Each image has a high resolution of 1624× 1240$. Our experimental results show that our approach is highly effective, achieving mAP and F1-Scores of 99.5% and 99.7%, respectively, compared to the baseline model and the comparative attention module. Moreover, the proposed model can significantly reduce the time required to recognize mura defects, fully meeting the manufacturer's production requirements within a mere 1-second timeframe.  © 2013 IEEE.",Computer vision; mura defect; smart manufacturing; TFT-LCD; vertical-line mura; Inspection; Liquid crystal displays; Liquid crystals; Process control; Smart manufacturing; Defect detection; Display screen; Features extraction; Level-1; Liquid-crystal display; Mura defect; Smart manufacturing; Thin film transistors liquid crystal display; Vertical lines; Vertical-line mura; Thin film transistors
Scopus,"Wang, S.-Y.; Qu, Z.; Gao, L.-Y.",Multi-Spatial Pyramid Feature and Optimizing Focal Loss Function for Object Detection,,2024,,,,10.1109/TIV.2023.3282996,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161600294&doi=10.1109%2fTIV.2023.3282996&partnerID=40&md5=edfe3bf5e9dc3abdd8802480ba9d73d5,"Previous deep convolutional neural network research has made significant progress toward improving the speed and accuracy of object detection. However, despite these advancements, the inaccurate detection of multi-object (small objects) remains challenging in the traffic environments. In this article, we propose a new architecture called YOLOM, which is specifically designed to achieve enhanced multi-object (small objects) detection precision. YOLOM incorporates several innovative features: a multi-spatial pyramid (MSP), an optimized focal loss (OFLoss) function, and an objectness loss that incorporates effective intersection over union (EIoU) calculations. These features collectively yield enhanced accuracy and reduce the miss rate of small objects, particularly in the multi-object cases. According to the sizes of receptive field features with different spatial scales with pooling layers, we propose the MSP module. We optimize the focal loss as a classification function instead of the cross-entropy loss, which solves some class imbalance problems caused by anchor-free detection when encountering disparate datasets. Due to the superior performance of EIoU in confidence scoring, we use EIoU to participate in the objectness loss calculation of our work. Therefore, our method substitutes EIoU for YOLOX's objectness loss. The experimental results demonstrate that our strategies significantly outperform some end-to-end object detection methods.  © 2016 IEEE.",Convolutional neural networks; multi-spatial pyramid feature; object detection; objectness loss with EIoU; optimizing focal loss function; Classification (of information); Convolution; Deep neural networks; Intelligent vehicle highway systems; Object detection; Object recognition; Convolutional neural network; Features extraction; Loss functions; Multi-spatial pyramid feature; Objectness loss with effective intersection over union; Objects detection; Optimizing focal loss function; Spatial pyramids; Transformer; Feature extraction
Scopus,"Xu, H.; Zhang, X.; He, J.; Geng, Z.; Yu, Y.; Cheng, Y.",Panoptic Water Surface Visual Perception for USVs Using Monocular Camera Sensor,,2024,,,,10.1109/JSEN.2024.3413088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196734650&doi=10.1109%2fJSEN.2024.3413088&partnerID=40&md5=d11a2924840dc5cc3e5553d60079d38a,"In recent years, the significance of unmanned surface vehicles (USVs) has grown substantially across a wide range of applications. Monocular cameras, as the most common perception sensors deployed in USVs, offer the inherent advantages of rich semantic information and low-cost deployment. However, visual perception methods for USVs face challenges from water surface application and lose efficiency in harsh weather cases. To achieve robust perception capabilities through monocular cameras for USVs, we propose a panoptic water surface visual perception framework that can accomplish various perception tasks, including drivable area segmentation, object detection, and raindrop segmentation in water surface scenes. In addition, our framework provides a segmentation anything model (SAM)-driven training method to improve the robustness of the proposed model through low-cost model iteration. Moreover, the proposed perception method demonstrates excellent accuracy, robustness, and high inference speed compared to other lightweight baseline methods and can be deployed to low-power embedded platforms in USVs. 1558-1748  © 2024 IEEE.",Lightweight network; multitask learning; segmentation anything model (SAM) segmentation; unmanned surface vehicles (USVs) perception; visual perception; Cameras; Iterative methods; Job analysis; Object detection; Object recognition; Robotics; Semantic Segmentation; Unmanned surface vehicles; Vision; Head; Images segmentations; Lightweight network; Multitask learning; SAM segmentation; Task analysis; Unmanned surface vehicle  perception; Visual perception; Water surface; Semantics
Scopus,"Wei, C.; Wu, G.; Barth, M.J.",Feature Corrective Transfer Learning: End-to-End Solutions to Object Detection in Non-Ideal Visual Conditions,,2024,,,,10.1109/CVPRW63382.2024.00007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206494782&doi=10.1109%2fCVPRW63382.2024.00007&partnerID=40&md5=9ea2de00aa4ab4a279b12b37b2848e5a,"A significant challenge in the field of object detection lies in the system's performance under non-ideal imaging conditions, such as rain, fog, low illumination, or raw Bayer images that lack ISP processing. Our study introduces 'Feature Corrective Transfer Learning', a novel approach that leverages transfer learning and a bespoke loss function to facilitate the end-to-end detection of objects in these challenging scenarios without the need to convert non-ideal images into their RGB counterparts. In our methodology, we initially train a comprehensive model on a pristine RGB image dataset. Subsequently, non-ideal images are processed by comparing their feature maps against those from the initial ideal RGB model. This comparison employs the Extended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function designed to quantify similarities and integrate them into the detection loss. This approach refines the model's ability to perform object detection across varying conditions through direct feature map correction, encapsulating the essence of Feature Corrective Transfer Learning. Experimental validation on variants of the KITTI dataset demonstrates a significant improvement in mean Average Precision (mAP), resulting in a 3.8-8.1% relative enhancement in detection under non-ideal conditions compared to the baseline model, and a less marginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved under ideal conditions by the standard Faster RCNN algorithm. © 2024 IEEE.",Feature Correction; Non-Ideal Visual Conditions; Object Detection; Transfer Learning; Federated learning; Image enhancement; Transfer learning; Condition; Feature correction; Feature map; Ideal images; Loss functions; Non-ideal visual condition; Nonideal; Objects detection; Transfer learning; Visual condition; Contrastive Learning
Scopus,"Deng, L.; Fu, R.; Li, Z.; Liu, B.; Xue, M.; Cui, Y.",Lightweight cross-modal multispectral pedestrian detection based on spatial reweighted attentionmechanism,,2024,,,,10.32604/cmc.2024.048200,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189247095&doi=10.32604%2fcmc.2024.048200&partnerID=40&md5=da963075dce58c9906cd9a1d62e73063,"Multispectral pedestrian detection technology leverages infrared images to provide reliable information for visible light images, demonstrating significant advantages in low-light conditions and background occlusion scenarios. However, while continuously improving cross-modal feature extraction and fusion, ensuring themodel s detection speed is also a challenging issue. We have devised a deep learning network model for cross-modal pedestrian detectionbasedonResnet50, aiming tofocus onmore reliable features andenhance themodel s detectionefficiency. This model employs a spatial attention mechanism to reweight the input visible light and infrared image data, enhancing the model s focus on different spatial positions and sharing the weighted feature data across different modalities, thereby reducing the interference of multi-modal features. Subsequently, lightweight modules with depthwise separable convolution are incorporated to reduce the model s parameter count and computational load through channel-wise and point-wise convolutions. The network model algorithm proposed in this paper was experimentally validated on the publicly available KAIST dataset and compared with other existing methods. The experimental results demonstrate that our approach achieves favorable performance in various complex environments, affirming the effectiveness of the multispectral pedestrian detection technology proposed in this paper. © 2024 Tech Science Press. All rights reserved.",Convolutional neural networks; Depth separable convolution; Multispectral pedestrian detection; Spatially reweighted attention mechanism; Convolutional neural networks; Deep learning; Feature extraction; Image enhancement; Learning systems; Scattering parameters; Attention mechanisms; Convolutional neural network; Cross-modal; Depth separable convolution; Detection technology; Multi-spectral; Multispectral pedestrian detection; Network models; Pedestrian detection; Spatially reweighted attention mechanism; Convolution
Scopus,"Li, H.; Yu, K.; Qiu, J.; Wang, Z.; Yang, Y.",IA-Det: Iterative Attention-Based Robust Object Detection in Adverse Traffic Scenes,,2024,,,,10.1109/TIM.2024.3438845,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201759313&doi=10.1109%2fTIM.2024.3438845&partnerID=40&md5=2486d2f0c72e4cae9bb9cf8064365edd,"Robust traffic object detection has significant meanings for applications like intelligent transportation and surveillance. However, detection in the open world faces various adverse factors like occlusion, bad weather, and low light. Current endeavors focus on proposing better learnable modules for better feature representation, but their generalization ability is limited by the quality and diversity of training data. In this article, we propose an iterative attention (IA)-Det to achieve robust detection at various challenging transportation scenes by fully exploring semantics and suppressing the noises. First, an IA based on the t-mixture probability model is proposed to collect rich semantics at the localization branch. Unlike earlier attention methods, the IA module inherits the robust characteristic of t-mixture and asymmetrically redistributes the attention to features based on their similarity to semantic centers. Therefore, out-of-distribution features can receive greater opportunities for clustering into their semantic neighbors rather than being suppressed as noises. For the classification branch, an effective and efficient classification responding block (CRB) is designed to activate classification-aware semantics from the collected attention map. Comprehensive experiments on normal, rainy, foggy, and low light fusion datasets show that the proposed IA-Det achieves state-of-the-art performance with an impressive generalization ability.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Attention mechanism; domain generalization; probability model; robust detection; transportation application; Image segmentation; Attention mechanisms; Domain generalization; Features extraction; Generalisation; Location awareness; Noise; Objects detection; Probability modelling; Robust detection; Transportation application; Normal distribution
Scopus,"Guan, R.; Yao, S.; Liu, L.; Zhu, X.; Man, K.L.; Yue, Y.; Smith, J.; Lim, E.G.; Yue, Y.",Mask-VRDet: A robust riverway panoptic perception model based on dual graph fusion of vision and 4D mmWave radar,,2024,,,,10.1016/j.robot.2023.104572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176149010&doi=10.1016%2fj.robot.2023.104572&partnerID=40&md5=40c8ea5a69d8cc4636a641bc6abba89d,"With the development of Unmanned Surface Vehicles (USVs), the perception of inland waterways has become significant to autonomous navigation. RGB cameras can capture images with rich semantic features, but they would fail in adverse weather and at night. As a perception sensor that has initially emerged in recent years, 4D millimeter-wave radar (4D mmWave radar) can work in all weather and has more abundant point-cloud features than ordinary radar, but it also suffers from water-surface clutter seriously. Furthermore, the shape and outline of dense point cloud captured by 4D mmWave radar are irregular. CNN-based neural networks treat features as 2D rectangle grids, which excessively favor image modality and are unfriendly to radar modality. Therefore, we transform both features of image and radar into non-Euclidean space as graph structures. In this paper, we focus on robust panoptic perception in inland waterways. Firstly, we propose the first Clutter-Point-Removal (CPR) algorithm for 4D mmWave radar, removing water-surface clutter and improving the recall of radar targets. Secondly, we propose a high-performance panoptic perception model based on the graph neural network called Mask-VRDet, fusing features of vision and radar to simultaneously perform object detection and semantic segmentation. To the best of our knowledge, Mask-VRDet is the first riverway panoptic perception model based on vision-radar graphical fusion. It outperforms other single-modal and fusion models, and achieves state-of-the-art performance on our collected dataset. We release our code at https://github.com/GuanRunwei/Mask-VRDet-Official. © 2023 Elsevier B.V.",Fusion of vision and radar; Graph convolution network; Radar clutter removal; Riverway panoptic perception; Clutter (information theory); Graphic methods; Image segmentation; Millimeter waves; Object detection; Object recognition; Radar imaging; Semantics; Surface waters; Unmanned surface vehicles; Fusion of vision and radar; Graph convolution network; Millimeter-wave radar; Millimetre-wave radar; Model-based OPC; Perception model; Point-clouds; Radar clutter removal; Riverway panoptic perception; Water surface; Radar clutter
Scopus,"Kaur, J.; Kaur, N.",Enhancing Object Detection in Autonomous Driving with Improved-YOLOv8,,2024,,,,10.1049/icp.2025.0845,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003572005&doi=10.1049%2ficp.2025.0845&partnerID=40&md5=6a96cb19c2dd8da6637a720fe4de4612,"Autonomous driving systems rely on object recognition to perform localization and classification, among other essential functions. Even though deep learning has greatly improved object identification, training periods are frequently rather lengthy due to the large number of parameters. The Improved-YOLOv8 model, which uses a pruning technique to decrease the number of parameters and speed up training, is presented to solve this problem. The effectiveness of the suggested method is shown by experiments run on the KITTI dataset, which is widely used in autonomous driving research. © The Institution of Engineering & Technology 2024.",autonomous driving; deep learning; object detection; yolo; Object recognition; Autonomous driving; Deep learning; Driving systems; Localisation; Object identification; Objects detection; Objects recognition; Pruning techniques; Speed up; Yolo; Object detection
Scopus,"Yang, B.; Yang, J.",SITAR: Evaluating the Adversarial Robustness of Traffic Light Recognition in Level-4 Autonomous Driving,,2024,,,,10.1109/IV55156.2024.10588456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199752181&doi=10.1109%2fIV55156.2024.10588456&partnerID=40&md5=e46c79189a7e605edcb22a76cc36ad9b,"Traffic Light Recognition (TLR) is vital for Autonomous Driving Systems as it supplies critical information at intersections. Modern TLRs leverage camera and geolocation data, incorporating complex pre-(post)-processing steps and multiple deep learning (DL) models for detecting, recognizing, and tracking traffic lights. While the adversarial robustness of standalone DL models has been extensively studied, the robustness of a modern TLR system, i.e., a complex software component with code and DL models, is rarely studied and hence requires research efforts.In this work, we propose a novel testing framework (namely SITAR) targeting TLR modules from a representative Level-4 ADS, such as Baidu Apollo and Autoware. We design a novel adversarial attack loss function to evaluate and improve the adversarial robustness of modern TLR systems. We applied SITAR on Apollo TLR and compared our novel loss function with the state-of-the-art approaches that can effectively attack object detection and image recognition models. SITAR is shown to be effective and our novel loss function performs better than previous SOTAs with a 93% to 100% success rate with a maximum of five-step iteration and eight pixels per perturbation.  © 2024 IEEE.",Autonomous vehicles; Deep learning; Function evaluation; Image recognition; Intelligent systems; Intelligent vehicle highway systems; Autonomous driving; Driving systems; Geolocations; Learning models; Level 4; Loss functions; Post-processing; Processing steps; Recognition systems; Traffic lights recognition; Object detection
Scopus,"Pang, J.; Zhou, Y.",SODet: A LiDAR-Based Object Detector in Bird’s-Eye View,,2024,,,,10.1007/978-981-99-8148-9_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178637306&doi=10.1007%2f978-981-99-8148-9_7&partnerID=40&md5=bfca90cc9ab34d6c95e36f56eac3b4cf,"LiDAR-based object detection is of paramount significance in the realm of autonomous driving applications. Nevertheless, the detection of small objects from a bird’s-eye view perspective remains challenging. To address this issue, the paper presents SODet, an efficient single-stage 3D object detector designed to enhance the perception of small objects like pedestrians and cyclists. SODet incorporates several key components and techniques. To capture broader context information and augment the capability of feature representation, the model constructs residual blocks comprising large-kernel depthwise convolutions and inverted bottleneck structures, forming the foundation of the CSP-based NeXtDark backbone network. Furthermore, the NeXtFPN feature extraction network is designed with the introduced SPPF module and the proposed special residual blocks, enabling the extraction and fusion of multi-scale information. Additionally, training strategies such as mosaic data augmentation and cosine annealing learning rate are employed to further improve small object detection accuracy. The effectiveness of SODet is demonstrated through experimental results on the KITTI dataset, showcasing a remarkable enhancement in detecting small objects from a bird’s-eye perspective while maintaining a detection speed of 20.6 FPS. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Autonomous Driving; Bird’s-Eye View; LiDAR Point Cloud; Object Detection; Autonomous vehicles; Birds; Extraction; Object recognition; Optical radar; 3D object; Autonomous driving; Bird’s-eye view; Context information; LiDAR point cloud; Object detectors; Objects detection; Point-clouds; Single stage; Small objects; Object detection
Scopus,"Han, Y.; Wang, F.; Wang, W.; Li, X.; Zhang, J.",YOLO-SG: Small traffic signs detection method in complex scene,,2024,,,,10.1007/s11227-023-05547-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165662470&doi=10.1007%2fs11227-023-05547-y&partnerID=40&md5=fa102339688667e9bcb8be26e6c93dd6,"Fast and accurate detection of traffic signs is crucial for the development of intelligent transportation systems. To address the issue of false detection and missing detection of small traffic signs in complex scenes, this paper proposes a YOLO-SG model based on YOLOv5. The YOLO-SG approach employs SPD-Conv as a down-sampling structure to mitigate the loss of feature information during the down-sampling process. This enhances the detection performance of small objects in complex scenes and improves the generalization and robustness of the model. The feature extraction architecture uses GhostNet, which effectively reduces the number of model parameters and weight, enhancing the feasibility of practical model deployment. Furthermore, this study optimizes the output feature structure by introducing a small object detection layer and removing the large object detection layer, enabling the detection of small objects. Extensive experiments conducted on the GTSDB and TT100K datasets demonstrate that YOLO-SG exhibits excellent detection performance. On the GTSDB dataset, YOLO-SG achieved a 2.3% increase in mAP compared to the baseline network, while reducing the number of parameters by 42%. Similarly, on the TT100K dataset, YOLO-SG increased mAP by 6.3% and reduced the number of parameters by 43%. These experimental results showcase the effectiveness and accuracy of YOLO-SG, particularly in detecting small traffic signs. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Deep learning; Small object detection; Traffic sign detection; YOLOv5; Complex networks; Deep learning; Feature extraction; Intelligent systems; Object recognition; Signal sampling; Traffic signs; Complex scenes; Deep learning; Detection methods; Detection performance; Down sampling; Intelligent transportation systems; Small object detection; Small objects; Traffic sign detection; YOLOv5; Object detection
Scopus,"Khow, Z.J.; Tan, Y.-F.; Karim, H.A.; Rashid, H.A.A.",Improved YOLOv8 Model for a Comprehensive Approach to Object Detection and Distance Estimation,,2024,,,,10.1109/ACCESS.2024.3396224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192151007&doi=10.1109%2fACCESS.2024.3396224&partnerID=40&md5=a5ca25aadb53811222c0a43949b7dc6d,"The rapid advancements in deep learning have revolutionized the field of computer vision. However, despite the significant progress in computer vision, there remains a scarcity of research focused on utilizing this technology for distance estimation. Exploring such studies can bring immense convenience to people, especially in applications like anomaly object detection. On that account, this research proposes an improved detection model based on You Only Look Once version 8 (YOLOv8) namely YOLOv8-CAW, which is capable of both detecting target objects and accurately calculating their distances. The proposed method involves incorporating the Coordinate Attention and Wise-IoU into the YOLOv8 network, enhancing the detection accuracy. Combined with the distance estimation algorithm, results in a comprehensive output that includes both detection results and calculated distances. At the end of the experiment, a substantial improvement in performance metrics was observed, the model achieved increases in recall (0.4%), precision (2.2%), and Mean Average Precision (mAP) (1.5%) within the 0.5 to 0.95 threshold range, while maintaining inference speeds similar to the baseline model in PASCAL VOC dataset. Besides that, distance estimation achieved an approximate average accuracy of 90% which shows the results are highly encouraging and promising. The successful integration of computer vision and distance estimation opens new possibilities for practical applications, showcasing the potential of this approach in real-world scenarios.  © 2013 IEEE.",attention module; deep learning; distance estimation; loss function; object detection; You only look once (YOLO); Computer vision; Deep learning; Feature extraction; Object recognition; Semantics; Attention module; Computational modelling; Context models; Deep learning; Distance estimation; Features extraction; Loss functions; Objects detection; You only look once; Object detection
Scopus,"Ge, Q.; Da, W.; Wang, M.",MARFPNet: Multiattention and Adaptive Reparameterized Feature Pyramid Network for Small Target Detection on Water Surfaces,,2024,,,,10.1109/TIM.2024.3485463,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208384132&doi=10.1109%2fTIM.2024.3485463&partnerID=40&md5=23a0d04895f9f2cad9c5869cd8643b99,"The images captured by unmanned aerial vehicles (UAVs) are often limited in scale and feature information, making it challenging for current detection algorithms to learn the features of objects effectively. This limitation hampers accurate identification of small objects on water surfaces. We introduce a multiattention and adaptive reparameterized feature pyramid network for small target detection on water surfaces (MARFPNet) to tackle this issue. First, to address the loss of small object features during extraction, we improved the attention mechanism based on the characteristics of small objects and proposed a multiattention module, integrating it into the feature extraction process. Second, to address the semantic information of small objects being retained mostly in shallow feature maps and not fully utilized, we introduced an adaptive reparameterized generalized feature pyramid network (Adaptive_RepGFPN). This module reorganizes features, expands the fusion scale, and incorporates adaptive weighting in the concat operation. Third, to overcome the challenge of ineffective restoration of feature map information by upsampling, we introduce the Dysample. Finally, to address the problem of the loss function being sensitive to scale changes, we propose the normalized Wasserstein distance (NWD) loss function to reduce the sudden drop in loss due to scale changes. We conducted experiments on VisDrone, SeaDronsSee, and the self-build dataset. MARFPNet showed higher accuracy compared to other detection algorithms. Notably, on the self-build dataset, mAP50 and mAP50:95 improved by 9.1% and 3.5% over the baseline network. This demonstrates MARFPNet's effectiveness and suitability for detecting small targets in drone aerial photography on water surfaces.  © 1963-2012 IEEE.",Deep learning; feature fusion; object detection; small target; Aerial photography; Aircraft detection; Image reconstruction; Medical imaging; Photomapping; Scales (weighing instruments); Deep learning; Feature map; Feature pyramid; Features fusions; Objects detection; Pyramid network; Small objects; Small target detection; Small targets; Water surface; Unmanned aerial vehicles (UAV)
Scopus,"Zhang, Q.; Wang, C.; Li, H.; Shen, S.; Cao, W.; Li, X.; Wang, D.",Improved YOLOv8-CR Network for Detecting Defects of the Automotive MEMS Pressure Sensors,,2024,,,,10.1109/JSEN.2024.3419806,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197549896&doi=10.1109%2fJSEN.2024.3419806&partnerID=40&md5=1237a3939998eb3bb42c9e535a25747b,"Micro-electro-mechanical system (MEMS) sensors have been widely used in the automotive field owing to their small size, low cost, and high reliability. Various defects are caused by complicated environments and multistep processes in the manufacturing process of automotive MEMS pressure sensors. These defects are often small in size but can significantly impact the performance of products. The traditional detection method is inefficient and inaccurate. In this study, an improved you only look once v8 (YOLOv8) C2f-RFCBAMConv (CR) network is proposed to detect the defects of the automotive MEMS pressure sensors. The network is an improved version of YOLOv8. The faster version of the CSP bottleneck with two convolutions (C2f) and receptive-field convolutional block attention (RFCBAMConv) modules are applied. The ablation experiments confirm the impact of C2f and RFCBAMConv modules on network performance. The improved YOLOv8-CR network can effectively enhance the accuracy and speed of defect detection of the automotive MEMS pressure sensor. Furthermore, comparisons are made between the improved YOLOv8-CR network and other networks such as YOLOv8-large, YOLOv5, and faster region-based convolutional neural network (Fast-RCNN), demonstrating its effectiveness in identifying five types of defects in the automotive MEMS pressure sensors. The improved YOLOv8-CR exhibits considerably superior defect detection capabilities with a mean average precision (mAP) of 94.98% and a single picture detection time of 42.68 ms on the test set. The improved YOLOv8-CR network is helpful in realizing real-time and accurate online monitoring of product quality in the key process of the automotive MEMS pressure sensors. © 2001-2012 IEEE.",Defect detection; faster version of CSP bottleneck with two convolutions (C2f); receptive-field convolutional block attention (RFCBAMConv); you only look once v8 (YOLOv8) network; Convolution; MEMS; Pressure sensors; Automotives; Defect detection; Detecting defects; Fast version of CSP bottleneck with two convolution (c2f); Low-costs; MEMS (microelectromechanical system); Receptive fields; Receptive-field convolutional block attention (RFCBAMConv); System Pressure sensors; You only look once v8  network; Defects
Scopus,"Sun, H.; Chen, W.; Zhang, C.; Zhang, Z.",Deep Hierarchical Rear-Lamp Tracking at Nighttime,,2024,,,,10.1002/tee.23951,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177825068&doi=10.1002%2ftee.23951&partnerID=40&md5=21f495c4bc2577f1bac399b46e5f7566,"Rear-lamp tracking at night is a research topic in night vision that is essential for Advanced Driver Assistance Systems (ADAS). Most current computer vision-based methods address this problem using color features because rear lamps are lit during night driving. However, such methods are sufficiently robust in complex environments owing to the lack of feature diversity. On the other hand, as there is no off-the-shelf dataset, the application of deep learning to nighttime rear-lamp tracking has been sparsely explored. To tackle the above issues, in this paper, we propose hierarchical rear-lamp tracking at nighttime (H-RTN) and create a novel dataset for rear-lamp tracking. Specifically, the H-RTN consists of a rough detection hierarchy (R-hierarchy), an accurate detection hierarchy (A-hierarchy), and an optimization hierarchy (O-hierarchy). The R-hierarchy determines the region of interest (ROI) containing the target rear lamps, the A-hierarchy samples the rear-lamp candidates in the ROI, and the O-hierarchy selects the best pair of candidates as the final location of the target rear lamps. The experimental results show that H-RTN outperforms the alternative existing methods. © 2023 Institute of Electrical Engineer of Japan and Wiley Periodicals LLC. © 2023 Institute of Electrical Engineer of Japan and Wiley Periodicals LLC.",deep learning; night vision; rear-lamp tracking; vehicle detection; Advanced driver assistance systems; Automobile drivers; Image segmentation; Lighting; Vision; 'current; Deep learning; Night vision; Optimisations; Rear-lamp tracking; Region-of-interest; Regions of interest; Research topics; Vehicles detection; Vision-based methods; Deep learning
Scopus,"Pei, C.; Zhang, S.; Cao, L.; Zhao, L.",ContextNet: Leveraging Comprehensive Contextual Information for Enhanced 3D Object Detection,,2024,,,,10.1109/ACCESS.2024.3437642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200207765&doi=10.1109%2fACCESS.2024.3437642&partnerID=40&md5=a1a753d878670d0b3fdcf79a25ffa126,"The progress in object detection for autonomous driving using LiDAR point cloud data has been remarkable. However, current voxel-based two-stage detectors have not fully capitalized on the wealth of contextual information present in the point cloud data. Typically, Voxel Feature Encoding (VFE) layers tend to focus exclusively on internal voxel information, neglecting the broader context. Additionally, the process of extracting 3D proposal features through Region of Interest (ROI) spatial quantization and pooling downsampling results in a loss of spatial detail within the proposed regions. This limitation in capturing contextual details presents challenges for accurate object detection and positioning, particularly over long distances. In this paper, we propose ContextNet, which leverages comprehensive contextual information for enhanced 3D object detection. Specifically, it comprises two modules: the Voxel Self-Attention Encoding module (VSAE) and the Joint Channel Self-Attention Re-weight module (JCSR). VSAE establishes dependencies between voxels through self-attention, expanding the receptive field and introducing substantial contextual information. JCSR employs joint attention to extract both local channel information and global context information from the raw point cloud within the RoI region. By integrating these two sets of information and re-weighting the point features, the 3D proposal is refined, enabling a more accurate estimation of the object's position and confidence. Extensive experiments conducted on the KITTI dataset demonstrate that our approach outperforms voxel-based two-stage methods, particularly with a 9.5% improvement in the mAP compared to the baseline on the nuScenes test dataset, and an improved 1.61% hard AP compared to the baseline on the KITTI benchmark. © 2013 IEEE.",3D object detection; Autonomous driving; LiDAR sensor; Computer vision; Encoding (symbols); Image segmentation; Object recognition; Optical radar; Signal encoding; Statistical tests; 'current; 3D object; 3d object detection; Autonomous driving; Contextual information; Encoding modules; Encodings; LiDAR sensor; Objects detection; Point cloud data; Object detection
Scopus,"Mukherjee, S.; Beard, C.; Li, Z.",MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO,,2024,,,,10.1109/ICIP51287.2024.10648081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216857933&doi=10.1109%2fICIP51287.2024.10648081&partnerID=40&md5=6cad6cdda4216af7781dd352411436b4,"Low-light conditions and occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing “YOLO Phantom"", one of the smallest YOLO models ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating-Point Operations (GFLOPs). YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection. Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the multimodal dataset are available on GitHub. © 2024 IEEE.",IoT; Low light object detection; Multimodal fusion; Phantom Convolution; YOLO; Benchmarking; Image coding; Image segmentation; Network security; Transfer learning; Low light; Low light conditions; Low light object detection; Multi-modal; Multi-modal fusion; Objects detection; Phantom convolution; Phantoms; Real-world; YOLO; Phantoms
Scopus,"Xiong, Z.; Zou, L.; Wei, Y.; Yu, H.",Optimizing Automotive Component Assembly Inspection via an Advanced Detection Model Based on Faster R-CNN,,2024,,,,10.1109/ICAIRC64177.2024.10900029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000631149&doi=10.1109%2fICAIRC64177.2024.10900029&partnerID=40&md5=18efcc80c46e4c803bfbef593da9c4f7,"In the field of automotive manufacturing, the precision of assembly processes is critical for ensuring product quality and efficient cost management.To address the inefficiency and unreliability of traditional manual inspection techniques,this study proposes an advanced detection model based on the Faster R-CNN framework for evaluating the assembly status of eight critical components within a vehicle's engine bay.Firstly, deformable convolutions(DCN) are incorporated into the backbone network to enhance adaptability to objects of varying scales.Secondly, a Balanced Feature Pyramid Network (BFPN) is adopted to improve feature integration and minimize information loss across different feature levels.Lastly, a Dynamic Label Assignment (DLA)mechanism is introduced, which adjusts the Intersection over Union (IoU) threshold to improve the quality of positive sample classification.Experimental results demonstrate that the improved model achieves a mean Average Precision (mAP) of 87.1%, representing a 2.3% improvement over the baseline Faster R-CNN model.  © 2024 IEEE.",Balanced Feature Pyramid Network; component; Deformable Convolutions; Faster R-CNN; Object Detection; Automobile engine manufacture; Automobiles; Failure analysis; Advanced detections; Balanced feature pyramid network; Component; Deformable convolution; Detection models; Fast R-CNN; Feature pyramid; Model-based OPC; Objects detection; Pyramid network; Inspection
Scopus,"Wang, H.; Liu, Y.; Li, G.; Diao, X.; Zheng, Q.",Multi-Level Feature Fusion Based UAV Object Detection Method Under Foggy Weather,,2024,,,,10.1109/ICFTIC64248.2024.10913360,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001670144&doi=10.1109%2fICFTIC64248.2024.10913360&partnerID=40&md5=8a6daf7cbf1564251bb40a87bfefcd30,"Adverse weather conditions can blur aerial images captured by optical cameras on UAVs, complicating the already challenging task of detecting small-sized targets from a UAV's perspective. In this paper, a novel model called DU-YOLO is proposed. Firstly, an FFA-Net-based defog module is introduced, ensuring effective image preprocessing while meeting real-time requirement. Then, a multi-level feature fusion module based on the squeeze-excitation attention mechanism is proposed to enhance information extraction and detail preservation. Additionally, the focaler-MPDIoU loss function is designed to enhance the learning ability for small target samples and accelerate convergence. Finally, experimental results are tested on the synthetic fog dataset VisDrone-Fog, demonstrating that DU-YOLO achieves a 4.9% increase on mAP compared to the baseline model, YOLOvlOs.  © 2024 IEEE.",feature fusion; foggy weather condition; object detection; synthetic fog dataset; UAV; Aircraft detection; Fog computing; Image enhancement; Adverse weather; Aerial images; Condition; Features fusions; Foggy weather condition; Multilevels; Object detection method; Objects detection; Optical camera; Synthetic fog dataset; Aerial photography
Scopus,"Tang, Z.; Zhu, L.; Wang, W.; Gong, Y.",PE-SSD:Improved SSD For Small Object Detection,,2024,,,,10.1109/CAC63892.2024.10865724,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000793969&doi=10.1109%2fCAC63892.2024.10865724&partnerID=40&md5=00536ba6e2d46bd521290c018a4a6203,"With the flourishing development of deep learning, object detection technology has achieved encouraging results. However, small objects remain a major challenge in the field of object detection due to their lack of texture and contextual information caused by their small size. Despite such fact, we observed that their edges typically exhibit high contrast in relation to the background. Specifically, we innovatively employ a Laplacian image pyramid incorporated with feature enhancement modules rather than the traditional image pyramid, with the aim of more effectively capturing image details and edge information, given that the Laplacian pyramid has the ability to retain such information within its lower levels. Furthermore, we introduce attention mechanisms to optimize the feature fusion process, thus improving the utilization efficiency of multi-scale features. Taking SSD as the baseline, we propose a novel method called PE-SSD, which integrates the advantages of featureized Laplacian pyramid. Experimental results show that our method achieves higher accuracy in detecting small objects compared to traditional SSD. For inputs of size 512 × 512, the extensive experiments on the VisDrone2019 dataset demonstrates that our method achieves an average precision of 15.4%, surpassing traditional SSD networks by 1.2%. Specifically, The mAP_s score reaches 6.6%, which is 0.4% higher than the baseline network, indicating the effectiveness of our method in the field of small object detection. © 2024 IEEE.",Attention Module; Feature Fusion; Feature Pyramid; Laplacian Pyramid; Small Object Detection; Contrastive Learning; Deep learning; Feature extraction; Image enhancement; Object detection; Object recognition; Attention module; Detection technology; Feature pyramid; Features fusions; Image pyramids; Laplacian Pyramid; Learning objects; Objects detection; Small object detection; Small objects; Laplace transforms
Scopus,"Fu, H.; Liu, H.; Yuan, J.; He, X.; Lin, J.; Li, Z.",YOLO-Adaptor: A Fast Adaptive One-Stage Detector for Non-Aligned Visible-Infrared Object Detection,,2024,,,,10.1109/TIV.2024.3393015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191685914&doi=10.1109%2fTIV.2024.3393015&partnerID=40&md5=cc0131af2815adcbddc1d211d9272faf,"Visible-infrared object detection has attracted increasing attention recently due to its superior performance and cost-efficiency. Most existing methods focus on the detection of strictly-aligned data, significantly limiting its practical applications. Although several researchers have attempted to explore weakly-aligned visible-infrared object detection, they are limited to small translational deviations and suffer from a low detection speed. This paper first explores non-aligned visible-infrared object detection with complex deviations in translation, scaling, and rotation, and proposes a fast one-stage detector YOLO-Adaptor, which introduces a lightweight multi-modal adaptor to simultaneously predict alignment parameters and confidence weights between modalities. The adaptor adopts a feature-level alignment during the feature extraction process, ensuring high alignment efficiency. Moreover, we introduce a feature contrastive learning loss to guide the alignment learning of the adaptor, aiming to reduce the representation gap between the two modalities in hyperbolic space to implement feature spatial and distributional consistency. Extensive experiments are conducted on three datasets, including one weakly-aligned and two non-aligned datasets, and the experimental results demonstrate that YOLO-Adaptor could achieve significant performance improvements in terms of speed and accuracy. © 2016 IEEE.",adaptor; cross-modal feature alignment; multi-modal fusion; Visible-infrared object detection; Alignment; Autonomous vehicles; Efficiency; Extraction; Feature extraction; Object recognition; Adaptor; Autonomous Vehicles; Cross-modal; Cross-modal feature alignment; Feature alignment; Features extraction; Infrared object; Multi-modal fusion; Objects detection; Self-supervised learning; Visible-infrared object detection; YOLO; Object detection
Scopus,"Rao Muvva, V.V.R.M.K.; Joseph, K.T.; Samal, K.; Wolf, M.; Pitla, S.",Adaptive Perception Control for Aerial Robots with Twin Delayed DDPG,,2024,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196550044&partnerID=40&md5=51f55098008c31d9e79a21ce3eef5a6b,"Robotic perception is commonly assisted by convo-lutional neural networks. However, these networks are static in nature and do not adjust to changes in the environment. Additionally, these are computationally complex and impose latency in inference. We propose an adaptive perception system that changes in response to the robot's requirements. The perception controller has been designed using a recently proposed reinforcement learning technique called Twin Delayed DDPG (TD3). Our proposed method outperformed the baseline approaches. © 2024 EDAA.",Closed loop systems; Control Systems; Deep Learning; Drone; Neural Networks; UAS; UAV; Adaptive control systems; Aircraft control; Antennas; Closed loop control systems; Drones; Learning systems; Reinforcement learning; Aerial robots; Closed-loop system; Deep learning; Neural-networks; Perception systems; Reinforcement learning techniques; UAS; Deep learning
Scopus,"Deshmukh, P.; Chaitanya Rayasam, K.; Kumar Sahoo, U.; Kumar Das, S.; Majhi, S.",Multi-Class Vehicle Detection Using VDnet in Heterogeneous Traffic,,2024,,,,10.1109/TITS.2024.3476122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207765663&doi=10.1109%2fTITS.2024.3476122&partnerID=40&md5=44e5fcbbe35b82b22ac6f461e286e320,"Intelligent vehicles detection (IVD) provides information to manage traffic efficiently, drive autonomous vehicles and feed data to intelligent traffic management systems (ITMS). IVD is a challenging task for the close proximity vehicles in lane-less traffic and heterogeneous systems. Most vehicle detection models are complex and limited to multi-scale feature extraction due to the involvement of existing feature extraction backbones. Also, they do not include heterogeneous traffic vehicles, usually present in developing countries. Therefore, this paper proposes a multi-class vehicle detection (MCVD) model to detect vehicles in heterogeneous traffic using a realistic traffic dataset from a developing country. MCVD is a deep learning (DL) model that consists of a convolutional neural network backbone called VDnet, a light fusion bi-directional feature pyramid network (LFBFPN) and a modified vehicle detection head (MVDH). VDnet extracts multi-scale features from the traffic input images using feature reuse methods. LFBFPN combines these features bi-directionally and provides robust feature maps. Finally, MVDH is applied to detect multi-class vehicles and classify them into respective categories. The proposed model achieves 91.45% mean average precision (mAP) on the heterogeneous traffic labeled dataset (HTLD). The proposed MCVD is tested over Nvidia Jetson TX2 edge computing boards to verify the real-time performance. It achieves 17 frames per second (FPS) on TX2. The performance evaluation results indicate that the proposed MCVD model is fast, accurate and better than the existing works.  © 2024 IEEE.",Convolutional neural network; feature reuse method; heterogeneous traffic; intelligent vehicle detection; Advanced traffic management systems; Deep neural networks; Highway administration; Reusability; Street traffic control; Bi-directional; Convolutional neural network; Detection models; Feature reuse; Feature reuse method; Features extraction; Heterogeneous traffic; Intelligent vehicle detection; Multi-scale features; Vehicles detection; Convolutional neural networks
Scopus,"Hanzla, M.; Yusuf, M.O.; Al Mudawi, N.; Sadiq, T.; Almujally, N.A.; Rahman, H.; Alazeb, A.; Algarni, A.",Vehicle recognition pipeline via DeepSort on aerial image datasets,,2024,,,,10.3389/fnbot.2024.1430155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202653909&doi=10.3389%2ffnbot.2024.1430155&partnerID=40&md5=35b92cd5fb4c22fcda5ddae1ec100991,"Introduction: Unmanned aerial vehicles (UAVs) are widely used in various computer vision applications, especially in intelligent traffic monitoring, as they are agile and simplify operations while boosting efficiency. However, automating these procedures is still a significant challenge due to the difficulty of extracting foreground (vehicle) information from complex traffic scenes. Methods: This paper presents a unique method for autonomous vehicle surveillance that uses FCM to segment aerial images. YOLOv8, which is known for its ability to detect tiny objects, is then used to detect vehicles. Additionally, a system that utilizes ORB features is employed to support vehicle recognition, assignment, and recovery across picture frames. Vehicle tracking is accomplished using DeepSORT, which elegantly combines Kalman filtering with deep learning to achieve precise results. Results: Our proposed model demonstrates remarkable performance in vehicle identification and tracking with precision of 0.86 and 0.84 on the VEDAI and SRTID datasets, respectively, for vehicle detection. Discussion: For vehicle tracking, the model achieves accuracies of 0.89 and 0.85 on the VEDAI and SRTID datasets, respectively. Copyright © 2024 Hanzla, Yusuf, Al Mudawi, Sadiq, Almujally, Rahman, Alazeb and Algarni.",deep learning; DeepSort; dynamic environments; object recognition; path planning; remote sensing; unmanned aerial vehicles; Adaptive boosting; Aerial photography; Aircraft detection; Image segmentation; Kalman filters; Motion planning; Aerial images; Aerial vehicle; Deep learning; Deepsort; Dynamic environments; Image datasets; Objects recognition; Remote-sensing; Unmanned aerial vehicle; Vehicle recognition; accuracy; aerial image dataset; Article; autonomous vehicle; computer vision; deep learning; DeepSORT; image analysis; image enhancement; image processing; image quality; image reconstruction; image segmentation; information processing; learning algorithm; machine learning; pipeline; recognition; traffic; unmanned aerial vehicle; vehicle tracking; virtual reality; Unmanned aerial vehicles (UAV)
Scopus,"Malligere Shivanna, V.; Guo, J.-I.","Object Detection, Recognition, and Tracking Algorithms for ADASs—A Study on Recent Trends",,2024,,,,10.3390/s24010249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181895213&doi=10.3390%2fs24010249&partnerID=40&md5=edf0aabc75d5a029ca16049ae6a6e102,"Advanced driver assistance systems (ADASs) are becoming increasingly common in modern-day vehicles, as they not only improve safety and reduce accidents but also aid in smoother and easier driving. ADASs rely on a variety of sensors such as cameras, radars, lidars, and a combination of sensors, to perceive their surroundings and identify and track objects on the road. The key components of ADASs are object detection, recognition, and tracking algorithms that allow vehicles to identify and track other objects on the road, such as other vehicles, pedestrians, cyclists, obstacles, traffic signs, traffic lights, etc. This information is then used to warn the driver of potential hazards or used by the ADAS itself to take corrective actions to avoid an accident. This paper provides a review of prominent state-of-the-art object detection, recognition, and tracking algorithms used in different functionalities of ADASs. The paper begins by introducing the history and fundamentals of ADASs followed by reviewing recent trends in various ADAS algorithms and their functionalities, along with the datasets employed. The paper concludes by discussing the future of object detection, recognition, and tracking algorithms for ADASs. The paper also discusses the need for more research on object detection, recognition, and tracking in challenging environments, such as those with low visibility or high traffic density. © 2023 by the authors.",advanced driver assistance system (ADAS); deep learning; object detection; object tracking; Accidents; Advanced driver assistance systems; Automobile drivers; Deep learning; Object recognition; Roads and streets; Tracking (position); Traffic signs; Vehicles; Advanced driver assistance system; Deep learning; Object detection algorithms; Object recognition algorithm; Object Tracking; Object tracking algorithm; Objects detection; Potential hazards; Recent trends; Traffic light; algorithm; camera; cyclist; deep learning; electric potential; pedestrian; review; sensor; telecommunication; visibility; Object detection
Scopus,"Zhang, L.J.; Fang, J.J.; Liu, Y.X.; Feng Le, H.; Rao, Z.Q.; Zhao, J.X.",CR-YOLOv8: Multiscale Object Detection in Traffic Sign Images,,2024,,,,10.1109/ACCESS.2023.3347352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181564845&doi=10.1109%2fACCESS.2023.3347352&partnerID=40&md5=9184a1356ad19b595e2c36262ff427b6,"Due to the large-scale changes of different forms of traffic signs and the rapid speed of vehicles, the detection accuracy and real-time performance of general object detectors are greatly challenged, especially the detection accuracy of small objects. In order to solve this problem, a multi-scale traffic sign detection model CR-YOLOv8 is proposed based on the latest YOLOv8. In the feature extraction stage, the attention module is introduced to enhance the channel and spatial features, so that the network can learn the key information of the small objects more easily. The RFB module is introduced in the feature fusion stage, which improves the feature diversity with less computational overhead and improves the network's ability to detect multi-scale objects. By improving the loss function to enable the model to effectively balance multi-scale objectives during training, the model generalization ability is improved.The experimental results on TT100k dataset show that compared with the baseline network, the average detection accuracy of the improved method is increased by 2.3 %, and the detection accuracy of small objects is increased by 1.6 %, which effectively reduces the detection accuracy gap among different scales.  © 2023 The Authors.",Traffic sign recognition; traffic sign recognition; YOLOv8; Extraction; Object detection; Object recognition; Traffic signs; Detection accuracy; Features extraction; Image color analysis; Sensitivity; Small objects; Solid modelling; Traffic sign recognition; YOLO; YOLOv8; Feature extraction
Scopus,"Eggert, M.; Schade, M.; Bröhl, F.; Moriz, A.",Generating Synthetic LiDAR Point Cloud Data for Object Detection Using the Unreal Game Engine,,2024,,,,10.1007/978-3-031-61175-9_20,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195277231&doi=10.1007%2f978-3-031-61175-9_20&partnerID=40&md5=d035b891b80bb2f96afa74320efe9b2a,"Object detection based on artificial intelligence is ubiquitous in today’s computer vision research and application. The training of the neural networks for object detection requires large and high-quality datasets. Besides datasets based on image data, datasets derived from point clouds offer several advantages. However, training datasets are sparse and their generation requires a lot of effort, especially in industrial domains. A solution to this issue offers the generation of synthetic point cloud data. Based on the design science research method, the work at hand proposes an approach and its instantiation for generating synthetic point cloud data based on the Unreal Engine. The point cloud quality is evaluated by comparing the synthetic cloud to a real-world point cloud. Within a practical example the applicability of the Unreal Game engine for synthetic point cloud generation could be successfully demonstrated. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",neural network; object detection; point cloud; synthetic data generation; Unreal; Large datasets; Object recognition; Game Engine; Neural-networks; Objects detection; Point cloud data; Point-clouds; Research and application; Synthetic data generations; Unreal; Vision applications; Vision research; Object detection
Scopus,"Yu, Z.; Zheng, X.; Yang, J.; Su, J.",Improved YOLOX-DeepSORT for Multitarget Detection and Tracking of Automated Port RTG,,2024,,,,10.1109/OJIES.2024.3388632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190725960&doi=10.1109%2fOJIES.2024.3388632&partnerID=40&md5=61e964472e6a36bc7db3cbe46f671ee0,"Rubber tire gantry (RTG) plays a pivotal role in facilitating efficient container handling within port operations. Conventional RTG, highly depending on human operations, is inefficient, labor-intensive, and also poses safety issues in adverse environments. This article introduces a multitarget detection and tracking (MTDT) algorithm specifically tailored for automated port RTG operations. The approach seamlessly integrates enhanced YOLOX for object detection and improved DeepSORT for object tracking to enhance the MTDT performance in the complex port settings. In particular, Light-YOLOX, an upgraded version of YOLOX incorporating separable convolution and attention mechanism, is introduced to improve real-time capability and small target detection. Subsequently, OSNet-DeepSORT, an enhanced version of DeepSORT, is proposed to mitigate ID switching challenges arising from unreliable data communication or occlusion in real port scenarios. The effectiveness of the proposed method is validated in various real-life port operations. Ablation studies and comparative experiments against typical MTDT algorithms demonstrate noteworthy enhancements in key performance metrics, encompassing small target detection, tracking accuracy, ID switching frequency, and real-time performance.  © 2020 IEEE.",DeepSORT; multitarget tracking; rubber tire gantry (RTG); target detection; YOLOX; Clutter (information theory); Convolution; Object detection; Object recognition; Ports and harbors; Rubber; Target tracking; DeepSORT; Features extraction; Multi-target detection and tracking; Multi-target-tracking; Objects detection; Rubber tire gantries; Seaport; Targets detection; Targets tracking; Yolox; Feature extraction
Scopus,"Wang, X.; Xuan, Y.; Huang, X.; Yan, Q.",YOLO-AFK: Advanced Fine-Grained Object Detection for Complex Solder Joints Defect,,2024,,,,10.1109/ACCESS.2024.3495540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209630354&doi=10.1109%2fACCESS.2024.3495540&partnerID=40&md5=b5f9def5225577c9f7b80c5e5d6fb309,"Welding processes significantly impact product quality as a crucial part of industrial production. Due to the reflection, diversity, complexity, and minuteness of solder defects, traditional detection methods struggle to detect surface defects in solder points effectively. Although object detection based on deep learning has made significant advances, detecting smaller objects remains challenging. To address these issues, we propose an improved defect detection network based on YOLOv9, named attention flexible kernel YOLO (YOLO-AFK). In particular, we propose a fusion attention network (FANet) that can enhance the model's ability to detect small defects by adaptively adjusting the receptive field of targets during feature extraction. Meanwhile, we use the alterable kernel convolution (AKConv), a variable kernel convolution, that breaks away from traditional convolutions limited to fixed local windows and sampling shapes. It can flexibly adjust the size and shape of the convolution kernels according to the solder targets, leading to more efficient feature extraction, thus achieving a lighter network. To gather more contextual and high-resolution information and enhance the detection accuracy and generalization ability for small objects and low-contrast targets, the cross-stage partial network fusion (C2f) module is designed to fuse feature maps from different levels. We evaluated the model using the publicly available NEU dataset and our proprietary solder point dataset, the fine-grain solder defect dataset (FG-SDD). Compared to previous studies, YOLO-AFK outperforms other state-of-the-art networks in terms of mean Average Precision (mAP) and Precision, with the parameter count increasing by only 12.4M, Precision improving by 10.1%, mAP increasing by 5.6%, and FPS improving by 23%. These results demonstrate the superior performance of the proposed network in detecting defects with complex structures. In particular, for industrial solder joint defect detection, YOLO-AFK not only improves detection accuracy but also significantly enhances the recognition of small targets and complex solder joint defects, showcasing the network's substantial potential and practical value in real-world production environments. The code is available at: https://github.com/Lwsk-wxy/yolo_afk.git.  ©2024 The Authors.",complex feature extraction; Deep learning; object detection; solder defect detection; Image coding; Image segmentation; Point defects; Soldering; Complex feature extraction; Deep learning; Defect detection; Features extraction; Kernel convolution; Objects detection; Small objects; Solder defect detection; Solder defects; Solder-joint defects; Deep learning
Scopus,"Nafaa, S.; Ashour, K.; Mohamed, R.; Essam, H.; Emad, D.; Elhenawy, M.; Ashqar, H.I.; Hassan, A.A.; Alhadidi, T.I.",Advancing Roadway Sign Detection with YOLO Models and Transfer Learning,,2024,,,,10.1109/ICMI60790.2024.10586105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199427434&doi=10.1109%2fICMI60790.2024.10586105&partnerID=40&md5=73405918738bbfbab16af9d19c8e1e45,"Roadway signs detection and recognition is an essential element in the Advanced Driving Assistant Systems (ADAS). Several artificial intelligence methods have been used widely among of them YOLOv5 and YOLOv8. In this paper, we used a modified YOLOv5 and YOLOv8 to detect and classify different roadway signs under different illumination conditions. Experimental results indicated that for the YOLOv8 model, varying the number of epochs and batch size yields consistent MAP50 scores, ranging from 94.6% to 97.1% on the testing set. The YOLOv5 model demonstrates competitive performance, with MAP50 scores ranging from 92.4% to 96.9%. These results suggest that both models perform well across different training setups, with YOLOv8 generally achieving slightly higher MAP50 scores. These findings suggest that both models can perform well under different training setups, offering valuable insights for practitioners seeking reliable and adaptable solutions in object detection applications.  © 2024 IEEE.",Assets Management; Deep Learning; Signs Detection; Advanced driver assistance systems; Automobile drivers; Deep learning; Intelligent systems; Traffic signs; Artificial intelligence methods; Assets management; Deep learning; Driving assistant systems; Essential elements; Illumination conditions; Model learning; Sign detection; Sign recognition; Transfer learning; Object detection
Scopus,"Xu, Y.; Hu, H.; Zhu, X.; Nan, Y.; Wang, K.; Liu, Z.; Lian, S.",RAOD: A Benchmark for Road Abandoned Object Detection From Video Surveillance,,2024,,,,10.1109/ACCESS.2024.3407955,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194849112&doi=10.1109%2fACCESS.2024.3407955&partnerID=40&md5=b932ac7453566499fdf0305bf75ea7b2,"Road abandoned objects are potential safety hazards in modern traffic transport, especially in highway scenes. Promptly detecting such obstacles on the road is of great significance for driving safety and Intelligent Transportation Systems (ITS). Current research primarily focuses on developing diverse road anomaly detection approaches to discriminate the unknown objects regarded as abandoned ones. However, previous efforts have been largely inadequate due to the absence of abundant datasets. In addition, prevailing benchmarks mainly provide data pertinent to autonomous driving, which might not effectively generalize to highway scenarios owing to camera perspective and scope limitations. To address these challenges, we introduce a large-scale Road Abandoned Object Detection (RAOD) benchmark derived from video surveillance. First, we collect abundant real-world video clips containing various potential abandoned object categories on the road from our commercial ITS, then assemble a road abandoned object dataset comprising 557 video sequences and 18,953 images with pixel-level manual annotations. Second, we conduct exhaustive evaluation experiments employing a range of baseline models from mainstream algorithms on our dataset to illustrate the performance of different approaches. Third, we propose a novel image segmentation framework based on an area-aware attention mechanism. Experimental results reveal that our method outperforms the UNet-based model by nearly 9% in terms of dice score. Our dataset represents the most extensive open-source resource dedicated to road abandoned object detection, accessible publicly at https://github.com/yajunbaby/A-Benchmark-for-Road-Abandoned-Object-Detection-from-Video-Surveillance.  © 2013 IEEE.",area-aware attention mechanism; intelligent transportation system; Road abandoned object detection; video surveillance; Intelligent systems; Intelligent vehicle highway systems; Object detection; Object recognition; Roads and streets; Security systems; Traffic control; Abandoned object detections; Area-aware attention mechanism; Attention mechanisms; Benchmark testing; Images segmentations; Intelligent transportation systems; Objects recognition; Road; Road abandoned object detection; Uncertainty; Video surveillance; Image segmentation
Scopus,"Li, Y.; Weng, D.; Tian, Z.; Hou, J.; Li, Z.",YOLO-Ti: an efficient object detection approach for tiny facial markers,,2024,,,,10.1117/12.3048940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210236113&doi=10.1117%2f12.3048940&partnerID=40&md5=5eaa50427b761c9260b0a26ce16d104d,"In this paper, an efficient object detection method YOLO-Ti is proposed to detect tiny facial markers. Our study is driven by the practical requirements of 3D face modeling, requiring the incorporation of as many facial features as possible for reference. This research can even provide information for facial expression recognition and joint deformation. To achieve this, we first present a feature fusion module called Cross-BiFPN, which incorporates additional cross-connecting branches between different network layers to utilize low-level features more effectively. Secondly, we add a high-resolution detection head and attention module to the YOLOv8 model to improve the ability of detecting tiny objects, while at the same time ensuring the lightweight detection model by reducing redundant network layers. Thirdly, we collect a dataset of facial markers with an average size much smaller than publicly available small object datasets. Ablation studies and comparison experiments are conducted to evaluate the performance of our approach. Compared with the baseline YOLOv8 model, YOLO-Ti shows a 30.4% improvement in mAP50 while reducing model parameters by 65.1%. The automatic feature extraction provided by our model facilitates the construction of digital humans, providing significant savings in manpower and time for modelers. © 2024 SPIE.",3D face reconstruction; Facial markers; improved YOLOv8 algorithm; tiny object detection; 3D modeling; Face recognition; Feature extraction; Object detection; Object recognition; Photons; 3D face modeling; 3D face reconstruction; Detection approach; Efficient object detections; Facial marker; Improved YOLOv8 algorithm; Object detection method; Objects detection; Practical requirements; Tiny object detection; 3D reconstruction
Scopus,"Wang, J.; Wang, Z.; Weng, Y.; Li, Y.",DRPDDet: Dynamic Rotated Proposals Decoder for Oriented Object Detection,,2024,,,,10.1007/978-981-99-8076-5_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190388429&doi=10.1007%2f978-981-99-8076-5_8&partnerID=40&md5=ba92952365833d81188218df0d219fef,"Oriented object detection has gained popularity in diverse fields. However, in the domain of two-stage detection algorithms, the generation of high-quality proposals with a high recall rate remains a formidable challenge, especially in the context of remote sensing images where sparse and dense scenes coexist. To address this, we propose the DRPDDet method, which aims to improve the accuracy and recall of proposals for Oriented target detection. Our approach involves generating high-quality horizontal proposals and dynamically decoding them into rotated proposals to predict the final rotated bounding boxes. To achieve high-quality horizontal proposals, we introduce the innovative HarmonyRPN module. This module integrates foreground information from the RPN classification branch into the original feature map, creating a fused feature map that incorporates multi-scale foreground information. By doing so, the RPN generates horizontal proposals that focus more on foreground objects, which leads to improved regression performance. Additionally, we design a dynamic rotated proposals decoder that adaptively generates rotated proposals based on the constraints of the horizontal proposals, enabling accurate detection in complex scenes. We evaluate our proposed method on the DOTA and HRSC2016 remote sensing datasets, and the experimental results demonstrate its effectiveness in complex scenes. Our method improves the accuracy of proposals in various scenarios while maintaining a high recall rate. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Dynamic rotated proposals decoder; Foreground information; Harmony RPN; Oriented object detection; Classification (of information); Decoding; Object recognition; Remote sensing; Rotation; Complex scenes; Diverse fields; Dynamic rotated proposal decoder; Feature map; Foreground information; Harmony RPN; High quality; Objects detection; Oriented object detection; Recall rate; Object detection
Scopus,"Wu, G.; Zhou, F.; Meng, C.; Li, X.-Y.",Precise UAV MMW-Vision Positioning: A Modal-Oriented Self-Tuning Fusion Framework,,2024,,,,10.1109/JSAC.2023.3322851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174826831&doi=10.1109%2fJSAC.2023.3322851&partnerID=40&md5=613c59641d44c3621ae2ac7ec431e44b,"Precise real-time unmanned aerial vehicle (UAV) positioning is crucial for preventing unauthorized UAVs from damaging cooperative intelligent transportation systems (C-ITSs). However, UAV positioning remains extremely challenging due to the small target size and high flexibility. Therefore, we develop a modal-oriented self-tuning fusion framework for precise UAV millimeter-wave(MMW)-vision positioning. The framework selects and extracts cross-modal features based on modality characters, and migrates the Doppler features of MMW radar data to the image features for precise pixel-level positioning. Based on the framework, a modal-oriented self-tuning fusion network is proposed to adaptively enhance UAV feature without direct supervision by exploiting the cross-modal correlations. A novel characteristic-based 3DMMW feature extraction method is presented to extract UAV Doppler motion characteristics while a self-tuning cross-modal affine transfer is proposed for UAV visual feature enhancement. Due to lack of dataset for our task, we establish a practical positioning platform and two novel datasets containing synchronized visual images and MMW radio frequency (RF) sequences in various scenarios. Experimental results confirm that our framework outperforms the benchmark methods in terms of positioning accuracy while maintaining real-time performance. Moreover, ablation studies also confirm the effectiveness of each module in the framework.  © 1983-2012 IEEE.",cooperative intelligent transportation systems; MMW-vision fusion; modal-oriented self-tuning framework; Precise UAV positioning; Antennas; Benchmarking; Extraction; Intelligent systems; Intelligent vehicle highway systems; Interactive computer systems; Millimeter waves; Radar imaging; Real time systems; Unmanned aerial vehicles (UAV); Aerial vehicle; Cooperative intelligent transportation system; Features extraction; Intelligent transportation systems; Millimeter-wave-vision fusion; Modal-oriented self-tuning framework; Precise unmanned aerial vehicle positioning; Real - Time system; Selftuning; Vehicle positioning; Feature extraction
Scopus,"Zhu, J.; Wu, Y.; Ma, T.",Multi-Object Detection for Daily Road Maintenance Inspection With UAV Based on Improved YOLOv8,,2024,,,,10.1109/TITS.2024.3437770,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204710166&doi=10.1109%2fTITS.2024.3437770&partnerID=40&md5=79f795d607f17393c783eb5306842105,"Daily road maintenance is essential to road safety and serviceability, particularly to highways. In daily road maintenance inspection tasks, the objectives include a variety of targets, such as pavement cracks, foreign objects, guardrail damages etc. There is a lack of rapid detection methods that allow for the uniform identification of multiple targets for road maintenance. This paper proposes an automatic multi-object detection method for road daily maintenance inspection assisted by unmanned aerial vehicles (UAV). A dataset of multiple roadway anomalies (UAVROAD) for daily road maintenance needs was constructed. UM-YOLO, an improved algorithm based on the YOLOv8n algorithm was created to better extract the features of multiple targets, as well as fuse features and reduce computation while maintaining accuracy. The improvements include adding EMA (Efficient multiscale Attention Module) in the C2f module in the backbone, employing Bi-FPN fusion mechanism in the neck and using GSConv, a lightweight convolutional network, for the convolution operation. Compared with the YOLOv8n, the proposed UM-YOLO improved mean average precision(mAP) by 4.6% and reduced the model computation by 14%.  © 2024 IEEE.",computer vision; Daily road maintenance; deep learning; multi-object detection; UAV; Aircraft detection; Automatic vehicle identification; Bismuth alloys; Deep learning; Highway accidents; Image annotation; Intelligent systems; Motor transportation; Risk analysis; Risk management; Aerial vehicle; Daily road maintenance; Deep learning; Maintenance inspections; Multi-object detection; Multiobject; Multiple targets; Objects detection; Road maintenance; Unmanned aerial vehicle; Unmanned aerial vehicles (UAV)
Scopus,"Xu, W.; Lu, P.; Zhao, Y.",E-RODNet: Lightweight Approach to Object Detection by Vehicular Millimeter-Wave Radar,,2024,,,,10.1109/JSEN.2024.3437474,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001210004&doi=10.1109%2fJSEN.2024.3437474&partnerID=40&md5=e014ebeac75def315221953cdec3b1ff,"The main sensors employed for achieving autonomous driving perception tasks include cameras, LiDAR, and millimeter-wave radar. Millimeter-wave radar offers the advantages of all-weather and all-day operation, helping to compensate for the shortcomings of cameras and LiDAR in adverse environmental conditions and ensuring the safety of autonomous driving technology. Currently, most object perception models based on millimeter-wave radar are overly complex, requiring substantial computational and storage resources during inference, which cannot meet the real-time requirements of autonomous driving. In light of this, this article proposes a more efficient new model, referred to as E-RODNet. The model employs the ConvFormer block as the primary feature extraction unit and adjusts the SepConv operator for spatiotemporal data modeling. To enhance the model’s global modeling capability, a global feature fusion module is introduced, adding global context information to each pixel after encoding features. In addition, the short-term sequence feature fusion module is improved based on the characteristics of the CRUW dataset. The application of these two techniques allows the model to achieve state-of-the-art performance with minimal parameters and computational resources. Compared to T-RODNet, E-RODNet achieves competitive performance with only 3.8% of the model parameters, improving average precision (AP) by 1.63%, and requiring only 18.2% of the GFLOPs of T-RODNet. Our model and training code can be available at https://github.com/lupeng-xm/E-RODNet.(Figure presented). © 2001-2012 IEEE.",Autonomous driving; ConvFormer block; global contextual information; millimeter-wave radar object detection; short-term sequence feature fusion; Digital elevation model; Image coding; Image compression; Image segmentation; Remote sensing; Autonomous driving; Contextual information; Convformer block; Features fusions; Global contextual information; Millimeter-wave radar; Millimeter-wave radar object detection; Millimetre-wave radar; Objects detection; Radar objects; Sequence features; Short-term sequence feature fusion; Autonomous vehicles
Scopus,"Kramar, V.A.; Kramar, O.A.; Alchakov, V.V.",The Computer Vision System for a Crewless Boat,,2024,,,,10.1109/ICIEAM60818.2024.10553783,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197250001&doi=10.1109%2fICIEAM60818.2024.10553783&partnerID=40&md5=cecc27b10f9ada76bed3fafddb658ece,"The article considers a computer vision system for a crewless ship. The existing approaches to the construction of such systems are described, and the strengths and weaknesses of different methods are analyzed. The hardware part of the system is considered. The functional scheme of the system and technical characteristics of the equipment are given. An approach to solving the problem of detecting a given class of objects based on the YOLOv5 algorithm is described. The accuracy of the obtained model is confirmed with the help of precision-recall curve. The paper also presents an algorithm for calculating the heading angle for the dynamic positioning of an uncrewed vessel based on data from the vision system. The advantage of the proposed approach is its fast performance, which allows the use of onboard computers in real-time. The highly effective strategy allows the onboard system's image to be broadcast over a local network without losing image quality. The performance of the computer vision system and the algorithm for detecting a given class of objects is verified using long-term hard-loaded testing. The test results showed good accuracy and stability of the developed system.  © 2024 IEEE.",computer vision; control system; crewless boat; image processing; object detection; onboard systems; YOLOv5; Boats; Computer control systems; Object detection; Computer vision system; Crewless boat; Functional scheme; Images processing; Object based; Objects detection; Objects-based; On-board systems; Performance; YOLOv5; Computer vision
Scopus,"Gong, H.; Wang, X.; Zhuang, W.",Research on Real-Time Detection of Maize Seedling Navigation Line Based on Improved YOLOv5s Lightweighting Technology,,2024,,,,10.3390/agriculture14010124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183188927&doi=10.3390%2fagriculture14010124&partnerID=40&md5=db0e7dfc68d3f02bfda118f0cf79075e,"This study focuses on real-time detection of maize crop rows using deep learning technology to meet the needs of autonomous navigation for weed removal during the maize seedling stage. Crop row recognition is affected by natural factors such as soil exposure, soil straw residue, mutual shading of plant leaves, and light conditions. To address this issue, the YOLOv5s network model is improved by replacing the backbone network with the improved MobileNetv3, establishing a combination network model YOLOv5-M3 and using the convolutional block attention module (CBAM) to enhance detection accuracy. Distance-IoU Non-Maximum Suppression (DIoU-NMS) is used to improve the identification degree of the occluded targets, and knowledge distillation is used to increase the recall rate and accuracy of the model. The improved YOLOv5s target detection model is applied to the recognition and positioning of maize seedlings, and the optimal target position for weeding is obtained by max-min optimization. Experimental results show that the YOLOv5-M3 network model achieves 92.2% mean average precision (mAP) for crop targets and the recognition speed is 39 frames per second (FPS). This method has the advantages of high detection accuracy, fast speed, and is light weight and has strong adaptability and anti-interference ability. It determines the relative position of maize seedlings and the weeding machine in real time, avoiding squeezing or damaging the seedlings. © 2024 by the authors.",autonomous navigation; crop row detection; deep learning; inter-row weeding; maize seedlings
Scopus,"Lambertenghi, S.C.; Stocco, A.",Assessing Quality Metrics for Neural Reality Gap Input Mitigation in Autonomous Driving Testing,,2024,,,,10.1109/ICST60714.2024.00024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193831439&doi=10.1109%2fICST60714.2024.00024&partnerID=40&md5=25840efaa189d15957228b81fcfc3bec,"Simulation-based testing of automated driving systems (ADS) is the industry standard, being a controlled, safe, and cost-effective alternative to real-world testing. Despite these advantages, virtual simulations often fail to accurately replicate real-world conditions like image fidelity, texture representation, and environmental accuracy. This can lead to significant differences in ADS behavior between simulated and real-world domains, a phenomenon known as the sim2real gap. Researchers have used Image-to-Image (I2I) neural translation to mitigate the sim2real gap, enhancing the realism of simulated environments by transforming synthetic data into more authentic representations of real-world conditions. However, while promising, these techniques may potentially introduce artifacts, distortions, or inconsistencies in the generated data that can affect the effectiveness of ADS testing. In our empirical study, we investigated how the quality of image-to-image (I2I) techniques influences the mitigation of the sim2real gap, using a set of established metrics from the literature. We evaluated two popular generative I2I architectures, pix2pix and CycleGAN, across two ADS perception tasks at a model level, namely vehicle detection and end-to-end lane keeping, using paired simulated and real-world datasets. Our findings reveal that the effectiveness of I2I architectures varies across different ADS tasks, and existing evaluation metrics do not consistently align with the ADS behavior. Thus, we conducted task-specific fine-tuning of perception metrics, which yielded a stronger correlation. Our findings indicate that a perception metric that incorporates semantic elements, tailored to each task, can facilitate selecting the most appropriate I2I technique for a reliable assessment of the sim2real gap mitigation.  © 2024 IEEE.",autonomous vehicles testing; generative adversarial networks; reality gap; sim2real; Virtual environments; Adversarial networks; Automated driving systems; Autonomous vehicle testing; Autonomous Vehicles; Condition; Real-world; Reality gaps; Sim2real; System behaviors; Vehicle testing; Automobile testing
Scopus,"Kaur, J.; Singh, W.",A systematic review of object detection from images using deep learning,,2024,,,,10.1007/s11042-023-15981-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169104184&doi=10.1007%2fs11042-023-15981-y&partnerID=40&md5=469fe7785dc708591b2a90cdedaeb7a9,"The development of object detection has led to huge improvements in human interaction systems. Object detection is a challenging task because it involves many parameters including variations in poses, resolution, occlusion, and daytime versus nighttime detection. This study surveys on various aspects of object detection that includes (1) basics of object detection, (2) object detection techniques, (3) datasets, (4) metrics and deep learning libraries. This study presents a systematic analysis of recent publications on object detection covering around 400 research articles and synthesised the findings to provide empirical answers to research questions. The review is based on relevant articles published from 2015 through 2022, as well as discussions of challenges and future directions in this field. Furthermore, the survey examined the contributions of various researchers concerning their respective application domains, while emphasizing the advantages and disadvantages of the research work. Despite the success of various methods proposed in literature for predicting results, there remains room for improvement in the accuracy of object detection. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Backbone architecture; Computer vision; deep learning; object detection; object detection application; Computer vision; Deep learning; Object recognition; Backbone architecture; Deep learning; Humaninteraction; Interaction systems; Object detection application; Objects detection; Synthesised; Systematic analysis; Systematic Review; Object detection
Scopus,"Wang, S.; Jiang, H.; Yang, J.; Ma, X.; Chen, J.; Li, Z.; Tang, X.",Lightweight tomato ripeness detection algorithm based on the improved RT-DETR,,2024,,,,10.3389/fpls.2024.1415297,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198833519&doi=10.3389%2ffpls.2024.1415297&partnerID=40&md5=cc8922bcecb466fc9ffc87fe78f82dd8,"Tomatoes, widely cherished for their high nutritional value, necessitate precise ripeness identification and selective harvesting of mature fruits to significantly enhance the efficiency and economic benefits of tomato harvesting management. Previous studies on intelligent harvesting often focused solely on identifying tomatoes as the target, lacking fine-grained detection of tomato ripeness. This deficiency leads to the inadvertent harvesting of immature and rotten fruits, resulting in economic losses. Moreover, in natural settings, uneven illumination, occlusion by leaves, and fruit overlap hinder the precise assessment of tomato ripeness by robotic systems. Simultaneously, the demand for high accuracy and rapid response in tomato ripeness detection is compounded by the need for making the model lightweight to mitigate hardware costs. This study proposes a lightweight model named PDSI-RTDETR to address these challenges. Initially, the PConv_Block module, integrating partial convolution with residual blocks, replaces the Basic_Block structure in the legacy backbone to alleviate computing load and enhance feature extraction efficiency. Subsequently, a deformable attention module is amalgamated with intra-scale feature interaction structure, bolstering the capability to extract detailed features for fine-grained classification. Additionally, the proposed slimneck-SSFF feature fusion structure, merging the Scale Sequence Feature Fusion framework with a slim-neck design utilizing GSConv and VoVGSCSP modules, aims to reduce volume of computation and inference latency. Lastly, by amalgamating Inner-IoU with EIoU to formulate Inner-EIoU, replacing the original GIoU to expedite convergence while utilizing auxiliary frames enhances small object detection capabilities. Comprehensive assessments validate that the PDSI-RTDETR model achieves an average precision mAP50 of 86.8%, marking a 3.9% enhancement over the original RT-DETR model, and a 38.7% increase in FPS. Furthermore, the GFLOPs of PDSI-RTDETR have been diminished by 17.6%. Surpassing the baseline RT-DETR and other prevalent methods regarding precision and speed, it unveils its considerable potential for detecting tomato ripeness. When applied to intelligent harvesting robots in the future, this approach can improve the quality of tomato harvesting by reducing the collection of immature and spoiled fruits. Copyright © 2024 Wang, Jiang, Yang, Ma, Chen, Li and Tang.",deep learning; deformable attention; Inner-EIoU; PConv; ripeness recognition; RT-DETR; slimneck; tomato
Scopus,"Kumar, A.; Meel, P.",Object Detection using SSD and Efficient net B7 as base Network,,2024,,,,10.1109/ICAC2N63387.2024.10894816,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000773130&doi=10.1109%2fICAC2N63387.2024.10894816&partnerID=40&md5=25d2e9b17ce8370bb53b55902f26306d,"Object detection is the method of recognizing and finding objects in digital images and video frames. Over the years, object detection techniques have made significant advances driven by breakthroughs in large-scale datasets, powerful computing resources, and deep learning architectures. This article provides an overview of target detection, its uses, applications and recent advances in the field.Object detection is an essential component of various real-world applications, like autonomous vehicle driving, military surveillance and radar systems, augmented reality, and robotics. By correctly identifying and finding objects of interest, machines can learn about their surroundings and make intelligent decisions. This article discusses the use of an efficient network as the central network for feature extraction and a single-shot multi-box detector for target detection in order to implement targeted detection. © 2024 IEEE.",Military radar; Object detection; Object recognition; Radar target recognition; Surveillance radar; Computing resource; Digital image; Digital videos; Image frames; Large-scale datasets; Learning architectures; Objects detection; Real-world; Targets detection; Video frame; Military vehicles
Scopus,"Song, K.; Xue, X.; Wen, H.; Ji, Y.; Yan, Y.; Meng, Q.",Misaligned Visible-Thermal Object Detection: A Drone-Based Benchmark and Baseline,,2024,,,,10.1109/TIV.2024.3398429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192971017&doi=10.1109%2fTIV.2024.3398429&partnerID=40&md5=82f64eb655e5978b97ae76a2045f2f36,"Multispectral object detection has achieved remarkable results due to its ability to fuse information from visible and thermal modalities in recent years. However, the existing visible-thermal datasets are constructed based on manually aligned image pairs, which cannot fully represent the challenges of real-world scenarios where image pairs are often misaligned. Existing methods for visible-thermal object detection are based on aligned data and are limited by the accuracy of registration. To address the above issues, we propose a dataset, namely DVTOD, which is a misaligned visible-thermal object detection dataset captured by drones. DVTOD includes 16 challenging attributes and 54 capture scenes. Furthermore, we introduce a cross-modal alignment detector (CMA-Det) for misaligned visible-thermal object detection. Firstly, we design an alignment network to estimate the visible-to-thermal deformation field, which is used to correct for misalignment of the corresponding visible and thermal features. Secondly, we propose a strategy called Object Search Rectification (OSR) to improve the robustness of feature alignment. To better remove the interference of complex backgrounds, a bi-directional feature correction fusion module (BFCFM) is designed to calibrate bimodal features by exploiting the correlation of channel and spatial information between two modalities. CMA-Det outperforms existing methods on the DVTOD dataset and two other visible-thermal object detection datasets. © 2016 IEEE.",cross-modal alignment; feature alignment; Multispectral object detection; visible-thermal dataset; Aircraft detection; Alignment; Drones; Feature extraction; Interactive computer systems; Object detection; Object recognition; Annotation; Cross-modal; Cross-modal alignment; Feature alignment; Features extraction; Multi-spectral; Multispectral object detection; Objects detection; Real - Time system; Thermal; Vibration; Visible-thermal dataset; Real time systems
Scopus,"Babu, M.A.A.; Pandey, S.K.; Durisic, D.; Koppisetty, A.C.; Staron, M.",Impact of Image Data Splitting on the Performance of Automotive Perception Systems,,2024,,,,10.1007/978-3-031-56281-5_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192177434&doi=10.1007%2f978-3-031-56281-5_6&partnerID=40&md5=15628e168be69da764bda23aa7ca5927,"Context: Training image recognition systems is one of the crucial elements of the AI Engineering process in general and for automotive systems in particular. The quality of data and the training process can have a profound impact on the quality, performance, and safety of automotive software. Objective: Splitting data between train and test sets is one of the crucial elements in this process as it can determine both how well the system learns and generalizes to new data. Typical data splits take into consideration either randomness or timeliness of data points. However, in image recognition systems, the similarity of images is of equal importance. Methods: In this computational experiment, we study the impact of six data-splitting techniques. We use an industrial dataset with high-definition color images of driving sequences to train a YOLOv7 network. Results: The mean average precision (mAP) was 0.943 and 0.841 when the similarity-based and the frame-based splitting techniques were applied, respectively. However, the object-based splitting technique produces the worst mAP score (0.118). Conclusion: There are significant differences in the performance of object detection methods when applying different data-splitting techniques. The most positive results are the random selections, whereas the most objective ones are splits based on sequences that represent different geographical locations. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Autonomous driving; Data splitting technique; Image perception system; Object detection; YOLOv7; Autonomous vehicles; Image recognition; Object recognition; Autonomous driving; Data splitting; Data splitting technique; Image perception; Image perception system; Objects detection; Perception systems; Performance; Splitting techniques; YOLOv7; Object detection
Scopus,"Adiuku, N.; Avdelidis, N.P.; Tang, G.; Plastropoulos, A.; Diallo, Y.",Mobile Robot Obstacle Detection and Avoidance with NAV-YOLO,,2024,,,,10.18178/ijmerr.13.2.219-226,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189462962&doi=10.18178%2fijmerr.13.2.219-226&partnerID=40&md5=244beff11293d476a3917b75fd597b6e,"Intelligent robotics is gaining significance in Maintenance, Repair, and Overhaul (MRO) hangar operations, where mobile robots navigate complex and dynamic environments for Aircraft visual inspection. Aircraft hangars are usually busy and changing, with objects of varying shapes and sizes presenting harsh obstacles and conditions that can lead to potential collisions and safety hazards. This makes Obstacle detection and avoidance critical for safe and efficient robot navigation tasks. Conventional methods have been applied with computational issues, while learning-based approaches are limited in detection accuracy. This paper proposes a vision-based navigation model that integrates a pre-trained Yolov5 object detection model into a Robot Operating System (ROS) navigation stack to optimise obstacle detection and avoidance in a complex environment. The experiment is validated and evaluated in ROS-Gazebo simulation and turtlebot3 waffle-pi robot platform. The results showed that the robot can increasingly detect and avoid obstacles without colliding while navigating through different checkpoints to the target location. © 2024 by the authors. This is an open access article distributed under the Creative Commons Attribution License (CC BY-NC-ND 4.0), which permits use, distribution and reproduction in any medium, provided that the article is properly cited, the use is noncommercial and no modifications or adaptations are made. All Rights Reserved.",autonomous navigation; deep learning; mobile robot; object detection; obstacle avoidance
Scopus,"Lu, Z.; Liu, F.; Wang, L.; Xu, L.; Liu, X.",A Novel Lung Nodule Detection and Recognition Model Based on Deep Learning,,2024,,,,10.1109/ACCESS.2024.3478358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207630037&doi=10.1109%2fACCESS.2024.3478358&partnerID=40&md5=06c5397f8e7417ddf93a4176f2fc23ee,"To solve the problems of missing and false detection of pulmonary nodules in complex lung environments, as well as trivial and inefficient detection procedures, an end-to-end pulmonary nodules detection and recognition model based on deep learning was proposed. Innovation and improvement are made on the basis of YOLOv5. In the feature extraction stage of the model, a convolutional structure integrating self-attention mechanism is proposed to capture the global feature and the dependence relationship of long-distance information, and screen the key pathological information. Then, a convolution structure integrating internal convolution operators is proposed to reduce the computational redundancy in the feature channel and improve the inference speed of the model. In the feature fusion stage of the model, the structure of cross-scale coordinate attention feature fusion is proposed, and the different features enhanced with attention are weighted by jumping links to promote the fusion of multi-scale feature information. The proposed model obtained 97.8% mAP@0.5 indexes in the self-built diagnosis and treatment data set of pulmonary nodules in Huaihai area. The pulmonary nodule detection model proposed in this paper can significantly reduce the false positive rate and obtain the location and classification results of diseased nodules with higher detection accuracy and faster detection speed, which has important practical value in clinical application.  © 2013 IEEE.",cross-scale feature fusion; involution; multi-head self-attention mechanism; Pulmonary nodule detection; Deep learning; Diagnosis; Attention mechanisms; Cross-scale feature fusion; Detection models; Features fusions; Involution; Model-based OPC; Multi-head self-attention mechanism; Pulmonary nodule detection; Pulmonary nodules; Recognition models; Lung cancer
Scopus,"Jhala, J.S.; Joshi, C.; Anand, D.",Deep Learning Driven Object Detection and Classification for Autonomous Vehicles in Diverse Traffic and Weather Conditions,,2024,,,,10.1109/ICETI63946.2024.10777214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214977783&doi=10.1109%2fICETI63946.2024.10777214&partnerID=40&md5=f6774ab61018499df1bdbd35510142e4,"The rapid development of self-driving vehicles necessitates integrating a sophisticated sensing system to address various obstacles posed by road traffic efficiently. While several datasets support object detection in autonomous vehicles, evaluating their suitability for different weather conditions globally is crucial. In this study, we present deep learning models trained on a novel dataset derived from YouTube videos recorded from Indian car’s dashcams. These videos capture a wide range of conditions, including rain, fog, daytime, hazy and night-time driving scenarios prevalent in India. The dataset comprises a total of 1450 annotated images depicting vehicles and other road assets across six different classes. In this work, performance analysis of the YOLOv8 models trained using an existing dataset was compared with the model trained on an expanded version using the proposed weather-specific dataset. The results demonstrate improved accuracy metrics of 91.3%, 84.5%, and 91.2% for Precision, Recall, and mean Average Precision (mAP) upon integrating the proposed dataset. The model trained on this diverse dataset exhibits heightened robustness, proving highly beneficial for autonomous and conventional vehicle operations in India’s dynamic traffic environments. This research contributes to advancing object detection capabilities crucial for autonomous driving technologies in real-world settings. © 2024 IEEE.",Autonomous-driving Cars; Deep Learning Neural Networks; Object Identification Model; Smart Transportation Systems; YOLOv8 Algorithm; Autonomous vehicles; Deep neural networks; Vehicle detection; Autonomous driving; Autonomous-driving car; Deep learning neural network; Identification modeling; Learning neural networks; Object identification; Object identification model; Smart transportation system; Transportation system; YOLOv8 algorithm; Image annotation
Scopus,"Chen, X.; Jiang, H.; Zheng, H.; Yang, J.; Liang, R.; Xiang, D.; Cheng, H.; Jiang, Z.",DET-YOLO: An Innovative High-Performance Model for Detecting Military Aircraft in Remote Sensing Images,,2024,,,,10.1109/JSTARS.2024.3462745,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204493009&doi=10.1109%2fJSTARS.2024.3462745&partnerID=40&md5=8ad401bc1f998456a3f535590a68d55a,"To address the challenges of low detection rate and high missed detection rate of military aircraft in current complex remote sensing data, and to meet the requirements of real-Time detection and easy deployment of models, this article introduces DET-you only look once (YOLO), an innovative detection model. First, to tackle the issue of reduced accuracy in identifying small targets amidst intricate backgrounds, a novel feature extraction component, C2fDEF, was devised. This module replaced all existing C2f components within YOLOv8n, thereby significantly enhancing the model's ability to cope with complicated environmental contexts. Second, to achieve the functionality of easy deployment of the model, some deep structures were simplified to make the model more lightweight. Afterward, to further improve the model's ability to handle complex backgrounds and dense environments in remote sensing images and to improve the model's detection accuracy for military aircraft, the DAT module was embedded in the model. Finally, this article also optimized the loss function and regmax to further reduce computational costs while improving the detection accuracy of the model. To verify the effectiveness and strong universality of DET-YOLO, extensive experimental verification was conducted on three publicly available datasets, namely MAR20, NWPU VHR-10, and NEU-DET. On the MAR20 dataset, compared with other advanced models, DET-YOLO achieved the highest mAP0.5 (namely 94.7%) with only 80 training epochs while meeting lightweight and real-Time requirements. While on the other two datasets, DET-YOLO also achieved the best detection performance.  © 2008-2012 IEEE.",Attention mechanism; deep learning; deformable convolution (DCN); loss function; remote sensing images (RSIs); you only look once (YOLO); Fighter aircraft; Military helicopters; Military photography; Remote sensing; Attention mechanisms; DCN; Deep learning; Detection accuracy; Detection rates; High performance modeling; Loss functions; Modeling abilities; Remote sensing images; YOLO; accuracy assessment; detection method; image analysis; image classification; machine learning; remote sensing; Aircraft detection
Scopus,"Li, X.; Yang, K.; Huang, R.; Zhou, B.; Xiao, J.; Gao, Z.",Detecting Small Objects using Multi-Scale Feature Fusion Mechanism with Convolutional Block Attention,,2024,,,,10.1109/SMC54092.2024.10831911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217843435&doi=10.1109%2fSMC54092.2024.10831911&partnerID=40&md5=3eae145891fd0ff46c009b37c9c16c52,"Small object detection is difficult because of their low resolution and the inclusion of unimportant background information. Aiming at the problem of small objects information loss in multiscale feature fusion and the impact of background information, this paper proposes a multi-scale feature fusion mechanism (MFFM) with the convolutional block attention modules (CBAM). The proposed approach efficiently utilizes the low three-layer feature information (P1-P3) output from the backbone network and the improved feature fusion technique to enhance the characterization ability of the single-layer feature information through the attention mechanism. This mechanism enables the single-layer features to carry the three-layer feature information, thereby improving the fusion ability among the feature layers. The experimental results demonstrate that MFFM enhances the overall accuracy mAP on the VisDrone2019-DET validation set by 1.4% and the accuracy APs on small objects by 1.5% in comparison to the baseline model YOLOX-X. This approach effectively improves the performance of small object detection. © 2024 IEEE.",Attention mechanism; Multi-scale feature fusion; Small object detection; Unmanned aerial vehicle imagery; Aerial photography; Aircraft detection; Image fusion; Information fusion; Object detection; Object recognition; Object tracking; Unmanned aerial vehicles (UAV); Aerial vehicle; Attention mechanisms; Feature information; Features fusions; Fusion mechanism; Multi-scale feature fusion; Multi-scale features; Small object detection; Small objects; Unmanned aerial vehicle imagery; Antennas
Scopus,"Li, Y.; Kong, B.; Yu, W.; Jiang, J.",Small Object Vehicle Detection Based on Improved YOLOv5,,2024,,,,10.1109/ICSP62122.2024.10743827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211453348&doi=10.1109%2fICSP62122.2024.10743827&partnerID=40&md5=1882746209acfc5b0e37a2464bc95dcb,"With the advent of deep learning, object detection has been extensively utilized in intelligent transportation systems. However, the performance of baseline object detection models in traffic scenes is suboptimal, particularly for small object detection. To address this issue, this paper introduces an improved method for small object detection in traffic scenes based on YOLOv5, designated as YOLOv5-CBS. Initially, the convolutional modules of the model network are replaced to enhance the capability of feature extraction. Subsequently, an attention mechanism is incorporated into the Neck section of the model network, which bolsters the network's ability to capture contextual information through bidirectional encoding and decoding. Additionally, the imbalance between simple and challenging samples is mitigated by introducing a weighting function, Slide, into the model's loss function. Experimental results on the SODA10M dataset demonstrate that YOLOv5-CBS achieves average detection accuracies of 64.3% for mAP@0.5, 46.3% for mAP@0.5:0.95, and 16.2% for small object detection, representing improvements of 4.3%, 7.8%, and 2.1% over YOLOv5, respectively. © 2024 IEEE.",Attention mechanism; Object detection; Small target; Deep learning; Object detection; Object recognition; Vehicle detection; Attention mechanisms; Intelligent transportation systems; Learning objects; Model networks; Objects detection; Small object detection; Small objects; Small targets; Traffic scene; Vehicles detection; Intelligent systems
Scopus,"Xu, Y.; Chen, Q.",Radar Can See and Hear as Well: A New Multimodal Benchmark Based on Radar Sensing,,2024,,,,10.1109/JIOT.2024.3396285,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192205171&doi=10.1109%2fJIOT.2024.3396285&partnerID=40&md5=8557fa1028c88ac6a9611c1c4a59a0e7,"Radar technology has emerged as a pivotal component for various applications within the Internet of Things (IoT). To promote the understanding and integration of radar sensing in developing multimodal applications, we introduce the Radar Can See and Hear (RACER) data set. This data set encompasses synchronized radar sensing, audio, and visual data. Radar, with its capability to detect vocal cord vibrations and lip movements, addresses scenarios where conventional microphone and camera setups may falter, such as through-wall or non-line-of-sight sensing. Specifically, the radar discerns and characterizes human lip and vocal cord movements in the range-Doppler domain. We employ deep neural networks to capture the inherent relationships among radar signatures, audio vocal sound, and visual lip movements during human pronunciations. We evaluate the performances of radar sensing using experiments on speech classification, cross-modality retrieval among audio, video, and radar, and cross-modality distillation from video or audio to radar. We summarize the findings and the limitations of using radar sensing in speech-related multimodal analysis applications. Our codes are available at: https://github.com/SPIresearch/RACER. © 2014 IEEE.",Deep neural networks; Internet of Things (IoT); radar sensing; speech classification; Advanced driver assistance systems; Audio acoustics; Deep neural networks; Distillation; Modal analysis; Tracking radar; Cross modality; Lip movements; Multi-modal; Radar applications; Radar detection; Radar sensing; Speech classification; Task analysis; Vibration; Vocal cords; Internet of things
Scopus,"Wang, S.; Jiao, H.; Su, X.; Yuan, Q.",An Ensemble Learning Approach With Attention Mechanism for Detecting Pavement Distress and Disaster-Induced Road Damage,,2024,,,,10.1109/TITS.2024.3391751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192213357&doi=10.1109%2fTITS.2024.3391751&partnerID=40&md5=f950b3336787b2077615d6f043525da8,"Road damage presents a significant risk to traffic safety, including pavement distress and disaster-induced damage. Thanks to their high efficiency, computer vision-based methods for pavement distress detection have been widely developed. In disaster scenarios, the automatic extraction of road damage information from extensive social media images plays a critical role in rescue efforts. However, few existing studies have focused on detecting object-level disaster-induced road damage. To fill the gap, this paper presents a Social media image dataset of Object detection for Disaster-induced Road damage (SODR), including 1,552 images and two categories (i.e., collapses and blockages). Additionally, this paper proposes an ensemble learning approach with attention mechanisms based on YOLOv5 (You Only Look Once) network. Initially, attention modules are employed to create two distinct detectors for ensemble learning. Subsequently, one standard YOLOv5 and two variant networks are trained with consistent settings, and test time augmentation is applied during the inference phase. The proposed method has been implemented across five scales of YOLOv5, offering alternatives for balancing accuracy and computational cost. To demonstrate the validity, comprehensive experiments were conducted on two datasets. Compared with some mainstream detectors and ensemble learning methods, our approach achieved competitive results with a fewer number of parameters and a simpler training and testing process. The SODR dataset and source code are available at (https://github.com/nonondayo/yolov5_SODRv1).  © 2024 IEEE.",attention mechanism; disaster response; ensemble learning; object detection; Road damage detection; Damage detection; Disasters; E-learning; Learning systems; Object detection; Object recognition; Pavements; Attention mechanisms; Disaster-response; Ensemble learning; Objects detection; Road; Road damage; Road damage detection; Social networking (online); YOLO; Social networking (online)
Scopus,"Chandrasekaran, N.; Alamsyah, N.; Haq, Q.M.U.; Fan, H.-W.; Chou, C.-Y.; Lu, K.-H.",Advancing Aerial Imagery Analysis: A Comparative Study of YOLOv3 and YOLOv8 on the VisDrone Dataset,,2024,,,,10.1109/ICE3IS62977.2024.10775447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215108958&doi=10.1109%2fICE3IS62977.2024.10775447&partnerID=40&md5=9a2efab8d40baf458605a8bf860c0cfe,"Object detection in aerial imagery is crucial for applications such as security monitoring, traffic oversight, and emergency response. This study investigates the effectiveness of the YOLOv3 model in identifying objects within the diverse VisDrone dataset, focusing on its ability to manage varying object scales, occlusions, and intricate backgrounds in real-time scenarios. YOLOv3°s architecture is leveraged for its proficiency in handling these challenges, making it highly suitable for practical applications such as identifying vehicles and pedestrians in traffic management systems, monitoring large areas for security purposes, and detecting critical situations in disaster response operations. Comprehensive experimentation demonstrates YOLOv3s capability to address the unique challenges of aerial imagery. Data augmentation and transfer learning techniques are employed to enhance detection accuracy. A comparative analysis with the YOLOv8 model reveals that YOLOv8 consistently outperforms YOLOv3 in terms of mean Average Precision (mAP), underscoring advancements in the latest model. These findings highlight the potential of both models to significantly improve aerial surveillance, traffic management, and disaster response, providing robust tools for precise and efficient object detection in drone-captured images. © 2024 IEEE.",Aerial Imagery; Deep Learning; Object Detection; VisDrone Dataset; YOLOv3; Aircraft accidents; Emergency traffic control; Highway administration; Image analysis; Image enhancement; Intelligent systems; Network security; Aerial imagery; Comparatives studies; Deep learning; Emergency response; Imagery analysis; Objects detection; Real- time; Security monitoring; Visdrone dataset; YOLOv3; Aerial photography
Scopus,"Dixit, I.A.; Bhoite, S.",Analysis of Performance of YOLOv8 Algorithm for Pedestrian Detection,,2024,,,,10.1109/ICCES63552.2024.10859981,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218420349&doi=10.1109%2fICCES63552.2024.10859981&partnerID=40&md5=aeb632c3747a291a4ce56383cb9db316,"Pedestrian detection is widely used nowadays in several applications like Robotics, Security systems and autonomous driving systems. Pedestrian detection is a rapidly advancing field within computer vision. Many algorithms have been developed for pedestrian detection using various deep learning approaches. Despite this development, the task of pedestrian detection from a mixed object dataset containing pedestrian and non-pedestrian images still poses a big challenge. After going through latest papers, it has been found that there is a need for enhancement in the correctness and speed of pedestrian detection for use in real world scenarios like autonomous driving systems. This paper proposes to analyze the execution of the Yolo V8algorithm with an aim of pedestrian detection. To achieve this objective, we have used machine learning pipeline from data collection to evaluation with the help of Yolo V8. The objectives of this paper are to observe and analyse the performance of the Yolo V8 model for pedestrian detection, to compare the time required by the model for two different datasets and to fine tune the model as per requirement. The Yolo v8 algorithm has demonstrated great success in the domain of computer vision for real time object detection. Initially, this algorithm was applied on the Roboflow Pedestrian Yolo V5 dataset and has shown to effectively improve pedestrian detection accuracy as the number of epochs increases. Later the algorithm was applied on the Yolo dataset Sindhi Label containing pedestrian as well as non-pedestrian images. This was done to avoid the chances of overfitting the model. © 2024 IEEE.",Computer Vision; Deep learning; Mixed Object Dataset; Pedestrian detection; Security System; Yolo V8; Adversarial machine learning; Deep learning; Object detection; Autonomous driving; Deep learning; Driving systems; Learning approach; Machine-learning; Mixed object dataset; Pedestrian detection; Performance; Real-world scenario; Yolo v8; Machine vision
Scopus,"Li, M.; Lan, J.; Wang, L.; Zhang, Y.; Huang, K.",Infrared Multiobject Contrast Enhancement and Detection Based on Layered Visual Transformer Network for Autonomous Driving,,2024,,,,10.1109/JSEN.2024.3466397,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205826122&doi=10.1109%2fJSEN.2024.3466397&partnerID=40&md5=df181e27121ad5ff0531eccf335c918f,"For autonomous driving in urban environments, achieving reliable object detection under various lighting conditions is crucial. Thermal infrared cameras, capable of capturing images passively without relying on external light sources, face challenges due to the low contrast of infrared images, which complicates the detection and recognition of multiple objects. To address this challenge, we propose the layered visual transformer network (LVTN), which is divided into three components: backbone, encoder, and decoder and detection head: 1) the backbone network, LVTN, transforms image pixels into vectors and employs layering techniques combined with feature aggregation from each layer to maintain the integrity of features and information in infrared images; 2) the encoder introduces adaptive-feedback attention (AFA) to replace traditional attention mechanisms, focusing on subtle object features and enhancing the contrast between the object and the background; and 3) the decoder and detection head introduces the fine-grained matching loss function (FMLF), which dynamically adjusts training weights, gives higher attention to object-dense regions, and addresses the issues of multiscale and dense object detection. We trained and validated our model on the FLIR-ADAS and KAIST datasets, achieving mAP scores of 62.6% and 75.6%, respectively, surpassing other state-of-the-art infrared detection algorithms. © 2001-2012 IEEE.",Attention mechanism; automatic driving; infrared (IR) object detection; thermal infrared camera; transformer network; Image coding; Image enhancement; Infrared detectors; Infrared radiation; Laser beams; Network coding; Remote sensing; Temperature indicating cameras; Attention mechanisms; Automatic driving; Infra-red cameras; Infrared  object detection; Infrared cameras; Infrared object; Objects detection; Thermal infrared camera; Thermal-infrared; Transformer network; Infrared imaging
Scopus,"Kumar, A.K.; Palanisamy, V.","Detection of lanes, obstacles and drivable areas for self-driving cars using multifusion perception metrics",,2024,,,,10.32629/jai.v7i3.1059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186233295&doi=10.32629%2fjai.v7i3.1059&partnerID=40&md5=ec31450ae10d440aee0381249ae766f5,"Autonomous vehicles have been a recent trend and active research area from the onset of machine learning and deep learning algorithms. Computer vision and deep learning techniques have simplified the operations of continuous monitoring and decision-making capabilities of autonomous vehicles. A navigation system is facilitated by a visual system, where sensors and collectors process input in form of images or videos, and the navigation system will be making certain decisions to adhere to the safety of drivers and passers-by. This research article contemplates the model of obstacle detection, lane detection, and how the vehicle is supposed to act in terms of autonomous driving situation. This situation should resemble human driving conditions and should ensure maximum safety to both the stakeholders. A unified neural network for detecting lanes, objects, obstacles and to advise the driving speed is defined in this architecture. As far as autonomous driving is considered, these target elements are considered to be the predominant areas of focus for autonomous driving vehicles. Since capturing the images or videos have to be performed in real-time scenarios and processing them for relevant decision making have to be completed at a swift pace, a concept of context tensors is introduced in the decoders for discriminating the tasks based on priority. Every task is associated with the other tasks and also the decision-making process and hence this architecture will continue to learn every day. From the obtained results, it is evident that multitask networks can be improved using the proposed method in terms of accuracy, decision-making capability and reduced computational time. This model investigates the performance using Berkeley deep drive datasets which are considered to be a challenging dataset. © 2024 by author(s).","autonomous vehicle; deep learning; image processing; self-driving car, multi-task network"
Scopus,"Liu, H.; Cheng, W.; Li, C.; Xu, Y.; Fan, S.",Lightweight Detection Model RM-LFPN-YOLO for Rebar Counting,,2024,,,,10.1109/ACCESS.2024.3349978,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182349392&doi=10.1109%2fACCESS.2024.3349978&partnerID=40&md5=13592489c1f79fce8dd07c5bd73e9c3a,"In this study, we propose a novel lightweight detection model for rebar counting, which is rectified mobilenet lightweight feature pyramid network based on YOLO (RM-LFPN-YOLO). The model incorporates a lightweight backbone network that integrates the coordinate attention (CA) mechanism, a lightweight feature pyramid network (LFPN), and a loss function that combines focal loss and efficient intersection over union (EIOU) loss, all meticulously designed to enhance the model's performance. Experimental results demonstrate that our improved algorithm, with a mere 25.08M parameters, computes efficiently at 7.60G with an input size of 416 pixels. Additionally, it achieves an impressive average precision (AP) of 99.03% at an IOU of 0.5. The proposed lightweight model can be deployed on embedded devices and achieve efficient rebar detection and counting performance.  © 2013 IEEE.",attention; focal loss; LFPN; YOLO; Job analysis; Attention; Computational modelling; Feature pyramid; Features extraction; Focal loss; Lightweight feature pyramid network; Pyramid network; Task analysis; YOLO; Feature extraction
Scopus,"He, X.; Wu, D.; Wu, D.; You, Z.; Zhong, S.; Liu, Q.",Millimeter-Wave Radar and Camera Fusion for Multiscenario Object Detection on USVs,,2024,,,,10.1109/JSEN.2024.3444826,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201757832&doi=10.1109%2fJSEN.2024.3444826&partnerID=40&md5=e751946d2fb59069624d3be042b83e9f,"Accurate object detection is fundamental for unmanned surface vehicles (USVs) to achieve intelligent perception. This article proposes an object detection network that integrates millimeter-wave radar and a camera. The method utilizes the complementary advantages of millimeter-wave radar and camera data modalities to realize multiscenario object detection for USVs applications. To address the drawback of sparse point clouds in millimeter-wave radar and improve the suboptimal performance of the camera in adverse weather conditions and small object detection, as well as to effectively utilize the features of both millimeter-wave radar and camera, a multisensor deep learning fusion object detection network [fusion mixture with AFPN (FMA)-fully convolutional one-stage (FCOS)] is proposed. To validate the effectiveness of FMA-FCOS, training, and testing are conducted on the multiscenario vessel dataset collected specifically for this study and the nuScenes dataset. In comparison with methods solely relying on a camera, such as the original FCOS object detection framework and YOLOv9, as well as other fusion methodologies combining camera and radar, the results demonstrate that FMA-FCOS delivers notable advantages, achieving a superior or comparable detection accuracy in the datasets.  © 2001-2012 IEEE.",Deep learning; fusion mixture with AFPN (FMA)-fully convolutional one-stage (FCOS); multiscenario; object detection; sensor fusion; unmanned surface vehicle (USV); Deep learning; Image coding; Image segmentation; Remote sensing; Sensor data fusion; Ship testing; Unmanned surface vehicles; Deep learning; Features extraction; FMA-FCOS; Millimeter-wave radar; Millimetre-wave radar; Multi scenarios; Objects detection; Radar detection; Sensor fusion; Surface vehicles; Radar imaging
Scopus,"Li, Y.; Kong, Z.; Xu, Q.",A Novel Collaborative Heterogeneous Supervision Network for Small Object Detection Method Based on Panchromatic and Hyperspectral Images,,2024,,,,10.1109/ICCSSE63803.2024.10823779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217238054&doi=10.1109%2fICCSSE63803.2024.10823779&partnerID=40&md5=f9ca0ffc55fa6f375a3fe556aeeafb53,"With the increasing availability of simultaneous panchromatic and hyperspectral images, object detection methods based on them have demonstrated significant application advantages. However, they still face several challenges that limit detection performance: 1) the sizes of small objects remain very small even in the fused images, insufficient texture information and spectral information that is easily confused, leading to lower accuracy in object detection; 2) etection-by-Preprocess (DBP) methods often suffer from spectral and spatial detail distortions, compromising target features; 3) Preprocess-free detection (PFD) methods extract panchromatic and hyperspectral features directly through networks, but the black-box nature of deep network makes it difficult to ensure precise alignment of these two types of features, thereby hindering further improvements in detection accuracy. Therefore, this paper proposed a novel Collaborative Heterogeneous Supervision Network (CHS-Net) for small object detection on panchromatic and hyperspectral images. First, integrating fusion and detection components into a heterogeneous supervision network enhances learning capabilities by incorporating more empirical knowledge. Second, a unified joint regulation strategy was introduced to enhance integrated learning capabilities using optimized feedback loss functions. This approach enhanced the attention of different components to target features, effectively improving weak small target detection performance. Finally, comparative experiments based on EO-1 dataset demonstrate that the proposed method outperforms many start-of-the-art approaches. © 2024 IEEE.",Collaborative supervision; Deep learning; Object Detection; Contrastive Learning; Deep learning; Image fusion; Image texture; Supervised learning; Collaborative supervision; Deep learning; Detection performance; HyperSpectral; Learning capabilities; Object detection method; Objects detection; Preprocess; Small object detection; Target feature; Self-supervised learning
Scopus,"Zhang, H.; Li, Y.; Leng, L.; Che, K.; Liu, Q.; Guo, Q.; Liao, J.; Cheng, R.",Automotive Object Detection via Learning Sparse Events by Spiking Neurons,,2024,,,,10.1109/TCDS.2024.3410371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195394926&doi=10.1109%2fTCDS.2024.3410371&partnerID=40&md5=60c834f76394e69420a3c55573c6f1be,"Event-based sensors, distinguished by their high temporal resolution of 1 μ s and a dynamic range of 120 dB, stand out as ideal tools for deployment in fast-paced settings such as vehicles and drones. Traditional object detection techniques that utilize artificial neural networks (ANNs) face challenges due to the sparse and asynchronous nature of the events these sensors capture. In contrast, spiking neural networks (SNNs) offer a promising alternative, providing a temporal representation that is inherently aligned with event-based data. This article explores the unique membrane potential dynamics of SNNs and their ability to modulate sparse events. We introduce an innovative spike-triggered adaptive threshold mechanism designed for stable training. Building on these insights, we present a specialized spiking feature pyramid network (SpikeFPN) optimized for automotive event-based object detection. Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional SNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPN achieves a mean average precision (mAP) of 0.477 on the GEN1 automotive detection (GAD) benchmark dataset, marking significant increases over the selected SNN baselines. Moreover, the efficient design of SpikeFPN ensures robust performance while optimizing computational resources, attributed to its innate sparse computation capabilities.  © 2016 IEEE.",Deep learning; dynamical vision sensor (DVS); object detection; spiking neural networks (SNNs); Deep learning; Job analysis; Neural networks; Object detection; Object recognition; Adaptation models; Deep learning; Dynamical vision sensor; Features extraction; Neural-networks; Objects detection; Spiking neural network; Task analysis; Vehicle's dynamics; Vision sensors; Feature extraction
Scopus,"Chen, Y.; Wang, H.",Accurate and Robust Roadside 3-D Object Detection Based on Height-Aware Scene Reconstruction,,2024,,,,10.1109/JSEN.2024.3444816,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201784278&doi=10.1109%2fJSEN.2024.3444816&partnerID=40&md5=7155aa34cbe393ae2ffb08b7bf944766,"Roadside 3-D object detection allows for a drastic expansion of the visibility range and a reduction in occlusions for autonomous vehicles. Recent approaches are based on bird's-eye view (BEV) fusion, which unifies multimodal features in the shared BEV representation space. However, the camera-to-BEV projection throws away the geometric information of camera features, hindering the effectiveness of such methods. Besides, depth-based camera lifting results in inefficiency and instability in disrupted roadside scenarios. To address these challenges, this article introduces a novel 3-D object detection framework based on height-aware scene reconstruction, dubbed HSRDet. Specifically, we leverage height-aware 3-D reconstruction to ensure geometric consistency in BEV feature mapping and employ a fast camera-to-BEV transformation based on feature distillation to boost efficiency without compromising performance. In addition, we integrate a novel data augmentation method, namely, View Shake (VS), to further improve the performance of our model. Extensive experiments on the DAIR-V2X dataset demonstrate that HSRDet not only achieves state-of-the-art detection accuracy but also exhibits strong robustness in disturbance scenarios. Further experiments on the intelligent roadside units (RSUs) have revealed that our method runs stably at 11.8 frames/s on a RTX 3090 Ti GPU, thus promising vast engineering application prospects.  © 2001-2012 IEEE.",3-D object detection; bird's-eye view (BEV); intelligent transport system; multimodality fusion; 3D object; 3d ojbect detection; Accuracy; BEV; Intelligent transport; Intelligent transport system; Multi-modality fusion; Objects detection; Three-dimensional display; Transport systems
Scopus,"Ardic, O.; Cetinel, G.",Deep Learning-Based Real-Time Engine Part Inspection With Collaborative Robot Application,,2024,,,,10.1109/ACCESS.2024.3489714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208389742&doi=10.1109%2fACCESS.2024.3489714&partnerID=40&md5=e55ce0c668feae444927074b4f5986ca,"Vehicle manufacturing requires error-free processes, as modern vehicles are made up of thousands of parts, including around 280 critical components for safe driving. According to the National Highway Traffic Safety Administration (NHTSA), 2% of vehicle accidents will be caused by defective parts. Current inspection systems in manufacturing plants have limitations, with a high risk of defective parts reaching consumers and leading to recalls. This study aims to develop a real-time, deep learning-based engine part inspection system to improve accuracy and efficiency in mass production. The system, implemented in a large automotive manufacturing plant, uses a Fanuc CR-15ia collaborative robot to inspect engine parts. Combining the single-shot detector (SSD) and faster region-based convolutional neural network (R-CNN) algorithms, the system achieves 99.9% accuracy measured after four months of use, with an Average Precision (AP) of 0.994 for Faster R-CNN and 0.955 for SSD. The inspection system addresses cycle time concerns and is integrated with factory systems for real-time data sharing. Ongoing enhancements aim to improve system performance further.  © 2013 IEEE.",Collaborative robot; deep learning; part inspection; real-time applications; Automobile engine manufacture; Automobile plants; Energy security; Highway accidents; Industrial robots; Inspection; Intelligent robots; Robot learning; Collaborative robots; Deep learning; Defective parts; Engine parts; Inspection system; Manufacturing plant; Part inspection; Real- time; Real-time application; Single-shot; Collaborative robots
Scopus,"Li, K.; Dai, Z.; Zuo, C.; Wang, X.; Cui, H.; Song, H.; Cui, M.",Scene adaptation in adverse conditions: a multi-sensor fusion framework for roadside traffic perception,,2024,,,,10.1080/15472450.2024.2390844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201549876&doi=10.1080%2f15472450.2024.2390844&partnerID=40&md5=9f0145f245b6fe0b16a89a66bc341fd1,"Robust roadside traffic perception requires integrating the strengths of multi-source sensors under various adverse conditions, which is challenging but indispensable for formulating effective traffic management strategies. One limitation of existing radar-camera perception systems is that they focus on integrating multi-source information without directly considering scene information, leading to difficulties in achieving scene adaptive fusion. How to establish the connection between scene information and multi-source information is the key challenge to solving this problem. In this article, we propose a Scene adaptive Sensor Fusion (SSF) framework that characterizes scene information and integrates it into radar-camera fusion schemes, aiming to achieve high-quality roadside traffic perception. Specifically, we introduce a multi-source object association method that accurately associates multi-source sensor information on the roadside. We then utilize coding techniques to characterize the scene information, including visibility characterization regarding lighting and weather conditions, and road characterization regarding sensor viewpoint. By incorporating sensor and scene information into the fusion model, the SSF framework effectively establishes the connection between them. We evaluate the SSF framework on the Roadside Radar and Video Dataset (RRVD) and the Traffic flow Parameter Estimation Dataset (TPED), both collected from real-world traffic scenarios. Experiments demonstrate that SSF significantly improves vehicle detection accuracy under various adverse conditions compared to traditional single-source sensing methods and other state-of-the-art fusion techniques. Furthermore, vehicle trajectories based on SSF detection results enable accurate traffic parameter estimation, such as volume, speed, and density, in complex and dynamic environments. © 2024 Taylor & Francis Group, LLC.",multi-sensor fusion; scene adaptation; traffic parameter estimation; traffic perception; Air traffic control; Highway administration; Highway traffic control; Motor transportation; Street traffic control; Time difference of arrival; Vehicle detection; Condition; Multi-sensor fusion; Multi-source informations; Multi-Sources; Parameters estimation; Scene adaptation; Sensor fusion; Traffic parameter estimation; Traffic parameters; Traffic perception
Scopus,"Devi, S.; Dayana, R.; Vadivukkarasi, K.; Malarvezhi, P.",Navigating the Domain Shift: Object Detection in Indian Road Datasets,,2024,,,,10.1007/978-3-031-64067-4_21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202190456&doi=10.1007%2f978-3-031-64067-4_21&partnerID=40&md5=0b8c2e06c6c53bfc2fcb479f3e3d9bd8,"Object detection plays a vital role in several applications, including autonomous driving systems and intelligent transportation systems. However, accurate Object detection in unstructured road environments, particularly on Indian roads, poses a significant challenge due to the diverse and complex nature of the surroundings and perpetually changing environment. Moreover, the limited availability of annotated data exacerbates the problem. Real-world deployment of object detection models often introduces a significant challenge: domain shift. To address this issue, we introduced a new augmentation technique called InterAug. The suggested method may use region of an object using a bounding box to define a “semantic context” for each object in contrast to existing policies. InterAug approach illustrates the feasibility of choosing the most suitable context for each object within a specific scene through annotation, as opposed to altering the entire scene or solely specifying bounding boxes. To prove effectiveness of proposed work makes use of an extensive Indian road dataset JUVDsi V1, that includes a variety of terrains, weather patterns, and road kinds. Through extensive experiments and evaluations using benchmark models Faster R-CNN and YOLOV5, we’ve demonstrated our approach’s remarkable effectiveness, achieving a substantial 4.3% accuracy boost compared to the baseline. We address the critical issue of domain shift, emphasizing the importance of day-to-night adaptation in challenging Indian road scenarios. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",data augmentation; Data scarcity; Day to Night adaptation; Domain Shift; Indian Road conditions; Object Detection; Motor transportation; Autonomous driving; Bounding-box; Data augmentation; Data scarcity; Day to night adaptation; Domain shift; Driving systems; Indian road condition; Objects detection; Road condition; Intelligent systems
Scopus,"Li, F.; Zhao, Y.; Wei, J.; Li, S.; Shan, Y.",SNCE-YOLO: An Improved Target Detection Algorithm in Complex Road Scenes,,2024,,,,10.1109/ACCESS.2024.3481642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207423589&doi=10.1109%2fACCESS.2024.3481642&partnerID=40&md5=fc4b6362e0585c523ec50eac49c165c3,"Autonomous driving is a prominent research area, with the primary challenge being the precise perception and interpretation of the surrounding environment. To address issues such as low resolution, dense occlusion, and small targets in complex road scenes, we propose an improved road target detection algorithm, SNCE-YOLO, based on YOLOv5. Firstly, to avoid the loss of feature information during training for low-resolution and small targets, the Space-to-Depth Convolution(SPD-Conv) is introduced to replace the original convolutional layer for retaining global feature information. Secondly, to address the issue of confusing complex backgrounds with detection targets when features are extracted too early and deeply, the Normalization-based Attention Module (NAM) is incorporated into the backbone network without increasing the computational effort, focusing on the channel and spatial information of interest. Thirdly, to enhance the capture and fusion of multi-level feature information, we optimized the neck network's feature fusion module by implementing the C2f structure. Finally, to reduce the bias in the regression of the occluded target positions, we introduced the EIOU loss function to better define the relationship between the predicted and real frames. The SNCE-YOLO algorithm achieves a mean Average Precision (mAP@0.5) of 91.9% on the KITTI dataset, which represents a 2.6% improvement over the baseline model. Additionally, with a detection speed of 95.24 frames per second (FPS), it meets the requirements for real-time target detection in complex road scenes. Comparisons are also made with other mainstream target detection algorithms to verify the effectiveness of SNCE-YOLO. © 2013 IEEE.",Autonomous driving; C2f; EIOU; NAM; SPD-Conv; YOLOv5; Autonomous driving; C2f; EIOU; Feature information; Lower resolution; Normalisation; Normalization-based attention module; Space-to-depth convolution; Target detection algorithm; YOLOv5; Autonomous vehicles
Scopus,"Li, K.; Cui, H.; Dai, Z.; Song, H.",RVIFNet: Radar-Visual Information Fusion Network for All-Weather Vehicle Perception in Roadside Monitoring Scenarios,,2024,,,,10.1109/JSEN.2024.3481492,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207376332&doi=10.1109%2fJSEN.2024.3481492&partnerID=40&md5=98e90ee663f199d89becf2ff75654343,"Achieving all-weather vehicle perception in roadside monitoring systems composed of cameras and millimeter-wave radars is challenging, primarily due to the ineffective integration of information from these two sources. Particularly in adverse weather conditions, this lack of integration can lead to the monitoring system's inability to promptly identify and address hazardous situations. However, current fusion methods often have the problem of being dominated by visual information, and they do not sufficiently utilize the complementary aspects of the two-source information. In this article, we present the radar and visual information fusion network (RVIFNet), a novel method that tackles these challenges through enhanced radar data representation and multilevel fusion strategies. First, we develop a pseudoimage representation method for sparse radar data and its feature extraction technique, which enhances its expressiveness and lays the groundwork for feature-level fusion with visual features. Second, we propose a multilevel fusion approach that leverages the complementary attributes of the dual-modal data in terms of spatial localization, resolution, and semantic understanding to achieve fusion at levels of low-level semantics, high-level semantics, and anchor box level. In additiona, we introduce the monitoring perspective for radar and camera (MPRC) dataset, collected and annotated specifically for roadside monitoring scenarios, and elaborate on the spatial-temporal synchronization method for the dual-source data. We evaluate RVIFNet on MPRC and the widely used in-vehicle dataset NuScenes, confirming its effectiveness for all-weather vehicle detection. To best of the authors' knowledge, this work is among the early attempts to fuse radar and camera data for all-weather vehicle perception in the roadside monitoring scenarios. © 2024 IEEE.",All-weather vehicle perception; deep learning; radar-visual fusion; roadside monitoring; spatial-temporal synchronization; Network security; All-weather vehicle perception; Deep learning; Monitoring system; Radar information; Radar-visual fusion; Roadside monitoring; Spatial temporals; Spatial-temporal synchronization; Temporal synchronization; Visual information fusion; Data fusion
Scopus,"Pu, X.; Xu, X.; Yu, Y.",LinkedFormer: Radar Communication and Multiscale Imaging for Object Detection under Complex Sea Background,,2024,,,,10.18494/SAM5062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201163478&doi=10.18494%2fSAM5062&partnerID=40&md5=a2bebf4c01fbcb577fbed25ebf01b653,"The advent of deep learning has propelled significant advancements in object detection, thereby enhancing the intelligence of underwater autonomous driving systems. In this paper, we explore the cutting-edge applications of autonomous driving technology in the field of underwater exploration, addressing the pivotal role of target detection in navigating and executing tasks within challenging marine environments. In this study, the object detection capability of such systems is enhanced by integrating deep learning and multisensor fusion technology, especially by combining high-precision sensor data with multitask learning models to achieve efficient and robust detection. Our study has three principal contributions. First, we introduce a novel light perception detection system that combines monocular camera technology with 4D radar. It enriches environmental perception by weaving in radar signals and significantly enhances the accuracy and stability of target detection. Second, we have developed a dual-modal detection framework, named Radar-Picture Detection, which utilizes a parallel sequence prediction method. This approach prioritizes radar signal processing, aiding in the improvement of target detection accuracy in intricate underwater environments. Third, we conducted a comprehensive evaluation of our model’s performance using the FloW Dataset, which is specifically curated for identifying floating waste in inland waters through unmanned vessel footage. We not only propel forward the field of target detection for underwater autonomous systems but also establish new avenues and a solid foundation for deploying deep learning and multisensor fusion technology in marine environmental perception. Insights and methodologies from this study are poised to spearhead further developments in autonomous marine exploration, enhancing safety, efficiency, and our understanding of underwater environments. © MYU K.K.",4D radar; multisensor fusion techniques; object detection; underwater autonomous communication; Autonomous vehicles; Deep learning; Marine applications; Marine radar; Military applications; Object recognition; Radar imaging; Tracking radar; Underwater acoustics; Underwater imaging; 4d radar; Autonomous communications; Autonomous driving; Environmental perceptions; Fusion technology; Multi-sensor fusion techniques; Objects detection; Targets detection; Underwater autonomous communication; Underwater environments; Object detection
Scopus,"Widodo, H.; Taufiqurrohman, H.; Muis, A.; Wijayanto, Y.N.; Prihantoro, G.; Dwiyanti, H.; Cahya, Z.; Widaryanto, A.; Nugroho, T.H.",Experimental Evaluation of Pothole Detection and Its Dimension Estimation Using YOLOv8 and Depth Camera for Road Surface Analysis,,2024,,,,10.1109/ICRAMET62801.2024.10809331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215943526&doi=10.1109%2fICRAMET62801.2024.10809331&partnerID=40&md5=89fe54267b767c91e80d8a360a0b9793,"Pothole detection and dimension estimation are essential to improve the safety and comfort of autonomous vehicles. This paper uses a depth camera to present a new approach for position estimation and pothole detection using the You Only Look Once (YOLO) method. Experimental results on detection applications using real cars show good pothole detection accuracy in both bright and dimly lit road conditions covered by trees with precision. This object detection uses the YOLO version 8 nano model accompanied by coco as pre-trained training. In the training dataset, only one class, namely the pothole dataset, is used. A depth camera from Intel Realsense type D455 will be employed, and the Jetson Orin Nano will subsequently apply the training results. During the field test, data about the position of the pothole coordinate inside the pixel frame, the pothole's width, and its distance from the camera will be displayed, in addition to pothole-detecting objects. Every pothole object found will have its data shown in real time. Validating the width and length of the pothole involves taking actual measurements with a meter. The estimated distance and width of potholes showed good agreement with direct manual measurements with R -squared values above 0.97 and gradients approaching unity. © 2024 IEEE.",autonomous vehicle; pothole detection; pothole distance; pothole width; road surface; unstructured path; Autonomous Vehicles; Depth camera; Detection estimation; Dimension estimation; Experimental evaluation; Pothole detection; Pothole distance; Pothole width; Road surfaces; Unstructured path; Magnetic levitation vehicles
Scopus,"Wen, L.; Peng, Y.; Lin, M.; Gan, N.; Tan, R.",Multi-Modal Contrastive Learning for LiDAR Point Cloud Rail-Obstacle Detection in Complex Weather,,2024,,,,10.3390/electronics13010220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181920159&doi=10.3390%2felectronics13010220&partnerID=40&md5=4549e001b848e517543856cd2d27da05,"Obstacle intrusion is a serious threat to the safety of railway traffic. LiDAR point cloud 3D semantic segmentation (3DSS) provides a new method for unmanned rail-obstacle detection. However, the inevitable degradation of model performance occurs in complex weather and hinders its practical application. In this paper, a multi-modal contrastive learning (CL) strategy, named DHT-CL, is proposed to improve point cloud 3DSS in complex weather for rail-obstacle detection. DHT-CL is a camera and LiDAR sensor fusion strategy specifically designed for complex weather and obstacle detection tasks, without the need for image input during the inference stage. We first demonstrate how the sensor fusion method is more robust under rainy and snowy conditions, and then we design a Dual-Helix Transformer (DHT) to extract deeper cross-modal information through a neighborhood attention mechanism. Then, an obstacle anomaly-aware cross-modal discrimination loss is constructed for collaborative optimization that adapts to the anomaly identification task. Experimental results on a complex weather railway dataset show that with an mIoU of 87.38%, the proposed DHT-CL strategy achieves better performance compared to other high-performance models from the autonomous driving dataset, SemanticKITTI. The qualitative results show that DHT-CL achieves higher accuracy in clear weather and reduces false alarms in rainy and snowy weather. © 2024 by the authors.",complex weather; contrastive learning; multi-modal; point clouds; rail-obstacle detection; semantic segmentation
Scopus,"Xu, H.; Zhang, X.; He, J.; Geng, Z.; Pang, C.; Yu, Y.","Surround-View Water Surface BEV Segmentation for Autonomous Surface Vehicles: Dataset, Baseline and Hybrid-BEV Network",,2024,,,,10.1109/TIV.2024.3395653,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192186582&doi=10.1109%2fTIV.2024.3395653&partnerID=40&md5=892f88dd7e772ada9e33ec3c9609e373,"Autonomous surface vessels (ASVs) are growing rapidly due to their ability to execute hazardous and time-consuming missions over water surfaces. Recently, camera-based surround-view bird's eye view (BEV) segmentation has attracted increasing attention because of its popular application in autonomous driving. However, most existing surround-view BEV segmentation studies have focused on road scenes for autonomous cars, and the development of water surface surround-view BEV segmentation has been relatively slow due to the absence of water surface BEV datasets. Different from road scenes with complex dynamic scenes, water surface BEV segmentation task also face new interference like vessel swaying and reflection interference. To address these problems and stimulate relevant research, we first introduce the WSBEV dataset, a surround-view visual segmentation dataset on water surface scenes. Notably, our compiled WSBEV utilizes a hybrid camera configuration and contains pinhole cameras and fisheye cameras with different focal lengths, which also brings new challenges for the surround-view BEV segmentation method. To address the issues above, we propose a novel water surface BEV segmentation network named Hybrid-BEV that supports hybrid camera inputs. The geometry-based view transformer module is designed to directly transfer the extracted feature from the perspective view to BEV space. The pose query adjustment module introduces vessel pose data to suppress vessel swaying interference. Compared to other baseline methods, our Hybrid-BEV achieves excellent accuracy in our WSBEV dataset. Extensive experiments validate the effectiveness of our approach and the exceptional generalizability of the WSBEV dataset. © 2016 IEEE.",Autonomous surface vessels; deep learning; surround-view BEV segmentation; visual perception dataset; Autonomous vehicles; Deep learning; Pinhole cameras; Roads and streets; Surface waters; Unmanned surface vehicles; Vision; Autonomous surface vessels; Deep learning; Images segmentations; Interference; Road; Sea surfaces; Surround-view BEV segmentation; Visual perception; Visual perception dataset; Image segmentation
Scopus,"Gheorghe, C.; Duguleana, M.; Boboc, R.G.; Postelnicu, C.C.",Analyzing Real-Time Object Detection with YOLO Algorithm in Automotive Applications: A Review,,2024,,,,10.32604/cmes.2024.054735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208374812&doi=10.32604%2fcmes.2024.054735&partnerID=40&md5=1dbb6200abcce710e766a7958f9bc358,"Identifying objects in real-time is a technology that is developing rapidly and has a huge potential for expansion in many technical fields. Currently, systems that use image processing to detect objects are based on the information from a single frame. A video camera positioned in the analyzed area captures the image, monitoring in detail the changes that occur between frames. The You Only Look Once (YOLO) algorithm is a model for detecting objects in images, that is currently known for the accuracy of the data obtained and the fast-working speed. This study proposes a comprehensive literature review of YOLO research, as well as a bibliometric analysis to map the trends in the automotive field from 2020 to 2024. Object detection applications using YOLO were categorized into three primary domains: road traffic, autonomous vehicle development, and industrial settings. A detailed analysis was conducted for each domain, providing quantitative insights into existing implementations. Among the various YOLO architectures evaluated (v2-v8, H, X, R, C), YOLO v8 demonstrated superior performance with a mean Average Precision (mAP) of 0.99.  © 2024 The Authors.",automotive; autonomous vehicles; industry; traffic; YOLO; Automotive industry; Object detection; Automotive applications; Automotives; Autonomous Vehicles; Images processing; Objects detection; Real- time; Single frames; Technical fields; Traffic; You only look once
Scopus,"Gong, D.; Li, J.; Wang, C.; Wang, Z.",PVSA : A General and Elegant Sampling Algorithm for Voxel-Based 3D Object Detection,,2024,,,,10.1109/ICCAR61844.2024.10569566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198229865&doi=10.1109%2fICCAR61844.2024.10569566&partnerID=40&md5=73c389ea7cc7bedae1c34f3c0941ce10,"Perceiving the environment is vital for autonomous vehicles as it serves as the foundation for decision making and path planning. LiDAR is a widely employed sensor, which produces a voluminous and sparsely populated point cloud. For voxel-based 3D object detection methods, the initial step involves the division of the raw point cloud into voxels, the process known as voxelization. Nevertheless, once the number of point clouds contained within a voxel reaches the certain threshold, the allocation of additional point clouds to that voxel ceases. This leads to a greater degree of information loss. Scholars primarily focus on the subsequent stages following voxelization, such as feature extraction and utilization. We first focus on the sampling issue during the voxelization. In the paper, we propose a general and elegant Points in Voxel Sampling Algorithm module named PVSA. During the voxelization, the assignment of all points into their respective voxels continues even after the maximum number of points in a voxel has been reached. For voxels in which the number of internal point clouds exceeds the certain threshold, the farthest distance sampling method is utilized as it ensures a genuine and uniform distribution of the point cloud within the voxel. We conducted an evaluation of the proposed module using the Kitti dataset. Experimental findings suggest that the incorporation of the PVSA module enhances the object detection capabilities of the voxel-based model, particularly in the identification of samll targets like pedestrians. The incorporation of PVSA modules significantly enhances Pillarnet's capacity to recognize pedestrians, resulting in a 46.2% pt improvement in performance at a distance of 20 meters. On average, there is an enhancement of 1.43% pt.  © 2024 IEEE.",Autonomous Vehicle; Sampling Algorithm; Small Object Detection; Voxelization; Autonomous vehicles; Decision making; Learning algorithms; Motion planning; Object recognition; 3D object; Autonomous Vehicles; Decision paths; Decisions makings; Object detection method; Objects detection; Point-clouds; Sampling algorithm; Small object detection; Voxelization; Object detection
Scopus,"Kim, J.; Kim, J.",Perception and Sensing Technologies for Maritime Autonomous Ships,,2024,,,,10.5302/J.ICROS.2024.24.0024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191201499&doi=10.5302%2fJ.ICROS.2024.24.0024&partnerID=40&md5=1b00abdabfc4ec465c424116924e0d7e,"Recent advancements in artificial intelligence and sensor technology have promoted increasing interest in maritime autonomous ships. In this paper, we present an overview of the perception and sensing technologies applied in autonomous ship research. We introduce various datasets collected using perception sensors, such as a camera, LiDAR, and marine radar. Object detection and semantic segmentation methods for detecting floating objects and navigable regions using individual sensors are presented. Additionally, sensor fusion methods that integrate data from multiple sensors to enhance detection and navigation capabilities are addressed. © ICROS 2024.",autonomous ship; perception; sensing; sensor fusion; Autonomous vehicles; Object detection; Object recognition; Optical radar; Semantics; Artificial intelligence technologies; Autonomous ship; Object semantic; Objects detection; Segmentation methods; Semantic segmentation; Sensing; Sensing technology; Sensor fusion; Sensor technologies; Ships
Scopus,"Li, W.; Zhou, J.; Li, X.; Cao, Y.; Jin, G.; Zhang, X.",InfRS: Incremental Few-Shot Object Detection in Remote Sensing Images,,2024,,,,10.1109/TGRS.2024.3475482,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207150382&doi=10.1109%2fTGRS.2024.3475482&partnerID=40&md5=d20eb379c0b95a3d799cade1271673b4,"Few-shot detection in remote sensing images has witnessed significant advancements recently. Despite these progresses, the capacity for continuous conceptual learning still poses a significant challenge to existing methodologies. In this article, we explore the intricate task of incremental few-shot object detection (iFSOD) in remote sensing images. We present a pioneering transfer-learning-based technique, termed InfRS, designed to enable the incremental learning of novel classes using a restricted set of examples, while simultaneously preserving the knowledge learned from previously seen classes without the need to revisit old data. Specifically, we pretrain the detector using sufficient data from base datasets and then generate a set of classwise prototypes that represent the intrinsic characteristics of the data. In the incremental learning session, we design a hybrid prototypical contrastive (HPC) encoding module for learning discriminative representations. Furthermore, we develop a prototypical calibration strategy based on the Wasserstein distance to overcome the catastrophic forgetting problem. Comprehensive evaluations conducted with two aerial imagery datasets show that our InfRS effectively addresses the iFSOD issue in remote sensing imagery. Code is available at https://github.com/lyanna4869/InfRS.git.  © 1980-2012 IEEE.",Incremental few-shot object detection (iFSOD); prototypical contrastive learning; remote sensing images; Object detection; Object recognition; Proximity sensors; Conceptual learning; Incremental few-shot object detection; Incremental learning; Intrinsic characteristics; Learning sessions; Objects detection; Prototypical contrastive learning; Remote sensing images; Shot detection; Transfer learning; calibration; catastrophic event; detection method; image analysis; remote sensing; satellite imagery; Aerial photography
Scopus,"Meng, Z.; Xia, X.; Ma, J.",Toward Foundation Models for Inclusive Object Detection: Geometry- and Category-Aware Feature Extraction Across Road User Categories,,2024,,,,10.1109/TSMC.2024.3385711,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191298049&doi=10.1109%2fTSMC.2024.3385711&partnerID=40&md5=240ed02a4d75904b2fbc484f830d1ae3,"The safety of different categories of road users comprising motorized vehicles and vulnerable road users (VRUs) such as pedestrians and cyclists is one of the priorities of automated driving and smart infrastructure services. Three-dimensional (3-D) LiDAR-based object detection has been a promising approach to perceiving road users. Despite accurate 3-D geometry information, the point cloud from LiDAR is usually nonuniform, and learning the effective point cloud abstract representations for diverse road users remains challenging for 3-D object detection, particularly for small objects such as VRUs. For inclusive object detection (IDetect), we propose a general foundation convolution component, called geometry-aware convolution (GA Conv) toward a foundation feature extraction model, to serve as basic convolution operations of the neutral network for inclusive 3-D object detection. Further, the GA Conv operations are then utilized as the elementary feature extraction layers to build a novel elegant and pyramid network for IDetect. It learns the effective geometric-related features from the unstructured point cloud data by implicitly learning the distribution property and geometry-related features from different categories of road users in particular for VRUs. The proposed IDetect is comprehensively evaluated on the large-scale benchmark Waymo open datasets with all categories of road users. The qualitative and quantitative experiment results demonstrate that IDetect can effectively consider the nonuniform distributed point clouds and learn the geometric features to assist the different categories of road user detection. In addition, the GA Conv has been integrated with other state-of-the-art neural networks and a performance boost for VRU detection has been demonstrated, showing the foundation functionality of the GA Conv and making it a general component in the future inclusive 3-D object detection foundation model.  © 2013 IEEE.",3-dimensional (3-D) object detection; automated driving; geometry-aware convolution (GA Conv); vulnerable road user (VRU) perception; Convolution; Extraction; Feature extraction; Foundations; Geometry; Large datasets; Object recognition; Optical radar; Pedestrian safety; Roads and streets; 3-dimensional; 3-dimensional (3-D) object detection; Automated driving; Features extraction; Geometry-aware convolution; Kernel; Objects detection; Pedestrian; Point cloud compression; Point-clouds; Road; Road users; User perceptions; Vulnerable road user  perception; Object detection
Scopus,"Gautam, V.; Prasad, S.; Sinha, S.",Joint-YODNet: A Light-Weight Object Detector for UAVs to Achieve Above 100fps,,2024,,,,10.1007/978-3-031-58174-8_47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200667210&doi=10.1007%2f978-3-031-58174-8_47&partnerID=40&md5=0866f26eefb89dc4b6c5ee14c94eb4d1,"Small object detection via UAV (Unmanned Aerial Vehicle) images captured from drones and radar is a complex task. This domain encompasses numerous complexities, including size and scale variations, image resolution constraints, and occlusion issues, all of which impede the accurate detection and localization of small objects. To address these challenges, we propose a novel method called Joint-YODNet for UAVs to detect small objects, leveraging a joint loss function specifically designed for this task. Our method revolves around the development of a joint loss function tailored to enhance the detection performance of small objects. Through extensive experimentation on a diverse dataset of UAV images captured under varying environmental conditions, we evaluated different variations of the loss function and determined the most effective formulation. The results demonstrate that our proposed joint loss function outperforms existing methods in accurately localizing small objects. Specifically, Joint-YODNet achieves a recall of 0.971 and a F1Score of 0.975, surpassing state-of-the-art (SOTA) techniques. Additionally, our method achieves a mAP@.5(%) of 98.6, indicating it’s robustness in detecting small objects across varying scales. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Deep Neural Networks; Object Detection; SAR; SAR Ship Detection; Small Object Detection; Unmanned Aerial Vehicle; Aircraft detection; Antennas; Drones; Image resolution; Object detection; Object recognition; Radar imaging; Synthetic aperture radar; Tracking radar; Aerial vehicle; Light weight; Loss functions; Objects detection; SAR ship detection; Ship detection; Small object detection; Small objects; Unmanned aerial vehicle; Vehicle images; Deep neural networks
Scopus,"Bakirci, M.; Bayraktar, I.",Transforming Aircraft Detection Through LEO Satellite Imagery and YOLOv9 for Improved Aviation Safety,,2024,,,,10.1109/DSPA60853.2024.10510106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193021274&doi=10.1109%2fDSPA60853.2024.10510106&partnerID=40&md5=733e5414ebd97e79072abe601b5176c9,"The utilization of object detection algorithms in conjunction with satellite imagery to detect aircraft has become a crucial focus of investigation, bearing significant implications for the management of airports, as well as for the enhancement of aviation safety and security. Traditional monitoring methods, reliant on manual observation or radar systems, are laborintensive, error-prone, and may struggle to provide comprehensive coverage, particularly in vast or remote areas. In contrast, satellite imagery offers wide-area coverage and high-resolution imagery, ideal for capturing detailed views of airport facilities and surrounding areas. By coupling satellite imagery with advanced object detection algorithms like YOLO, significant improvements in aircraft detection capabilities are achievable. These algorithms streamline monitoring processes, enhance situational awareness, and enable prompt responses to potential safety or security threats, marking a paradigm shift in aircraft monitoring practices. Object detection algorithms, especially those based on deep learning techniques like convolutional neural networks (CNNs), have revolutionized object identification in images or video streams. Their adaptability to diverse environmental conditions and high accuracy make them invaluable across numerous applications, from surveillance to autonomous vehicles. Satellite imagery, particularly from Low Earth Orbit (LEO) satellites, offers exceptional detail and clarity, with shorter revisit times enabling near-real-time monitoring of dynamic events on the ground. Leveraging these advantages, this study focuses on enhancing airport security and aircraft safety through the detection of aircraft on the ground utilizing YOLOv9, chosen for its promising capabilities. While our findings demonstrate notable advancements, it is crucial to address certain limitations in the algorithm's detection capabilities to ensure its effectiveness in real-world applications. © 2024 IEEE.",aviation security; convolutional neural network; deep learning; satellite imagery; YOLOv9; Aircraft; Aircraft detection; Airport security; Convolution; Deep neural networks; Image enhancement; Learning systems; Object detection; Object recognition; Orbits; Security systems; Aviation safety; Aviation Security; Convolutional neural network; Deep learning; Detection capability; Low earth orbit satellites; Monitoring methods; Object detection algorithms; Safety and securities; YOLOv9; Convolutional neural networks
Scopus,"Becker, S.; Bayer, J.; Hug, R.; Huebner, W.; Arens, M.",Utilizing Dataset Affinity Prediction in Object Detection to Assess Training Data,,2024,,,,10.1007/978-3-031-59057-3_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194075509&doi=10.1007%2f978-3-031-59057-3_17&partnerID=40&md5=b9d33878e3c013cac8f412d58b90fc16,"Data pooling offers various advantages, such as increasing the sample size, improving generalization, reducing sampling bias, and addressing data sparsity and quality, but it is not straightforward and may even be counterproductive. Assessing the effectiveness of pooling datasets in a principled manner is challenging due to the difficulty in estimating the overall information content of individual datasets. Towards this end, we propose incorporating a data source prediction module into standard object detection pipelines. The module runs with minimal overhead during inference time, providing additional information about the data source assigned to individual detections. We show the benefits of the so-called dataset affinity score by automatically selecting samples from a heterogeneous pool of vehicle datasets. The results show that object detectors can be trained on a significantly sparser set of training samples without losing detection accuracy. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Ante-hoc explanation; Dataset label prediction; Dataset origin prediction; Object detection; Sample selection; Selection bias; Training data analysis; Object detection; Object recognition; Ante-hoc explanation; Data-source; Dataset label prediction; Dataset origin prediction; Label predictions; Objects detection; Samples selection; Selection bias; Training data; Training data analyse; Forecasting
Scopus,"Hou, X.; Guan, Y.; Choi, N.; Han, T.","BPS: Batching, Pipelining, Surgeon of Continuous Deep Inference on Collaborative Edge Intelligence",,2024,,,,10.1109/TCC.2024.3399616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192765162&doi=10.1109%2fTCC.2024.3399616&partnerID=40&md5=dfb7624592fd7e117974a0b91404ec3e,"Users on edge generate deep inference requests continuously over time. Mobile/edge devices located near users can undertake the computation of inference locally for users, e.g., the embedded edge device on an autonomous vehicle. Due to limited computing resources on one mobile/edge device, it may be challenging to process the inference requests from users with high throughput. An attractive solution is to (partially) offload the computation to a remote device in the network. In this paper, we examine the existing inference execution solutions across local and remote devices and propose an adaptive scheduler, a BPS scheduler, for continuous deep inference on collaborative edge intelligence. By leveraging data parallel, neurosurgeon, reinforcement learning techniques, BPS can boost the overall inference performance by up to 8.2× over the baseline schedulers. A lightweight compressor, FF, specialized in compressing intermediate output data for neurosurgeon, is proposed and integrated into the BPS scheduler. FF exploits the operating character of convolutional layers and utilizes efficient approximation algorithms. Compared to existing compression methods, FF achieves up to 86.9% lower accuracy loss and up to 83.6% lower latency overhead.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",convolutional neural networks; Edge computing; efficient AI; reinforcement learning; Approximation algorithms; Convolution; Deep learning; Edge computing; Neural networks; Neurosurgery; Collaboration; Computational modelling; Convolutional neural network; Deep inference; Edge computing; Edge intelligence; Efficient AI; Performances evaluation; Reinforcement learnings; Reinforcement learning
Scopus,"Falaschetti, L.; Manoni, L.; Palma, L.; Pierleoni, P.; Turchetti, C.",Embedded Real-Time Vehicle and Pedestrian Detection Using a Compressed Tiny YOLO v3 Architecture,,2024,,,,10.1109/TITS.2024.3447453,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204704255&doi=10.1109%2fTITS.2024.3447453&partnerID=40&md5=a9f1703bb3be9c96e4a4e19600235900,"Vehicle and pedestrian detection (VaPD) is one of the most critical tasks in an advanced driver assistance system which help the driver to drive safely and save the pedestrian life. VaPD is a typical object detection problem that requires a trade-off among accuracy, speed, and memory consumption. Most existing methods focus on improving detection accuracy, while ignoring VaPD requires real-time detection speed with limited computational resources. Thus, it is of primary importance to study light-weight and real-time VaPD methods for embedded devices, that is hardware platforms with limited computation and memory resources. To deal with these issues, this paper proposes a low-rank (LR) Tiny YOLO v3 architecture that meets the requirements of real-time VaPD on embedded systems. The architecture has been developed starting from Tiny YOLO v3 adopting a convolutional neural network compression technique based on Tucker tensor decomposition, able to reduce the computational complexity of the network. A wide experimentation has been carried out on two embedded platforms, Raspberry Pi 4 and NVIDIA Jetson Nano 2 GB, and two datasets commonly used for VaPD, PASCAL VOC and KITTI dataset, showing the superiority of the LR Tiny YOLO v3 with respect to the state-of-the-art networks in obtaining the best compromise between inference time, accuracy and memory occupancy. Moreover, the proposed architecture meets the requirements of VaPD on embedded systems using only 22% of the memory required by the baseline Tiny YOLO v3 Darknet, and always providing better inference time (36.46 FPS) with only a marginal decrease in accuracy (~2%). © 2000-2011 IEEE.",Autonomous driving; CNN compression; embedded smart sensors; object detection; pedestrian detection; YOLO; Autonomous vehicles; Convolutional neural networks; Image segmentation; Magnetic levitation vehicles; Tensors; Autonomous driving; CNN compression; Critical tasks; Embedded smart sensor; Embedded-system; Objects detection; Pedestrian detection; Real- time; Vehicles detection; YOLO; Advanced driver assistance systems
Scopus,"Wu, Y.; Gu, Z.; Wang, Z.; Watabe, D.",Enhancing Safe Driving of Autonomous Buses in Obstructed-View Conditions Using Distributed Monocular Cameras,,2024,,,,10.1109/IECON55916.2024.10905580,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000977610&doi=10.1109%2fIECON55916.2024.10905580&partnerID=40&md5=85a45ba9e6d29b862efa024a539e7540,"With the rapid advancement of autonomous driving technology, the application of automated urban transit has become a key component of modern transportation systems. As a representative of automated urban transit, the safe operation of autonomous buses is of great significance, especially in complex and variable traffic situations. This study proposes a low-cost method that combines a distributed architecture configuration based on monocular cameras with the latest YOLOv8 model to enhance the perception capabilities of fixed-route autonomous buses under specific conditions. The primary aim is to reduce traffic accidents caused by pedestrians suddenly appearing near the bus, particularly in blind spots. The methodology involves advanced object detection and transmission techniques. Video data captured by the monocular camera is processed in real-time, and data transmission is facilitated via Wi-Fi modules. The system centrally renders the relative positions of pedestrians and the bus on the receiving end, thereby enhancing the safety decision-making capabilities of the bus’s autonomous driving system. This approach not only advances the development of automated urban transit systems but also promotes the practical implementation of low-cost autonomous driving solutions, particularly in scenarios where the view is obstructed for autonomous buses. © 2024 IEEE.",autonomous driving; distributed architecture; monocular vision; object detection; real-time systems; YOLOv8; Autonomous vehicles; Bus transportation; Buses; Light rail transit; Pedestrian safety; Rapid transit; Autonomous driving; Condition; Distributed architecture; Monocular cameras; Monocular vision; Objects detection; Real - Time system; Safe driving; Urban transit; YOLOv8; Urban transportation
Scopus,"Narayanan, P.; Nikhil, V.; Veluchamy, S.",Vehicle Object Detection in Traffic Environments Using Low-Light Image Enhancement,,2024,,,,10.1109/ICCCNT61001.2024.10723924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212817116&doi=10.1109%2fICCCNT61001.2024.10723924&partnerID=40&md5=249c4e458d99e0ca0b123a6e249034a0,"Scene depth and the corresponding ambient lights generally degrade low-light photos taken in a busy setting with uneven lighting.Vehicle object recognition is made more challenging by this degradation, which results in a significant loss of object details in the deteriorated picture format due to the poor contrast and the appearance of artificial light. On the other hand, current methods for identifying prominent objects rely on the unreasonable assumption that the photographs were captured in an adequately lit atmosphere. In this research, we refer to a method for improving images in dim light image detection of vehicle objects. The suggested approach immediately embeds the physical lighting model into the convolution neural network, where the traffic environment light is represented as a point-wise fluctuation that varies with local content, to explain the degradation of low light images. A Sensor Filter is also used to record the difference between an object’s local content and its local neighborhood hotspots. We generate a low light Images dataset with pixel-level human labeled ground-truth annotations for quantitative assessment, in addition to our benchmark dataset. We also show encouraging results on four other publically available datasets. ©2024 IEEE.",Low light image; Pixel-level; Sensor Filter; Vehicle object; Convolutional neural networks; Image annotation; Image enhancement; Photography; Ambient light; Local contents; Low light; Low-light images; Objects detection; Pixel level; Scene depths; Sensor filter; Traffic environment; Vehicle object; Pixels
Scopus,"Yao, S.; Guan, R.; Wu, Z.; Ni, Y.; Huang, Z.; Wen Liu, R.; Yue, Y.; Ding, W.; Gee Lim, E.; Seo, H.; Lok Man, K.; Ma, J.; Zhu, X.; Yue, Y.",WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmarks for Autonomous Driving on Water Surfaces,,2024,,,,10.1109/TITS.2024.3415772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197611307&doi=10.1109%2fTITS.2024.3415772&partnerID=40&md5=8af930fceef24e1e9468f43d2e92a705,"Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivor rescue, environmental monitoring, hydrography mapping and waste cleaning. This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces. Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation. Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively. In addition to basic perception tasks, such as object detection, instance segmentation and semantic segmentation, we also provide annotations for free-space segmentation and waterline segmentation. Leveraging the multi-task and multi-modal data, we conduct benchmark experiments on the uni-modality of radar and camera, as well as the fused modalities. Experimental results demonstrate that 4D radar-camera fusion can considerably improve the accuracy and robustness of perception on water surfaces, especially in adverse lighting and weather conditions. WaterScenes dataset is public on https://waterscenes.github.io.  © 2024 IEEE.",4D radar-camera fusion; Autonomous driving; multi-task; unmanned surface vehicle; Cameras; Job analysis; Modal analysis; Object detection; Object recognition; Radar imaging; Semantics; Unmanned surface vehicles; 4d radar-camera fusion; Autonomous driving; Autonomous Vehicles; Environmental Monitoring; Maritime-surveillance; Monocular cameras; Multi tasks; Task analysis; Waste cleaning; Water surface; Autonomous vehicles
Scopus,"Nazeri, A.; Godwin, D.W.; Maria Panteleaki, A.; Anagnostopoulos, I.; Edidem, M.I.; Li, R.; Shu, T.",Exploration of TPU Architectures for the Optimized Transformer in Drainage Crossing Detection,,2024,,,,10.1109/BigData62323.2024.10826077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217988044&doi=10.1109%2fBigData62323.2024.10826077&partnerID=40&md5=0a274d49b6cc46635704256868e1de7e,"Understanding hydrologic connectivity within landscapes is crucial for managing environmental challenges. Despite advancements in high-resolution Digital Elevation Models (DEMs) derived from Light Detection and Ranging (LiDAR) technology, accurately delineating hydrologic connectivity remains challenging due to disruptions caused by virtual flow barriers, such as roads and bridges. This study addresses this issue by enhancing the detection performance and reducing the latency of Transformer models for image detection of drainage crossings. We retrained a Detection Transformer (DETR) with a specialized recipe to improve culvert detection performance. Owing to the high susceptibility of LiDAR-based DEMs to measurement noise and varying data modalities, we conducted extensive data preprocessing to ensure DETR compatibility with the culvert dataset. Ablation studies on input size indicate that the model performs optimally with 800×800 pixel inputs, demonstrating its adaptability to new data modalities. Additionally, we employed Tensor Processing Units (TPUs) to decrease the model's latency. We developed a novel strategy to optimize TPU architecture, utilizing genetic algorithms to expedite the discovery of optimal TPU configurations for detection deployment. Our model surpasses the performance of previous models on the same task. This work not only addresses the computational complexities of deploying advanced object detection in environmental contexts but also significantly contributes to the precise and efficient monitoring of hydrologic connectivity. © 2024 IEEE.",culvert; Detection Transformer; DETR; Digital Elevation Model; hydrologic connectivit; LiDAR; Object detection; TPU; Geological surveys; Image coding; Image enhancement; Levees; Photomapping; Steganography; Virtual addresses; Detection performance; Detection transformer; Digital elevation model; Hydrologic connectivit; Hydrologic connectivity; Light detection and ranging; Objects detection; Processing units; Tensor processing unit; Culverts
Scopus,"Zhou, W.; Cai, C.; Li, C.; Xu, H.; Shi, H.",AD-YOLO: A Real-Time YOLO Network With Swin Transformer and Attention Mechanism for Airport Scene Detection,,2024,,,,10.1109/TIM.2024.3472805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206328562&doi=10.1109%2fTIM.2024.3472805&partnerID=40&md5=3f0dd2194aa8a07312f4b2db3d2640fc,"Real-time acquisition of airport scene information is crucial for airport safety and optimization of airport utilization efficiency. However, detecting airport objects is still a challenging task due to the small size of person and vehicle targets in the airport scene images, insufficient public airport data, and so on, which makes it difficult to achieve high accuracy and real-time detection methods in the airport scene simultaneously. This article proposes a novel airport object detection approach to address the challenge by integrating the advantages of improved you only look once (YOLO), Swin Transformer, and attention mechanism [airport detector - YOLO (AD-YOLO)]. Specifically, we introduce the Swin Transformer, which retains the Transformer's ability of global attention to obtain features and reduces the drawbacks of computational complexity, into the head network based on YOLOv7 to improve the high-dimensional information feature fusion. We also design an efficient channel spatial attention (ECSA) module and introduce a small object detection layer (SODL) to improve the detection accuracy of small targets in the airport scene. We test the proposed method on the self-constructed multiple airport surveillance dataset (MASD) containing 5736 images captured by actual airport and online airport video. The experimental results show that AD-YOLO achieves 71.6% mean average precision (mAP), exceeding the mAP of the baseline method by 4.4%. The proposed method has 101.4 frames/s (FPS) on the NVIDIA RTX3080 GPU and 17.8 FPS on the Jetson Orin NX, meeting the real-time and accuracy requirements of the airport scene. Finally, the experimental results on the public airport surface surveillance (ASS) dataset show that AD-YOLO outperforms other detection methods, demonstrating its effectiveness. © 1963-2012 IEEE.",Airport scene; attention mechanism; object detection; Swin Transformer; YOLOv7; Airport runways; Airport security; Clutter (information theory); Electric transformer testing; Remote sensing; Security systems; Airport object; Airport scene; Attention mechanisms; Detection methods; Objects detection; Public airports; Real- time; Scene detection; Swin transformer; YOLOv7; Object detection
Scopus,"Fedorov, V.A.",Recognizing Railway Infrastructure Using CNN and Stereoscopic Vision,,2024,,,,10.1109/SmartIndustryCon61328.2024.10516208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193273357&doi=10.1109%2fSmartIndustryCon61328.2024.10516208&partnerID=40&md5=426b108c6b4ecbf04122564a5189f35a,"In the era of Industry 4.0, achieving Grade of Automation 4 (GoA 4) within the railway sphere is pivotal for the development of autonomous networks. This necessitates intelligent systems capable of real-time perception, precise object identification, and accurate determination of distances. This paper investigates railway object detection using Convolutional Neural Network (CNN), specifically the YOLOv8 architecture, extending prior research by integrating CNN with stereoscopic vision for distance determination to the detected objects. Implementing GoA 4 enables autonomous systems to interpret and react within railway environments. Our evaluation of YOLOv8 demonstrates consistent and robust object detection, achieving an mAP of 0.8 at IoU 0.5 and maintaining 0.54 across IoU 0.5-0.95. Utilizing stereoscopic vision, the determination of distances to detected objects within a 250-meter range exhibits an error margin of less than 10%, ensuring high precision in distance estimation. The integration of YOLOv8 with stereoscopic vision for accurate distance estimation, executed within 80 milliseconds, demonstrates remarkable computational efficiency, which is crucial for real-time applications. This efficient process, facilitated by the Nvidia RTX A5000 graphics accelerator, ensures both precision and high speed in dynamic railway settings. Our assessment emphasizes the adaptability and reliability of YOLOv8 in detecting railway infrastructure objects across diverse conditions, signifying its potential for real-world deployment. © 2024 IEEE.",CNN; computer vision; GoA4; machine vision; railway infrastructure; stereoscopic vision; YOLOv8; Computational efficiency; Convolutional neural networks; Intelligent systems; Object detection; Object recognition; Railroad transportation; Railroads; Real time systems; Autonomous networks; Convolutional neural network; Distance estimation; Goa4; Machine-vision; Network vision; Railway infrastructure; Real time perception; Stereoscopic vision; YOLOv8; Computer vision
Scopus,"Zhao, H.; Zhang, S.; Peng, X.; Lu, Z.; Li, G.",Improved object detection method for autonomous driving based on DETR,,2024,,,,10.3389/fnbot.2024.1484276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216516718&doi=10.3389%2ffnbot.2024.1484276&partnerID=40&md5=442dc18ae40eef21674fd538ffee0b3d,"Object detection is a critical component in the development of autonomous driving technology and has demonstrated significant growth potential. To address the limitations of current techniques, this paper presents an improved object detection method for autonomous driving based on a detection transformer (DETR). First, we introduce a multi-scale feature and location information extraction method, which solves the inadequacy of the model for multi-scale object localization and detection. In addition, we developed a transformer encoder based on the group axial attention mechanism. This allows for efficient attention range control in the horizontal and vertical directions while reducing computation, ultimately enhancing the inference speed. Furthermore, we propose a novel dynamic hyperparameter tuning training method based on Pareto efficiency, which coordinates the training state of the loss functions through dynamic weights, overcoming issues associated with manually setting fixed weights and enhancing model convergence speed and accuracy. Experimental results demonstrate that the proposed method surpasses others, with improvements of 3.3%, 4.5%, and 3% in average precision on the COCO, PASCAL VOC, and KITTI datasets, respectively, and an 84% increase in FPS. Copyright © 2025 Zhao, Zhang, Peng, Lu and Li.",feature extraction; loss function; object detection; parameter tuning; transformer encoder; Encoding (symbols); Autonomous driving; Based on detections; Critical component; Features extraction; Growth potential; Loss functions; Object detection method; Objects detection; Parameters tuning; Transformer encoder; article; controlled study; diagnosis; feature extraction; male; pareto optimality; velocity; Signal encoding
Scopus,"Roopa Devi, E.M.; Sedhumadhavan, V.; Sudharsan, V.K.; Rakesh, M.; Surendhar, D.",Wild Animal Recognition using Deep Learning Models,,2024,,,,10.1109/ICCCNT61001.2024.10725145,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211083650&doi=10.1109%2fICCCNT61001.2024.10725145&partnerID=40&md5=6fea304d456a5233fe1ec19679033be3,"Wildlife-vehicle collisions present a significant challenge at the intersection of road networks and natural habitats, necessitating innovative solutions to enhance road safety and mitigate the impact on wildlife populations. Traditional detection methods often fall short in achieving real-time effectiveness, prompting the exploration of advanced deep learning models such as EfficientDet, Single Shot MultiBox Detector (SSD), and YOLOv9 for wild animal recognition. This research addresses the limitations of existing models by proposing a comprehensive recognition system that leverages the capabilities of these state-of-the-art models. EfficientDet, SSD, and YOLOv9 represent a paradigm shift in automatically extracting intricate features from raw data, offering potential for robust and adaptive recognition capabilities. Our The experimental findings illustrate that the suggested YOLOv9 model outperforms EfficientDet and SSD, showcasing superior detection accuracy, robustness, and computational efficiency. This discovery emphasizes the effectiveness of YOLOv9 in tasks related to detecting wild animals, underscoring its potential as a reliable and efficient solution for mitigating wildlife-vehicle collisions and enhancing road safety in natural environments. © 2024 IEEE.","Wild Animal Recognition System, Deep Learning Models, EfficientDet, Single Shot MultiBox Detector (SSD), YOLOv9"
Scopus,"Song, G.; Song, S.",A Rain Image Generation Approach for Autonomous Driving Perception Sensors,,2024,,,,10.1109/EIECS63941.2024.10800702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216320748&doi=10.1109%2fEIECS63941.2024.10800702&partnerID=40&md5=88ea65c100c787e0fb54edf8ba7ab86b,"Proper rain image generation is important to assess the safety of autonomous perception system in rainy environment and improve perception performance. Rain image generation methods for autonomous driving vision sensors have become a research hotspot. The diversity of data can be extended and the performance of vision-based perception can be improved by effective real rain image generation. Although recent deep learning-based image generation of real rain has good performance, it is still an open problem for autonomous driving scenarios. Firstly, the existing rain images dataset only considers the streak characteristics produced by rain in modeling real rain features, and does not consider the interference of raindrops and fog on the image. Secondly, the distribution of the generated rain streaks, raindrops and fog lacks the influence of physical characteristics brought about by vehicle speed, rainfall and gravity during vehicle operation. Moreover, the lack of a real-time and efficient method for generating realistic raindrop images makes data enhancement and safety testing of sensing systems challenging. To solve the problem of real rain images generation for autonomous driving scenarios, we propose an efficient data generation method that combines physical properties and robot operating system to realize real-time generation of real rain for dynamic scenes and real-time data publish.  © 2024 IEEE.",Autonomous driving; Physical properties; Rain image generation; Real-time generation; Autonomous driving; Generation method; Image generations; Perception systems; Performance; Property; Rain image generation; Real- time; Real-time generation; Vision sensors; Robot Operating System
Scopus,"Hu, J.; He, Y.; Zeng, M.; Qian, Y.; Zhang, R.",Smoke and Fire Detection Based on YOLOv7 With Convolutional Structure Reparameterization and Lightweighting,,2024,,,,10.1109/LSENS.2024.3420883,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197538624&doi=10.1109%2fLSENS.2024.3420883&partnerID=40&md5=a857e340e7ae86b1914b7c4e8ed72b75,"This letter presents a YOLO-based detection model called Diverse branch block and slimneck-YOLO (DS-YOLO) [Diverse Branch Block (DBB) and SlimNeck], which enables fast and accurate identification of smoke and fire. First, Diverse branch block and partial convolution- efficient layer aggregation network (DP-ELAN) module in the backbone is constructed based on DBB and partial convolution. It is possible to enhance detection capability while reducing the number of parameters. Second, utilizing the SlimNeck based on ghost shuffle convolution to reduce model complexity. Finally, we use normalized Gaussian Wasserstein distance instead of the standard intersection over union to improve the detection capability for small-sized objects.The experimental results on our custom smoke and fire dataset indicate that the proposed DS-YOLO achieves a mean average precision of 70.1%, while reducing computational complexity. This represents an 1.3% improvement over the baseline model. © 2017 IEEE.",computer vision; Sensor applications; sensor networks; smoke and fire detection; YOLOv7; Complex networks; Computer vision; Convolution; Feature extraction; Fire detectors; Fires; Smoke; Accuracy; Computational modelling; Detection capability; Features extraction; Fire detection; Reparameterization; Sensors network; Smoke detection; YOLO; YOLOv7; Sensor networks
Scopus,"Duraisamy, P.; Deepika, A.; Shivadharshini, G.; Swetha, S.; Kumar, N.S.",Implementation of YOLO V8 for Advanced Autonomous Vehicle Detection Techniques,,2024,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209086445&partnerID=40&md5=3034cd10b911fe511e5bb38034693132,"Addressing the escalating necessity for proficient and dependable autonomous vehicle systems, this research proposes an advanced object detection approach harnessing the YOLOv8 algorithm. Object detection plays a pivotal role in the functionality of autonomous driving systems, enabling vehicles to perceive and react to their surroundings promptly. YOLOv8, an upgraded iteration of the YOLO algorithm, is acclaimed for its rapidity and precision in object detection endeavors. Our proposed model, distinguished by enhancements in network architecture and training methodologies, surpasses existing models in terms of detection precision and computational efficacy. Through an exhaustive examination, we delve into the network architecture, training regimen, and assessment metrics of the YOLOv8-based model. Experimental findings showcase the model's commendable performance in both quantitative and qualitative assessments, highlighting its resilience in identifying pedestrians, vehicles, traffic signs, and other pertinent objects across varied driving scenarios. The model's predicted outputs attest to its adeptness in precisely localizing and categorizing objects of interest, thereby augmenting the safety and efficiency of autonomous driving systems. © Grenze Scientific Society, 2024.",Autonomous vehicles; Computer vision; Object detection; Real-time systems; YOLOv8 algorithm; Advanced driver assistance systems; Autonomous driving; Autonomous Vehicles; Detection approach; Driving systems; In networks; Objects detection; Real - Time system; Vehicle system; Vehicles detection; YOLOv8 algorithm; Pedestrian safety
Scopus,"Xu, H.; Zhang, X.; He, J.; Yu, Y.; Cheng, Y.",Real-Time Volumetric Perception for Unmanned Surface Vehicles Through Fusion of Radar and Camera,,2024,,,,10.1109/TIM.2024.3381690,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189156410&doi=10.1109%2fTIM.2024.3381690&partnerID=40&md5=2a0674dba5e8842c806080b24b6359c5,"In recent years, unmanned surface vehicles (USVs) have played an increasingly important role in various applications. Due to the expansion of USV application scenes from common marine areas to inland waters with complex environments, environmental perception has become an essential requirement for autonomous navigation systems of USVs. Traditional perception methods utilize either light detection and ranging (LiDAR) or radar to construct volumetric maps for environmental perception. To improve the accuracy of perception systems and reduce deployment costs, this article proposes a novel radar and camera fusion volumetric map network named FVMNet for real-time volumetric perception. FVMNet is based on a novel radar and image fusion architecture and comprises four modules: 1) the radar and image encoders can extract different features; 2) only using in training stage without extra valid time costs, auxiliary segmentation head advances the image encoder; 3) to eliminate the representation difference between image features and radar features, the BEV spatial transformer module transfers image feature representations from the perspective view to BEV space; and 4) the fusion segmentation head predicts the volumetric perception results. Compared to other baseline methods that use a single modality, FVMNet achieves state-of-the-art accuracy in the public USVInland dataset and our collected wharf dataset. We conducted comprehensive ablation experiments to validate the efficacy of the designed modules. Moreover, the proposed method demonstrates generalization in zero-shot real-world scenarios and robustness under extreme weather conditions.  © 1963-2012 IEEE.",Millimeter-wave (MMW) radar; radar-camera fusion; supervised learning with light detection and ranging (LiDAR) data; unmanned surface vehicles (USVs) perception; volumetric perception; Cameras; Costs; Image segmentation; Navigation systems; Optical radar; Real time systems; Signal encoding; Space-based radar; Unmanned surface vehicles; MMW radar; Point cloud compression; Point-clouds; Radar-camera fusion; Real- time; Sea surfaces; Supervised learning with LiDAR data; Unmanned surface vehicle  perception; Volumetric perception; Volumetrics; Radar imaging
Scopus,"Fei, T.; Mukhopadhyay, S.C.; da Costa, J.P.J.; RoyChaudhuri, C.; Lan, L.; Demitri, N.",Spatial Environment Perception and Sensing in Automated Systems: A Review,,2024,,,,10.1109/JSEN.2024.3379222,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189538819&doi=10.1109%2fJSEN.2024.3379222&partnerID=40&md5=e69e0a0643c064ddfad3528072eb36bf,"—This article provides a concise yet comprehensive review of spatial environment sensing and perception (SESP) in automated systems, crucial for intelligent applications such as smart cities, smart homes, navigation, automated driving, and industry 4.0. With a focus on achieving dependable performance even in harsh environments and enabling affordable mass deployment, the review explores sensor technologies, calibration, diagnostics, performance degradation analysis, and machine-learning (ML)-based perception algorithms. Our contributions aim to deepen understanding and drive progress in automated systems, addressing challenges across diverse application scenarios. Through a meticulous analysis of the pros and cons of state-of-the-art technologies, this article sheds light on future trends in research and development. This work serves as a valuable resource for researchers, practitioners, and industry professionals seeking insights into the evolving landscape of SESP. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Automated sensor; sensor calibration; sensor perception; sensors; smart sensor; Intelligent buildings; Intelligent robots; Learning systems; Smart sensors; Automated sensors; Automated systems; Environment perceptions; Environment sensing; Intelligent sensors; Robot sensing system; Sensor calibration; Sensor perceptions; Sensor systems; Spatial environments; Calibration
Scopus,"Tang, G.; Ni, J.; Zhao, Y.; Gu, Y.; Cao, W.",A Survey of Object Detection for UAVs Based on Deep Learning,,2024,,,,10.3390/rs16010149,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181847264&doi=10.3390%2frs16010149&partnerID=40&md5=739dbe8348e6f04ea3c3019f056e0dd9,"With the rapid development of object detection technology for unmanned aerial vehicles (UAVs), it is convenient to collect data from UAV aerial photographs. They have a wide range of applications in several fields, such as monitoring, geological exploration, precision agriculture, and disaster early warning. In recent years, many methods based on artificial intelligence have been proposed for UAV object detection, and deep learning is a key area in this field. Significant progress has been achieved in the area of deep-learning-based UAV object detection. Thus, this paper presents a review of recent research on deep-learning-based UAV object detection. This survey provides an overview of the development of UAVs and summarizes the deep-learning-based methods in object detection for UAVs. In addition, the key issues in UAV object detection are analyzed, such as small object detection, object detection under complex backgrounds, object rotation, scale change, and category imbalance problems. Then, some representative solutions based on deep learning for these issues are summarized. Finally, future research directions in the field of UAV object detection are discussed. © 2023 by the authors.",computer vision; deep learning; object detection; unmanned aerial vehicles; Aircraft detection; Antennas; Computer vision; Deep learning; Object recognition; Unmanned aerial vehicles (UAV); Aerial Photographs; Aerial vehicle; Deep learning; Detection technology; Early warning; Geological exploration; Objects detection; Precision Agriculture; Recent researches; Unmanned aerial vehicle; Object detection
Scopus,"McDonnell, K.J.",Leveraging the Academic Artificial Intelligence Silecosystem to Advance the Community Oncology Enterprise,,2023,,,,10.3390/jcm12144830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166326559&doi=10.3390%2fjcm12144830&partnerID=40&md5=4d9e360cc4be9af4aefa2698972f0062,"Over the last 75 years, artificial intelligence has evolved from a theoretical concept and novel paradigm describing the role that computers might play in our society to a tool with which we daily engage. In this review, we describe AI in terms of its constituent elements, the synthesis of which we refer to as the AI Silecosystem. Herein, we provide an historical perspective of the evolution of the AI Silecosystem, conceptualized and summarized as a Kuhnian paradigm. This manuscript focuses on the role that the AI Silecosystem plays in oncology and its emerging importance in the care of the community oncology patient. We observe that this important role arises out of a unique alliance between the academic oncology enterprise and community oncology practices. We provide evidence of this alliance by illustrating the practical establishment of the AI Silecosystem at the City of Hope Comprehensive Cancer Center and its team utilization by community oncology providers. © 2023 by the author.",artificial intelligence; City of Hope; community practice; oncology; adult; article; artificial intelligence; cancer center; human; synthesis
Scopus,"Bai, Z.; Nayak, S.P.; Zhao, X.; Wu, G.; Barth, M.J.; Qi, X.; Liu, Y.; Sisbot, E.A.; Oguchi, K.",Cyber Mobility Mirror: A Deep Learning-Based Real-World Object Perception Platform Using Roadside LiDAR,,2023,,,,10.1109/TITS.2023.3268281,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159796681&doi=10.1109%2fTITS.2023.3268281&partnerID=40&md5=911aa5a439b3e025478bca95fdf7d90c,"Object perception plays a fundamental role in Cooperative Driving Automation (CDA) which is regarded as a revolutionary promoter for next-generation transportation systems. However, the vehicle-based perception may suffer from the limited sensing range and occlusion as well as low penetration rates in connectivity. In this paper, we propose Cyber Mobility Mirror (CMM), a next-generation real-world object perception system for 3D object detection, tracking, localization, and reconstruction, to explore the potential of roadside sensors for enabling CDA in the real world. The CMM system consists of six main components: i) the data pre-processor to retrieve and preprocess the raw data; ii) the roadside 3D object detector to generate 3D detection results; iii) the multi-object tracker to identify detected objects; iv) the global locator to generate geo-localization information; v) the mobile-edge-cloud-based communicator to transmit perception information to equipped vehicles, and vi) the onboard advisor to reconstruct and display the real-time traffic conditions. An automatic perception evaluation approach is proposed to support the assessment of data-driven models without human-labeling requirements and a CMM field-operational system is deployed at a real-world intersection to assess the performance of the CMM. Results from field tests demonstrate that our CMM prototype system can achieve 96.99% precision and 83.62% recall for detection and 73.55% ID-recall for tracking. High-fidelity real-time traffic conditions (at the object level) can be geo-localized with a root-mean-square error (RMSE) of 0.69m and 0.33m for lateral and longitudinal direction, respectively, and displayed on the GUI of the equipped vehicle with a frequency of 3-4 Hz.  © 2000-2011 IEEE.",3D object detection; cooperative driving automation; deep learning; Field operational system; localization; multi-object tracking; Automation; Deep learning; Image reconstruction; Intelligent vehicle highway systems; Laser applications; Mean square error; Object detection; Object recognition; Roadsides; Three dimensional computer graphics; Three dimensional displays; Vehicle to vehicle communications; Vehicles; 3D object; 3d object detection; Cooperative driving; Cooperative driving automation; Deep learning; Field operational system; Localisation; Multi-object tracking; Objects detection; Operational systems; Point cloud compression; Point-clouds; Three-dimensional display; Optical radar
Scopus,"Yan, G.; Liu, K.; Liu, C.; Zhang, J.",Edge Intelligence for Internet of Vehicles: A Survey,,2024,,,,10.1109/TCE.2024.3378509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188538736&doi=10.1109%2fTCE.2024.3378509&partnerID=40&md5=f08d3a4a28733eea4175050bf0b74b28,"The Internet of Vehicles (IoV) has become a fundamental platform for advancing Intelligent Transportation Systems (ITSs) and Intelligent Connected Vehicles (ICVs). However, the increasing volume of data generated by vehicle sensors and the computational demands of Artificial Intelligence (AI) algorithms present significant challenges for the platform. Edge Intelligence (EI), which brings intelligent computing and data processing closer to vehicles, has emerged as a potential solution. In this survey, we provide a comprehensive overview of Edge Intelligence for the Internet of Vehicles. We begin by discussing the motivations behind employing EI in the IoV for typical AI computations. To fully exploit the potential of EI in heterogeneous IoV environments, we present a layered vehicular EI architecture and discuss its benefits and challenges. Furthermore, we provide a taxonomy of EI approaches for vehicular networks, focusing on cooperative inference, distributed training, and collaborative sensing, in terms of their schemas and advanced frameworks. Finally, we explore emerging trends and research directions in this field, including vehicle-road-cloud integration, generative AI-driven IoV, and vehicular cyber-physical fusion. By offering insights into state-of-the-art techniques and trends, this survey aims to enable researchers to develop innovative solutions for transforming the intelligent IoV ecosystem.  © 1975-2011 IEEE.",edge intelligence (EI); inference; Internet of Vehicles (IoV); sensing; training; Data handling; Distributed computer systems; Intelligent systems; Intelligent vehicle highway systems; Job analysis; Network architecture; Vehicles; Collaboration; Computational demands; Edge intelligence; Inference; Intelligent transportation systems; Internet of vehicle; Sensing; Task analysis; Vehicle sensors; Computer architecture
Scopus,"Liang, M.; Meyer, F.",Neural Enhanced Belief Propagation for Multiobject Tracking,,2024,,,,10.1109/TSP.2023.3314275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181092781&doi=10.1109%2fTSP.2023.3314275&partnerID=40&md5=34736fd0feed62faf0427cbbc94b2c18,"Algorithmic solutions for multi-object tracking (MOT) are a key enabler for applications in autonomous navigation and applied ocean sciences. State-of-the-art MOT methods fully rely on a statistical model and typically use preprocessed sensor data as measurements. In particular, measurements are produced by a detector that extracts potential object locations from the raw sensor data collected at discrete time steps. This preparatory processing step reduces data flow and computational complexity but may result in a loss of information. State-of-the-art Bayesian MOT methods that are based on belief propagation (BP) systematically exploit graph structures of the statistical model to reduce computational complexity and improve scalability. However, as a fully model-based approach, BP can provide highly suboptimal estimates when there is a mismatch between the statistical model and the true data-generating process. Existing BP-based MOT methods can further only make use of preprocessed measurements. In this paper, we introduce a variant of BP that combines model-based with data-driven MOT. The proposed neural enhanced belief propagation (NEBP) method complements the statistical model of BP by information learned from raw sensor data. This approach conjectures that the learned information can reduce model mismatch and thus improve data association and false alarm rejection. Our NEBP method improves tracking performance compared to model-based methods. At the same time, it inherits the advantages of BP-based MOT, i.e., it scales only quadratically in the number of objects, and it can thus generate and maintain a large number of object tracks. We evaluate the performance of our NEBP approach for MOT on the nuScenes autonomous driving dataset and demonstrate that it can achieve state-of-the-art performance. In particular, an average multi-object tracking accuracy (AMOTA) of 0.683 was obtained and, compared with non-BP-based methods, identity switches (IDS) and track fragments (Frag) were reduced by 23% and 19%, respectively.  © 1991-2012 IEEE.",belief propagation; factor graphs; graph neural networks; Multiobject tracking; probabilistic data association; Backpropagation; Complex networks; Computational complexity; Data reduction; Graph neural networks; Graphic methods; Large dataset; Object detection; Tracking (position); Belief propagation; Factor graphs; Graph neural networks; Multi-object tracking; Probabilistic data association; Raw sensor; Sensors data; State of the art; Statistic modeling; Tracking method; Belief propagation
Scopus,"Yuan, M.; Yue, P.; Yang, C.; Li, J.; Yan, K.; Cai, C.; Wan, C.",Generating lane-level road networks from high-precision trajectory data with lane-changing behavior analysis,,2024,,,,10.1080/13658816.2023.2279977,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176562390&doi=10.1080%2f13658816.2023.2279977&partnerID=40&md5=b765100c2dbbd24e3c260fa8e3386d03,"Abstract–: Recent advances in mobile mapping systems have facilitated the collection of high-precision trajectory data in centimeter positioning accuracy. It provides the potential to infer lane-level road networks, which are essential for autonomous driving navigation. This task is challenging due to the complicated lane merging and diverging structures as well as the lane-changing patterns in trajectory data. This paper presents a lane-level road network generation method from high-precision trajectory data with lane-changing behavior analysis. Trajectories are firstly partitioned by detecting road intersections and changes in lane structure. Subsequently, in regions with consistent lane structure, a principal curve fitting algorithm is developed to extract lane centerlines. Erroneous lanes generated by lane-changing behavior are pruned based on a constructed lane intersection graph. In regions with merging and diverging lanes, a lane-group fitting algorithm is designed. This algorithm estimates lane locations by incorporating a Gaussian mixture model with lane width prior knowledge and then infers lane-level topological structures using trajectory flow information. The proposed method is evaluated on a real-world high-precision trajectory dataset. Comprehensive experiments demonstrate that it outperforms state-of-the-art methods in four metrics. Under complex scenarios, the method is capable of generating lane-level road networks with higher completeness and fewer fragments. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",high-precision trajectory data; lane-changing behavior; Lane-level road network; algorithm; Gaussian method; navigation; numerical model; precision; road; topology; trajectory
Scopus,"Cheng, K.; Zhu, X.; Pei, Y.; Zhan, Y.",Research progress of agricultural automatic machinery obstacle detection,,2023,,,,10.3969/j.issn.1671-7775.2023.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171848645&doi=10.3969%2fj.issn.1671-7775.2023.04.007&partnerID=40&md5=0b08f965a05b74609bda0d7a534a1bb0,"Automatic navigation technology of agricultural machinery is concerned in the field of intelligent agriculture, while the obstacle detection is an important part. The shortcomings of sensor detection technology in the early years were analyzed, and the application and application prospects of computer vision in agricultural machinery intelligent obstacle detection methods were summarized. The results show that sensor technology is now developed from single sensor to multi-sensor fusion at very mature stage, but it is still affected by obstacles surface and too high detection cost. In recent years, computer vision and deep learning have been used in agriculture of convolutional neural network, but it should be improved much in occlusion, remote detection, moving obstacle detection and other aspects. According to the agricultural obstacle detection techniques in the past 20 years, the existing problems are summarized, and a new idea for the future development of smart agriculture in China is proposed. © 2023 Journal of Jiangsu University (Natural Science Edition). All rights reserved.",agricultural machinery; computer vison; information fusion; obstacle detection; sensor
Scopus,"Yuan, Y.; Wu, Y.; Zhao, L.; Chen, J.; Zhao, Q.",Research progress of UAV aerial video multi⁃object detection and tracking based on deep learning,,2023,,,,10.7527/S1000-6893.2023.28334,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175035439&doi=10.7527%2fS1000-6893.2023.28334&partnerID=40&md5=119dda6ba471fdcf769d188e7ae660a4,With the increasing convenience of data acquisition for aerial photography of Unmanned Aerial Vehicle （UAV），the multi-target detection and tracking technology based on the UAV platform has developed rapidly and has broad prospects for applications in civil and military fields. In recent years，the rapid progress of in-depth learning has also provided a variety of more effective solutions. However，the challenging problems such as sudden changes in the appearance of the target，serious occlusion of the target area，and disappearance and reappearance of the target from the perspective of UAV have not been completely solved. In this paper，we summarize the algorithms for multi-target detection and tracking in UAV aerial video based on deep learning，and summarize the latest progress in this field，including multi-target detection and multi-object tracking. The multi-object detection module is divided into two parts：two-stage and one-stage detection. For the multi-object tracking module，according to the two classical frameworks of tracking-based detection and joint-detection tracking，the principles of the two algorithms are described and their advantages and disadvantages are analyzed. Then，the existing public data sets are statistically analyzed，and the optimal schemes of the benchmark challenge VisDrone Challenge in the field of multi-target detection and tracking based on UAV aerial video in recent years are compared and analyzed. Finally，the paper discusses the urgent problems of multi-object detection and tracking from the perspective of UAV and the possible research directions in the future，providing a reference for the follow-up researchers. © 2023 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.,deep learning; detection tracking; joint-detection tracking; multi object detection; multi target tracking; one-stage detection; two-stage detection; UAV aerial video; Aerial photography; Aircraft detection; Antennas; Data acquisition; Deep learning; Military applications; Military photography; Object recognition; Unmanned aerial vehicles (UAV); Aerial video; Deep learning; Detection tracking; Joint-detection; Joint-detection tracking; Multi object detection; Multi-target-tracking; Multiobject; Objects detection; One-stage detection; Two-stage detections; UAV aerial video; Object detection
Scopus,"Wang, L.; Hua, S.; Zhang, C.; Yang, G.; Ren, J.; Li, J.",YOLOdrive: A Lightweight Autonomous Driving Single-Stage Target Detection Approach,,2024,,,,10.1109/JIOT.2024.3439863,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002092407&doi=10.1109%2fJIOT.2024.3439863&partnerID=40&md5=b68860615f7797c981c54f02b0c28547,"With the continuous development of autonomous driving, real-time target detection has become increasingly critical in autonomous driving systems. However, traditional target detection algorithms usually require huge computational resources, limiting their application on embedded autonomous driving platforms. To address this challenge, a lightweight single-stage target detection algorithm is proposed YOLOdrive. The inverted residual structure, linear bottleneck layer, and depth-separable convolution in MobileNetv2 are utilized to improve the YOLOv8 backbone network, while the spatial channel reconstructed convolution is used to improve the C2f module of YOLOv8, and the convolution of YOLOv8 neck and detection head is replaced by the depth-separable convolution. Experimental results verify that the average accuracy of YOLOdrive algorithm on MS COCO2017 data set and VOC2007 data set is improved compared with the baseline model YOLOv8-Nano. The amount of model parameters has been reduced by more than 50%, and the amount of computation in the model has been reduced by more than 70%. The algorithm drastically reduces the amount of parameters and computational complexity of the network, improves the operational efficiency, saves the storage space of the network, and maintains a high-detection performance. © 2014 IEEE.",Autonomous driving; deep learning; target detection; YOLOv8; Computational efficiency; Continuous time systems; Convolution; Deep learning; Feature extraction; Interactive computer systems; Internet of things; Object detection; Parameter estimation; Signal detection; Accuracy; Autonomous driving; Classification algorithm; Deep learning; Features extraction; Objects detection; Real - Time system; Single stage; Targets detection; YOLOv8; Real time systems
Scopus,"An, X.; Li, S.; Zhang, Y.; Zhang, A.",Review of Fault Diagnosis Techniques for UAV Flight Control Systems,,2023,,,,10.3778/j.issn.1002-8331.2305-0137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007336975&doi=10.3778%2fj.issn.1002-8331.2305-0137&partnerID=40&md5=82f94cb56d1baea612de1ec37c69d0f1,"In recent years, unmanned aerial vehicles（UAVs）have been widely used in various complex fields of military and civilian applications due to their unique advantages such as low operating costs and high mobility. At the same time, the complex and diverse missions have put forward higher requirements for the reliability and safety of UAV systems. The UAV fault diagnosis technology can provide timely and accurate diagnosis results, which helps the maintenance, repair and servicing of UAVs, and is of great significance in enhancing the combat effectiveness of UAVs. Therefore, this paper firstly analyses UAV flight control systems, and classifies the faults. Secondly, the research methods and status quo of UAV fault diagnosis technology are analysed and summarised. Finally, the main challenges faced by UAV fault diagnosis technology are discussed and the future development direction is pointed out; the aim is to provide some reference for researchers in the field of UAV fault diagnosis technology and to promote the improvement of UAV fault diagnosis technology level in China. © 2016 Chinese Medical Journals Publishing House Co.Ltd. All rights reserved.",actuator fault; fault diagnosis; sensor fault; unmanned aerial vehicle; Aircraft control; Antennas; Failure analysis; Fault detection; Flight control systems; Military applications; Military vehicles; Operating costs; Repair; Actuator fault; Aerial vehicle; Complex fields; Fault diagnosis technique; Fault diagnosis technology; Faults diagnosis; Flight-control systems; Lower operating costs; Sensors faults; Unmanned aerial vehicle; Unmanned aerial vehicles (UAV)
Scopus,"Hao, L.-Y.; Yang, J.-R.; Zhang, Y.; Zhang, J.",Multi-target vehicle detection based on corner pooling with attention mechanism,,2023,,,,10.1007/s10489-023-05084-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174607346&doi=10.1007%2fs10489-023-05084-4&partnerID=40&md5=cdcdb6f1b9adadc3e014f1be3ae4ea9e,"Abstract: Multi-target detection based on corner pooling provides a distinctive framework without anchor boxes, which has achieved wide application in the area of intelligent transportation system. To effectively detect small vehicles in the distant view, we propose an improved detection network termed corner pooling with attention mechanism (CPAM). A newly devised network called Hourglass with Coordinate Attention(Hourglass-CA) is proposed as an alternative to the Hourglass-104 backbone network. This one incorporates a multi-level attention mechanism to optimize the efficiency of feature extraction. Additionally, a novel multi-level attention loss(MLA loss) is presented, which dynamically adjusts the offsets during the feature extraction process. The experimental results demonstrate that our proposed CPAM achieves lightweight detection, reducing the parameters from 201M to 117M with an FPS from 4.2 to 16.1. Moreover, the AP can reach 51.6 % , surpassing several existing detectors. Graphical abstract: [Figure not available: see fulltext.]. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Coordinate attention; Corner pooling; Intelligent transportation system; Small object detection; Vehicle detection; Edge detection; Extraction; Intelligent systems; Intelligent vehicle highway systems; Object detection; Object recognition; Vehicles; Attention mechanisms; Coordinate attention; Corner pooling; Features extraction; Intelligent transportation systems; Multi-targets; Multilevels; Small object detection; Target vehicles; Vehicles detection; Feature extraction
Scopus,"Xu, C.; Zhang, Y.; Chen, S.",A multi-strategy integrated improved Yolov8n algorithm and its application to automatic driving detection,,2024,,,,10.1109/ICISCAE62304.2024.10761211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214700452&doi=10.1109%2fICISCAE62304.2024.10761211&partnerID=40&md5=a7b3e106c10b9d8a93125e73ddb6b360,"For the existing target detection algorithms with low detection accuracy and slow recognition efficiency, it is difficult to meet the needs of autonomous driving to enable vehicles to quickly and accurately identify and localize the surrounding objects. For this reason, this paper proposes a multi-strategy integrated and improved Yolov8n algorithm to meet the demand for fast and accurate identification and localisation of surrounding objects for autonomous driving. Firstly, for the problem of low detection accuracy of Yolov8n, the GAM attention mechanism is introduced to improve the model's ability to capture key information. Subsequently, for the problem of poor model robustness, the CIOU loss function is replaced with the GIOU loss function to improve the model robustness. Finally, the SGD optimizer is applied to optimize the hyperparameters of the model to improve the model training efficiency. The proposed improved algorithm is applied to the VOC2007 dataset for validation, and the experimental results show that the improved model on the validation set mAP@0.5 can be 86.5%. The effectiveness and applicability of the proposed algorithm is verified by the improved model in the validation set. The Precision, Recall and mAP@0.5 all have significantly improved. The proposed improved Yolov8n model was compared with other target detection models, mAP@0.5 improved 2.8%~12.3%, and the superiority of the model is verified. ©2024 IEEE.",GAM attention; GIOU; VOC2007; Yolov8n; Automatic driving; Autonomous driving; Detection accuracy; GAM attention; GIOU; ITS applications; Loss functions; Model robustness; Validation sets; Yolov8n; Automatic target recognition
Scopus,"Chang, I.-C.; Yen, C.-E.; Song, Y.-J.; Chen, W.-R.; Kuo, X.-M.; Liao, P.-H.; Kuo, C.; Huang, Y.-F.",An Effective YOLO-Based Proactive Blind Spot Warning System for Motorcycles,,2023,,,,10.3390/electronics12153310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167783693&doi=10.3390%2felectronics12153310&partnerID=40&md5=e4ef30c333b95291e4717cd95f010c7a,"Interest in advanced driver assistance systems (ADAS) is booming in recent years. One of the most effervescent ADAS features is blind spot detection (BSD), which uses radar sensors or cameras to detect vehicles in the blind spot area and alerts the driver to avoid a collision when changing lanes. However, this kind of BSD system fails to notify nearby vehicle drivers in this blind spot of the possible collision. The goal of this research is to design a proactive bus blind spot warning (PBSW) system that will immediately notify motorcyclists when they enter the blind spot or the area of the inner wheel difference of a target vehicle, i.e., a bus. This will increase the real-time functionality of BSD and can have a significant impact on enhancing motorcyclist safety. The proposed hardware is placed on the motorcycle and consists of a Raspberry Pi 3B+ and a dual-lens stereo camera. We use dual-lens cameras to capture and create stereoscopic images then transmit the images from the Raspberry Pi 3B+ to an Android phone via Wi-Fi and to a cloud server using a cellular network. At the cloud server, we use the YOLOv4 image recognition model to identify the position of the rear-view mirror of the bus and use the lens imaging principle to estimate the distance between the bus and the motorcyclist. Finally, the cloud server returns the estimated distance to the PBSW app on the Android phone. According to the received distance value, the app will display the visible area/blind spot, the area of the inner wheel difference of the bus, the position of the motorcyclist, and the estimated distance between the motorcycle and the bus. Hence, as soon as the motorcyclist enters the blind spot of the bus or the area of the inner wheel difference, the app will alert the motorcyclist immediately to enhance their real-time safety. We have evaluated this PBSW system implemented in real life. The results show that the average position accuracy of the rear-view mirror is 92.82%, the error rate of the estimated distance between the rear-view mirror and the dual-lens camera is lower than 0.2%, and the average round trip delay between the Android phone and the cloud server is about 0.5 s. To the best of our knowledge, this proposed system is one of few PBSW systems which can be applied in the real world to protect motorcyclists from the danger of entering the blind spot and the area of the inner wheel difference of the target vehicle in real time. © 2023 by the authors.",Android app; area of the inner wheel difference; blind spot; distance estimation; dual-lens camera; motorcycle; proactive warning system; Raspberry Pi; real-time; YOLO
Scopus,"Tallat, R.; Hawbani, A.; Wang, X.; Al-Dubai, A.; Zhao, L.; Liu, Z.; Min, G.; Zomaya, A.Y.; Hamood Alsamhi, S.","Navigating Industry 5.0: A Survey of Key Enabling Technologies, Trends, Challenges, and Opportunities",,2024,,,,10.1109/COMST.2023.3329472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181841986&doi=10.1109%2fCOMST.2023.3329472&partnerID=40&md5=057d4174ea29dc6179afb0cfd66a385d,"This century has been a major avenue for revolutionary changes in technology and industry. Industries have transitioned towards intelligent automation, relying less on human intervention, resulting in the fourth industrial revolution, Industry 4.0. That is why IoT has been the researcher's arena for quite some time. With Industry 4.0 still in motion, the world is on the verge of the 5{textit {th}} industrial revolution, a relatively new concept with many unclear opinions regarding its potential benefits, challenges, opportunities, trends, and impact on society. There is a dire need for a broader and more critical perspective. This research paints a bigger picture of 'What is happening?' and 'What to expect?' during the transition phase of Industry 5.0. In this comprehensive review, we have addressed the state-of-the-art practices in Industry 4.0 and the transitional phase of Industry 5.0. We have highlighted the most promising key enabling technologies, trends, research topics, rising challenges, and unfolding opportunities that can help prepare society for this paradigm shift. The paper then surveys the work toward the outstanding key enablers, challenges, trends, and opportunities in the IoT evolution for Industry 5.0. To spur further avenues for researchers and industrialists, the paper offers conclusive insights at the end. In addition, the article has a precise set of research questions answered in consequent sections and subsections for the reader's clarity.  © 1998-2012 IEEE.",6G; blockchain; digital twin; federated learning; Industrial Internet of Things (IIoT); industrial wireless sensor networks; industry 4.0; Industry 5.0; intelligent sensing; Internet of Robotic Things (IoRT); Blockchain; E-learning; Internet of things; Wireless sensor networks; 6g; Block-chain; Federated learning; Fourth industrial revolution; Industrial internet of thing; Industrial revolutions; Industrial wireless sensor network; Industrial wireless sensors; Industry 5.0; Intelligent sensing; Internet of robotic thing; Robot sensing system; Service robots; Industry 4.0
Scopus,"Jiang, Y.; Zhu, B.; Zhao, X.; Deng, W.",Pixel-wise content attention learning for single-image deraining of autonomous vehicles,,2023,,,,10.1016/j.eswa.2023.119990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151630648&doi=10.1016%2fj.eswa.2023.119990&partnerID=40&md5=2ec5dd6d441a54ceadd099a4a4b58d91,"Improving the performance of autonomous vehicles in adverse weather conditions is vital for the commercialization of such automated systems. Existing synthetic datasets for developing rain-tolerant vision are of limited value. To address this deficiency, a closed environment capable of simulating different degrees of rainfall is constructed. And a new Closed Field Rain dataset is collected in 36 testing cycles. Inspired by the idea that human can infer the content of rainy images directly without removing the raindrops. A new single-image deraining method is proposed, that does not require ground truth images. This method incorporates an image content estimation module applied to predict the scene content representation, and a pixel-wise content attention block used to evaluate the significance of each pixel. After that, an encoder-decoder network is applied to complete the image. On the other hand, it is almost impossible to obtain the ground truth of rainy images because of the dynamic characteristics of real traffic environment. Thus, the model is trained by employing PatchGAN, using a patch-based loss. Using common no-reference and feature point metrics as performance indicators, this paper conducts a comprehensive evaluation on both synthetic and real-world datasets including Closed Field Rain dataset. Results show the effectiveness of our model quantitatively and qualitatively. © 2023 Elsevier Ltd",Closed field rain; Content representation; GAN; Pixel-wise content attention; Single-image deraining; Autonomous vehicles; Rain; Statistical tests; Adverse weather; Autonomous Vehicles; Closed field rain; Content representation; GAN; Ground truth; Performance; Pixel-wise content attention; Single images; Single-image deraining; Pixels
Scopus,"Yang, Y.; Yang, F.; Sun, L.; Xiang, T.; Lv, P.",Echoformer: Transformer Architecture Based on Radar Echo Characteristics for UAV Detection,,2023,,,,10.1109/JSEN.2023.3254525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151392959&doi=10.1109%2fJSEN.2023.3254525&partnerID=40&md5=4c4e7d8712b24c808fa850c05ec641f7,"While recent years have witnessed an increasing number of commercial applications of unmanned aerial vehicles (UAVs), an imperative problem people have to face is the rapid growth of malicious use. So, it is imperative for security agencies to develop anti-UAV technology. The introduction of deep learning (DL) has a positive influence on radar signal processing, but DL-based methodologies have yet to be widespread in radar target detection because of the lack of unique architecture based on radar echo characteristics and the annotation method of radar data. In this article, a novel Transformer-based architecture is proposed, which transforms the problem of UAV detection into a binary classification task in each range cell. The complex encoder architecture and the Transformer-based extractor are designed to extract the Doppler frequency shift feature and the micro-Doppler signature (mDS) of a UAV simultaneously. The well-designed architecture based on radar echo characteristics can achieve a combination training of echoes with different coherent processing intervals (CPIs). In addition, we provide an annotation method and a data augmentation skill for our real measured dataset. The results of the experiment demonstrate that the proposed method has better detection performance and measuring accuracy under different SNRs in comparison with traditional radar target detection and other DL-based methods.  © 2001-2012 IEEE.",Doppler frequency shift; micro-Doppler signature (mDS); radar echo processing; Transformer; unmanned aerial vehicle (UAV) detection; Aircraft detection; Antennas; Clutter (information theory); Commercial vehicles; Continuous wave radar; Deep learning; Forestry; Radar measurement; Tracking radar; Unmanned aerial vehicles (UAV); Aerial vehicle; Clutter; Doppler frequency shift; Doppler signatures; Micro-dopple signature; Micro-Doppler; Radar detection; Radar echo processing; Radar echoes; Task analysis; Transformer; Unmanned aerial vehicle detection; Vehicles detection; Frequency modulation
Scopus,"Khalid, S.; Shah, J.H.; Sharif, M.; Dahan, F.; Saleem, R.; Masood, A.",A Robust Intelligent System for Text-Based Traffic Signs Detection and Recognition in Challenging Weather Conditions,,2024,,,,10.1109/ACCESS.2024.3401044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193222591&doi=10.1109%2fACCESS.2024.3401044&partnerID=40&md5=45dcd3245f7a602f9e4e3e0aa60bb05c,"Traffic signs have great importance regarding smooth traffic flow and safe driving. However, due to many distractions and capricious factors, spotting and perceiving them may become hazardous. Traffic sign detection and recognition have gained popularity to put an end or to lessen the issue, and massive efforts have been realized in this regard. Despite considerable endeavors put together for traffic sign detection and recognition, there is a lack of attention in this area where these traffic signs contain text in them. A handful of studies may be found in state-of-the-art (SOTA) methods for text-based traffic sign detection, and particularly lesser for text recognition of detected text. The proposed method focuses on developing a robust semi-pipeline intelligent system to detect and understand text from traffic road signs boards in various weather conditions. For this purpose, a customized YOLOv5s is deployed for initial panel detection. Subsequently, MSER with preprocessing techniques is used for localization of text. Finally, OCR with NLP is utilized to recognize the text. The proposed method employed the ASAYAR dataset for training and different datasets for testing. The proposed approach produced satisfactory outcomes on them in contrast with SOTA approaches.  © 2013 IEEE.",automated road signs/panels detection; deep learning; MSER; natural language processing; Text recognition; YOLOV5s; Character recognition; Deep learning; Intelligent systems; Meteorology; Natural language processing systems; Roads and streets; Statistical tests; Traffic signs; Automated road sign/panel detection; Automated roads; Deep learning; Features extraction; Language processing; MSER; Natural language processing; Natural languages; Road; Road signs; Shape; Symbol; Text detection; Text recognition; YOLOV5; Feature extraction
Scopus,"Elhanashi, A.; Dini, P.; Saponara, S.; Zheng, Q.",Integration of Deep Learning into the IoT: A Survey of Techniques and Challenges for Real-World Applications,,2023,,,,10.3390/electronics12244925,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180682394&doi=10.3390%2felectronics12244925&partnerID=40&md5=dd55893c500d3a712fde72b0cab5ea0e,"The internet of things (IoT) has emerged as a pivotal technological paradigm facilitating interconnected and intelligent devices across multifarious domains. The proliferation of IoT devices has resulted in an unprecedented surge of data, presenting formidable challenges concerning efficient processing, meaningful analysis, and informed decision making. Deep-learning (DL) methodologies, notably convolutional neural networks (CNNs), recurrent neural networks (RNNs), and deep-belief networks (DBNs), have demonstrated significant efficacy in mitigating these challenges by furnishing robust tools for learning and extraction of insights from vast and diverse IoT-generated data. This survey article offers a comprehensive and meticulous examination of recent scholarly endeavors encompassing the amalgamation of deep-learning techniques within the IoT landscape. Our scrutiny encompasses an extensive exploration of diverse deep-learning models, expounding on their architectures and applications within IoT domains, including but not limited to smart cities, healthcare informatics, and surveillance applications. We proffer insights into prospective research trajectories, discerning the exigency for innovative solutions that surmount extant limitations and intricacies in deploying deep-learning methodologies effectively within IoT frameworks. © 2023 by the authors.",convolutional neural networks; deep learning; healthcare; internet of things; recurrent neural networks; surveillance
Scopus,"Yang, J.; Pan, Z.; Liu, Y.; Niu, B.; Lei, B.",Single Object Tracking in Satellite Videos Based on Feature Enhancement and Multi-Level Matching Strategy,,2023,,,,10.3390/rs15174351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170353774&doi=10.3390%2frs15174351&partnerID=40&md5=27a6e3df7a08d1c950ec775bd819266f,"Despite significant advancements in remote sensing object tracking (RSOT) in recent years, achieving accurate and continuous tracking of tiny-sized targets remains a challenging task due to similar object interference and other related issues. In this paper, from the perspective of feature enhancement and a better feature matching strategy, we present a tracker SiamTM specifically designed for RSOT, which is mainly based on a new target information enhancement (TIE) module and a multi-level matching strategy. First, we propose a TIE module to address the challenge of tiny object sizes in satellite videos. The proposed TIE module goes along two spatial directions to capture orientation and position-aware information, respectively, while capturing inter-channel information at the global 2D image level. The TIE module enables the network to extract discriminative features of the targets more effectively from satellite images. Furthermore, we introduce a multi-level matching (MM) module that is better suited for satellite video targets. The MM module firstly embeds the target feature map after ROI Align into each position of the search region feature map to obtain a preliminary response map. Subsequently, the preliminary response map and the template region feature map are subjected to the Depth-wise Cross Correlation operation to get a more refined response map. Through this coarse-to-fine approach, the tracker obtains a response map with a more accurate position, which lays a good foundation for the prediction operation of the subsequent sub-networks. We conducted extensive experiments on two large satellite video single-object tracking datasets: SatSOT and SV248S. Without bells and whistles, the proposed tracker SiamTM achieved competitive results on both datasets while running at real-time speed. © 2023 by the authors.",feature enhancement; matching strategy; object tracking; satellite video; siamese network; Large dataset; Remote sensing; Target tracking; Feature enhancement; Feature map; Level matching; Matching strategy; Matchings; Multilevels; Object Tracking; Satellite video; Siamese network; Target information; Satellites
Scopus,"Xing, J.; Liu, Y.; Zhang, G.-Z.",Improved YOLOV5-Based UAV Pavement Crack Detection,,2023,,,,10.1109/JSEN.2023.3281585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161564254&doi=10.1109%2fJSEN.2023.3281585&partnerID=40&md5=cf04b109702444e850d042cf9f0fc744,"In terms of highway crack detection, the combination of unmanned aerial vehicles (UAVs) and deep learning networks has become a powerful detection means. However, in the actual detection, in order to take into account the detection efficiency, it is necessary to ensure that the detection area is large enough, which makes the crack occupy few pixels in the image, and the image background is complex. Therefore, in this article, DJI Mavic3 is used to establish the image data set of highway pavement cracks under complex background. And, the YOLOV5 deep learning model is improved by adding a swin transformer structure and bidirectional feature pyramid network (BIFPN) feature pyramid. The improved YOLOV5 model achieved real-time pixel-level detection with a detection accuracy of 90% and a detection speed of 43.5 FPS. In terms of crack detection ability, the accuracy of the improved YOLOV5 reaches four pixels, and cracks of 1.2 mm can be detected in the experiment. Compared with the YOLOV7 model, the detection accuracy of the improved YOLOV5 model is increased by 15.4%. Compared with the YOLOV6 model, the calculated parameters of the improved YOLOV5 model are reduced by 59.25%. The proposed model is superior to other advanced models in crack detection. © 2001-2012 IEEE.",Bidirectional feature pyramid network (BIFPN); crack; swin transformer; unmanned aerial vehicle (UAV); YOLOV5; Aircraft detection; Antennas; Autonomous vehicles; Complex networks; Crack detection; Deep learning; Pavements; Pixels; Aerial vehicle; BIFPN; Computational modelling; Optimisations; Proposal; Road transportation; Swin transformer; Transformer; Unmanned aerial vehicle; YOLOV5; Unmanned aerial vehicles (UAV)
Scopus,"Selvaraj, R.; Kuthadi, V.M.; Duraisamy, A.; Selvaraj, B.; Pethuraj, M.S.",Learning Optimizer-Based Visual Analytics Method to Detect Targets in Autonomous Unmanned Aerial Vehicles,,2024,,,,10.1109/MITS.2023.3271447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160228003&doi=10.1109%2fMITS.2023.3271447&partnerID=40&md5=ea15d0daa74a5d39bf0b3099904d1267,"Visual analytics is vital for identifying targets by differentiating their structure and texture from unmanned aerial vehicles (UAVs). Object sensing disturbance and swift texture differentiation are tedious due to the UAV displacements. For improving the accuracy of target detection, this article introduces a learning optimizer-based visual analytics method. The proposed method assimilates deep learning and a gradient descent algorithm for feature differentiation and error minimization concurrently. The captured images are identified using multiple structural feature variations and are correlated with similar stored images. The features are extracted at different displacement and structural changing instances for leveraging accuracy. The learning process trains the similarity features during different differentiation factors. In the feature extraction, the minimum slope points are identified using a gradient descent algorithm by assigning random weights. As the differentiation increases using similar features, the minimum similarity value is detection. Postdetection, the weights are incremental and linear across different feature slopes. Therefore, the accuracy increases under varying displacement instances, preventing target misdetection. The gradient function is invariable between the minimum and maximum values for identifying high-precision features. This ensures optimal detection of different buildings and structures with high accuracy.  © 2009-2012 IEEE.",Aircraft detection; Antennas; Autonomous vehicles; Deep learning; Energy utilization; Extraction; Feature extraction; Interactive computer systems; Object detection; Object recognition; Optimization; Textures; Unmanned aerial vehicles (UAV); Visualization; Aerial vehicle; Analytic method; Energy-consumption; Features extraction; Objects detection; Optimizers; Real - Time system; Surveillance; Targets tracking; Visual analytics; Real time systems
Scopus,"Saini, V.; Kantipudi, M.P.; Meduri, P.",Enhanced SSD Algorithm-Based Object Detection and Depth Estimation for Autonomous Vehicle Navigation,,2023,,,,10.18280/ijtdi.070408,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182652677&doi=10.18280%2fijtdi.070408&partnerID=40&md5=40ec75062cae16029c72767e497bfb58,"Autonomous vehicles necessitate robust stability and safety mechanisms for effective navigation, relying heavily upon advanced perception and precise environmental awareness. This study addresses the object detection challenge intrinsic to autonomous navigation, with a focus on the system architecture and the integration of cutting-edge hardware and software technologies. The efficacy of various object recognition algorithms, notably the Single Shot Detector (SSD) and You Only Look Once (YOLO), is rigorously compared. Prior research has indicated that SSD, when augmented with depth estimation techniques, demonstrates superior performance in real-time applications within complex environments. Consequently, this research proposes an optimized SSD algorithm paired with a Zed camera system. Through this integration, a notable improvement in detection accuracy is achieved, with a precision increase to 87%. This advancement marks a significant step towards resolving the critical challenges faced by autonomous vehicles in object detection and distance estimation, thereby enhancing their operational safety and reliability. © 2023 WITPress. All rights reserved.",autonomous vehicle; camera; LiDAR; object recognition; perception & vision; RADAR; safe driving performance; SSD; YOLO; ZED-Camera
Scopus,"Liao, L.; Luo, L.; Su, J.; Xiao, Z.; Zou, F.; Lin, Y.",Eagle-YOLO: An Eagle-Inspired YOLO for Object Detection in Unmanned Aerial Vehicles Scenarios,,2023,,,,10.3390/math11092093,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159216871&doi=10.3390%2fmath11092093&partnerID=40&md5=7606d68011da7b6d2192eb69a4e4da88,"Object detection in images taken by unmanned aerial vehicles (UAVs) is drawing ever-increasing research interests. Due to the flexibility of UAVs, their shooting altitude often changes rapidly, which results in drastic changes in the scale size of the identified objects. Meanwhile, there are often many small objects obscured from each other in high-altitude photography, and the background of their captured images is also complex and variable. These problems lead to a colossal challenge with object detection in UAV aerial photography images. Inspired by the characteristics of eagles, we propose an Eagle-YOLO detection model to address the above issues. First, according to the structural characteristics of eagle eyes, we integrate the Large Kernel Attention Module (LKAM) to enable the model to find object areas that need to be focused on. Then, in response to the eagle’s characteristic of experiencing dramatic changes in its field of view when swooping down to hunt at high altitudes, we introduce a large-sized feature map with rich information on small objects into the feature fusion network. The feature fusion network adopts a more reasonable weighted Bi-directional Feature Pyramid Network (Bi-FPN). Finally, inspired by the sharp features of eagle eyes, we propose an IoU loss named Eagle-IoU loss. Extensive experiments are performed on the VisDrone2021-DET dataset to compare it with the baseline model YOLOv5x. The experiments showed that Eagle-YOLO outperformed YOLOv5x by 2.86% and 4.23% in terms of the mAP and AP50, respectively, which demonstrates the effectiveness of Eagle-YOLO for object detection in UAV aerial image scenes. © 2023 by the authors.",attentional mechanisms; Eagle-YOLO; object detection; unmanned aerial vehicle
Scopus,"Leng, J.; Mo, M.; Zhou, Y.; Ye, Y.; Gao, C.; Gao, X.",Recent advances in drone-view object detection,,2023,,,,10.11834/jig.220836,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172764983&doi=10.11834%2fjig.220836&partnerID=40&md5=a50918020831a26b73bf221f753b5f96,Given the support of artificial intelligence technology，drones have initially acquired intelligent sensing capabilities and have demonstrated efficient and flexible data collection in practical applications. Drone-view object detection，which aims to locate specific objects in aerial images，plays an irreplaceable role in many fields and has important research significance. For example，drones with highly mobile and flexible deployment have remarkable advantages in accident handling，order management，traffic guidance，and flow detection，making them irreplaceable in traffic monitoring. As for disaster emergency rescue，drones with aerial vision and high mobility can achieve efficient search and safe rescue in large areas，locate people quickly and accurately in distress，and help rescuers control the situation，thereby ensuring the safety of people in distress. This study provides a comprehensive summary of the challenges in object detection based on the unmanned aerial vehicle（UAV）perspective to portray further the development of drone-view object detection. The existing algorithms and related datasets are also introduced. First，this study briefly introduces the concept of object detection in drone view and summarizes the five imbalance challenges in object detection in drone view，such as scale imbalance，spatial imbalance，class imbalance，semantic imbalance，and objective imbalance. This study analyzes and summarizes the challenges of drone-view object detection based on the aforementioned imbalances by using quantitative data analysis and visual qualitative analysis. 1）Object scale imbalance is the most focused challenge in current research. It comes from the unique aerial view of drones. The changes in the drone’s height and angle bring drastic changes to the object scale in the acquired images. The distance of the lens from the photographed object under the drone view is often far. This scenario results in numerous small objects in the image and makes capturing useful features for object detection difficult for the existing detectors. 2）Different regions of drone-view images have great differences，and most objects are concentrated in the minor area of images，i. e. ，the spatial distribution of objects is enormously uneven. On the one hand，the clustering of dense objects in small areas generates occlusion. The detection model needs to devote considerable attention to this occlusion to distinguish different objects effectively. On the other hand，treating equally different areas wastes many computational resources in vanilla areas，limiting the improvement of object detection performance. 3）The problem of class imbalance in the drone view is divided into two categories. One is the positive-negative sample imbalance problem caused by the gap between the front and rear views shared in the image. The other is the imbalanced numbers of different categories caused by the number of samples in the real world. 4）The semantic pieces of information defined by different category labels in the drone-view object detection dataset are often similar，resulting in only subtle differences between different categories. However，significantly different representations of objects exist in the same category，which together form the semantic imbalance problem. 5）Drone-view object detection often faces the problem of unbalanced optimization targets，i. e. ，the contradiction between the high computational demand for high-resolution images and the limited computing power of low-power chips is difficult to balance. These unbalanced problems bring enormous challenges to object detection from the UAV viewpoint. However，even the most advanced object detection algorithms currently available can hardly achieve an average accuracy rate of 40% on aerial images，which is far below the performance of general object detection tasks. Therefore，many scholars have conducted many studies. These research methods can be summarized as optimization ideas to solve these imbalance problems. In this study，we collect relevant research works，which are sorted and analyzed according to the countries of authors，institutions，published journals or conferences，years，the category of methods，and the solved problem. The present study presents the challenging problems solved by previous research and the development trends of existing methods. This study also focuses on the methods of improving drone-view object detection performance in terms of data augmentation，multiscale feature fusion，region searching strategies，multitask learning，and lightweight model. The advantages and disadvantages of these methods for different problems are systematically summarized and analyzed. Besides introducing existing methods，the present study compiles and introduces the applications of drone-view object detection in practical scenarios，such as traffic monitoring，power inspection，crop analysis，and disaster rescue. These applications further emphasize the significance of object detection in drone view. Then，this study collects and organizes UAV datasets suitable for object detection tasks. These datasets are present from various perspectives，such as year，published journals or conferences，annotation information，and number of citations. In particular，the present study provides the performance evaluation of the existing algorithms on two commonly used public datasets. The presentation of these performance data is expected to help researchers understand the current state of development of drone-view object detection and promote further development in this field. Finally，this study provides an outlook on the future direction of drone-view object detection by considering the aforementioned imbalance problems. The promising research includes the following：1）data augmentation：providing the network with enough high-quality learning samples by considering the specific characteristics of drone-view images based on the conventional data augmentation strategy is a good idea；2）multiscale representation：how to avoid the interference of background noise in feature fusion and effectively extract information at different scales using an efficient fusion strategy is an urgent problem to be solved；3）visual inference：using information unique to the viewpoint of drones，mining contextual information from images to facilitate image recognition，and using easy-to-detect objects to improve the performance of difficult-to-detect objects are directions worthy of deep consideration. © 2023 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.,aerial image; computer vision; deep learning; object detection; review
Scopus,"Chen, H.; Liu, Y.; Cheng, Y.",DRIO: Robust Radar-Inertial Odometry in Dynamic Environments,,2023,,,,10.1109/LRA.2023.3301290,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166746147&doi=10.1109%2fLRA.2023.3301290&partnerID=40&md5=c3f6974187aedaaedfeeeeffa5756474,"Accurate and robust localization is essential for mobile robots. Recently, millimeter wave (mmWave) radars have been widely used for odometry, owing to their robustness to all-weather conditions, lightweight and low cost. However, existing radar-based odometry methods degrade severely in high-dynamic environments. In this letter, we propose a robust radar-inertial odometry method for high-dynamic environments (DRIO) by exploiting the ground, an ever-present static target that is unaffected by the dynamic environments. The points of the ground surface were traditionally treated as clutter points in previous works due to their unstable distribution. We overcome this limitation by detecting ground points using both Doppler and geometric characteristics. During the detection process, accurate radar velocity is jointly estimated, which is then fused with inertial data to obtain the odometry. The real-world evaluations indicate that the proposed method achieves robust and Lidar-level localization in complex dynamic environments. In addition to odometry, our method can effectively improve the quality of radar point clouds for subsequent perception tasks.  © 2016 IEEE.",Dynamic environments; localization; millimeter wave radar; odometry; Doppler radar; Millimeter waves; Optical radar; Tracking radar; Dynamic environments; Localisation; Millimeter-wave radar; Millimeterwave communications; Millimetre-wave radar; Odometry; Point cloud compression; Point-clouds; Radar measurement
Scopus,"Luo, T.; Wang, H.; Cai, Y.; Chen, L.; Wang, K.; Yu, Y.",Binary residual feature pyramid network: An improved feature fusion module based on double-channel residual pyramid structure for autonomous detection algorithm,,2023,,,,10.1049/itr2.12291,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139703301&doi=10.1049%2fitr2.12291&partnerID=40&md5=0fdc8e69ce21c9a4d5d7764368686e51,"The vehicle detection algorithm based on visual perception has been applied in all types of automatic driving scenes. However, there are still flaws in the current detection algorithm model, especially for small objects. The detection effect of vehicle objects with small pixels in the image is often missed and wrongly detected. This research proposes an improved feature fusion module based on double-channel residual pyramid (DRP) structure for autonomous detection algorithm which named binary residual feature pyramid network (BiResFPN) to solve the above problems. Firstly, a DRP structure, which can effectively supplement the shallow information of the network, is proposed. The residual structure is added to the output feature map for further supplement. Then, an average sampling method of positive and negative samples based on intersection-over-union (IOU) value is proposed on the basis of this structure, aimed at the unbalanced sampling of positive and negative samples in the training stage of faster regions with CNN features (RCNN). It leads to the reduction of the interference of a large number of simple negative samples, which makes the learned model better. The experimental results based on the KITTI and BDD100K dataset datasets show that the capability of the feature fusion module based on DRP structure is strong for small object detection. Compared with Faster-RCNN (FPN), the detection algorithm of small object detection accuracy APsmall was increased by 2.6%, APmedium and APlarge was increased by 1.1% and 0.3%. © 2022 The Authors. IET Intelligent Transport Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",Automobile drivers; Feature extraction; Intelligent systems; Object recognition; Signal detection; Traffic control; Autonomous detection; Detection algorithm; Double-channel; Feature pyramid; Features fusions; Fusion modules; Module-based; Negative samples; Pyramid network; Pyramid structure; Object detection
Scopus,"Zhang, Y.; Fang, X.; Guo, J.; Wang, L.; Tian, H.; Yan, K.; Lan, Y.",CURI-YOLOv7: A Lightweight YOLOv7tiny Target Detector for Citrus Trees from UAV Remote Sensing Imagery Based on Embedded Device,,2023,,,,10.3390/rs15194647,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174145889&doi=10.3390%2frs15194647&partnerID=40&md5=d21f3fd3372a46ce1f6ec6ebb0948256,"Data processing of low-altitude remote sensing visible images from UAVs is one of the hot research topics in precision agriculture aviation. In order to solve the problems of large model size with slow detection speed that lead to the inability to process images in real time, this paper proposes a lightweight target detector CURI-YOLOv7 based on YOLOv7tiny which is suitable for individual citrus tree detection from UAV remote sensing imagery. This paper augmented the dataset with morphological changes and Mosica with Mixup. A backbone based on depthwise separable convolution and the MobileOne-block module was designed to replace the backbone of YOLOv7tiny. SPPF (spatial pyramid pooling fast) was used to replace the original spatial pyramid pooling structure. Additionally, we redesigned the neck by adding GSConv and depth-separable convolution and deleted its input layer from the backbone with a size of (80, 80) and its output layer from the head with a size of (80, 80). A new ELAN structure was designed, and the redundant convolutional layers were deleted. The experimental results show that the GFLOPs = 1.976, the parameters = 1.018 M, the weights = 3.98 MB, and the mAP = 90.34% for CURI-YOLOv7 in the UAV remote sensing imagery of the citrus trees dataset. The detection speed of a single image is 128.83 on computer and 27.01 on embedded devices. Therefore, the CURI-YOLOv7 model can basically achieve the function of individual tree detection in UAV remote sensing imagery on embedded devices. This forms a foundation for the subsequent UAV real-time identification of the citrus tree with its geographic coordinates positioning, which is conducive to the study of precise agricultural management of citrus orchards. © 2023 by the authors.",citrus trees; lightweight; remote sensing; target detector; YOLOv7; Aircraft detection; Chemical detection; Convolution; Data handling; Remote sensing; Citrus tree; Detection speed; Embedded device; Lightweight; Remote sensing imagery; Remote-sensing; Spatial pyramids; Target detectors; UAV remote sensing; YOLOv7; Unmanned aerial vehicles (UAV)
Scopus,"Gao, X.; Kanu-Asiegbu, A.M.; Du, X.",MambaST: A Plug-and-Play Cross-Spectral Spatial-Temporal Fuser for Efficient Pedestrian Detection,,2024,,,,10.1109/ITSC58415.2024.10920115,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001671411&doi=10.1109%2fITSC58415.2024.10920115&partnerID=40&md5=8f672dafcbf01823eae019971ae2e56c,"This paper proposes MambaST, a plug-and-play cross-spectral spatial-temporal fusion pipeline for efficient pedestrian detection. Several challenges exist for pedestrian detection in autonomous driving applications. First, it is difficult to perform accurate detection using RGB cameras under dark or low-light conditions. Cross-spectral systems must be developed to integrate complementary information from multiple sensor modalities, such as thermal and visible cameras, to improve the robustness of the detections. Second, pedestrian detection models are latency-sensitive. Efficient and easy-to-scale detection models with fewer parameters are highly desirable for real-time applications such as autonomous driving. Third, pedestrian video data provides spatial-temporal correlations of pedestrian movement. It is beneficial to incorporate temporal as well as spatial information to enhance pedestrian detection. This work leverages recent advances in the state space model (Mamba) and proposes a novel Multi-head Hierarchical Patching and Aggregation (MHHPA) structure to extract both fine-grained and coarse-grained information from both RGB and thermal imagery. Experimental results show that the proposed MHHPA is an effective and efficient alternative to a Transformer model for cross-spectral pedestrian detection. Our proposed model also achieves superior performance on small-scale pedestrian detection. The code is available at https://github.com/XiangboGaoBarry/MambaST © 2024 IEEE.",Image coding; Image enhancement; Video recording; Autonomous driving; Low light conditions; Multiple sensors; Pedestrian detection; Plug-and-play; RGB cameras; Sensor modality; Spatial temporals; Thermal camera; Visible cameras
Scopus,"Kan, X.; Shi, G.; Yang, X.",Dense Mapping Method for Indoor Dynamic Scenes Using an Improved ORB-SLAM2 Algorithm Based on RGB-D,,2024,,,,10.1109/CCSSTA62096.2024.10691846,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207457782&doi=10.1109%2fCCSSTA62096.2024.10691846&partnerID=40&md5=2557be1aa8aef51887481990c4a71550,"Leveraging instance segmentation techniques within Simultaneous Localization and Mapping (SLAM) frameworks effectively tackles the challenges associated with dynamic environments. While instance segmentation algorithms enhance SLAM capabilities, it may come at the expense of real-time performance. Additionally, camera motion-induced jitter may lead to inaccurate keyframe selection and tracking loss. To address the performance bottleneck, this work introduces a novel algorithm that integrates YOLOv5 with ORB-SLAM2, aiming to achieve both real-time operation and improved localization accuracy. In the tracking thread, dynamic objects are identified and their feature points are filtered out, striking a balance between real-time performance and localization accuracy. Furthermore, a keyframe selection criterion based on inter-frame relative motion is introduced to improve the accuracy of keyframe selection. Finally, the algorithm achieves the construction of dense static point cloud maps in dynamic scenes, thereby obtaining stable dense point cloud maps. Experimental results on the walking_xyz dataset show that compared to ORBSLAM2, the proposed algorithm reduces the average processing time by 38.5% and improves the root mean square accuracy by 95.2%. The experimental results indicate that compared to ORB-SLAM2, the proposed algorithm reduces the average processing time by 38.5% and improves the root mean square accuracy by 95.2% on the walking_xyz dataset. The proposed algorithm effectively addresses the aforementioned challenges, en hancing the localization accuracy and precision of SLAM algorithms, and thereby improving the usability of maps. © 2024 IEEE.",dynamic scene; keyframe selection; ORB-SLAM2; VSLAM; YOLOv5; Image coding; Image segmentation; SLAM robotics; Timing jitter; Cloud map; Dynamic scenes; Key frame selection; Localization accuracy; ORB-SLAM2; Point-clouds; Real time performance; Simultaneous localization and mapping; VSLAM; YOLOv5; Mapping
Scopus,"Beg, M.S.; Ismail, M.Y.; Saef Ullah Miah, M.; Peeie, M.H.",Enhancing Driving Assistance System with YOLO V8-Based Normal Visual Camera Sensor,,2023,,,,10.37934/ARASET.31.1.226236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170053893&doi=10.37934%2fARASET.31.1.226236&partnerID=40&md5=ec998974a41a7df78c307b6aafbc999b,"One of the safety features that can alert drivers to the presence of other vehicles and reduce the risk of collisions is vehicle detection. In this study, the objective is to setup a driving support system for detecting vehicles, motorcycles, and traffic signals on the roads near to Universiti Malaysia Pahang using object detection techniques. The video was taken through a direct camera to capture video footage of traffic objects on the roads in the district, which was then analysed using the YOLO-V8 deep learning algorithm. The system was trained on a primary dataset of 1,068 images, with 70% of the dataset used for training, 20% for testing and 10% for validation. After conducting a performance validation, the system achieved a mean average precision (mAP) of 88.2% on train dataset and was able to detect different types of vehicles such as cars, motorcycles, and traffic lights. The results of this study could be beneficial for road safety authorities and researchers interested in developing intelligent transportation systems. © 2023, Penerbit Akademia Baru. All rights reserved.",Deep Learning; driving assisting; image processing; Object detection; Yolo-V8
Scopus,"Ma, N.; Han, T.; Xia, Q.; Han, Y.",Road Information Recognition and Decision Making Based on YOLO Algorithm,,2024,,,,10.1109/AIHCIR65563.2024.00041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004930140&doi=10.1109%2fAIHCIR65563.2024.00041&partnerID=40&md5=431420ef36258ba3b8c820a580155689,"Autonomous driving technology is a revolutionary technology in the field of modern transportation, which has developed rapidly in recent years. However, it is greatly affected by the environment, such as the change of lighting (the difference in detection performance under different lighting conditions) and weather conditions (the impact of bad weather such as rain, fog, and snow). Hence, we present an exclusively visual, real-time perception system that can simultaneously identify traffic signals, pedestrians, signage, road markings, vehicles and delineate areas suitable for driving. By integrating YOLOv5s with YOLOP, we have trained multiple datasets to develop a sophisticated detection model. Leveraging this refined model along with a monocular distance measurement algorithm, we have successfully implemented decision control. The training results demonstrate that the system has effectively mitigated the impact of environmental factors, exhibiting strong robustness. Furthermore, it has met the stringent requirements for speed and accuracy in autonomous driving, yielding commendable outcomes. © 2024 IEEE.",monocular distance measurement; vehicle behavior decision; yolo; Traffic signs; Vehicle performance; Autonomous driving; Behavior decision; Decisions makings; Detection performance; Information recognition; Monocular distance measurement; Revolutionary technology; Vehicle behavior; Vehicle behavior decision; Yolo; Autonomous vehicles
Scopus,"Heo, J.; Novlan, T.; Akoum, S.; Gavrilovska, A.",GT-Craft: A Framework for Fast Prototyping Geospatial-Based Digital Twins in Unity 3D,,2024,,,,10.1109/SEC62691.2024.00043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216721694&doi=10.1109%2fSEC62691.2024.00043&partnerID=40&md5=cec19794eb4d92f8da2cc819ddaee055,"A digital twin presents promising opportunities and potential benefits for various industrial use cases by enabling simulation and prediction on the virtual representation of the real-world environment. However, the implementation and maintenance costs for the digital twin are prohibitively high, restricting its widespread adoption. To address this issue, we present a framework, GT-Craft, which enables fast prototyping the geospatial-based digital twin at scale. GT-Craft automates the generation of the digital twin by using the streamed geospatial data and the semantic information extracted from deep neural network (DNN) models. As GT-Craft generates digital twins on the Unity game engine, the Unity-based simulators and game applications can seamlessly use the digital twins generated by GT-Craft. The presented framework is compatible with non-Unity-based applications and existing 3D software and simulation tools, e.g., Blender, Apple Reality Composer, and NVIDIA Omniverse, as it supports exporting the generated digital twin in the universal scene description (USD) format, which is an emerging industrial open standard for exchanging and editing 3D contents.  © 2024 IEEE.",cyber physical system; digital twin; edge computing; game engine; unity; universal scene description; Deep neural networks; Digital elevation model; Edge computing; Three dimensional computer graphics; Virtual reality; Cybe-physical systems; Cyber-physical systems; Edge computing; Fast prototyping; Game Engine; Geo-spatial; Potential benefits; Scene description; Unity; Universal scene description; Application programs
Scopus,"Saltori, C.; Galasso, F.; Fiameni, G.; Sebe, N.; Poiesi, F.; Ricci, E.",Compositional Semantic Mix for Domain Adaptation in Point Cloud Segmentation,,2023,,,,10.1109/TPAMI.2023.3310261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169678738&doi=10.1109%2fTPAMI.2023.3310261&partnerID=40&md5=22dab8de0d5024d171ec2f85b1a0623f,"Deep-learning models for 3D point cloud semantic segmentation exhibit limited generalization capabilities when trained and tested on data captured with different sensors or in varying environments due to domain shift. Domain adaptation methods can be employed to mitigate this domain shift, for instance, by simulating sensor noise, developing domain-agnostic generators, or training point cloud completion networks. Often, these methods are tailored for range view maps or necessitate multi-modal input. In contrast, domain adaptation in the image domain can be executed through sample mixing, which emphasizes input data manipulation rather than employing distinct adaptation modules. In this study, we introduce compositional semantic mixing for point cloud domain adaptation, representing the first unsupervised domain adaptation technique for point cloud segmentation based on semantic and geometric sample mixing. We present a two-branch symmetric network architecture capable of concurrently processing point clouds from a source domain (e.g. synthetic) and point clouds from a target domain (e.g. real-world). Each branch operates within one domain by integrating selected data fragments from the other domain and utilizing semantic information derived from source labels and target (pseudo) labels. Additionally, our method can leverage a limited number of human point-level annotations (semi-supervised) to further enhance performance. We assess our approach in both synthetic-to-real and real-to-real scenarios using LiDAR datasets and demonstrate that it significantly outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.  © 1979-2012 IEEE.",Domain adaptation; point cloud; semantic segmentation; semi-supervised learning; unsupervised learning; Computer architecture; Deep learning; Mixing; Network architecture; Semantic Segmentation; Supervised learning; Compositional semantics; Domain adaptation; Point cloud compression; Point-clouds; Semantic segmentation; Semi-supervised learning; Task analysis; Three-dimensional display; article; human; Semantics
Scopus,"Wang, J.; Li, L.; Xu, P.",Visual Sensing and Depth Perception for Welding Robots and Their Industrial Applications,,2023,,,,10.3390/s23249700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180252930&doi=10.3390%2fs23249700&partnerID=40&md5=90d758248938af1214b013662d001018,"With the rapid development of vision sensing, artificial intelligence, and robotics technology, one of the challenges we face is installing more advanced vision sensors on welding robots to achieve intelligent welding manufacturing and obtain high-quality welding components. Depth perception is one of the bottlenecks in the development of welding sensors. This review provides an assessment of active and passive sensing methods for depth perception and classifies and elaborates on the depth perception mechanisms based on monocular vision, binocular vision, and multi-view vision. It explores the principles and means of using deep learning for depth perception in robotic welding processes. Further, the application of welding robot visual perception in different industrial scenarios is summarized. Finally, the problems and countermeasures of welding robot visual perception technology are analyzed, and developments for the future are proposed. This review has analyzed a total of 2662 articles and cited 152 as references. The potential future research topics are suggested to include deep learning for object detection and recognition, transfer deep learning for welding robot adaptation, developing multi-modal sensor fusion, integrating models and hardware, and performing a comprehensive requirement analysis and system evaluation in collaboration with welding experts to design a multi-modal sensor fusion architecture. © 2023 by the authors.",3D reconstruction; deep learning; depth perception; industrial applications; welding robot; welding sensor; Binocular vision; Image reconstruction; Industrial robots; Intelligent robots; Machine design; Object detection; Robot vision; Stereo image processing; Welding; 3D reconstruction; Deep learning; Multimodal sensor; Sensor fusion; Vision sensing; Visual depth; Visual perception; Visual sensing; Welding robots; Welding sensor; adaptation; artificial intelligence; binocular vision; computer; controlled study; deep learning; depth perception; human; monocular vision; review; robotics; sensor; welding; Deep learning
Scopus,"Li, J.; Sun, W.","Analysis of aerial images for identification of houses using big data, UAV photography and neural network",,2023,,,,10.1007/s00500-023-08967-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164938690&doi=10.1007%2fs00500-023-08967-3&partnerID=40&md5=ddc8e6a019bfc2e8a5446160f9af9d03,"Computer vision has undergone significant transformation owing to deep learning in the last two decades. Deep convolutional networks have been successfully applied for various applications to learn different tasks related to vision, such as image classification, image segmentation, and object detection. Deep learning models can generate fine-tuned results by transferring knowledge to large generic datasets. This study aims to conduct an in-depth analysis of a big data tracking algorithm for aerial images of unmanned aerial vehicles (UAVs) to detect houses using neural networks to address the low accuracy and efficiency of manual detection in remote areas by mitigating the associated security risks. In the context of big data, a UAV-based preprocessing method is discussed for images using guided filtering. In order to reduce the impact of radiation distortion on the color and brightness of UAV-based aerial images of houses, a histogram matching method was applied. The guided filtering method is used to solve the problem of imaging details of houses that are not apparent after smoothing and denoising the aerial images. A house detection algorithm based on a deep neural network is then applied to the UAV images to detect the images of houses, and the time consumption of the deep learning operation is examined within the context of big data. Combining deep separation convolution and calculation optimization with YOLOv2 improves the house's image detection in real-time while preserving an accurate performance of UAV-based aerial images to detect houses by combining the YOLOv2 detection framework. The results of the experiments indicate that the proposed method can improve the efficiency and accuracy of house detection using aerial images and has certain practical applications. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Big data tracking; Conventional neural network; Deep neural network detection algorithm; Guided filtering method; House inspection; UAV aerial photography; Aerial photography; Aircraft detection; Antennas; Convolution; Deep neural networks; Houses; Image enhancement; Image segmentation; Large dataset; Learning systems; Object detection; Risk assessment; Unmanned aerial vehicles (UAV); Aerial vehicle; Big data tracking; Conventional neural network; Data-tracking; Deep neural network detection algorithm; Detection algorithm; Filtering method; Guided filtering; Guided filtering method; House inspection; Network detections; Neural-networks; Unmanned aerial vehicle aerial photography; Efficiency
Scopus,"Zhu, J.; Yang, Y.; Cheng, Y.",A Millimeter-Wave Radar-Aided Vision Detection Method for Water Surface Small Object Detection,,2023,,,,10.3390/jmse11091794,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172779700&doi=10.3390%2fjmse11091794&partnerID=40&md5=5376a7f079ba21d35da939450fcdea5f,"Unmanned surface vehicles (USVs) have wide applications in marine inspection and monitoring, terrain mapping, and water surface cleaning. Accurate and robust environment perception ability is essential for achieving autonomy in USVs. Small object detection on water surfaces is an important environment perception task, typically achieved by visual detection using cameras. However, existing vision-based small object detection methods suffer from performance degradation in complex water surface environments. Therefore, in this paper, we propose a millimeter-wave (mmWave) radar-aided vision detection method that enables automatic data association and fusion between mmWave radar point clouds and images. Through testing on real-world data, the proposed method demonstrates significant performance improvement over vision-based object detection methods without introducing more computational costs, making it suitable for real-time application on USVs. Furthermore, the image–radar data association model in the proposed method can serve as a plug-and-play module for other object detection methods. © 2023 by the authors.",object detection; unmanned surface vehicle; visual–radar fusion
Scopus,"Zhang, T.; Pan, Y.",Real-time detection of a camouflaged object in unstructured scenarios based on hierarchical aggregated attention lightweight network,,2023,,,,10.1016/j.aei.2023.102082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243140&doi=10.1016%2fj.aei.2023.102082&partnerID=40&md5=9a1f8a0fb407d44dbcf2fd1828ef2141,"Accurate and real-time grasping of household items is a challenge for home service robots in complex indoor environments. In this paper, using transparent cups as the recognition object, an accurate, reliable, and real-time visual algorithm is proposed for the elderly who needs nursing care to solve the recognition problems of overlapping, blurred, and small cups in complex unstructured indoor scenes. In order to meet the lightweight deployment of home service robots while considering both high-scale and low-scale semantic features, this paper designs a multi-scale fusion lightweight backbone network based on the Split Shuffle Block (SSB) and Group Shuffle Block (GSB) feature encoding units. Among them, depthwise separable convolution (DwConv) uses to reduce the amount of coding unit parameters, and feature shuffle uses to promote information exchange and the ability of information expression. In order to accurately identify the transparent cup camouflaged in complex backgrounds, this paper proposed a lightweight feature enhancement module that combines multi-scale hierarchical aggregation attention and multi-branch parallel convolution structure. The proposed module used an adaptive weighting strategy and a channel normalization weighting strategy to highlight the active regions of boundary features in each branch feature map, enhance the exchange of boundary information, and reduce the loss of detailed information. The experimental results on the IndoorCup and MobileCup datasets show that the detection accuracy of the proposed method is 93.6% and 92.6%, respectively, and the model calculation amount is only 0.91 M, which can lightweight deploy on indoor mobile robots for real-time detection. From the results of qualitative comparisons, the proposed method has strong robustness. It can effectively suppress false and missed detection caused by background interference. Likewise, it also effectively identifies camouflage objects and small objects in complex backgrounds. © 2023 Elsevier Ltd",Camouflaged object; Hierarchical aggregated attention; Indoor mobile robot; Lightweight backbone; Unstructured scenarios; Complex networks; Machine design; Mobile robots; Object detection; Semantics; Signal detection; Camouflaged object; Complex background; Hierarchical aggregated attention; Home service robot; Indoor mobile robots; Lightweight backbone; Real- time; Real-time detection; Unstructured scenario; Weighting strategies; Convolution
Scopus,"Jia, F.; Afaq, M.; Ripka, B.; Huda, Q.; Ahmad, R.",Vision- and Lidar-Based Autonomous Docking and Recharging of a Mobile Robot for Machine Tending in Autonomous Manufacturing Environments,,2023,,,,10.3390/app131910675,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174214905&doi=10.3390%2fapp131910675&partnerID=40&md5=66eb5ca7e384e1f5b3d27d06dbc54f33,"Autonomous docking and recharging are among the critical tasks for autonomous mobile robots that work continuously in manufacturing environments. This requires robots to demonstrate the following abilities: (i) detecting the charging station, typically in an unstructured environment and (ii) autonomously docking to the charging station. However, the existing research, such as that on infrared range (IR) sensor-based, vision-based, and laser-based methods, identifies many difficulties and challenges, including lighting conditions, severe weather, and the need for time-consuming computation. With the development of deep learning techniques, real-time object detection methods have been widely applied in the manufacturing field for the recognition and localization of target objects. Nevertheless, those methods require a large amount of proper and high-quality data to achieve a good performance. In this study, a Hikvision camera was used to collect data from a charging station in a manufacturing environment; then, a dataset for the wireless charger was built. In addition, the authors of this paper propose an autonomous docking and recharging method based on the deep learning model and the Lidar sensor for a mobile robot operating in a manufacturing environment. In the proposed method, a YOLOv7-based object detection method was developed, trained, and evaluated to enable the robot to quickly and accurately recognize the charging station. Mobile robots can achieve autonomous docking to the charging station using the proposed Lidar-based approach. Compared to other methods, the proposed method has the potential to improve recognition accuracy and efficiency and reduce the computation costs for the mobile robot system in various manufacturing environments. The developed method was tested in real-world scenarios and achieved an average accuracy of 95% in recognizing the target charging station. This vision-based charger detection method, if fused with the proposed Lidar-based docking method, can improve the overall accuracy of the docking alignment process. © 2023 by the authors.",3D Lidar; autonomous docking; autonomous recharging; computer vision; manufacturing environments; mobile robots
Scopus,"Guo, Y.; Tian, X.; Xiao, Y.",DBCR-YOLO: Improved YOLOv5 based on double-sampling and broad-feature coordinate-attention residual module for water surface object detection,,2023,,,,10.1117/1.JEI.32.4.043013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173242522&doi=10.1117%2f1.JEI.32.4.043013&partnerID=40&md5=f53d47cb832234161262d12167c33b61,"Unmanned missions have become more and more popular in recent years. The related technologies of unmanned ground vehicles and unmanned aerial vehicles are growing rapidly, but research on unmanned surface vehicles (USVs) is rare. Water surface object detection algorithms play a crucial role in the field of USVs. However, achieving an object detection algorithm that balances speed and accuracy in the presence of interference is a difficult challenge. We proposed a network, DBCR-YOLO, that improved the detection accuracy while meeting real-time requirements. Based on YOLOv5, we added an additional detection head for detecting tiny objects. Then, we replaced the downsampling in YOLOv5's backbone network with the proposed double sampling mechanism to solve the problem that paying attention to the key features of objects cannot be done in the downsampling process of YOLOv5. Finally, we substituted the proposed BCR neck for YOLOv5's neck, thus improving the fusion of features between different scales based on fewer parameters and fewer calculations. We tested our network on the water surface object detection dataset. Compared with YOLOv5, DBCR-YOLO improved the detection accuracy by 3.4%. At the same time, DBCR-YOLO achieved the highest accuracy in comparison with other networks.  © 2023 SPIE and IS&T.",unmanned surface vehicles; water surface object detection; YOLOv5; Antennas; Ground vehicles; Object recognition; Signal detection; Signal sampling; Aerial vehicle; Detection accuracy; Double-sampling; Down sampling; Object detection algorithms; Objects detection; Real time requirement; Water surface; Water surface object detection; YOLOv5; Object detection
Scopus,"Kim, M.; Kim, H.; Sung, J.; Park, C.; Paik, J.",High-resolution processing and sigmoid fusion modules for efficient detection of small objects in an embedded system,,2023,,,,10.1038/s41598-022-27189-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145645329&doi=10.1038%2fs41598-022-27189-5&partnerID=40&md5=b86d5a24c70196ffb8d4da471dd922f5,"Recent advances in deep learning realized accurate, robust detection of various types of objects including pedestrians on the road, defect regions in the manufacturing process, human organs in medical images, and dangerous materials passing through the airport checkpoint. Specifically, small object detection implemented as an embedded system is gaining increasing attention for autonomous vehicles, drone reconnaissance, and microscopic imagery. In this paper, we present a light-weight small object detection model using two plug-in modules: (1) high-resolution processing module (HRPM) and (2) sigmoid fusion module (SFM). The HRPM efficiently learns multi-scale features of small objects using a significantly reduced computational cost, and the SFM alleviates mis-classification errors due to spatial noise by adjusting weights on the lost small object information. Combination of HRPM and SFM significantly improved the detection accuracy with a low amount of computation. Compared with the original YOLOX-s model, the proposed model takes a two-times higher-resolution input image for higher mean average precision (mAP) using 57% model parameters and 71% computation in Gflops. The proposed model was tested using real drone reconnaissance images, and provided significant improvement in detecting small vehicles. © 2023, The Author(s).","Airports; Autonomous Vehicles; Colon, Sigmoid; Commerce; Excipients; Humans; excipient; article; classification error; human; human experiment; noise; sigmoid; unmanned aerial vehicle; airport; commercial phenomena; sigmoid"
Scopus,"Chu, C.; Hui, S.; Liu, X.; Jia, X.; Zhong, C.",Optimization of YOLOv8 Vehicle Detection Algorithm in Autonomous Driving Technology,,2024,,,,10.1109/ICAICA63239.2024.10823036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216868403&doi=10.1109%2fICAICA63239.2024.10823036&partnerID=40&md5=c20e4c05c0997537a90b303935fb858c,"In autonomous driving, accurate vehicle target perception matters a lot. This paper devises an enhanced vehicle target detection algorithm using multi-sensor fusion, seeking to address the high computational expense, weak generalization, and poor small target detection of traditional algorithms. Firstly, to bolster YOLOv8's global feature extraction, its backbone network was swapped for Swin Transformer. Then, a GSConv module paired with the channel attention SE replaced the regular convolutions in YOLOv8, trimming computational complexity and enhancing generalization. Finally, high-resolution feature layers were incorporated into the adjusted backbone network. These furnished higher spatial resolution and snagged more details. Multi-scale fusion of the added layers then upped small target detection. The improved YOLOv8 model's Precision, Recall, Map@0.5, and mAP@0.5:0.95 climbed by 3%, 5%, 3.5%, and 2.5% respectively, topping out at 88.3%, 86%, 89.9%, and 64.9%. Against traditional algorithms' drawbacks like high cost and weak detection, these gains were notable, validating the improved method. © 2024 IEEE.",Autonomous Driving; Grouped Shuffling Convolution; Squeeze-and-Excitation; Swin Transformer; YOLOv8; Autonomous driving; Back-bone network; Generalisation; Grouped shuffling convolution; Optimisations; Small target detection; Squeeze-and-excitation; Swin transformer; Vehicle targets; YOLOv8
Scopus,"Olayode, I.O.; Du, B.; Severino, A.; Campisi, T.; Alex, F.J.","Systematic literature review on the applications, impacts, and public perceptions of autonomous vehicles in road transportation system",,2023,,,,10.1016/j.jtte.2023.07.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180446260&doi=10.1016%2fj.jtte.2023.07.006&partnerID=40&md5=0c7e945b42d29d59339a7a01fdaa0650,"As the advancement of driverless technology, together with information and communication technology moved at a fast pace, autonomous vehicles have attracted great attention from both industries and academic sectors during the past decades. It is evident that this emerging technology has great potential to improve the pedestrian safety on roads, mitigate traffic congestion, increase fuel efficiency, and reduce greenhouse gas emissions. However, there is limited systematic research into the applications and public perceptions of autonomous vehicles in road transportation. The purpose of this systematic literature review is to synthesise and analyse existing research on the applications, implications, and public perceptions of autonomous vehicles in road transportation system. It is found that autonomous vehicles are the future of road transportation and that the negative perception of humans is rapidly changing towards autonomous vehicles. Moreover, to fully deploy autonomous vehicles in a road transportation system, the existing road transportation infrastructure needs significant improvement. This systematic literature review contributes to the comprehensive knowledge of autonomous vehicles and will assist transportation researchers and urban planners to understand the fundamental and conceptual framework of autonomous vehicle technologies in road transportation systems. © 2023 Periodical Offices of Changâ€™an University",Autonomous mobility; Autonomous vehicles; Public perception; Road transportation; Traffic safety; Gas emissions; Greenhouse gases; Motor transportation; Pedestrian safety; Roads and streets; Traffic congestion; Urban transportation; Vehicle to vehicle communications; Autonomous mobilities; Autonomous Vehicles; Driverless; Industry sectors; Information and Communication Technologies; Public perception; Road transportation; Systematic literature review; Traffic safety; Transportation system; Autonomous vehicles
Scopus,"Jiang, W.; Han, D.; Han, B.; Wu, Z.",YOLOv8-FDF: A Small Target Detection Algorithm in Complex Scenes,,2024,,,,10.1109/ACCESS.2024.3448619,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201764379&doi=10.1109%2fACCESS.2024.3448619&partnerID=40&md5=af19ff2c22bdb8bbae26548b8bdca9e6,"Synthetic Aperture Radar (SAR) finds widespread applications in environmental monitoring, disaster management, ship surveillance, and military intelligence. However, existing target detection methods are ineffective in SAR scenes due to the intricate background environments, small target displays, and irregular appearances. To address these challenges, this thesis introduces a target detection model named YOLOv8-FDF, tailored for SAR scenes based on the YOLOv8 architecture. The model effectively incorporates the FADC module to distinguish targets from complex backgrounds and integrates a deformable feature adaptive mechanism to focus on irregular targets. Additionally, this thesis devised a specialized detection head designed to identify small targets in SAR-wide scenes, thereby improving the effectiveness of detecting such targets. The proposed YOLOv8-FDF model is evaluated on the HRSID dataset. Experiment results show a 3.6% improvement in Map75 on both the training and test sets. Furthermore, under the COCO standard, the model achieves improvements of 4.1%, 2.9%, and 5.5% on AP, AP50, and AP75, along with 6.8%, 1.2%, and 1.2% improvements on small, medium, and large-sized ship detection. An accuracy enhancement of 6.8%, 1.0%, and 14.9% is achieved. These experimental findings validate the efficacy of the proposed YOLOv8-FDF model in SAR scenarios.  © 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",complex environment; deep learning; SAR target detection; Small targets; YOLOv8; Deep learning; Marine applications; Marine radar; Military radar; Radar target recognition; Ships; Surveillance radar; Tracking radar; Accuracy; Adaptation models; Complex environments; Deep learning; Features extraction; Marine vehicles; Radar target detection; Small targets; Synthetic aperture radar target detection; YOLO; YOLOv8; Synthetic aperture radar
Scopus,"Wang, H.; Wang, H.; Zhang, X.; Ruan, R.; Wang, Y.; Yin, Y.",Multiagent Detection System Based on Spatial Adaptive Feature Aggregation,,2024,,,,10.1109/JSYST.2024.3423752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211468281&doi=10.1109%2fJSYST.2024.3423752&partnerID=40&md5=4e2be8c0fbedc12ec74ab88387143c8e,"Detection systems based on computer vision play important roles in Large-Scale Multiagent Systems. In particular, it can automatically locate and identify key objects and enhance intelligent collaboration and coordination among multiple agents. However, classification and localization in object detection may produce inconsistent prediction results due to different learning focus. Therefore, we propose a Spatial Decoupling and Boundary Feature Aggregation Network (SDBA-Net) to achieve spatial decoupling and task alignment. SDBA-Net includes a spatially sensitive region-aware module (SSRM) and a boundary feature aggregation module (BFAM). SSRM predicts sensitive regions for each task while minimizing computational cost. BFAM extracts valuable boundary features within sensitive regions and aligns them with corresponding anchors. These two modules are combined to spatially decouple and align the features of two tasks. In addition, a significance dependency complementary module (SDCM) is introduced. It enables SSRM to quickly adjust the sensitive region of the classification task to the significant feature region. Experiments are conducted on a large-scale complex real-world dataset MS COCO (Lin et al., 2014). The results show that SDBA-Net achieves better results than the baselines. Using the ResNet-50 backbone, our method improves the average precision (AP) of the single-stage detector VFNet by 1.0 point (from 41.3 to 42.3). In particular, when using the Res2Net-101-DCN backbone, SDBA-Net achieves an AP of 51.8 on the MS COCO test-dev.  © 2007-2012 IEEE.",Complex scenarios; feature aggregation; large scale; multiagent; object detection; spatial decoupling; Classification (of information); Computer vision; Object detection; Aggregation network; Complex scenario; Decouplings; Detection system; Feature aggregation; Large-scales; Multi agent; Objects detection; Sensitive regions; Spatial decoupling; Object recognition
Scopus,"Zhang, Z.; Chen, S.; Wang, Z.; Yang, J.",PlaneSeg: Building a Plug-In for Boosting Planar Region Segmentation,,2024,,,,10.1109/TNNLS.2023.3262544,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153406367&doi=10.1109%2fTNNLS.2023.3262544&partnerID=40&md5=46b521226cd471a6bd6c03f56a9963d8,"Existing methods in planar region segmentation suffer the problems of vague boundaries and failure to detect small-sized regions. To address these, this study presents an end-to-end framework, named PlaneSeg, which can be easily integrated into various plane segmentation models. Specifically, PlaneSeg contains three modules, namely, the edge feature extraction module, the multiscale module, and the resolution-adaptation module. First, the edge feature extraction module produces edge-aware feature maps for finer segmentation boundaries. The learned edge information acts as a constraint to mitigate inaccurate boundaries. Second, the multiscale module combines feature maps of different layers to harvest spatial and semantic information from planar objects. The multiformity of object information can help recognize small-sized objects to produce more accurate segmentation results. Third, the resolution-adaptation module fuses the feature maps produced by the two aforementioned modules. For this module, a pairwise feature fusion is adopted to resample the dropped pixels and extract more detailed features. Extensive experiments demonstrate that PlaneSeg outperforms other state-of-the-art approaches on three downstream tasks, including plane segmentation, 3-D plane reconstruction, and depth prediction. Code is available at https://github.com/nku-zhichengzhang/PlaneSeg. © 2012 IEEE.",Deep learning; depth prediction; planar region segmentation; plane reconstruction; plug-in; Deep learning; Edge detection; Extraction; Image reconstruction; Job analysis; Semantic Segmentation; Semantics; Deep learning; Depth prediction; Features extraction; Image edge detection; Images reconstruction; Images segmentations; Planar region; Planar region segmentation; Plane reconstruction; Plug-ins; Region segmentation; Task analysis; article; feature extraction; prediction; Feature extraction
Scopus,"Gai, K.; Yu, J.; Zhu, L.",Introduction to Cybersecurity in the Internet of Things,,2024,,,,10.1201/9781032694818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191244437&doi=10.1201%2f9781032694818&partnerID=40&md5=9e18fd814f7e4c8fe84c23a2044c8d67,"This book addresses the security challenges facing the rapidly growing Internet of Things (IoT) industry, including the potential threats and risks arising from its complex architecture. The authors discuss the overall IoT architecture, covering networking, computing, and security threats and risks to hardware such as sensors, actuators, and portable devices, as well as infrastructure layers. They cover a range of technical concepts such as cryptography, distributed storage, and data transmission, and offer practical advice on implementing security solutions such as authentication and access control. By exploring the future of cybersecurity in the IoT industry, with insights into the importance of big data and the threats posed by data mining techniques, this book is an essential resource for anyone interested in, or working in, the rapidly evolving field of IoT security. © 2024 Keke Gai, Jing Yu and Liehuang Zhu.",
Scopus,"Xing, Z.; Zhao, S.; Guo, W.; Meng, F.; Guo, X.; Wang, S.; He, H.",Coal resources under carbon peak: Segmentation of massive laser point clouds for coal mining in underground dusty environments using integrated graph deep learning model,,2023,,,,10.1016/j.energy.2023.128771,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168139719&doi=10.1016%2fj.energy.2023.128771&partnerID=40&md5=1d1a800119fa22eb254a9ec8180b30e4,"With the background of China's carbon peak, the low-carbon and sustainable development of the coal industry is vital to China's national energy security. Because the underground visibility is low and the dust is continuously spreading, coal mine point cloud segmentation can provide a key basis for underground environment perception, and then provides a premise for the construction of green coal mines. In this study, we propose to segment the coal mining face (CMF) point cloud under the harsh environment based on the advanced dynamic graph convolution neural network (DGCNN) and to obtain the information of the coal cutting roof line. The results show that the multi-level and series pooling DGCNN (ML&SP-DGCNN) which was constructed on the basis of a large number of previous studies shows the best performance. In this study, the coal cutting roof line obtained by segmenting the CMF point cloud provides a key basis for dynamically correcting the underground geological model and straightening the CMF. More importantly, the established CMF point cloud segmentation model lays a foundation for perceiving the underground environment, which is of great help to realize the sustainable green production of coal resources. © 2023 Elsevier Ltd",Coal; Deep learning; Environmental perception; Low-carbon development; Unmanned mining; China; Carbon; Coal deposits; Coal dust; Coal industry; Coal mines; Deep learning; Energy security; Mine roof control; Roofs; Carbon peaks; Coal mining face; Coal resources; Deep learning; Environmental perceptions; Low-carbon development; Point cloud segmentation; Point-clouds; Underground environment; Unmanned mining; carbon emission; coal industry; coal mine; coal mining; dust; sustainable development; Coal
Scopus,"Hanzla, M.; Yusuf, M.O.; Sadiq, T.; Mudawi, N.A.; Rahman, H.; Alazeb, A.; Alarfaj, A.A.; Algarni, A.",UAV Detection Using Template Matching and Centroid Tracking,,2024,,,,10.1109/ACCESS.2024.3450580,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202738342&doi=10.1109%2fACCESS.2024.3450580&partnerID=40&md5=92eb74642220c59b7a9aa93efb654382,"In computer vision and image processing, vehicle detection and tracking in complicated aerial images have become important subjects. The need for automated systems that can precisely detect and track vehicles in aerial image data is growing due to the abundance of data coming from numerous sources, including drones and satellites. This study introduces a new method for lane extraction that relies on centroid tracking and template matching, followed by co-registration and geo-referencing. Our approach offers robust vehicle detection and tracking over a range of sizes and positions in complicated backgrounds, while also efficiently segmenting the region of interest. Our suggested method, which makes use of machine learning and feature extraction techniques, shows excellent precision and effectiveness when it comes to detecting and tracking vehicles in complex aerial images. This finding has important implications for traffic management and urban planning, going beyond computer vision and image processing. Our technology has the potential to transform traffic management procedures by making it simpler to detect traffic bottlenecks and monitor traffic flow. Furthermore, our method can help identify damaged vehicles in disaster response scenarios, which will help prioritize rescue and recovery activities. All things considered, our suggested approach is a significant addition to the domains of computer vision and image processing, with a broad range of uses in traffic control, urban planning, and disaster management. © 2013 IEEE.",Georeferencing; multiple object detection; segmentation; smart traffic monitoring; vehicle detection; Advanced traffic management systems; Aerial photography; Aircraft detection; Emergency traffic control; Highway administration; Highway traffic control; Image registration; Image segmentation; Intelligent systems; Motor transportation; Unmanned aerial vehicles (UAV); Accuracy; Features extraction; Georeferencing; Images segmentations; Multiple-object detections; Real - Time system; Road; Segmentation; Smart traffic; Smart traffic monitoring; Traffic monitoring; Vehicles detection; Template matching
Scopus,"Zhang, L.; Wen, F.; Zhang, Q.; Gui, G.; Sari, H.; Adachi, F.",Constrained Multiobjective Decomposition Evolutionary Algorithm for UAV-Assisted Mobile Edge Computing Networks,,2024,,,,10.1109/JIOT.2024.3417009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002093892&doi=10.1109%2fJIOT.2024.3417009&partnerID=40&md5=38b444e1f0c55ac0f0e49ef26b416b2c,"The increasing significance of unmanned aerial vehicles (UAVs) in mobile edge computing (MEC) has captured considerable attention. Nevertheless, the effectiveness of UAVs-assisted MEC networks is hampered by challenges, such as limited communication capacity and onboard power. To tackle these issues, this study develops a constrained multiobjective optimization model designed to enhance the performance of UAVs-assisted MEC networks, focusing on system capacity, energy consumption, and task latency. As a result, this problem manifests as a complex constrained multiobjective optimization problem. The study then proposes a constrained multiobjective decomposition evolutionary algorithm (CMODEA) with low-computational complexity. This algorithm employs an adaptive individual comparison strategy, balancing diversity and convergence, and integrates an optimally guided differential evolution strategy for efficiently approximating optimal solutions. Additionally, it incorporates an adaptive constraint handling method, effectively managing existing constraints. The CMODEA aims to simultaneously optimize system capacity, energy consumption, and task latency while meeting the computational resource requirements of UAVs and ensuring acceptable user task latency levels. Simulation results demonstrate the algorithm's effectiveness in significantly enhancing capacity, reducing energy consumption and latency, without greatly increasing algorithm complexity. © 2014 IEEE.",6G; constrained multiobjective optimization; evolutionary computation; mobile edge computing (MEC); unmanned aerial vehicles (UAVs); Antennas; Complex networks; Computational complexity; Constrained optimization; Evolutionary algorithms; Internet of things; Job analysis; Mobile edge computing; Multiobjective optimization; 6g; Aerial vehicle; Constrained multi-objective optimizations; Convergence; Delay; Energy-consumption; Multi objective; Optimisations; Task analysis; Unmanned aerial vehicle; Energy utilization
Scopus,"Atik, S.T.; Brocanelli, M.; Grosu, D.",Are Turn-by-Turn Navigation Systems of Regular Vehicles Ready for Edge-Assisted Autonomous Vehicles?,,2023,,,,10.1109/TITS.2023.3275367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161055799&doi=10.1109%2fTITS.2023.3275367&partnerID=40&md5=32e97e881292a9a0f4b6baa71cc0786e,"Private and public transportation will be dominated by Autonomous Vehicles (AV), which are safer than regular vehicles. However, ensuring good performance for the autonomous features requires fast processing of heavy tasks. Providing each AV with powerful computing resources may result in increased AV cost and decreased driving range. An alternative solution is to install low-power computing hardware on each AV and offload the heavy tasks to powerful nearby edge servers. In this case, the AV's reaction time depends on how quickly the navigation tasks are completed in the edge server. To reduce task completion latency, the edge servers must be equipped with enough network and computing resources to handle the vehicle demands, which show large spatio-temporal variations. Thus, deploying the same resources in different locations may lead to unnecessary resource over-provisioning. In this paper, we leverage simulations using real traffic data to discuss the implications of deploying heterogeneous resources in different city areas to sustain peak versus average demand of edge-assisted AVs. Our analysis indicates that a reduction in network bandwidth and computing cores of up to 60% and 50%, respectively, is achieved by deploying edge resources for the average demand rather than peak demand. We also investigate how the peak-hour demand affects the safe travel time of AVs and find that it can be reduced by approximately 20% if they would be rerouted to areas with a lower edge-resource load. Thus, future research must consider that traditional turn-by-turn navigation systems may not provide the fastest routes for edge-assisted AVs.  © 2000-2011 IEEE.",Autonomous vehicles; edge computing; navigation systems; Autonomous vehicles; Cost benefit analysis; Edge computing; Job analysis; Travel time; Autonomous Vehicles; Computing resource; Edge computing; Edge resources; Edge server; Private transportation; Public transportation; Task analysis; Urban areas; Navigation systems
Scopus,"Feng, R.; Zhao, F.; Chen, S.; Zhang, S.; Zhu, S.",A handwritten ancient text detector based on improved feature pyramid network,,2023,,,,10.1016/j.patrec.2023.06.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163837856&doi=10.1016%2fj.patrec.2023.06.013&partnerID=40&md5=ef9a0a6bfe61d350ee705b5d28d92889,"Text detection is the primary task for digitization of ancient books. Different from the common scene text detection tasks (ICDAR, TotalText, etc.), the texts in handwritten ancient documents are more densely distributed and generally small objects; at the same time, the layout structure is also more complex, with problems such as mixed arrangement of pictures and texts and high background noise, all of which pose challenges for detection. According to the characteristics of ancient book images, this paper proposes a new fusion structure based on Feature Pyramid Networks, and takes FCOS as the baseline model to form a new detector (named RFCOS). We enhance the detection capability for dense and small text instances by adding bottom-up fusion paths, cross-layer connections and weighted fusion. Meanwhile, the loss of high-level feature maps during fusion is reduced by new upsampling method and lateral connections. We verified the effectiveness of our RFCOS on the HWAD (Handwritten Ancient Books Dataset), a dataset containing samples in four languages - Yi, Chinese, Tibetan and Tangut, and verify the generalization of RFCOS on another public dataset MTHv2. The results show that RFCOS outperformed most of the existing text detectors in terms of precision, recall and F-measure. © 2023 Elsevier B.V.",FCOS; Feature pyramid networks; Handwritten text detection; Small object detection; Feature extraction; Digitisation; FCOS; Feature pyramid; Feature pyramid network; Handwritten text detection; Handwritten texts; Primary task; Pyramid network; Small object detection; Text detection; Object detection
Scopus,"Azurmendi, I.; Zulueta, E.; Lopez-Guede, J.M.; González, M.",Simultaneous Object Detection and Distance Estimation for Indoor Autonomous Vehicles,,2023,,,,10.3390/electronics12234719,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179330617&doi=10.3390%2felectronics12234719&partnerID=40&md5=0c47d95bafc36a4f6d3dec170c5a4ad3,"Object detection is an essential and impactful technology in various fields due to its ability to automatically locate and identify objects in images or videos. In addition, object-distance estimation is a fundamental problem in 3D vision and scene perception. In this paper, we propose a simultaneous object-detection and distance-estimation algorithm based on YOLOv5 for obstacle detection in indoor autonomous vehicles. This method estimates the distances to the desired obstacles using a single monocular camera that does not require calibration. On the one hand, we train the algorithm with the KITTI dataset, which is an autonomous driving vision dataset that provides labels for object detection and distance prediction. On the other hand, we collect and label 100 images from a custom environment. Then, we apply data augmentation and transfer learning to generate a fast, accurate, and cost-effective model for the custom environment. The results show a performance of mAP0.5:0.95 of more than 75% for object detection and 0.71 m of mean absolute error in distance prediction, which are easily scalable with the labeling of a larger amount of data. Finally, we compare our method with other similar state-of-the-art approaches. © 2023 by the authors.",AGV; autonomous vehicles; distance estimation; indoor navigation; object detection; YOLO
Scopus,"Pan, X.; Jia, N.; Mu, Y.; Gao, X.",Survey of small object detection,,2023,,,,10.11834/jig.220455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172780895&doi=10.11834%2fjig.220455&partnerID=40&md5=4879bef52890c03b019cfdb3508a3da3,In recent years，object detection has attracted increasing attention because of the rapid development of computer vision and artificial intelligence technology. Early traditional object detection methods，such as histogram of oriented gradient（HOG）and deformable parts model（DPM）usually adopt three steps：region selection，manual feature extraction，and classification regression. However，manual feature extraction has great limitations for small object detection. The object detection algorithm based on the convolutional neural network can be divided into two-stage and one-stage detection algorithms. Two-stage detection algorithms，such as faster region with convolutional neural network（Faster RCNN）and cascade region with convolutional neural network（Cascade RCNN），select candidate regions through the region proposal network. Then，they classify and regress these regions to obtain the detection results. However，the problem of low accuracy still exists in small object detection. One-stage detection algorithms，such as single shot MultiBox detector（SSD）and you only look once（YOLO），can directly locate the object and output the category detection information of the object，thereby improving the speed of object detection to a certain extent. However，small object detection has always been a huge challenge in the field of object detection because of the small proportion of small object pixels，little semantic information，and small objects that are easily disturbed by complex scenes. In particular，the challenges in object detection are as follows：First，the characteristics of small objects are few. Given the small scale of small objects and the small coverage area in data images，extracting favorable semantic feature information in network training is difficult. Second，small object detection is susceptible to interference. Most of the small objects have low resolution，blurred images，and little visual information. Thus，they are easily disturbed during difficult feature extraction. Thus，the detection model cannot easily locate and identify small objects accurately. Moreover，many false detections and missed detections exist. Third，a shortage of small object datasets exists. At present，most of the mainstream object datasets，such as PASCAL VOC and MS-COCO，are aimed at normal-scale objects. In particular，the proportion of small-scale objects is insufficient，and the distribution is uneven. However，some datasets mentioned in this study that can be used for small object detection are all aimed at specific scenes or tasks. These datasets include DOTA remote sensing object detection dataset，face detection dataset and benchmark，which are not universal for small object detection. Fourth，small objects are easy to gather and block. A serious occlusion problem occurs when small objects gather. After many downsampling and pooling operations，quite a lot of feature information is lost，resulting in some detection difficulties. At present，visual small object detection is increasingly important in all fields of life. Aiming at the problems in small object detection，this study combs the research status and achievements of small object detection at home and abroad to promote the development of small object detection further，improve the speed and accuracy of small object detection，and optimize its algorithm model. The methods of small object detection are analyzed and summarized from the aspects of data enhancement，super resolution，multiscale feature fusion，contextual semantic information，anchor frame mechanism，attention，and specific detection scenarios. Data enhancement is the method proposed for solving the problems of a few general small object datasets，a small number of small objects in public datasets，and uneven distribution of small objects in images. The earliest data enhancement strategy is to increase the number of object training and improve the performance of object detection by deforming，rotating，scaling，cutting，and translating object instances. Then，other effective data augmentation methods emerged，which included oversampling the images containing small objects in the experiment，scaling and rotating the small objects，and copying the objects to any new position in order to augment the data. Data enhancement helps improve the robustness of a model to a certain extent. Moreover，it solves the problems of unobvious visual features of small objects and less object information. It also achieves good results in the final detection performance. However，the improper design of data enhancement strategy in practical applications may lead to new noise，impairing the performance of feature extraction. This scenario also brings some challenges to the design of the algorithm. The small object detection method based on multiscale fusion needs to make full use of the detailed information in the image because the characteristic information of small-scale objects is little. In the existing convolutional neural network（CNN）model of general object detection，multiscale detection can help the model to obtain accurate positioning information and discriminating feature information by using a low-level feature layer. This scenario is conducive to the detection and recognition of small-scale objects. First，a feature pyramid network（FPN）with strong semantic features at all scales is introduced. Then，an fpn-based path aggregation network（PANet），which not only achieved good results in case segmentation but also improved the detection of small objects. In feature fusion，the residual feature enhancement method extracts the context information with a constant ratio to reduce the information loss of the highest pyramid feature map. At present，many methods are based on multiscale feature fusion，which uses the low-level high-resolution and high-level strong feature semantic information of the network to improve the accuracy of small objects. In small object detection，the target’s feature expression ability is weak. Thus，the network structure must be deepened to learn considerable feature information. Introducing an attention mechanism can often make the network model pay considerable attention to the channels and areas related to the task. In the object detection network，the shallow feature map lacks the contextual semantic information of small objects. By incorporating attention mechanisms into the SSD model，irrelevant information in feature fusion is suppressed，leading to an improvement in the detection accuracy of small objects. In general，the attention mechanism can reasonably allocate the used resources，quickly find the region of interest，and ignore disturbing information. However，the improper design in use increases the cost of network calculation and affects the extraction of object features by the model. Finally，the future research direction of small object detection is prospected. Visual small object detection is becoming increasingly important in all fields of life，and it will develop in other directions in the future. © 2023 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.,data enhancement; multiscale characteristic fusion; object detection; small object detection; super-resolution
Scopus,"Cheng, L.; Zhang, D.; Zheng, Y.",Road Object Detection in Foggy Complex Scenes Based on Improved YOLOv8,,2024,,,,10.1109/ACCESS.2024.3438612,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200825413&doi=10.1109%2fACCESS.2024.3438612&partnerID=40&md5=ccaf2df4837060e4f103c897e3906395,"Focusing on the challenges of vehicle detection in foggy weather, especially the algorithm of low accuracy caused by small and incomplete targets in adverse weather conditions, a foggy weather vehicle detection algorithm based on improved lightweight YOLOv8 was proposed. Firstly, the dataset was processed through a combination of data transformation, Dehaze Formers and dark channel preprocessing. Secondly, in the main body of YOLOv8, the C2f component was replaced with the dynamic convolution C2f- DCN, enhancing its adaptability to geometric changes in the image. To further improve the detection performance of the classifier, an improved S5attention module based on S2-MLP was introduced. This module utilizes contextual information to capture long-range dependencies and assign weights to different channels based on their relevance to the task at hand. By considering non-local features, the S5attention module helps the model better capture important spatial relationships within the image. Additionally, the feature extraction module was updated to FasterNext, improving the differential convolution's feature extraction capabilities. The Involution module was also introduced to reduce FLOPs during feature channel fusion and reduce the model's parameter count. Experimental results show that on the RESIDE foggy weather dataset, the improved algorithm has an mAP50 increase of 4.1% compared with the original algorithm, and the model's parameter quantity is only 9.06m, with a computational cost reduced from 28.7G to 28.1G. The research model in this article will provide technical support for detecting vehicle targets in foggy weather, ensuring fast and accurate operation. © 2013 IEEE.",Deep learning; feature extraction; foggy weather vehicle detection; YOLOv8; Convolution; Deep learning; Extraction; Image enhancement; Metadata; Object detection; Scattering parameters; Vehicles; Accuracy; Computational modelling; Deep learning; Features extraction; Foggy weather vehicle detection; Kernel; Vehicles detection; YOLO; YOLOv8; Feature extraction
Scopus,"Tang, C.; Hu, Q.; Zhou, G.; Yao, J.; Zhang, J.; Huang, Y.; Ye, Q.",Transformer Sub-Patch Matching for High-Performance Visual Object Tracking,,2023,,,,10.1109/TITS.2023.3264664,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153373340&doi=10.1109%2fTITS.2023.3264664&partnerID=40&md5=ba9e8c6583023e3122a2051079af6f26,"Visual tracking is a core component of intelligent transportation systems, especially for unmanned driving and road surveillance. Numerous convolutional neural network (CNN) trackers have achieved unprecedented performance. However, CNN features with regular spatial context relationships experience difficulty matching the rigid target templates when dramatic deformation and occlusion occur. In this paper, we propose a novel full Transformer Sub-patch Matching network for tracking (TSMtrack), which decomposes the tracked object into sub-patches, and interlaced matches the extracted sub-patches by leveraging the attention mechanism born with the Transformer. Roots in Transformer architecture, TSMtrack consists of image patch decomposition, sub-patch matching, and position prediction. Specifically, TSMtrack converts the whole frame into sub-patches and extracts the sub-patch features independently. By sub-patch matching and FFN-like prediction, TSMtrack enables independent similarity measurement between sub-patch features in an interlaced and iterative fashion. With a full Transformer pipeline implemented, we achieve a high-quality trade-off between tracking speed performance. Experiments on nine benchmarks demonstrate the effectiveness of our Transformer sub-patch matching framework. In particular, it realizes an AO of 75.6 on GOT-10K and SR of 57.9 on WebUAV-3M with 48 FPS on GPU RTX-2060s.  © 2000-2011 IEEE.",full transformer; siamese network; sub-patch matching; Visual tracking; Intelligent systems; Neural networks; Convolutional neural network; Full transformer; Matching networks; Patch-matching; Performance; Siamese network; Sub-patch matching; Sub-patches; Visual object tracking; Visual Tracking; Economic and social effects
Scopus,"Han, T.; Dong, Q.; Sun, L.",SenseLite: A YOLO-Based Lightweight Model for Small Object Detection in Aerial Imagery,,2023,,,,10.3390/s23198118,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174024646&doi=10.3390%2fs23198118&partnerID=40&md5=1644148fe4179e7a58ac86692ef89174,"In the field of aerial remote sensing, detecting small objects in aerial images is challenging. Their subtle presence against broad backgrounds, combined with environmental complexities and low image resolution, complicates identification. While their detection is crucial for urban planning, traffic monitoring, and military reconnaissance, many deep learning approaches demand significant computational resources, hindering real-time applications. To elevate the accuracy of small object detection in aerial imagery and cater to real-time requirements, we introduce SenseLite, a lightweight and efficient model tailored for aerial image object detection. First, we innovatively structured the YOLOv5 model for a more streamlined structure. In the backbone, we replaced the original structure with cutting-edge lightweight neural operator Involution, enhancing contextual semantics and weight distribution. For the neck, we incorporated GSConv and slim-Neck, striking a balance between reduced computational complexity and performance, which is ideal for rapid predictions. Additionally, to enhance detection accuracy, we integrated a squeeze-and-excitation (SE) mechanism to amplify channel communication and improve detection accuracy. Finally, the Soft-NMS strategy was employed to manage overlapping targets, ensuring precise concurrent detections. Performance-wise, SenseLite reduces parameters by 30.5%, from 7.05 M to 4.9 M, as well as computational demands, with GFLOPs decreasing from 15.9 to 11.2. It surpasses the original YOLOv5, showing a 5.5% mAP0.5 improvement, 0.9% higher precision, and 1.4% better recall on the DOTA dataset. Compared to other leading methods, SenseLite stands out in terms of performance. © 2023 by the authors.",aerial images; GSConv; involution; SE; small objects; Soft-NMS; YOLOv5; Aerial photography; Antennas; Deep learning; Military applications; Military photography; Object detection; Object recognition; Remote sensing; Semantics; Aerial imagery; Aerial images; Gsconv; Involution; Performance; Small object detection; Small objects; Soft-NMS; Squeeze-and-excitation; YOLOv5; Image resolution
Scopus,"Rathee, M.; Bačić, B.; Doborjeh, M.",Automated Road Defect and Anomaly Detection for Traffic Safety: A Systematic Review,,2023,,,,10.3390/s23125656,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163947663&doi=10.3390%2fs23125656&partnerID=40&md5=1e6b780f9710de80833fd3c38b016432,"Recently, there has been a substantial increase in the development of sensor technology. As enabling factors, computer vision (CV) combined with sensor technology have made progress in applications intended to mitigate high rates of fatalities and the costs of traffic-related injuries. Although past surveys and applications of CV have focused on subareas of road hazards, there is yet to be one comprehensive and evidence-based systematic review that investigates CV applications for Automated Road Defect and Anomaly Detection (ARDAD). To present ARDAD’s state-of-the-art, this systematic review is focused on determining the research gaps, challenges, and future implications from selected papers (N = 116) between 2000 and 2023, relying primarily on Scopus and Litmaps services. The survey presents a selection of artefacts, including the most popular open-access datasets (D = 18), research and technology trends that with reported performance can help accelerate the application of rapidly advancing sensor technology in ARDAD and CV. The produced survey artefacts can assist the scientific community in further improving traffic conditions and safety. © 2023 by the authors.","ARDAD; computer vision; deep learning; machine learning; motorist safety; on-road anomaly detection; structural damage detection; transfer learning; Accidents, Traffic; Safety; Anomaly detection; Damage detection; Deep learning; Defects; Roads and streets; Structural analysis; Anomaly detection; Automated road defect and anomaly detection; Automated roads; Deep learning; Defect detection; Machine-learning; Motorist safety; On-road anomaly detection; Structural damage detection; Transfer learning; prevention and control; safety; traffic accident; Computer vision"
Scopus,"He, W.; Deng, Z.; Ye, Y.; Pan, P.",ConCs-Fusion: A Context Clustering-Based Radar and Camera Fusion for Three-Dimensional Object Detection,,2023,,,,10.3390/rs15215130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176326926&doi=10.3390%2frs15215130&partnerID=40&md5=5a93f90784cb6fb5334379d90aef95f2,"Multi-modality three-dimensional (3D) object detection is a crucial technology for the safe and effective operation of environment perception systems in autonomous driving. In this study, we propose a method called context clustering-based radar and camera fusion for 3D object detection (ConCs-Fusion) that combines radar and camera sensors at the intermediate fusion level to achieve 3D object detection. We extract features from heterogeneous sensors and input them as feature point sets into the fusion module. Within the fusion module, we utilize context cluster blocks to learn multi-scale features of radar point clouds and images, followed by upsampling and fusion of the feature maps. Then, we leverage a multi-layer perceptron to nonlinearly represent the fused features, reducing the feature dimensionality to improve model inference speed. Within the context cluster block, we aggregate feature points of the same object from different sensors into one cluster based on their similarity. All feature points within the same cluster are then fused into a radar–camera feature fusion point, which is self-adaptively reassigned to the originally extracted feature points from a simplex sensor. Compared to previous methods that only utilize radar as an auxiliary sensor to camera, or vice versa, the ConCs-Fusion method achieves a bidirectional cross-modal fusion between radar and camera. Finally, our extensive experiments on the nuScenes dataset demonstrate that ConCs-Fusion outperforms other methods in terms of 3D object detection performance. © 2023 by the authors.",autonomous driving; bidirectional cross-modal fusion; context clustering; radar–camera fusion; three-dimensional object detection; Autonomous vehicles; Cameras; Object recognition; Radar imaging; 3D object; Autonomous driving; Bidirectional cross-modal fusion; Context clustering; Cross-modal; Fusion modules; Objects detection; Radar–camera fusion; Three-dimensional object; Three-dimensional object detection; Object detection
Scopus,"Wang, H.; Guo, E.; Chen, F.; Chen, P.",Depth Completion in Autonomous Driving: Adaptive Spatial Feature Fusion and Semi-Quantitative Visualization,,2023,,,,10.3390/app13179804,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170395317&doi=10.3390%2fapp13179804&partnerID=40&md5=3d1f3098f5158cdd84463871314880cc,"The safety of autonomous driving is closely linked to accurate depth perception. With the continuous development of autonomous driving, depth completion has become one of the crucial methods in this field. However, current depth completion methods have major shortcomings in small objects. To solve this problem, this paper proposes an end-to-end architecture with adaptive spatial feature fusion by encoder–decoder (ASFF-ED) module for depth completion. The architecture is built on the basis of the network architecture proposed in this paper, and is able to extract depth information adaptively with different weights on the specified feature map, which effectively solves the problem of insufficient depth accuracy of small objects. At the same time, this paper also proposes a depth map visualization method with a semi-quantitative visualization, which makes the depth information more intuitive to display. Compared with the currently available depth map visualization methods, this method has stronger quantitative analysis and horizontal comparison ability. Through experiments of ablation study and comparison, the results show that the method proposed in this paper exhibits a lower root-mean-squared error (RMSE) and better small object detection performance on the KITTI dataset. © 2023 by the authors.",autonomous driving; computer vision; depth completion; image processing; multi-source information fusion for sensing
Scopus,"Mohapatra, J.B.; Monikantan, J.; Nishchal, N.K.",Object recognition under bad weather conditions with wavelet-modified logarithmic fringe-adjusted joint transform correlator,,2024,,,,10.1007/s12596-024-02065-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202467273&doi=10.1007%2fs12596-024-02065-9&partnerID=40&md5=9f01f8a4e03e97cb1f1b015d534fbcf7,"In real world applications, pattern recognition can be truly challenging when objects are embedded with backgrounds, fog, haze, sun-glare, and dust etc. It becomes very difficult to identify the object in these scenarios as auto-correlation is heavily affected by the noise. In this paper, our objective is to enhance the efficiency of object recognition in noisy environments by incorporating wavelet transform into the logarithmic fringe-adjusted joint transform correlator (LFJTC). The proposed study includes experimental demonstration of object detection and its investigation under ambient noise and different weather conditions. Wavelet function is integrated to the joint power spectrum of LFJTC. For eliminating the undesired dc term, differential processing is carried out. To quantify the effectiveness of the wavelet processing on LFJTC, three performance measure parameters have been calculated; correlation peak intensity, peak-to-sidelobe ratio, and signal-to-clutter ratio. The findings of the correlation analysis have been investigated using the target with Gaussian noise, speckle noise, simulated fog, haze, sun-glare, and dust. © The Author(s), under exclusive licence to The Optical Society of India 2024.",Correlation; Fringe-adjusted filter; Joint transform correlator; Wavelet transform; Correlators; Gaussian noise (electronic); Glare effects; Image coding; Laser beams; Radar clutter; Signal to noise ratio; Wavelet transforms; Auto correlation; Condition; Correlation; Fringe-adjusted filter; Fringe-adjusted joint transform correlator; Joint transform correlators; Objects recognition; Real-world; Sun glare; Wavelets transform; Glare
Scopus,"Chen, D.; Yan, X.; Shi, S.",Review on Detection and Identification Positioning Technology of Surface Unexploded Submunitions,,2023,,,,10.12132/ISSN.1673-5048.2023.0069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179999385&doi=10.12132%2fISSN.1673-5048.2023.0069&partnerID=40&md5=d944c8e7deb29fdccbe52989843cadca,"Long-range multi-purpose cluster munitions with reconnaissance, identification, strike and evaluation functions are widely used, and a large number of unexploded submunitions will be generated on the surface, which will greatly hinder the battlefield maneuver, deployment and depth attack. Due to its special visual characteristics, surface unexploded submunitions use imaging technology and deep learning technology for long-distance, large-area, non-contact rapid and accurate detection and recognition. This paper introduces the common methods of unexploded submunition detection and their advantages and disadvantages, summarizes the imaging characteristics of unexploded submunitions under different imaging technology conditions and their identification and positioning methods, analyzes the signifi-cant advantages of deep learning in the field of unexploded submunition detection and recognition. Then, a UAV-borne rapid detection, identification and positioning method based on deep learning for surface unexploded submunitions is proposed. © The Author(s) 2023.",deep learning; identification and positioning; imaging technology; unexploded submunition
Scopus,"Yuan, L.; Tang, H.; Chen, Y.; Gao, R.; Wu, W.",Improved YOLOv5 for Road Target Detection in Complex Environments,,2023,,,,10.3778/j.issn.1002-8331.2304-0251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007363901&doi=10.3778%2fj.issn.1002-8331.2304-0251&partnerID=40&md5=dbc0a50e668304b15660e14fadba58fc,"To solve the problem of missed detection in road object detection tasks in complex environments due to diverse target scale changes, dense occlusion and uneven lighting, an improved method for road object detection CTC-YOLO（context transformer and convolutional block attention module based on YOLOv5）is proposed. Firstly, for small targets, improve the network detection head structure, add a multi-scale target detection layer, and improve the accuracy of small target detection. Secondly, in order to fully utilize the input contextual information, introduce a context transformer networks（CoTNet）module in the feature extraction section, and design a CoT3 module to guide dynamic attention matrix learning and improve visual representation ability. Finally, the C3 module in the Neck section integrates the convolutional block attention module（CBAM）to locate attention regions in complex scenes. To further validate the CTC-YOLO method proposed in this paper, some useful strategies are adopted, such as model integration position selection and comparison with other attention mechanisms. The experimental results show that the mAP@0.5 on the publicly available datasets KITTI, Cityscapes and BDD100K reaches 89.6%, 46.1% and 57.0%, respectively, which are 3.1, 2.0 and 1.2 percentage points higher than the baseline model, respectively. Compared with other models, the detection efficiency is higher and effectively improves the problem of object detection in complex environments. © 2023 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.",attentional mechanism; complex environment; target detection; YOLOv5; Complex networks; Convolution; Object recognition; Roads and streets; Attentional mechanism; Complex environments; Detection tasks; Missed detections; Module-based; Objects detection; Road targets; Small targets; Targets detection; YOLOv5; Object detection
Scopus,"Shih, C.-H.; Lin, C.-J.; Jhang, J.-Y.",Ackerman Unmanned Mobile Vehicle Based on Heterogeneous Sensor in Navigation Control Application,,2023,,,,10.3390/s23094558,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159165676&doi=10.3390%2fs23094558&partnerID=40&md5=6c860ad195e2cd4b293cbe9394a93ee1,"With the advancement of science and technology, the development and application of unmanned mobile vehicles (UMVs) have emerged as topics of crucial concern in the global industry. The development goals and directions of UMVs vary according to their industrial uses, which include navigation, autonomous driving, and environmental recognition; these uses have become the priority development goals of researchers in various fields. UMVs employ sensors to collect environmental data for environmental analysis and path planning. However, the analysis function of a single sensor is generally affected by natural environmental factors, resulting in poor identification results. Therefore, this study introduces fusion technology that employs heterogeneous sensors in the Ackerman UMV, leveraging the advantages of each sensor to enhance accuracy and stability in environmental detection and identification. This study proposes a fusion technique involving heterogeneous imaging and LiDAR (laser imaging, detection, and ranging) sensors in an Ackerman UMV. A camera is used to obtain real-time images, and YOLOv4-tiny and simple online real-time tracking are then employed to detect the location of objects and conduct object classification and object tracking. LiDAR is simultaneously used to obtain real-time distance information of detected objects. An inertial measurement unit is used to gather odometry information to determine the position of the Ackerman UMV. Static maps are created using simultaneous localization and mapping. When the user commands the Ackerman UMV to move to the target point, the vehicle control center composed of the robot operating system activates the navigation function through the navigation control module. The Ackerman UMV can reach the destination and instantly identify obstacles and pedestrians when in motion. © 2023 by the authors.",Ackerman unmanned mobile vehicle; deep learning; heterogeneous sensor; navigation control; object detection; Deep learning; Environmental technology; Motion planning; Navigation; Optical radar; Robot Operating System; Robots; Ackerman unmanned mobile vehicle; Deep learning; Heterogeneous sensors; Imaging detections; Laser detection; Laser imaging; Laser ranging; Mobile vehicle; Navigation controls; Objects detection; Object detection
Scopus,"Wang, Y.; Qiu, Y.; Jiang, H.; Lu, Y.",Small object detection for autonomous driving under hazy conditions on mountain motorways,,2023,,,,10.1117/1.OE.62.11.113101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179468303&doi=10.1117%2f1.OE.62.11.113101&partnerID=40&md5=a08e5e6e9b4989bb48f4f81d8951e0d2,"To address the issue of high miss rates for distant small objects and the diminished system detection performance due to the influence of hazy when autonomous vehicles operate on mountain highways. We propose a framework for small object vehicle detection in hazy traffic environments (SHTDet). This framework aims to enhance small object detection for autonomous driving under hazy conditions on mountainous motorways. Specifically, to restore the clarity of hazy images, we designed an image enhancement (IE), and its parameters are predicted by a convolutional neural network [filter parameter estimation (FPE)]. In addition, to enhance the detection accuracy of small objects, we introduce a cascaded sparse query (CSQ) mechanism, which effectively utilizes high-resolution features while maintaining fast detection speed. We jointly optimize the IE and the detection network (CSQ-FCOS) in an end-to-end manner, ensuring that FPE module can learn a suitable IE. Our proposed SHTDet method is adept at adaptively handling sunny and hazy conditions. Extensive experiments demonstrate the efficacy of the SHTDet method in detecting small objects on hazy sections of mountain highways. © 2023 SPIE.",autonomous driving; hazy images; mountain motorways; small object detection; small object vehicle detection in hazy traffic environments; Autonomous vehicles; Convolutional neural networks; Landforms; Object detection; Object recognition; Autonomous driving; Condition; Hazy image; Mountain highway; Mountain motorway; Small object detection; Small object vehicle detection in hazy traffic environment; Small objects; Traffic environment; Vehicles detection; Image enhancement
Scopus,"Ma, S.; Wang, W.; Pan, Z.; Hu, Y.; Zhou, G.; Wang, Q.",A Recognition Model Incorporating Geometric Relationships of Ship Components,,2024,,,,10.3390/rs16010130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181871571&doi=10.3390%2frs16010130&partnerID=40&md5=be31227346dc6e637d342354349b83ef,"Ship recognition with optical remote sensing images is currently widely used in fishery management, ship traffic surveillance, and maritime warfare. However, it currently faces two major challenges: recognizing rotated targets and achieving fine-grained recognition. To address these challenges, this paper presents a new model called Related-YOLO. This model utilizes the mechanisms of relational attention to stress positional relationships between the components of a ship, extracting key features more accurately. Furthermore, it introduces a hierarchical clustering algorithm to implement adaptive anchor boxes. To tackle the issue of detecting multiple targets at different scales, a small target detection head is added. Additionally, the model employs deformable convolution to extract the features of targets with diverse shapes. To evaluate the performance of the proposed model, a new dataset named FGWC-18 is established, specifically designed for fine-grained warship recognition. Experimental results demonstrate the excellent performance of the model on this dataset and two other public datasets, namely FGSC-23 and FGSCR-42. In summary, our model offers a new route to solve the challenging issues of detecting rotating targets and fine-grained recognition with remote sensing images, which provides a reliable foundation for the application of remote sensing images in a wide range of fields. © 2023 by the authors.",fine-grained ship dataset; optical remote sensing; rotated ship recognition; Clustering algorithms; Warships; Fine grained; Fine-grained ship dataset; Fisheries management; Geometric relationships; Optical remote sensing; Performance; Recognition models; Remote sensing images; Rotated ship recognition; Ship recognition; Optical remote sensing
Scopus,"Song, K.; Zhang, Y.; Bao, Y.; Zhao, Y.; Yan, Y.",Self-Enhanced Mixed Attention Network for Three-Modal Images Few-Shot Semantic Segmentation,,2023,,,,10.3390/s23146612,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165987993&doi=10.3390%2fs23146612&partnerID=40&md5=c50b8da3703f6f640ed0bd667a43306c,"As an important computer vision technique, image segmentation has been widely used in various tasks. However, in some extreme cases, the insufficient illumination would result in a great impact on the performance of the model. So more and more fully supervised methods use multi-modal images as their input. The dense annotated large datasets are difficult to obtain, but the few-shot methods still can have satisfactory results with few pixel-annotated samples. Therefore, we propose the Visible-Depth-Thermal (three-modal) images few-shot semantic segmentation method. It utilizes the homogeneous information of three-modal images and the complementary information of different modal images, which can improve the performance of few-shot segmentation tasks. We constructed a novel indoor dataset VDT-2048-5i for the three-modal images few-shot semantic segmentation task. We also proposed a Self-Enhanced Mixed Attention Network (SEMANet), which consists of a Self-Enhanced module (SE) and a Mixed Attention module (MA). The SE module amplifies the difference between the different kinds of features and strengthens the weak connection for the foreground features. The MA module fuses the three-modal feature to obtain a better feature. Compared with the most advanced methods before, our model improves mIoU by 3.8% and 3.3% in 1-shot and 5-shot settings, respectively, which achieves state-of-the-art performance. In the future, we will solve failure cases by obtaining more discriminative and robust feature representations, and explore achieving high performance with fewer parameters and computational costs. © 2023 by the authors.",few-shot semantic segmentation; multi-modal images; three-modal registration; Image enhancement; Large dataset; Semantics; Computer vision techniques; Few-shot semantic segmentation; Images segmentations; Large datasets; Multimodal images; Performance; Semantic segmentation; Supervised methods; Thermal; Three-modal registration; article; attention; attention network; computer vision; illumination; image segmentation; Semantic Segmentation
Scopus,"Saidani, T.",Deep Learning Approach: YOLOv5-based Custom Object Detection,,2023,,,,10.48084/etasr.6397,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179989862&doi=10.48084%2fetasr.6397&partnerID=40&md5=79c4d352a900f822b64b43749ec2a1da,"Object detection is of significant importance in the field of computer vision, since it has extensive applications across many sectors. The emergence of YOLO (You Only Look Once) has brought about substantial changes in this domain with the introduction of real-time object identification with exceptional accuracy. The YOLOv5 architecture is highly sought after because of its increased flexibility and computational efficiency. This research provides an in-depth analysis of implementing YOLOv5 for object identification. This research delves deeply into the architectural improvements and design ideas that set YOLOv5 apart from its predecessors to illuminate its unique benefits. This research examines the training process and the efficiency of transfer learning techniques, among other things. The detection skills of YOLOv5 may be greatly improved by including these features. This study suggests the use of YOLOv5, a state-of-the-art object identification framework, as a crucial tool in the field of computer vision for accurate object recognition. The results of the proposed framework demonstrate higher performance in terms of mAP (60.9%) when evaluated with an IoU criterion of 0.5 and when compared to current methodologies in terms of reliability, computing flexibility, and mean average precision. These advantages make it applicable in many real-world circumstances. © 2023, Dr D. Pylarinos. All rights reserved.",computer vision; deep learning; object detection; YOLOv5
Scopus,"Zhu, X.; Kundu, S.K.",Road Anomaly Detection and Localization for Connected Vehicle Applications,,2023,,,,10.4271/2023-01-0719,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160731341&doi=10.4271%2f2023-01-0719&partnerID=40&md5=ee2ed70d51a4777f3a8efbcf12dd5ad1,"Road anomalies pose significant challenges for on-road safety, ride comfort, and fuel economy. The recent advancement of Connected Vehicle technology has made it feasible to overcome this challenge by sharing the detected road hazards information with other vehicles and entities. However, localization accuracies of the detected road hazards are often very low due to noisy detection results and limited GPS sensor performances. In this paper, a cloud based data management system with in-vehicle and on-cloud data processing modules is presented for road hazards detection and localization. Stereo camera and a consumer-grade GPS sensor on a testing vehicle are used to detect road anomaly information, e.g., type, size, and location, where a novel in-vehicle data processing module is implemented based on Kalman Filter and Phase Adjustment. For hazards data shared from all connected vehicles, an on-cloud data processing module is designed to further improve anomaly localization accuracy based on clustering. The whole system was tested in a parking lot with potholes, debris, and road bumps. Experimental results show that the hazards localization accuracy could be significantly improved from 7.4m to 1.4m with 84% accuracy using the proposed system. The proposed real-time system could bring significant benefits for commercial vehicles, and transportation companies with improved safety and ride quality. © 2023 SAE International. All Rights Reserved.",bump; Connected Vehicle; debris; GPS; Kalman filter; pothole; road anomaly; sensor fusion; stereo camera; Anomaly detection; Cameras; Commercial vehicles; Data handling; Debris; Fuel economy; Hazards; Information management; Landforms; Motor transportation; Real time systems; Roads and streets; Stereo image processing; Anomaly localizations; Bump; Connected vehicle; Localization accuracy; Pothole; Processing modules; Road anomaly; Road hazards; Sensor fusion; Stereo cameras; Kalman filters
Scopus,"Wang, B.; Ma, G.; Sui, H.; Zhang, Y.; Zhang, H.; Zhou, Y.",Few-Shot Object Detection in Remote Sensing Imagery via Fuse Context Dependencies and Global Features,,2023,,,,10.3390/rs15143462,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166259832&doi=10.3390%2frs15143462&partnerID=40&md5=684c1adc372ce90ed81e14c854a09b86,"The rapid development of Earth observation technology has promoted the continuous accumulation of images in the field of remote sensing. However, a large number of remote sensing images still lack manual annotations of objects, which makes the strongly supervised deep learning object detection method not widely used, as it lacks generalization ability for unseen object categories. Considering the above problems, this study proposes a few-shot remote sensing image object detection method that integrates context dependencies and global features. The method can be used to fine-tune the model with a small number of sample annotations based on the model trained in the base class, as a way to enhance the detection capability of new object classes. The method proposed in this study consists of three main modules, namely, the meta-feature extractor (ME), reweighting module (RM), and feature fusion module (FFM). These three modules are respectively used to enhance the context dependencies of the query set features, improve the global features of the support set that contains annotations, and finally fuse the query set features and support set features. The baseline of the meta-feature extractor of the entire framework is based on the optimized YOLOv5 framework. The reweighting module of the support set feature extraction is based on a simple convolutional neural network (CNN) framework, and the foreground feature enhancement of the support sets was made in the preprocessing stage. This study achieved beneficial results in the two benchmark datasets NWPU VHR-10 and DIOR. Compared with the comparison methods, the proposed method achieved the best performance in the object detection of the base class and the novel class. © 2023 by the authors.",context dependencies; feature fusion; few-shot object detection (FSOD); global features; graph convolutional unit (GCU); Convolution; Convolutional neural networks; Deep learning; Feature extraction; Object recognition; Remote sensing; Base class; Context dependency; Features fusions; Few-shot object detection; Global feature; Graph convolutional unit; Metafeature; Object detection method; Objects detection; Remote sensing images; Object detection
Scopus,"Liu, J.; Liu, X.; Chen, Q.; Niu, S.",A Traffic Parameter Extraction Model Using Small Vehicle Detection and Tracking in Low-Brightness Aerial Images,,2023,,,,10.3390/su15118505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161660499&doi=10.3390%2fsu15118505&partnerID=40&md5=94a5f95d31ea6cb778f9e4ed1a9f3e4b,"It is still a challenge to detect small-size vehicles from a drone perspective, particularly under low-brightness conditions. In this context, a YOLOX-IM-DeepSort model was proposed, which improved the object detection performance in low-brightness conditions accurately and efficiently. At the stage of object detection, this model incorporates the data enhancement algorithm as well as an ultra-lightweight subspace attention module, and optimizes the number of detection heads and the loss function. Then, the ablation experiment was conducted and the analysis results showed that the YOLOX-IM model has better mAP than the baseline model YOLOX-s for multi-scale object detection. At the stage of object tracking, the DeepSort object-tracking algorithm is connected to the YOLOX-IM model, which can extract vehicle classification data, vehicle trajectory, and vehicle speed. Then, the VisDrone2021 dataset was adopted to verify the object-detection and tracking performance of the proposed model, and comparison experiment results showed that the average vehicle detection accuracy is 85.00% and the average vehicle tracking accuracy is 71.30% at various brightness levels, both of which are better than those of CenterNet, YOLOv3, FasterR-CNN, and CascadeR-CNN. Next, a field experiment using an in-vehicle global navigation satellite system and a DJI Phantom 4 RTK drone was conducted in Tianjin, China, and 12 control experimental scenarios with different drone flight heights and vehicle speeds were designed to analyze the effect of drone flight altitude on speed extraction accuracy. Finally, the conclusions and discussions were presented. © 2023 by the authors.",low brightness images; traffic information and control; unmanned aerial vehicles; vehicle detection and tracking; China; Tianjin; detection method; GNSS; image analysis; satellite imagery; tracking; unmanned vehicle
Scopus,"Hong, J.-W.; Kim, S.-H.; Han, G.-T.",Detection of Multiple Respiration Patterns Based on 1D SNN from Continuous Human Breathing Signals and the Range Classification Method for Each Respiration Pattern,,2023,,,,10.3390/s23115275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161584072&doi=10.3390%2fs23115275&partnerID=40&md5=5b6723ff4d942f27f4b08ed12ae50efe,"Human respiratory information is being used as an important source of biometric information that can enable the analysis of health status in the healthcare domain. The analysis of the frequency or duration of a specific respiration pattern and the classification of respiration patterns in the corresponding section for a certain period of time are important for the utilization of respiratory information in various ways. Existing methods require window slide processing to classify sections for each respiration pattern from the breathing data for a certain time period. In this case, when multiple respiration patterns exist within one window, the recognition rate can be lowered. To solve this problem, a 1D Siamese neural network (SNN)-based human respiration pattern detection model and a merge-and-split algorithm for the classification of multiple respiration patterns in each region for all respiration sections are proposed in this study. When calculating the accuracy based on intersection over union (IOU) for the respiration range classification result for each pattern, the accuracy was found to be improved by approximately 19.3% compared with the existing deep neural network (DNN) and 12.4% compared with a 1D convolutional neural network (CNN). The accuracy of detection based on the simple respiration pattern was approximately 14.5% higher than that of the DNN and 5.3% higher than that of the 1D CNN. © 2023 by the authors.","1D SNN; MASRP; mmWave sensor; one-dimensional (1D) CNN; respiration patterns; Algorithms; Humans; Neural Networks, Computer; Recognition, Psychology; Respiration; Respiratory Rate; Classification (of information); Convolutional neural networks; Frequency domain analysis; Pattern recognition; 1d siamese neural network; Breathing signals; Convolutional neural network; Human breathing; MASRP; Mm-wave sensors; Neural-networks; One-dimensional; One-dimensional (1d) convolutional neural network; Respiration pattern; algorithm; artificial neural network; breathing; breathing rate; human; recognition; Deep neural networks"
Scopus,"Shen, Y.; Chandaka, B.; Lin, Z.-H.; Zhai, A.; Cui, H.; Forsyth, D.; Wang, S.",Sim-on-Wheels: Physical World in the Loop Simulation for Self-Driving,,2023,,,,10.1109/LRA.2023.3325689,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174804455&doi=10.1109%2fLRA.2023.3325689&partnerID=40&md5=4f49a72e9c1f4495852205ab83a900e7,"We present Sim-on-Wheels, a safe, realistic, and vehicle-in-loop framework to test autonomous vehicles' performance in the real world under safety-critical scenarios. Sim-on-wheels runs on a self-driving vehicle operating in the physical world. It creates virtual traffic participants with risky behaviors and seamlessly inserts the virtual events into images perceived from the physical world in real-time. The manipulated images are fed into autonomy, allowing the self-driving vehicle to react to such virtual events. The full pipeline runs on the actual vehicle and interacts with the physical world, but the safety-critical events it sees are virtual. Sim-on-Wheels is safe, interactive, realistic, and easy to use. The experiments demonstrate the potential of Sim-on-Wheels to facilitate the process of testing autonomous driving in challenging real-world scenes with high fidelity and low risk.  © 2016 IEEE.",Autonomous agents; robot safety; simulation and animation; Autonomous agents; Interactive computer systems; Pipelines; Rendering (computer graphics); Robots; Safety engineering; Three dimensional computer graphics; Three dimensional displays; Vehicle wheels; Physical world; Real - Time system; Real-world; Rendering (computer graphic); Road; Robot safety; Robot sensing system; Self drivings; Simulation and animation; Three-dimensional display; Real time systems
Scopus,"Sithiwichankit, C.; Chanchareon, R.",Advanced Stiffness Sensing through the Pincer Grasping of Soft Pneumatic Grippers,,2023,,,,10.3390/s23136094,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164846358&doi=10.3390%2fs23136094&partnerID=40&md5=4b52d56587f5c65a05b4dea722b8701b,"In this study, a comprehensive approach for sensing object stiffness through the pincer grasping of soft pneumatic grippers (SPGs) is presented. This study was inspired by the haptic sensing of human hands that allows us to perceive object properties through grasping. Many researchers have tried to imitate this capability in robotic grippers. The association between gripper performance and object reaction must be determined for this purpose. However, soft pneumatic actuators (SPA), the main components of SPGs, are extremely compliant. SPA compliance makes the determination of the association challenging. Methodologically, the connection between the behaviors of grasped objects and those of SPAs was clarified. A new concept of SPA modeling was then introduced. A method for stiffness sensing through SPG pincer grasping was developed based on this connection, and demonstrated on four samples. This method was validated through compression testing on the same samples. The results indicate that the proposed method yielded similar stiffness trends with slight deviations in compression testing. A main limitation in this study was the occlusion effect, which leads to dramatic deviations when grasped objects greatly deform. This is the first study to enable stiffness sensing and SPG grasping to be carried out in the same attempt. This study makes a major contribution to research on soft robotics by progressing the role of sensing for SPG grasping and object classification by offering an efficient method for acquiring another effective class of classification input. Ultimately, the proposed framework shows promise for future applications in inspecting and classifying visually indistinguishable objects. © 2023 by the authors.",object classification; pincer grasping; pneumatic gripper; sensible grasping; soft gripper; stiffness sensing; Equipment Design; Hand; Hand Strength; Humans; Pressure; Robotics; Association reactions; Compression testing; Grippers; Pneumatic actuators; Pneumatics; Grasped object; Haptic sensing; Human hands; Object classification; Object property; Pincer grasping; Pneumatic grippers; Sensible grasping; Soft gripper; Stiffness sensing; equipment design; hand; hand strength; human; pressure; procedures; robotics; Stiffness
Scopus,"Yang, Q.; Zhu, H.; Zhao, H.; Tang, X.; Dai, S.; Chen, J.","CG-YOLO: A Lightweight, High-Accuracy Gesture Recognition Method for Human-Computer Interaction",,2024,,,,10.1109/CAC63892.2024.10864609,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000752012&doi=10.1109%2fCAC63892.2024.10864609&partnerID=40&md5=6da1ffb2e5f8f0b503e6c26dc233e604,"Although gesture recognition technology has made significant progress, its accuracy and robustness in practical applications are still insufficient due to factors such as complex environments, lighting variations, and background interference. This paper proposes a lightweight and high-accuracy human-computer interaction gesture recognition method called CG-YOLO to address these issues. We reconstruct the backbone network of YOLOv8s by introducing a Contextual Feature Aggregation Backbone Network (ContextRepViT) to enhance the model’s accuracy in recognizing small-sized hand targets, thus solving the problems of missed and false detections in complex backgrounds. Additionally, we propose a lightweight feature extraction module (GSRepNCSP) to redesign the Contextual 2D Features (C2f) modules in the Head network of YOLOv8s, improving parameters utilization and reducing computational load. Experimental results on the public HaGRID dataset show that, compared to YOLOv8s, the CG-YOLO reduces the number of parameters by 1.3M, and improves Precision, Recall, mAP@0.5, and mAP@0.5-0.95 by 3.2%, 1.5%, 2%, and 3.9%, respectively. Moreover, the human-robot interaction experiments with the quadruped robot also verified the effectiveness of the CG-YOLO. © 2024 IEEE.",Gesture recognition; Human-computer interaction; Lightweight; YOLOv8; Gesture recognition; Human robot interaction; Modular robots; Palmprint recognition; Back-bone network; Complex environments; Computer interaction; Environment lighting; Gesture recognition technologies; Gestures recognition; High-accuracy; Lightweight; Recognition methods; YOLOv8; Multipurpose robots
Scopus,"Huang, T.; Wang, G.; Wu, L.; Pu, H.; Luo, J.; Liu, H.; Zou, X.; Luo, J.",MD-TLCF: Miner Distance Detection Based on Trajectory-Based Low-Confidence Filter,,2024,,,,10.1109/TIM.2024.3412212,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196118141&doi=10.1109%2fTIM.2024.3412212&partnerID=40&md5=32f56e6132b62a9c891c59203e8da922,"— The confidence level of the detection result generally indicates the reliability of the detected object. However, in underground coal mines with low light intensity and uneven light distribution, the confidence level of detection results is low and exhibits significant fluctuation. Conventional 2-D miner detection methods are inadequate in delivering comprehensive 3-D miner positions to coal mine robots. This article proposes a real-time detection framework on RGBD images to generate a miner detection box and corresponding distance (miner state). Moreover, a trajectory-based low-confidence filter (TLCF) is proposed to refilter the miner state with a low confidence value based on the predicted state of the tracked miner trajectory. To evaluate the performance of distance detection methods in underground coal mines under various lighting conditions, a miner tracking dataset MINERTKRGBD is proposed. In the evaluation of this dataset, the proposed framework based on RGBD images outperforms the state-of-the-art vision methods. The miner distance detection method using TLCF (MD-TLCF) outperforms the method that solely relied on a confidence threshold filter (CTF). The Recall50 of the MD-TLCF method improved by approximately 4.2% compared to the latter, without any decrease in Precision50. Our code repository and dataset are publicly available at https://github.com/HT-hlf/MD-TLCF.git. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Kalman filter; low confidence; miner distance detection; underground coal mine; Bandpass filters; Coal; Coal mines; Feature extraction; Miners; Object detection; Three dimensional displays; Trajectories; Coal-mining; Distances detections; Features extraction; Information filter; Low confidence; Miner distance detection; Point cloud compression; Point-clouds; Three-dimensional display; Underground coal mine; Kalman filters
Scopus,"Mohammed, M.A.; Mohammed, M.H.; Abed Alsultani, H.A.; Kassim Ahmad, H.; Hikmat, R.; Migo, P.; Zhyrov, G.",Analyzing the Role of Arduino and LTE in IoT-Powered Adaptive Traffic Solutions,,2024,,,,10.23919/FRUCT64283.2024.10749917,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210842923&doi=10.23919%2fFRUCT64283.2024.10749917&partnerID=40&md5=cee8be1d76d5440535434e52ea570d33,"Background: Urban traffic demands efficient management solutions to reduce congestion and improve flow. Traditional traffic signal systems, mostly static, struggle to track urban activity.Objective: This article uses IoT technologies, Arduino microcontrollers, and LTE connection to create an adaptive traffic light system that constantly adjusts traffic signal lengths to maximize traffic flow.Methodology: We created a prototype adaptive traffic light system using Arduino microcontrollers with LTE modules and sensors. The sensors send Real-time traffic data over LTE to a cloud server. The technology uses machine learning algorithms to assess data and traffic conditions and remotely alter traffic signal timings via IoT.Results: The prototype improved traffic flow and reduced congestion during peak hours at chosen junctions. In quantitative terms, traffic throughput rose 25%, and intersection waiting times decreased by 35%. Idling time reduction was anticipated to lower vehicle emissions.Conclusion: Arduino and LTE connection in an IoT-based adaptive traffic signal system show promise for urban traffic management. Traffic flow, waiting times, and emissions improve, proving its scalability and enabling cities to a sustainable and effective traffic management plan as vehicle loads rise. Further study is needed to determine its efficacy in different metropolitan topologies and traffic patterns. © 2024 FRUCT Oy.",adaptive systems; Arduino; cloud computing; emissions reduction; IoT (Internet of Things); LTE (Long-Term Evolution); machine learning; real-time data; traffic management; vehicular flow; 4G mobile communication systems; Advanced traffic management systems; Air traffic control; Bioluminescence; Emission control; Highway administration; Highway traffic control; Image thinning; Information management; Long Term Evolution (LTE); Luminescence of gases; Luminescence of liquids and solutions; Luminescence of solids; Motor transportation; Phosphorescence; Street traffic control; Traffic congestion; Urban transportation; Vehicle actuated signals; Arduino; Cloud-computing; Emission reduction; Internet of thing; Long-term evolution; Machine-learning; Real-time data; Traffic management; Vehicular flow; Microcontrollers
Scopus,"Ruan, J.; Cui, H.; Huang, Y.; Li, T.; Wu, C.; Zhang, K.",A review of occluded objects detection in real complex scenarios for autonomous driving,,2023,,,,10.1016/j.geits.2023.100092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160397104&doi=10.1016%2fj.geits.2023.100092&partnerID=40&md5=8f750cc3bae9e34e146eaf769ca59501,"Autonomous driving is a promising way to future safe, efficient, and low-carbon transportation. Real-time accurate target detection is an essential precondition for the generation of proper following decision and control signals. However, considering the complex practical scenarios, accurate recognition of occluded targets is a major challenge of target detection for autonomous driving with limited computational capability. To reveal the overlap and difference between various occluded object detection by sharing the same available sensors, this paper presents a review of detection methods for occluded objects in complex real-driving scenarios. Considering the rapid development of autonomous driving technologies, the research analyzed in this study is limited to the recent five years. The study of occluded object detection is divided into three parts, namely occluded vehicles, pedestrians and traffic signs. This paper provided a detailed summary of the target detection methods used in these three parts according to the differences in detection methods and ideas, which is followed by the comparison of advantages and disadvantages of different detection methods for the same object. Finally, the shortcomings and limitations of the existing detection methods are summarized, and the challenges and future development prospects in this field are discussed. © 2023 The Author(s)",Autonomous driving; Object detection; Occluded objects; Pedestrians; Traffic signs; Vehicles; Autonomous vehicles; Object recognition; Pedestrian safety; Traffic signs; Autonomous driving; Control signal; Detection methods; Low carbon transportations; Objects detection; Occluded objects; Pedestrian; Real- time; Targets detection; Time-accurate; Object detection
Scopus,"Cheng, G.; Yuan, X.; Yao, X.; Yan, K.; Zeng, Q.; Xie, X.; Han, J.",Towards Large-Scale Small Object Detection: Survey and Benchmarks,,2023,,,,10.1109/TPAMI.2023.3290594,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163766263&doi=10.1109%2fTPAMI.2023.3290594&partnerID=40&md5=8629804ba8256a873bf0f9f07799f64e,"With the rise of deep convolutional neural networks, object detection has achieved prominent advances in past years. However, such prosperity could not camouflage the unsatisfactory situation of Small Object Detection (SOD), one of the notoriously challenging tasks in computer vision, owing to the poor visual appearance and noisy representation caused by the intrinsic structure of small targets. In addition, large-scale dataset for benchmarking small object detection methods remains a bottleneck. In this paper, we first conduct a thorough review of small object detection. Then, to catalyze the development of SOD, we construct two large-scale Small Object Detection dAtasets (SODA), SODA-D and SODA-A, which focus on the Driving and Aerial scenarios respectively. SODA-D includes 24828 high-quality traffic images and 278433 instances of nine categories. For SODA-A, we harvest 2513 high resolution aerial images and annotate 872069 instances over nine classes. The proposed datasets, as we know, are the first-ever attempt to large-scale benchmarks with a vast collection of exhaustively annotated instances tailored for multi-category SOD. Finally, we evaluate the performance of mainstream methods on SODA. We expect the released benchmarks could facilitate the development of SOD and spawn more breakthroughs in this field.  © 1979-2012 IEEE.",Benchmark; convolutional neural networks; deep learning; object detection; small object detection; Antennas; Benchmarking; Convolution; Deep neural networks; Job analysis; Object detection; Object recognition; Benchmark; Benchmark testing; Convolutional neural network; Deep learning; Features extraction; Objects detection; Pedestrian; Small object detection; Task analysis; article; catalysis; Feature extraction
Scopus,"Wang, S.; Wang, C.; Shi, C.; Liu, Y.; Lu, M.",Mask-Guided Mamba Fusion for Drone-Based Visible-Infrared Vehicle Detection,,2024,,,,10.1109/TGRS.2024.3452550,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202732974&doi=10.1109%2fTGRS.2024.3452550&partnerID=40&md5=e78e01bb92c9775596b23d87f270fd68,"Drone-based vehicle detection is a critical task within intelligent transportation systems. The existing methods that rely solely on single visible or infrared modalities often struggle to achieve both precise and robust detection. Effectively integrating cross-modal information to assist in vehicle detection remains a significant challenge. In this article, we propose a mask-guided Mamba fusion (MGMF) method for visible-infrared vehicle detection in aerial scenes. The proposed MGMF framework consists of two key components: the masked regularization constraint module (MRCM) and the state-space fusion module (SSFM). First, in MAEM, we use candidate regions from one modality to cover corresponding regions of intermediate-level features from another modality, while a regularization constraint extracts cross-modal guidance. This design allows cross-modal features focused on vehicle areas to be extracted from both modalities for fusion. Second, in SSFM, we propose mapping cross-modal features into a shared hidden state for interaction. This reduces disparities between the cross-modal features and enhances the representation, enabling better perception of intermodal correlations. When evaluated on the DroneVehicle dataset, our MGMF achieves an 80.24% with respect to mAP, establishing a new benchmark for state-of-the-art performance. Ablation studies further demonstrate the effectiveness of our MAEM and SSFM in enhancing visible-infrared fusion for vehicle detection. © 2024 IEEE.",Cross-modal information; drone-based vehicle detection; masked regularization constraint module (MRCM); state-space fusion module (SSFM); Air navigation; Aircraft detection; Automatic guided vehicles; Conformal mapping; Job analysis; Magnetic levitation vehicles; Object tracking; Vehicle locating systems; Accuracy; Constraint module; Cross-modal; Cross-modal information; Drone-based vehicle detection; Features extraction; Fusion modules; Masked regularization constraint module; Objects detection; Regularisation; State space fusion module; State-space; Task analysis; Transformer; Vehicles detection; aerial photography; detection method; image analysis; information system; infrared imagery; intelligent transportation system; remotely operated vehicle; Vehicle detection
Scopus,"Xiao, Y.; Liu, Y.; Luan, K.; Cheng, Y.; Chen, X.; Lu, H.",Deep LiDAR-Radar-Visual Fusion for Object Detection in Urban Environments,,2023,,,,10.3390/rs15184433,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172995795&doi=10.3390%2frs15184433&partnerID=40&md5=e5e1126de76b30347b28107be4ae6725,"Robust environmental sensing and accurate object detection are crucial in enabling autonomous driving in urban environments. To achieve this goal, autonomous mobile systems commonly integrate multiple sensor modalities onboard, aiming to enhance accuracy and robustness. In this article, we focus on achieving accurate 2D object detection in urban autonomous driving scenarios. Considering the occlusion issues of using a single sensor from a single viewpoint, as well as the limitations of current vision-based approaches in bad weather conditions, we propose a novel multi-modal sensor fusion network called LRVFNet. This network effectively combines data from LiDAR, mmWave radar, and visual sensors through a deep multi-scale attention-based architecture. LRVFNet comprises three modules: a backbone responsible for generating distinct features from various sensor modalities, a feature fusion module utilizing the attention mechanism to fuse multi-modal features, and a pyramid module for object reasoning at different scales. By effectively fusing complementary information from multi-modal sensory data, LRVFNet enhances accuracy and robustness in 2D object detection. Extensive evaluations have been conducted on the public VOD dataset and the Flow dataset. The experimental results demonstrate the superior performance of our proposed LRVFNet compared to state-of-the-art baseline methods. © 2023 by the authors.",deep learning method; multi-modal sensing; multi-sensor fusion; object detection; Autonomous vehicles; Deep learning; Learning systems; Object recognition; Optical radar; Sensor data fusion; Urban planning; 2D objects; Autonomous driving; Deep learning method; Learning methods; Multi-modal; Multi-sensor fusion; Multimodal sensing; Objects detection; Sensor modality; Urban environments; Object detection
Scopus,"Liu, Q.; Li, D.; Jiang, R.; Liu, S.; Liu, H.; Li, S.",MT-FANet: A Morphology and Topology-Based Feature Alignment Network for SAR Ship Rotation Detection,,2023,,,,10.3390/rs15123001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164147045&doi=10.3390%2frs15123001&partnerID=40&md5=2b6d6dd22b240884d7d9c2fa56670f14,"In recent years, ship target detection in synthetic aperture radar (SAR) images has significantly progressed due to the rapid development of deep learning (DL). However, since only the spatial feature information of ship targets is utilized, the current DL-based SAR ship detection approaches cannot achieve a satisfactory performance, especially in the case of multiscale, rotations, or complex backgrounds. To address these issues, in this paper, a novel deep-learning network for SAR ship rotation detection, called a morphology and topology-based feature alignment network, is proposed which can better exploit the morphological features and inherent topological structure information. This network consists of the following three main steps: First, deformable convolution is introduced to improve the representational ability for irregularly shaped ship targets, and subsequently, a morphology and topology feature pyramid network is developed to extract inherent topological structure information. Second, based on the aforementioned features, a rotation alignment feature head is devised for fine-grained processing as well as aligning and distinguishing the features; to enable regression prediction of rotated bounding boxes; and to adopt a parameter-sharing mechanism to improve detection efficiency. Therefore, utilizing morphological and inherent topological structural information enables a superior detection performance to be achieved. Finally, we evaluate the effectiveness of the proposed method using the rotated ship detection dataset in SAR images (RSDD-SAR). Our method outperforms other DL-based algorithms with fewer parameters. The overall average precision is 90.84% and recall is 92.21%. In inshore and offshore scenarios, our method performs well for the detection of multi-scale and rotation-varying ship targets, with its average precision reaching 66.87% and 95.72%, respectively. © 2023 by the authors.",morphology features; rotating bounding boxes; ship target detection; synthetic aperture radar (SAR); topological structure information; Alignment; Deep learning; Feature extraction; Offshore oil well production; Radar imaging; Rotation; Ships; Space-based radar; Topology; Tracking radar; Bounding-box; Morphology feature; Rotating bounding box; Ship target detection; Ship targets; Structure information; Synthetic aperture radar; Targets detection; Topological structure; Topological structure information; Synthetic aperture radar
Scopus,"Xu, X.; Song, Y.; Ge, Q.; Huang, Y.",Optimization of Ship Small Target Detection Based on YOLOv10 in Complex Ocean Environment,,2024,,,,10.1109/ICUS61736.2024.10839864,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217989659&doi=10.1109%2fICUS61736.2024.10839864&partnerID=40&md5=99ca24eba08608a6e07bf0dad1b3686d,"Ship target detection technology plays a vital role in civilian maritime traffic monitoring and military maritime security protection, and it is the key to ensuring the safety and order of marine activities. In a complex ocean environment, the background information of optical images is complex. The ship images are missed and wrongly detected due to the change in UAV shooting height, so a CSCGhost target detection algorithm is proposed. Experiments denote that compared with the traditional YOLOv10, the mAP increases by 3.2%, the accuracy increases by 9.1%, and the recall increases by 3.2%, which has a good detection effect and is especially suitable for offshore operations. © 2024 IEEE.",Ghost; object detection; sea ships; SSFF; YOLOv10; Change detection; Military photography; Waterway transportation; Ghost; Objects detection; Ocean environment; Optimisations; Sea ship; Ship targets; Small target detection; SSFF; Targets detection; YOLOv10; Marine safety
Scopus,"Sahu, S.; Sahu, S.P.; Dewangan, D.K.",Pedestrian detection using ResNet-101 based Mask R-CNN,,2023,,,,10.1063/5.0134276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166934718&doi=10.1063%2f5.0134276&partnerID=40&md5=cee41ef11396a9f60f15f8e17e5447c3,"Pedestrian detection is the most important process of any intelligent surveillance or advanced autonomous vehicle system. Autonomous vehicles observe the surroundings using camera, lidar, radar or sensor to detect the pedestrian from a certain distance so that vehicle can take the appropriate action. There are many frameworks that have been proposed by the researcher in the past years to make a better pedestrian detection model. The enhancement in the deep learning detection process becomes more accurate, but still, it is lacking in terms of accuracy and computational speed. To resolve this problem we are introducing a new deep learning model that uses the advantage of two most popular deep learning algorithms to detect the pedestrian more accurately in less time. In this paper we are using ResNet101 as a backbone of Mask R-CNN. Use of ResNet-101 here to extract the feature map as well as it overcomes the vanishing gradient and exploding gradient problem because of skipping connection features. Mask RCNN does masking on objects after classification to provide better visibility. The main aim of this model is to reduce the computational cost and increase the accuracy without affecting the robustness of the system. Based on the experimental result on the INRIA dataset the accuracy is reported 98.9% to 100% of the proposed model with 3.57% error rate that is less than the human error rate. We hope this model will get more improvement in the future to deal with the upcoming challenges in autonomous vehicles. © 2023 Author(s).",Autonomous vehicle; Classification; Deep learning; Pedestrian detection; ResNet-101; Segmentation
Scopus,"Ren, K.-Y.; Gu, M.-Y.; Yuan, Z.-Q.; Yuan, S.",3D object detection algorithms in autonomous driving: A review,,2023,,,,10.13195/j.kzyjc.2022.0618,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160825452&doi=10.13195%2fj.kzyjc.2022.0618&partnerID=40&md5=dbba5d1ca9eae452e72c782adb3f20e4,"Accurate and real-time object detection is one of the important functions for autonomous vehicles to accurately perceive the surrounding complex environment. Nevertheless, how to get the accurate size, distance, position, posture and other 3D information of surrounding objects is a classic problem. 3D object detection for autonomous driving has become a popular research field in recent years. Main research progress in this field is reviewed. Firstly, the characteristics of relevant sensors in the surrounding environment of autonomous driving is introduced. Then, the development of object detection from 2D to 3D is introduced and the loss functions is applied for optimization. According to the type of data acquired by the sensor, 3D object detection algorithms is categorized into three types, which are algorithms based on monocular/stereo images, point clouds, image and point cloud fusion. Futhermore, the classic and improved algorithms for each type of 3D object detection are reviewed, analyzed, and compared in detail. Simultaneously, the mainstream autonomous driving datasets and the evaluation criteria of their 3D object detection algorithms are summarized. Extensive experiment results of KITTI and NuScenes datasets are also compared and analyzed, which is widely used inpresent literature, summarizing the difficulties and problems of the existing algorithms. Besides, the opportunities and challenges of 3D object detection in data processing, feature extraction strategy, multi-sensor fusion and data distribution problems are proposed in hope of inspiring more future work. © 2023 Northeast University. All rights reserved.",3D object detection; autonomous driving; computer vision; deep learning; object detection; Autonomous vehicles; Computer vision; Deep learning; Feature extraction; Object recognition; Sensor data fusion; Signal detection; 3D object; 3d object detection; Autonomous driving; Autonomous Vehicles; Deep learning; Object detection algorithms; Objects detection; Point-clouds; Real- time; Object detection
Scopus,"Thoo, Y.-J.; Jeanneret Medina, M.; Froehlich, J.E.; Ruffieux, N.; Lalanne, D.",A Large-Scale Mixed-Methods Analysis of Blind and Low-vision Research in ACM and IEEE,,2023,,,,10.1145/3597638.3608412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177838134&doi=10.1145%2f3597638.3608412&partnerID=40&md5=e3aa4a7dc9a1794ff1a53c616ac32a04,"Technologies for blind and low-vision (BLV) people have long been a focus of Human-Computer Interaction (HCI) and accessibility (ASSETS) research. To map and assess this cross-disciplinary field, prior literature reviews have focused on specific BLV research areas (e.g., navigation assistance) or study methodologies (e.g., qualitative methods). In this paper, we provide a more holistic examination, combining both quantitative bibliometric analyses with qualitative assessments. Using keyword queries of terms focused on the human (e.g., people) and their visual status (e.g., blind, low-vision), we first derived a dataset of 880 papers published between 2010-2022 from ACM and IEEE conferences and journals. We then apply a programmatic analysis of this dataset followed by a qualitative analysis of the 100 most-cited papers. Our findings highlight four major research areas: Accessibility at Home & on the Go, Non-Visual Interaction, Orientation & Mobility, and Education. We also capture the diversity of denominations used to refer to the BLV community and their co-occurrences, as well as computer systems targeting both blind and low-vision users with a focus on visual substitution. We close by suggesting areas for future work and hope to stimulate discussions in our field.  © 2023 Owner/Author.",bibliographic coupling; blind; low-vision; systematic review; visual impairment; Bibliographic couplings; Blind; Large-scales; Low vision; Method analysis; Mixed method; Research areas; Systematic Review; Vision research; Visual impairment; Human computer interaction
Scopus,"Madan, C.R.",Memories That Matter: How We Remember Important Things,,2024,,,,10.4324/9780429032028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191251146&doi=10.4324%2f9780429032028&partnerID=40&md5=50acd92e5f63db88a7921f8a9fbbcc71,"What makes some experiences more memorable than others? How can you better remember specific information later? Memories That Matter addresses these questions and more. The book is divided into three main parts, with each part focusing on a different aspect of memory. After the introductory first part, Part II discusses everyday uses of memory and why we remember, establishing a foundation for how memory is structured and stored in the brain. Part III dives into what makes us remember. Emotional and rewarding experiences are both more memorable than mundane experiences but are often studied using different approaches. Self-relevance and objects we can interact with are remembered better than less relevant information. The author explores these motivation-related influences on memory and considers whether a common mechanism underlies them all. Part IV changes the focus, discussing how we sometimes want to remember specific information that does not automatically capture our attention. The book considers evidence-based learning strategies and memory strategies, whilst also exploring real-world applications, with discussion of professions that accomplish amazing memory feats daily. The book concludes with a reflection on how the role of memory is changing as our world makes information increasingly accessible, particularly with the ever-expanding influence of the internet. Drawing from a variety of literatures and perspectives, this important book will be relevant for all students of memory from psychology, cognitive neuroscience, and related health backgrounds. © 2023 Christopher R. Madan.",
Scopus,"Luo, Y.; Xu, D.; Zhou, G.; Sun, Y.; Lu, S.",Impact of Raindrops on Camera-Based Detection in Software-Defined Vehicles,,2024,,,,10.1109/MOST60774.2024.00028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201224774&doi=10.1109%2fMOST60774.2024.00028&partnerID=40&md5=1f94d70a49c911fd28820403214a4aaa,"Raindrops adhering to windshields or camera lenses substantially impair visibility, leading to significant camera-based detection challenges for software-defined vehicles in both daytime and nighttime conditions. Addressing the impact of raindrops is thus crucial. This work begins by classifying four prevalent types of raindrops within the BDD100K dataset, identifying microsphere raindrops as particularly impactful in rainy conditions. We then conduct a quantitative analysis focusing on the density and diameter of raindrops, underscoring the pronounced impacts of small-density raindrops on detection performance. To mitigate raindrop interference, we introduce and assess the SR3 model for raindrop removal, applying it to both synthetic raindrop-degraded data and real-world rainy data. Besides, we propose YOLO-RA, a novel and fast model to resolve the issues of missing small-size objects and erroneous detections in irrelevant regions. Next, a novel pipeline that combines SR3 with YOLO-RA markedly improves accuracy and processing speed. Finally, we discuss our experimental observations extensively and offer detailed explanations, contributing to understanding SDVs' operational effectiveness in adverse weather conditions.  © 2024 IEEE.",adverse weather; computer vision; impact mitigation; raindrops; software-defined vehicles; Cameras; Classification (of information); Computer software; Computer vision; Drops; Object detection; Adverse weather; Camera-based; Condition; Detection performance; FAST model; Impact mitigation; Raindrop; Rainy conditions; Real-world; Software-defined vehicle; Vehicles
Scopus,"Bolbot, V.; Sandru, A.; Saarniniemi, T.; Puolakka, O.; Kujala, P.; Valdez Banda, O.A.",Small Unmanned Surface Vessels—A Review and Critical Analysis of Relations to Safety and Safety Assurance of Larger Autonomous Ships,,2023,,,,10.3390/jmse11122387,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180642087&doi=10.3390%2fjmse11122387&partnerID=40&md5=489488d7fdbc9c698f6b0f11e49318ff,"Autonomous ships represent an emerging paradigm within the maritime sector, poised to bring multiple advantages. Although numerous prototypes have been developed, the deployment of large autonomous ships has predominantly remained confined to domestic waters or specialized military applications. The extensive adoption of autonomous ships is hampered by several challenges, primarily centered around safety. However, the direct assessment of autonomous technologies on large-scale vessels can be very costly. Small-scale autonomy testing may provide a cheaper option. This study reviews the current small autonomous ship models used by maritime researchers and industry practitioners. It aims to evaluate how these autonomous models currently augment and can augment safety assurances on larger autonomous ships. The review identifies relevant very small Unmanned Surface Vessels (USVs), the main research groups behind them and their applications. Then, the current use of USVs for safety and safety assurance is analyzed. Finally, the paper suggests innovative strategies and research directions for using USVs for the safety assurance of larger autonomous ships. © 2023 by the authors.",applications; bibliometric analysis; cybersecurity; safety; systematic review; unmanned surface vessels
Scopus,"Jin, G.; Tan, L.; Wang, X.; Zhao, J.",Multi-scale SAR road extraction method based on Duda operator,,2023,,,,10.12305/j.issn.1001-506X.2023.10.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176233659&doi=10.12305%2fj.issn.1001-506X.2023.10.10&partnerID=40&md5=fac0f78672e2cffd20a7685110188868,"Aiming at the problem of extracting different levels of roads in synthetic aperture radar (SAR) images, an effective multi-scale extraction method for SAR roads from coarse to fine is proposed. Based on Duda operator, the method improves the extraction probability of different levels of roads and increases the link probability of potential road segments through multi-scale extraction and results, integration of road in SAR images. On this basis, the effective extraction of roads in SAR images is realized by combing the hierarchical design of scale filtering, morphological filtering, and fragment linking. The proposed method is tested on real SAR images, and the effectiveness of the method is verified by continuous and complete extraction results of road in SAR images. © 2023 Chinese Institute of Electronics. All rights reserved.",morphology; multi-scale; road extraction; synthetic aperture radar (SAR); Extraction; Image enhancement; Radar imaging; Roads and streets; Synthetic aperture radar; Coarse to fine; Extraction method; Link probability; Multi-scales; Result integrations; Road extraction; Road extraction method; Road segments; Synthetic aperture radar; Synthetic aperture radar images; Morphology
Scopus,"Zhao, H.; Morgenroth, J.; Pearse, G.; Schindler, J.",A Systematic Review of Individual Tree Crown Detection and Delineation with Convolutional Neural Networks (CNN),,2023,,,,10.1007/s40725-023-00184-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151571890&doi=10.1007%2fs40725-023-00184-3&partnerID=40&md5=ee84152dddc835b185737212978c6747,"Purpose of Review: Crown detection and measurement at the individual tree level provide detailed information for accurate forest management. To efficiently acquire such information, approaches to conduct individual tree detection and crown delineation (ITDCD) using remotely sensed data have been proposed. In recent years, deep learning, specifically convolutional neural networks (CNN), has shown potential in this field. This article provides a systematic review of the studies that used CNN for ITDCD and identifies major trends and research gaps across six perspectives: accuracy assessment methods, data types, platforms and resolutions, forest environments, CNN models, and training strategies and techniques. Recent Findings: CNN models were mostly applied to high-resolution red–green–blue (RGB) images. When compared with other state-of-the-art approaches, CNN models showed significant improvements in accuracy. One study reported an increase in detection accuracy of over 11%, while two studies reported increases in F1-score of over 16%. However, model performance varied across different forest environments and data types. Several factors including data scarcity, model selection, and training approaches affected ITDCD results. Summary: Future studies could (1) explore data fusion approaches to take advantage of the characteristics of different types of remote sensing data, (2) further improve data efficiency with customised sample approaches and synthetic samples, (3) explore the potential of smaller CNN models and compare their learning efficiency with commonly used models, and (4) evaluate impacts of pre-training and parameter tunings. © 2023, The Author(s).",Crown delineation; Deep learning; Forestry; Instance segmentation; Object detection; Remote sensing; Tree detection
Scopus,"Bai, C.; Bai, X.; Wu, K.",A Review: Remote Sensing Image Object Detection Algorithm Based on Deep Learning,,2023,,,,10.3390/electronics12244902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180690509&doi=10.3390%2felectronics12244902&partnerID=40&md5=c69c6b84b7b736d5808db7464aea766a,"Target detection in optical remote sensing images using deep-learning technologies has a wide range of applications in urban building detection, road extraction, crop monitoring, and forest fire monitoring, which provides strong support for environmental monitoring, urban planning, and agricultural management. This paper reviews the research progress of the YOLO series, SSD series, candidate region series, and Transformer algorithm. It summarizes the object detection algorithms based on standard improvement methods such as supervision, attention mechanism, and multi-scale. The performance of different algorithms is also compared and analyzed with the common remote sensing image data sets. Finally, future research challenges, improvement directions, and issues of concern are prospected, which provides valuable ideas for subsequent related research. © 2023 by the authors.",comparative analysis of performance; deep learning; object detection; optical remote sensing image
Scopus,"Sun, A.; Ding, J.; Liu, J.; Zhou, H.; Zhang, J.; Zhang, P.; Dong, J.; Sun, Z.",Improved Detector Based on Yolov5 for Typical Targets on the Sea Surfaces,,2023,,,,10.3390/app13137695,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165208248&doi=10.3390%2fapp13137695&partnerID=40&md5=3a9a71c4b4b5ae38355d9fd1be7f367a,"Detection of targets on sea surfaces is an important area of application that can bring great benefits to the management and control systems in marine environments. However, there are few open-source datasets accessible for the purpose of object detection on seas and rivers. In this paper, a study is conducted on the improved detection algorithms based on the YOLOv5 model. The dataset for the tests contains ten categories of typical objects that are commonly seen in the contexts of seas, including ships, devices, and structures. Multiple augmentation methods are employed in the pre-processing of the input data, which are verified to be effective in enhancing the generalization ability of the algorithm. Moreover, a new form of the loss function is proposed that highlights the effects of the high-quality boxes during training. The results demonstrate that the adapted loss function contributes to a boost in the model performance. According to the ablation studies, the synthesized methods raise the inference accuracy by making up for several shortcomings of the baseline model for the detection tasks of single or multiple targets from varying backgrounds. © 2023 by the authors.",augmentation; loss function; marine applications; object detection
Scopus,"Yang, W.; Wu, J.; Zhang, J.; Gao, K.; Du, R.; Wu, Z.; Firkat, E.; Li, D.",Deformable convolution and coordinate attention for fast cattle detection,,2023,,,,10.1016/j.compag.2023.108006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235566&doi=10.1016%2fj.compag.2023.108006&partnerID=40&md5=9506ce73d2cc48ee4b7c48e76c691625,"Cattle detection is an important task in precision livestock farming, but it remains challenging due to the varying appearance and poses of cattle in different scenarios. In this paper, we propose a novel approach for fast cattle detection using deformable convolution and coordinate attention within YOLOv8, a SOTA object detection model. Our proposed method enhances the YOLOv8 architecture by introducing deformable convolution to capture more fine-grained spatial information and coordinate attention to emphasize important features in the detection process. We evaluate our method on a cattle dataset collected in a cattle farm and achieve superior performance compared to the baseline YOLOv8 and several SOTA object detection models. Specifically, our approach achieves a mean average precision (mAP) of 72.9% at 62.5 frames per second (FPS), which demonstrates its effectiveness and efficiency for fast cattle detection. By deploying our method on the farm's monitoring computer, our proposed approach has the potential to facilitate the development of automated cattle monitoring systems for improving animal welfare and farm management. © 2023 Elsevier B.V.",Computer vision; Precision agriculture; YOLOv8; Computer vision; Farms; Object detection; Object recognition; Precision agriculture; Detection models; Detection process; Fine grained; Important features; Objects detection; Precision Agriculture; Precision livestock farming; Spatial coordinates; Spatial informations; YOLOv8; cattle; computer vision; detection method; livestock farming; precision agriculture; Convolution
Scopus,"Huang, P.; Wang, S.; Chen, J.; Li, W.; Peng, X.",Lightweight Model for Pavement Defect Detection Based on Improved YOLOv7,,2023,,,,10.3390/s23167112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168732156&doi=10.3390%2fs23167112&partnerID=40&md5=875b84153b7c5e9f11c14382a69ca1bb,"Existing pavement defect detection models face challenges in balancing detection accuracy and speed while being constrained by large parameter sizes, hindering deployment on edge terminal devices with limited computing resources. To address these issues, this paper proposes a lightweight pavement defect detection model based on an improved YOLOv7 architecture. The model introduces four key enhancements: first, the incorporation of the SPPCSPC_Group grouped space pyramid pooling module to reduce the parameter load and computational complexity; second, the utilization of the K-means clustering algorithm for generating anchors, accelerating model convergence; third, the integration of the Ghost Conv module, enhancing feature extraction while minimizing the parameters and calculations; fourth, introduction of the CBAM convolution module to enrich the semantic information in the last layer of the backbone network. The experimental results demonstrate that the improved model achieved an average accuracy of 91%, and the accuracy in detecting broken plates and repaired models increased by 9% and 8%, respectively, compared to the original model. Moreover, the improved model exhibited reductions of 14.4% and 29.3% in the calculations and parameters, respectively, and a 29.1% decrease in the model size, resulting in an impressive 80 FPS (frames per second). The enhanced YOLOv7 successfully balances parameter reduction and computation while maintaining high accuracy, making it a more suitable choice for pavement defect detection compared with other algorithms. © 2023 by the authors.",CBAM convolution module; defect detection; Ghost Conv module; K-means; pavement defect detection; SPPCSPC_Group; YOLOv7; Defects; K-means clustering; Parameter estimation; Pavements; Semantics; CBAM convolution module; Defect detection; Detection models; Detection speed; Ghost conv module; K-means; Pavement defect detection; SPPCSPC_group; YOLOv7; article; calculation; feature extraction; k means clustering; Convolution
Scopus,"Liu, W.; Zhou, B.; Wang, Z.; Yu, G.; Yang, S.",FPPNet: A Fixed-Perspective-Perception Module for Small Object Detection Based on Background Difference,,2023,,,,10.1109/JSEN.2023.3263539,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153401485&doi=10.1109%2fJSEN.2023.3263539&partnerID=40&md5=1e8c9c3ce4c0932ea450a45b2ec94060,"A roadside sensing unit can provide over-the-horizon perception information for autonomous vehicles due to its high perception perspective. However, numerous challenges need to be overcome such as the missing detection of small objects and occluded objects. To this end, this study proposed a fixed perspective perception (FPP) module, which considered background subtraction and a fixed camera for small object detection. The proposed FPP module was divided into two parts: a grayscale background subtraction (GBS) submodule and a background-current image fusion (BCF) submodule. Specifically, the GBS submodule introduces background spatial information into a current frame, and the BCF submodule combines feature maps of a current frame and background by using channel attention. Moreover, we designed an object detection network called FPPNet which uses the FPP module to facilitate small object detection. Experimental results demonstrate that the FPPNet achieved 39.8% average precision small ( AP) and 65.7% AP in a Dair-V2X-I dataset. Futhermore, we conducted an extension of the FPP module to mainstream object detection networks such as CenterNet, Faster-Rcnn, and RetinaNet. Experimental results show that the proposed module can effectively improve small object detection accuracy of the networks mentioned earlier.  © 2001-2012 IEEE.",Background difference; feature fusion; object detection; roadside sensor; Cameras; Image fusion; Object detection; Object recognition; Roadsides; Semantics; Background differences; Features extraction; Features fusions; Gray scale; Location awareness; Objects detection; Roadside sensor; Small object detection; Submodules; Feature extraction
Scopus,"Liu, J.; Cai, Q.; Zou, F.; Zhu, Y.; Liao, L.; Guo, F.",BiGA-YOLO: A Lightweight Object Detection Network Based on YOLOv5 for Autonomous Driving,,2023,,,,10.3390/electronics12122745,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163856667&doi=10.3390%2felectronics12122745&partnerID=40&md5=597196b24393bf20f44ce8bbbc4cd2cb,"Object detection in autonomous driving scenarios has become a popular task in recent years. Due to the high-speed movement of vehicles and the complex changes in the surrounding environment, objects of different scales need to be detected, which places high demands on the performance of the network model. Additionally, different driving devices have varying performance capabilities, and a lightweight model is needed to ensure the stable operation of devices with limited computing power. To address these challenges, we propose a lightweight network called BiGA-YOLO based on YOLOv5. We design the Ghost-Hardswish Conv module to simplify the convolution operations and incorporate spatial coordinate information into feature maps using Coordinate Attention. We also replace the PANet structure with the BiFPN structure to enhance the expression ability of features through different weights during the process of fusing multi-scale feature maps. Finally, we conducted extensive experiments on the KITTI dataset, and our BiGA-YOLO achieved a mAP@0.5 of 92.2% and a mAP@0.5:0.95 of 68.3%. Compared to the baseline model YOLOv5, our proposed model achieved improvements of 1.9% and 4.7% in mAP@0.5 and mAP@0.5:0.95, respectively, while reducing the model size by 15.7% and the computational cost by 16%. The detection speed was also increased by 6.3 FPS. Through analysis and discussion of the experimental results, we demonstrate that our proposed model is superior, achieving a balance between detection accuracy, model size, and detection speed. © 2023 by the authors.",attention mechanism; BiFPN; CA; ghost module; lightweight network; object detection; YOLOv5
Scopus,"Wang, P.; Hu, Y.; Peng, S.; Zhou, L.",EMANet: An Ancient Text Detection Method Based on Enhanced-EfficientNet and Multidimensional Scale Fusion,,2024,,,,10.1109/JIOT.2024.3423667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198269841&doi=10.1109%2fJIOT.2024.3423667&partnerID=40&md5=0b1a97491460cc12352777d51ae48dfa,"Digitization through the Internet of Things (IoT) is a key measure for preserving ancient books. As a crucial aspect of digitization, text detection plays an essential role in the preservation and dissemination of ancient culture. Nevertheless, common detectors for ancient books struggle to meet the dual demands of speed and accuracy required by the IoT. Moreover, the robustness of these text detectors to capture complex features still needs to be strengthened. To tackle these issues, we introduce a new detector called EMANet. First, we substituted the feature extraction module in the core of EfficientNet-B3 with an improved Feature extraction (MBConv++) module to better capture the dependencies between channels. By incorporating this module, the network can concentrate on crucial features within the entire data sets. This significantly contributes to fulfilling the requirements for both speed and accuracy in ancient text detection. Additionally, we devised a multidimensional scale fusion (MDSF) module, effectively bolstering the scale robustness of the network. Finally, we construct a mobile ancient text digitization app suitable for the IoT. The proposed EMANet achieves an f-measure of 94.9% on shoot handwritten ancient book data set data sets, demonstrating its effectiveness. Simultaneously, we evaluate the generalization capability of EMANet using the public MTHv2 data sets. Results reveal that EMANet outperforms the majority of existing text detectors. Furthermore, our model demonstrates exceptional computational speed and precision in detection when deployed within the IoT framework, offering significant contributions to the holistic digitization of ancient texts.  © 2014 IEEE.",Ancient text detection; Enhanced-EfficientNet; Internet of Things (IoT); multidimensional scale fusion (MDSF); Extraction; Feature extraction; Ancient cultures; Ancient text detection; Detection methods; Digitisation; Enhanced-efficientnet; Features extraction; Multi dimensional; Multi-dimensional scale fusion; New detectors; Text detection; Internet of things
Scopus,"Wang, Y.; Tian, H.; Yin, T.; Song, Z.; Hauwa, A.S.; Zhang, H.; Gao, S.; Zhou, L.",The transmission line foreign body detection algorithm based on weighted spatial attention,,2024,,,,10.3389/fnbot.2024.1424158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198743784&doi=10.3389%2ffnbot.2024.1424158&partnerID=40&md5=547a8739a041e7bb0db267dd12820dc6,"Introduction: The secure operation of electric power transmission lines is essential for the economy and society. However, external factors such as plastic film and kites can cause damage to the lines, potentially leading to power outages. Traditional detection methods are inefficient, and the accuracy of automated systems is limited in complex background environments. Methods: This paper introduces a Weighted Spatial Attention (WSA) network model to address the low accuracy in identifying extraneous materials within electrical transmission infrastructure due to background texture occlusion. Initially, in the model preprocessing stage, color space conversion, image enhancement, and improved Large Selective Kernel Network (LSKNet) technology are utilized to enhance the model's proficiency in detecting foreign objects in intricate surroundings. Subsequently, in the feature extraction stage, the model adopts the dynamic sparse BiLevel Spatial Attention Module (BSAM) structure proposed in this paper to accurately capture and identify the characteristic information of foreign objects in power lines. In the feature pyramid stage, by replacing the feature pyramid network structure and allocating reasonable weights to the Bidirectional Feature Pyramid Network (BiFPN), the feature fusion results are optimized, ensuring that the semantic information of foreign objects in the power line output by the network is effectively identified and processed. Results: The experimental outcomes reveal that the test recognition accuracy of the proposed WSA model on the PL (power line) dataset has improved by three percentage points compared to that of the YOLOv8 model, reaching 97.6%. This enhancement demonstrates the WSA model's superior capability in detecting foreign objects on power lines, even in complex environmental backgrounds. Discussion: The integration of advanced image preprocessing techniques, the dynamic sparse BSAM structure, and the BiFPN has proven effective in improving detection accuracy and has the potential to transform the approach to monitoring and maintaining power transmission infrastructure. Copyright © 2024 Wang, Tian, Yin, Song, Hauwa, Zhang, Gao and Zhou.",BiFPN; BSAM; LSKNet; transmission lines; WSA; Automation; Complex networks; Electric lines; Electric power transmission networks; Image enhancement; Object detection; Power transmission; Semantics; Signal detection; Statistical tests; Textures; Bidirectional feature pyramid network; Bilevel; Bilevel spatial attention module; Feature pyramid; Large selective kernel network; Pyramid network; Spatial attention; Transmission-line; Weighted spatial attention; accuracy; algorithm; Article; artificial neural network; body weight gain; computer model; foreign body; human; interneuron; learning; learning algorithm; receptive field; satellite imagery; spatial attention; speech intelligibility; three-dimensional imaging; training; visual field; Electric power transmission
Scopus,"Chung, M.-A.; Wang, T.-H.; Lin, C.-W.",Advancing ESG and SDGs Goal 11: Enhanced YOLOv7-Based UAV Detection for Sustainable Transportation in Cities and Communities,,2023,,,,10.3390/urbansci7040108,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180248486&doi=10.3390%2furbansci7040108&partnerID=40&md5=135e847b03c842ce7a8a368d90120820,"Environmental, social, and governance issues have gained significant prominence recently, particularly with a growing emphasis on environmental protection. In the realm of heightened environmental concerns, unmanned aerial vehicles have emerged as pivotal assets in addressing transportation challenges with a sustainable perspective. This study focuses on enhancing unmanned aerial vehicles’ object detection proficiency within the realm of sustainable transportation. The proposed method refines the YOLOv7 E-ELAN model, tailored explicitly for traffic scenarios. Leveraging strides in deep learning and computer vision, the adapted model demonstrates enhancements in mean average precision, outperforming the original on the VisDrone2019 dataset. This approach, encompassing model component enhancements and refined loss functions, establishes an efficacious strategy for precise unmanned aerial vehicles object detection. This endeavor aligns seamlessly with environmental, social, and governance principles. Moreover, it contributes to the 11th Sustainable Development Goal by fostering secure urban spaces. As unmanned aerial vehicles have become integral to public safety and surveillance, enhancing detection algorithms cultivates safer environments for residents. Sustainable transport encompasses curbing traffic congestion and optimizing transportation systems, where unmanned aerial vehicle-based detection plays a pivotal role in managing traffic flow, thereby supporting extended Sustainable Development Goal 11 objectives. The efficient utilization of unmanned aerial vehicles in public transit significantly aids in reducing carbon footprints, corresponding to the “Environmental Sustainability” facet of Environmental, Social, and Governance principles. © 2023 by the authors.",environmental; object detection; social and governance (ESG); sustainable development goals (SDGs); sustainable transport; traffic monitoring; unmanned aerial vehicles (UAV); YOLOv7
Scopus,"Li, K.; Wang, Y.; Hu, Z.",Improved YOLOv7 for Small Object Detection Algorithm Based on Attention and Dynamic Convolution,,2023,,,,10.3390/app13169316,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169137554&doi=10.3390%2fapp13169316&partnerID=40&md5=c2856cf95ea4328f81b0bb91b277afe9,"The rapid advancement of deep learning has significantly accelerated progress in target detection. However, the detection of small targets remains challenging due to their susceptibility to size variations. In this paper, we address these challenges by leveraging the latest version of the You Only Look Once (YOLOv7) model. Our approach enhances the YOLOv7 model to improve feature preservation and minimize feature loss during network processing. We introduced the Spatial Pyramid Pooling and Cross-Stage Partial Channel (SPPCSPC) module, which combines the feature separation and merging ideas. To mitigate missed detections in small target scenarios and reduce noise impact, we incorporated the Coordinate Attention for Efficient Mobile Network Design (CA) module strategically. Additionally, we introduced a dynamic convolutional module to address misdetection and leakage issues stemming from significant target size variations, enhancing network robustness. An experimental validation was conducted on the FloW-Img sub-dataset provided by Okahublot. The results demonstrated that our enhanced YOLOv7 model outperforms the original network, exhibiting significant improvement in leakage reduction, with a mean Average Precision (mAP) of 81.1%. This represents a 5.2 percentage point enhancement over the baseline YOLOv7 model. In addition, the new model also has some advantages over the latest small-target-detection algorithms such as FCOS and VFNet in some respects. © 2023 by the authors.",attention module; dynamic convolution; small target detection; target detection techniques; YOLOv7 network model
Scopus,"Zhuang, L.; Jiang, T.; Qiu, M.; Wang, A.; Huang, Z.",Transformer Generates Conditional Convolution Kernels for End-to-End Lane Detection,,2024,,,,10.1109/JSEN.2024.3430234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199491645&doi=10.1109%2fJSEN.2024.3430234&partnerID=40&md5=4f384e9d177b71c07da754f1060a746a,"Environmental perception is crucial in autonomous driving technology, providing essential prior information for vehicle control and decision-making. Lane detection is gaining increasing attention as a critical component of environmental perception tasks. Mainstream frameworks rely on CNN-based architectures that often require preprocessing and postprocessing, complicating the implementation of end-to-end detection. Although Transformer-based structures address this issue from a global perspective, achieving higher accuracy remains challenging. This article proposes a novel Transformer-based end-to-end architecture, CondFormer, that processes each lane line separately, enhancing both accuracy and speed. First, we design a new Transformer structure to generate lane-by-lane parameter matrices from a global perspective instead of extracting features, efficiently constructing per-lane conditional convolution kernels. Second, to fully utilize multiscale information, CondFormer performs conditional convolution on the fused feature data. Then, each lane is further processed to obtain lane detection results using the ROW-wise method. Extensive experiments on the CULane, TuSimple, and CurveLanes datasets demonstrate that our method outperforms all Transformer-based end-to-end methods, offering a superior tradeoff between accuracy and speed. Our code is available at https://github.com/Zhuanglong2/Condformer. © 2001-2012 IEEE.",Autonomous driving; end to end; environmental perception; lane detection; Autonomous vehicles; Control system synthesis; Convolution; Economic and social effects; Feature extraction; Job analysis; Accuracy; Autonomous driving; End to end; Environmental perceptions; Features extraction; Kernel; Lane detection; Task analysis; Transformer; Decision making
Scopus,"Jagtap, S.; Chopade, N.B.; Deshmukh, V.; Dhavale, K.; Hange, P.","Object Detection, Tracking, and Prediction Analysis System for Sports Performance and Coaching",,2023,,,,10.1109/ICCUBEA58933.2023.10392158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187261408&doi=10.1109%2fICCUBEA58933.2023.10392158&partnerID=40&md5=85f82eafc291fee496f525f14fffc241,"Object detection and tracking systems have diverse real-world applications, including surveillance and security, autonomous vehicles, robotics, sports analysis, and medical imaging. These systems can detect and track objects of interest, providing valuable insights into various domains and enabling autonomous operation. Current object analysis systems often have limitations in accuracy and, efficiency, and realtime processing capabilities. These limitations include occlusion, motion blur, lighting conditions, scale and orientation, computational complexity, false positives and false negatives, and limited object types. These limitations can affect the applicability of object analysis systems in various domains and limit their ability to detect and track objects accurately. The proposed system aims to overcome these limitations by leveraging the powerful processing capabilities of Jetson Xavier and combining the Kalman filter and polynomial regression techniques for more accurate and efficient object analysis. The proposed system uses state-of-the-art computer vision techniques for object tracking and trajectory prediction. The proposed approach can be valuable in sports analysis, providing accurate and efficient object detection, tracking, and prediction analysis.  © 2023 IEEE.",Image pre-processing; Jetson Xavier NX module; Kalman filter; NumPy; Object detection; OpenCv; Computer vision; Forecasting; Kalman filters; Medical imaging; Object recognition; Sports; Tracking (position); Analysis system; Image preprocessing; Jetson xavy NX module; Numpy; Object analysis; Objects detection; Opencv; Processing capability; Sports analysis; Tracking and predictions; Object detection
Scopus,"Kaur, R.; Singh, S.",A comprehensive review of object detection with deep learning,,2022,,,,10.1016/j.dsp.2022.103812,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141923901&doi=10.1016%2fj.dsp.2022.103812&partnerID=40&md5=b525e123d0ca34e5a49bb4b79066534b,"In the realm of computer vision, Deep Convolutional Neural Networks (DCNNs) have demonstrated excellent performance. Video Processing, Object Detection, Image Segmentation, Image Classification, Speech Recognition and Natural Language Processing are some of the application areas of CNN. Object detection is the most crucial and challenging task of computer vision. It has numerous applications in the field of security, military, transportation and medical sciences. In this review, object detection and its different aspects have been covered in detail. With the gradual increase in the evolution of deep learning algorithms for detecting objects, a significant improvement in the performance of object detection models has been observed. However, this does not imply that the conventional object detection methods, which had been evolving for decades prior to the emergence of deep learning, had become outdated. There are some cases where conventional methods with global features are superior choice. This review paper starts with a quick overview of object detection followed by object detection frameworks, backbone convolutional neural network, and an overview of common datasets along with the evaluation metrics. Object detection problems and applications are also studied in detail. Some future research challenges in designing deep neural networks are discussed. Lastly, the performance of object detection models on PASCAL VOC and MS COCO datasets is compared and conclusions are drawn. © 2022 Elsevier Inc.",Computer vision; Conventional methods; Deep convolutional neural network; Deep learning; Object detection; Computer vision; Convolution; Convolutional neural networks; Deep neural networks; Image segmentation; Learning algorithms; Military applications; Natural language processing systems; Object recognition; Speech recognition; Video signal processing; Conventional methods; Convolutional neural network; Deep convolutional neural network; Deep learning; Detection models; Images segmentations; Objects detection; Performance; Segmentation images; Video processing; Object detection
Scopus,"Kochan, O.; Osolinskyi, O.; Sachenko, A.; Kochan, V.; Romanets, I.",Simulator of Microcontroller's Power Consumption,,2023,,,,10.1109/IDAACS58523.2023.10348884,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184811472&doi=10.1109%2fIDAACS58523.2023.10348884&partnerID=40&md5=779194e8d5542d7570a823d8ea5b1baa,"This paper proposed a simulator of power consumption for pulse consumers (in particular, microcontrollers, henceforth MC). It simulates the character of their power consumption. Due to the function of self-correction of error of circuit components, the simulator provides high accuracy. At the same time, the simulator provides a simulation of the MC operation at frequencies that considerably exceed the frequency of the MC clock generator. This eliminates frequency errors when calibrating the measuring channels of devices and systems designed to measure the MC's power consumption. Due to the calibration, these measurement channels can provide traceability of measurements when studying the MC's power consumption when executing instructions, commands, programs, and their fragments. The simulator can be a reference for creating the system of metrological service of the specified above measurement channels. © 2023 IEEE.",MCU; power consumption; pulse consumer; simulator; Clocks; Microcontrollers; Circuit components; Clock generator; Correction of errors; Frequency errors; High-accuracy; MCU; Measurement channels; Measuring channel; Pulse consumer; Self-correction; Electric power utilization
Scopus,"Sai Haritha, K.S.D.V.N.; Ravulapalli, G.; Sunny, N.",Deep Learning Virtual Assistant for Visually Impaired with Object Detection and Distance Estimation,,2023,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174259330&partnerID=40&md5=a4ae18e35bb029ff149c9bb7034098f8,"According to WHO (World Health Organization) 285 million individuals worldwide suffer from vision impairment. These metrics may triple in the coming 30 years. Thus, there is a need to develop a cost-effective guiding system for those people. This system provided a solution using large dataset COCO (Common Objects in Context) dataset which consists of 90 classes of real-time objects to cover all the day-to-day life objects It used the Single Shot Detector algorithm for object detection. It makes the work of blind people easier, and more efficient by transferring wireless voice-based feedback about whether an object is too close to them or at a safer distance. Google's text-to-speech engine is being used to provide audio-based output. It estimates the distance from person to object using depth extraction methodology with focal length as a scaling factor, real object width. This model provides a guiding system to help visually impaired people. © Grenze Scientific Society, 2023.",Depth Estimation; Google Text to Speech engine(gTTS); Object detection; Scaling Factor; Single Shot Detector (SSD); Cost effectiveness; Deep learning; E-learning; Engines; Large dataset; Object recognition; Speech recognition; Depth Estimation; Google text to speech engine; Google+; Guiding systems; Objects detection; Scaling factors; Single shot detector; Single-shot; Text-to-speech engines; Virtual assistants; Object detection
Scopus,"Borade, J.L.; Muddana, A.",Performance Analysis of Different Optimization Algorithms for Multi-Class Object Detection,,2023,,,,10.17762/ijritcc.v11i4.6400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163661741&doi=10.17762%2fijritcc.v11i4.6400&partnerID=40&md5=6b1e62af58f0f5316b7ccbcbae8b78b1,"Object recognition is a significant approach employed for recognizing suitable objects from the image. Various improvements, particularly in computer vision, are probable to diagnose highly difficult tasks with the assistance of local feature detection methodologies. Detecting multi-class objects is quite challenging, and many existing researches have worked to enhance the overall accuracy. But because of certain limitations like higher network loss, degraded training ability, improper consideration of features, less convergent and so on. The proposed research introduced a hybrid convolutional neural network (H-CNN) approach to overcome these drawbacks. The collected input images are pre-processed initially through Gaussian filtering to eradicate the noise and enhance the image quality. Followed by image preprocessing, the objects present in the images are localized using Grid Guided Localization (GGL I. The effective features are extracted from the localized objects using the AlexNet model. Different objects are classified by replacing the concluding softmax layer of AlexNet with Support Vector Regression (SVR) model. The losses present in the network model are optimized using the Improved Grey Wolf (IGW) optimization procedure. The performances of the proposed model are analyzed using PYTHON. Various datasets are employed, including MIT-67. PASCAL VOC2010. Microsoft (MS)-COCO and MSRC. The performances are analyzed by varying the loss optimization algorithms like improved Particle Swarm Optimization (IPSO), unproved Genetic Algorithm (IGA). and improved dragon fly algorithm (IDFA). unproved simulated annealing algorithm (ISAA) and improved bacterial foraging algorithm iIBFA). to choose the best algorithm. The proposed accuracy outcomes are attained as PASCAL VOC2010 (95.04%), MIT-67 dataset (96.02%). MSRC (97.37%), and MS COCO (94.53%), respectively. © 2023 Auricle Global Society of Education and Research. All rights reserved.",Gaussian filtering; Grey wolf optimization; Hybrid deep learning; improved optimization algorithms; Multi-class classification; Object recognition
Scopus,"Liu, P.; Xie, Z.; Li, T.",UCN-YOLOv5: Traffic Sign Object Detection Algorithm Based on Deep Learning,,2023,,,,10.1109/ACCESS.2023.3322371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174815830&doi=10.1109%2fACCESS.2023.3322371&partnerID=40&md5=fdffa952bf57cd77e800049433c8b7c8,"Traffic sign detection plays an important role in traffic safety and traffic management. In view of the complex and changeable environment and detection accuracy of traffic sign detection, this paper proposes UCN-YOLOv5 model based on the framework of YOLOv5.This model first replaces a new backbone network, which uses the core module RSU of U2Net to enhance the feature extraction of the network. Then, ConvNeXt-V2 is integrated, and the C3 module of its Block and YOLOv5 network is used to construct the C3_CN2 structure. The utilization of the proposed lightweight receptive field attention module LPFAConv in the Head Section represents a potential enhancement for the extraction of receptive field features. Finally, for small targets in traffic signs, Normalized Wasserstein Distance (NWD), which is insensitive to targets of different scales, is added to calculate the position loss function to replace the IoU metric to a certain extent, which further improves the detection ability of our model for traffic signs. Experiments on the TT100K dataset show that UCNYOLOv5 has excellent detection performance. Compared with the baseline model (Y0Lov5s, YOLOV5m, YOLOV5l), it improves the Map.5 index by 5.9 %, 4.9 % and 4.6 %; in the Map.5:.95 index, it is 4.4 %, 3.5 % and 2.8 % better. Moreover, the enhanced algorithm demonstrated favorable performance on the LISA and CCTSDB2021 traffic sign datasets. This research has important value for the accurate detection of traffic sign detection, and has guiding significance for in-depth research in related fields. © 2013 IEEE.",Convnext; Object detection; RFAConv; traffic sign detection; U2Net; YOLO; YOLOv5; Accident prevention; Deep learning; Extraction; Indium compounds; Job analysis; Object detection; Object recognition; Traffic signs; Convnext; Features extraction; Objects detection; RFAConv; Road; Task analysis; Traffic sign detection; U2net; Vehicle safety; YOLO; YOLOv5; Feature extraction
Scopus,"Heslinga, F.G.; Ruis, F.; Ballan, L.; van Leeuwen, M.C.; Masini, B.; van Woerden, J.E.; den Hollander, R.J.M.; Berndsen, M.; Baan, J.; Dijk, J.; Huizinga, W.",Leveraging temporal context in deep learning methodology for small object detection,,2023,,,,10.1117/12.2675589,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176500066&doi=10.1117%2f12.2675589&partnerID=40&md5=0edc05defad94692df8a72db88ee2ee9,"Automated object detection is becoming more relevant in a wide variety of applications in the military domain. This includes the detection of drones, ships, and vehicles in video and IR video. In recent years, deep learning-based object detection methods, such as YOLO, have shown to be promising in many applications for object detection. However, current methods have limited success when objects of interest are small in number of pixels, e.g. objects far away or small objects closer by. This is important, since accurate small object detection translates to early detection and the earlier an object is detected the more time is available for action. In this study, we investigate novel image analysis techniques that are designed to address some of the challenges of (very) small object detection by taking into account temporal information. We implement six methods, of which three are based on deep learning and use the temporal context of a set of frames within a video. The methods consider neighboring frames when detecting objects, either by stacking them as additional channels or by considering difference maps. We compare these spatio-temporal deep learning methods with YOLO-v8 that only considers single frames and two traditional moving object detection methods. Evaluation is done on a set of videos that encompasses a wide variety of challenges, including various objects, scenes, and acquisition conditions to show real-world performance. © 2023 SPIE. All rights reserved.",3D U-Net; Deep learning; Moving object detection; Small object detection; Spatio-temporal; T2-YOLO; YOLO; Deep learning; Learning systems; Military applications; Military vehicles; Object recognition; 3d U-net; Deep learning; Military domains; Moving-object detection; Object detection method; Objects detection; Small object detection; Spatio-temporal; T2-YOLO; YOLO; Object detection
Scopus,"Allebosch, G.; Van Hamme, D.; Veelaert, P.; Philips, W.",Efficient detection of crossing pedestrians from a moving vehicle with an array of cameras,,2023,,,,10.1117/1.OE.62.3.031210,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151674361&doi=10.1117%2f1.OE.62.3.031210&partnerID=40&md5=068e7316c94ae84dc1853ad9fe889ab7,"We describe a method for detecting crossing pedestrians and, in general, any object that is moving perpendicular to the driving direction of the vehicle. This is achieved by combining video snapshots from multiple cameras that are placed in a linear configuration and from multiple time instances. We demonstrate that the proposed array configuration imposes tight constraints on the expected disparity of static objects in a certain image region for a given camera pair. These regions are distinct for different camera pairs. In that manner, static regions can generally be distinguished from moving targets throughout the entire field of view when analyzing enough pairs, requiring only straightforward image processing techniques. On a self-captured dataset with crossing pedestrians, our proposed method reaches an F1 detection score of 83.66% and a mean average precision (MAP) of 84.79% on an overlap test when used stand-alone, being processed at 59 frames per second without GPU acceleration. When combining it with the Yolo V4 object detector in cooperative fusion, the proposed method boosts the maximal F1 scores of this detector on this same dataset from 87.86% to 92.68% and the MAP from 90.85% to 94.30%. Furthermore, combining it with the lower power Yolo-Tiny V4 detector in the same way yields F1 and MAP increases from 68.57% to 81.16% and 72.32% to 85.25%, respectively.  © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.",autonomous vehicles; epipolar geometry; motion segmentation; multicamera; pedestrian detection; Acceleration; Cameras; Motion analysis; Object detection; Pedestrian safety; Statistical tests; Autonomous Vehicles; Efficient detection; Epipolar geometry; Linear configuration; Motion segmentation; Moving vehicles; Multi-cameras; Multiple cameras; Pedestrian detection; Video snapshots; Autonomous vehicles
Scopus,"Xie, B.; Yang, L.; Wei, A.; Weng, X.; Li, B.",MuTrans: Multiple Transformers for Fusing Feature Pyramid on 2D and 3D Object Detection,,2023,,,,10.1109/TIP.2023.3299190,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166734491&doi=10.1109%2fTIP.2023.3299190&partnerID=40&md5=1f3a190645da6b52e32eb9df5036590c,"One of the major components of the neural network, the feature pyramid plays a vital part in perception tasks, like object detection in autonomous driving. But it is a challenge to fuse multi-level and multi-sensor feature pyramids for object detection. This paper proposes a simple yet effective framework named MuTrans (Mu ltiple Trans formers) to fuse feature pyramid in single-stream 2D detector or two-stream 3D detector. The MuTrans based on encoder-decoder focuses on the significant features via multiple Transformers. MuTrans encoder uses three innovative self-attention mechanisms: S patial-wise B oxAlign attention (SB) for low-level spatial locations, C ontext-wise A ffinity attention (CA) for high-level context information, and high-level attention for multi-level features. Then MuTrans decoder processes these significant proposals including the RoI and context affinity. Besides, the L ow and H igh-level F usion (LHF) in the encoder reduces the number of computational parameters. And the Pre-LN is utilized to accelerate the training convergence. LHF and Pre-LN are proven to reduce self-attention's computational complexity and slow training convergence. Our result demonstrates the higher detection accuracy of MuTrans than that of the baseline method, particularly in small object detection. MuTrans demonstrates a 2.1 higher detection accuracy on APS index in small object detection on MS-COCO 2017 with ResNeXt-101 backbone, a 2.18 higher 3D detection accuracy (moderate difficulty) for small object-pedestrian on KITTI, and 6.85 higher RC index (Town05 Long) on CARLA urban driving simulator platform.  © 1992-2012 IEEE.",feature pyramid; object detection; sensor fusion; Transformers; C (programming language); Decoding; Object detection; Object recognition; Signal encoding; Three dimensional displays; 2D objects; Decoding; Detection accuracy; Feature pyramid; Features extraction; Multilevels; Objects detection; Small object detection; Three-dimensional display; Transformer; Feature extraction
Scopus,"Bożko, A.; Ambroziak, L.",Influence of Insufficient Dataset Augmentation on IoU and Detection Threshold in CNN Training for Object Detection on Aerial Images,,2022,,,,10.3390/s22239080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143817129&doi=10.3390%2fs22239080&partnerID=40&md5=1f839fcaeecc1748a3467609c22a3581,"The objects and events detection tasks are being performed progressively often by robotic systems like unmanned aerial vehicles (UAV) or unmanned surface vehicles (USV). Autonomous operations and intelligent sensing are becoming standard in numerous scenarios such as supervision or even search and rescue (SAR) missions. The low cost of autonomous vehicles, vision sensors and portable computers allows the incorporation of the deep learning, mainly convolutional neural networks (CNN) in these solutions. Many systems meant for custom purposes rely on insufficient training datasets, what may cause a decrease of effectiveness. Moreover, the system’s accuracy is usually dependent on the returned bounding boxes highlighting the supposed targets. In desktop applications, precise localisation might not be particularly relevant; however, in real situations, with low visibility and non-optimal camera orientation, it becomes crucial. One of the solutions for dataset enhancement is its augmentation. The presented work is an attempt to evaluate the influence of the training images augmentation on the detection parameters important for the effectiveness of neural networks in the context of object detection. In this research, network appraisal relies on the detection confidence and bounding box prediction accuracy (IoU). All the applied image modifications were simple pattern and colour alterations. The obtained results imply that there is a measurable impact of the augmentation process on the localisation accuracy. It was concluded that a positive or negative influence is related to the complexity and variability of the objects classes. © 2022 by the authors.","aerial images; data augmentation; deep neural networks; image classification; object detection; unmanned aerial vehicle; Neural Networks, Computer; Aircraft detection; Antennas; Convolution; Convolutional neural networks; Deep neural networks; Image classification; Microcomputers; Object recognition; Aerial images; Aerial vehicle; Bounding-box; Convolutional neural network; Data augmentation; Detection threshold; Images classification; Neural networks trainings; Objects detection; Unmanned aerial vehicle; Object detection"
Scopus,"Cao, S.; Jin, Y.; Trautmann, T.; Liu, K.",Design and Experiments of Autonomous Path Tracking Based on Dead Reckoning,,2023,,,,10.3390/app13010317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145691058&doi=10.3390%2fapp13010317&partnerID=40&md5=a8ac1bbba3b83594be2102bf7289cb52,"Path tracking is an important component of autonomous driving and most current path tracking research is based on different positioning sensors, such as GPS, cameras, and LIDAR. However, in certain extreme cases (e.g., in tunnels or indoor parking lots), if these sensors are unavailable, achieving accurate path tracking remains a problem that is worthy of study. This paper addresses this problem by designing a dead reckoning method that is solely reliant on wheel speed for localization. Specifically, a differential drive model is first used for estimating the current relative vehicle position in real time by rear wheel speed, and the deviation between the current path and the reference path is then calculated using the pure pursuit algorithm as a means of obtaining the target steering wheel angle and vehicle speed. The steering wheel and vehicle speed signals are then output by two PID controllers in order to control the vehicle, and the automatic driving path tracking is ultimately realized. Through exhaustive tests and experiments, the stop position error and tracking process error are compared under different conditions, and the effects of vehicle speed, look-ahead distance, starting position angle, and driving mode on tracking accuracy are analyzed. The experimental results show the average error of the end position to be 0.26 m, 0.383 m, and 0.505 m when using BMW-i3 to drive one lap automatically at speeds of 5 km/h, 10 km/h, and 15 km/h in a test area with a perimeter of approximately 200 m. © 2022 by the authors.",autonomous driving; dead reckoning; differential drive kinematics; pure pursuit
Scopus,"Lele, A.S.; Fang, Y.; Anwar, A.; Raychowdhury, A.",Bio-mimetic high-speed target localization with fused frame and event vision for edge application,,2022,,,,10.3389/fnins.2022.1010302,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143730328&doi=10.3389%2ffnins.2022.1010302&partnerID=40&md5=ce82a68f26347a16db4e930f2a4cd799,"Evolution has honed predatory skills in the natural world where localizing and intercepting fast-moving prey is required. The current generation of robotic systems mimics these biological systems using deep learning. High-speed processing of the camera frames using convolutional neural networks (CNN) (frame pipeline) on such constrained aerial edge-robots gets resource-limited. Adding more compute resources also eventually limits the throughput at the frame rate of the camera as frame-only traditional systems fail to capture the detailed temporal dynamics of the environment. Bio-inspired event cameras and spiking neural networks (SNN) provide an asynchronous sensor-processor pair (event pipeline) capturing the continuous temporal details of the scene for high-speed but lag in terms of accuracy. In this work, we propose a target localization system combining event-camera and SNN-based high-speed target estimation and frame-based camera and CNN-driven reliable object detection by fusing complementary spatio-temporal prowess of event and frame pipelines. One of our main contributions involves the design of an SNN filter that borrows from the neural mechanism for ego-motion cancelation in houseflies. It fuses the vestibular sensors with the vision to cancel the activity corresponding to the predator's self-motion. We also integrate the neuro-inspired multi-pipeline processing with task-optimized multi-neuronal pathway structure in primates and insects. The system is validated to outperform CNN-only processing using prey-predator drone simulations in realistic 3D virtual environments. The system is then demonstrated in a real-world multi-drone set-up with emulated event data. Subsequently, we use recorded actual sensory data from multi-camera and inertial measurement unit (IMU) assembly to show desired working while tolerating the realistic noise in vision and IMU sensors. We analyze the design space to identify optimal parameters for spiking neurons, CNN models, and for checking their effect on the performance metrics of the fused system. Finally, we map the throughput controlling SNN and fusion network on edge-compatible Zynq-7000 FPGA to show a potential 264 outputs per second even at constrained resource availability. This work may open new research directions by coupling multiple sensing and processing modalities inspired by discoveries in neuroscience to break fundamental trade-offs in frame-based computer vision1. Copyright © 2022 Lele, Fang, Anwar and Raychowdhury.",accuracy-speed tradeoff; design space exploration; ego-motion cancelation; event camera; high-speed target tracking; hybrid neural network; neuromorphic vision; retinomorphic systems; accuracy speed tradeoff; Article; artificial neural network; biomimetics; computer vision; controlled study; convolutional neural network; deep learning; design space exploration; drone; ego motion cancelation; feature extraction; frame based computer vision; high speed target localization; high speed target tracking; house fly; hybrid neural network; insect; neuromorphic vision; noise; nonhuman; predation; predator; prey; primate; retinomorphic system; robotics; simulation; spatiotemporal analysis; spiking neural network; temporal analysis; vestibular system
Scopus,"Li, G.; Zhu, D.",Research on road defect detection based on improved YOLOv8,,2023,,,,10.1109/ITAIC58329.2023.10408744,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186065191&doi=10.1109%2fITAIC58329.2023.10408744&partnerID=40&md5=3103eecf1c217e96e7acbbaf6606e8cc,"Currently, the use of autonomous driving technology in China is generally limited by strict lane division and road conditions. In contrast to Tesla's vision-based autonomous driving technology, autonomous driving in China generally relies on radar signal reflection for positioning. As target detection algorithms continue to advance, it is a future trend to introduce computer vision methods into autonomous driving. This article proposes an improved YOLOv8 network model for research using relevant Chinese road datasets. By introducing a better backbone network, convnextv2, based on YOLOv8, it can effectively extract features from the data. Additionally, by introducing the novel WIOU computation function, it can better calculate the rectangular boxes. In this experiment, the improved YOLOv8-ATE achieved an average precision of mAPO.5 and mAPO.5:0.95, which were 18.1 % and 19% higher, respectively, compared to YOLOv8-base. The proposed YOLOv8-ATE model can effectively detect road defects and provide theoretical and technical support for future visual research in autonomous driving. © 2023 IEEE.",Attention mechanism; Computer vision; Object detection; Road defect detection; YOLOv8; Autonomous vehicles; Computer vision; Defects; Roads and streets; Attention mechanisms; Autonomous driving; Defect detection; Objects detection; Radar signals; Road condition; Road defect detection; Signal reflection; Vision based; YOLOv8; Object detection
Scopus,"Nahata, D.; Othman, K.",Exploring the challenges and opportunities of image processing and sensor fusion in autonomous vehicles: A comprehensive review,,2023,,,,10.3934/electreng.2023016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177487365&doi=10.3934%2felectreng.2023016&partnerID=40&md5=454aadc5174e3a72b39be0b6923c9f26,"Autonomous vehicles are at the forefront of future transportation solutions, but their success hinges on reliable perception. This review paper surveys image processing and sensor fusion techniques vital for ensuring vehicle safety and efficiency. The paper focuses on object detection, recognition, tracking, and scene comprehension via computer vision and machine learning methodologies. In addition, the paper explores challenges within the field, such as robustness in adverse weather conditions, the demand for real-time processing, and the integration of complex sensor data. Furthermore, we examine localization techniques specific to autonomous vehicles. The results show that while substantial progress has been made in each subfield, there are persistent limitations. These include a shortage of comprehensive large-scale testing, the absence of diverse and robust datasets, and occasional inaccuracies in certain studies. These issues impede the seamless deployment of this technology in real-world scenarios. This comprehensive literature review contributes to a deeper understanding of the current state and future directions of image processing and sensor fusion in autonomous vehicles, aiding researchers and practitioners in advancing the development of reliable autonomous driving systems. © 2023 the Author(s), licensee AIMS Press.",adverse weather; autonomous parking; autonomous vehicles; image processing; localization; sensor fusion; Computer vision; Large datasets; Object detection; Object recognition; Vehicle safety; Adverse weather; Autonomous Parking; Autonomous Vehicles; Fusion techniques; Images processing; Localisation; Paper surveys; Review papers; Sensor fusion; Vehicle efficiency; Autonomous vehicles
Scopus,"Liu, R.; Li, H.; Lv, Z.",Modeling Methods of 3D Model in Digital Twins,,2023,,,,10.32604/cmes.2023.023154,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148225357&doi=10.32604%2fcmes.2023.023154&partnerID=40&md5=ecef5d3967c4f5d9993f51d700242105,"To understand the current application and development of 3D modeling in Digital Twins (DTs), abundant literatures on DTs and 3D modeling are investigated by means of literature review. The transition process from 3D modeling to DTs modeling is analyzed, as well as the current application of DTs modeling in various industries. The application of 3D DTs modeling in the fields of smart manufacturing, smart ecology, smart transportation, and smart buildings in smart cities is analyzed in detail, and the current limitations are summarized. It is found that the 3D modeling technology in DTs has broad prospects for development and has a huge impact on all walks of life and even human lifestyles. At the same time, the development of DTs modeling relies on the development and support capabilities of mature technologies such as Big Data, Internet of Things, Cloud Computing, Artificial Intelligence, and game technology. Therefore, although some results have been achieved, there are still limitations. This work aims to provide a good theoretical support for the further development of 3D DTs modeling. © 2023 Tech Science Press. All rights reserved.",3D modeling; Digital twins; smart buildings; smart city; smart manufacturing; Computer games; Smart city; Three dimensional computer graphics; 'current; 3d modeling technologies; 3D models; 3d-modeling; Current limitation; Literature reviews; Model method; Smart manufacturing; Support capability; Transition process; 3D modeling
Scopus,"Balasubramaniam, A.; Pasricha, S.",Object Detection in Autonomous Cyber-Physical Vehicle Platforms: Status and Open Challenges,,2023,,,,10.1007/978-3-031-28016-0_17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195994005&doi=10.1007%2f978-3-031-28016-0_17&partnerID=40&md5=c6b2756d5081a2259123a809a643b55e,"Object detection is a computer vision task that has become an integral part of many consumer applications today such as surveillance and security systems, mobile text recognition, and diagnosing diseases from MRI/CT scans. Object detection is also one of the critical components to support autonomous driving. Autonomous vehicles rely on the perception of their surroundings to ensure safe and robust driving performance. This perception system uses object detection algorithms to accurately determine objects such as pedestrians, vehicles, traffic signs, and barriers in the vehicle’s vicinity. Deep learning-based object detectors play a vital role in finding and localizing these objects in real-time. This chapter discusses the state-of-the-art in object detectors and open challenges for their integration into autonomous vehicles. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.",Autonomous vehicles; Computer vision; Model compression; Object detection
Scopus,"Ji, W.; Li, J.; Bian, C.; Zhou, Z.; Zhao, J.; Yuille, A.; Cheng, L.",Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline,,2023,,,,10.1109/CVPR52729.2023.00112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173930754&doi=10.1109%2fCVPR52729.2023.00112&partnerID=40&md5=37aa867d380fe2a242b0cfffd0c4755b,"Robust and reliable semantic segmentation in complex scenes is crucial for many real-life applications such as autonomous safe driving and nighttime rescue. In most approaches, it is typical to make use of RGB images as input. They however work well only in preferred weather conditions; when facing adverse conditions such as rainy, overexposure, or low-light, they often fail to deliver satisfactory results. This has led to the recent investigation into multispectral semantic segmentation, where RGB and thermal infrared (RGBT) images are both utilized as input. This gives rise to significantly more robust segmentation of image objects in complex scenes and under adverse conditions. Nevertheless, the present focus in single RGBT image input restricts existing methods from well addressing dynamic real-world scenes. Motivated by the above observations, in this paper, we set out to address a relatively new task of semantic segmentation of multispectral video input, which we refer to as Multispectral Video Semantic Segmentation, or MVSS in short. An in-house MVSeg dataset is thus curated, consisting of 738 calibrated RGB and thermal videos, accompanied by 3,545 fine-grained pixel-level semantic annotations of 26 categories. Our dataset contains a wide range of challenging urban scenes in both daytime and nighttime. Moreover, we propose an effective MVSS baseline, dubbed MVNet, which is to our knowledge the first model to jointly learn semantic representations from multispectral and temporal contexts. Comprehensive experiments are conducted using various semantic segmentation models on the MVSeg dataset. Empirically, the engagement of multispectral video input is shown to lead to significant improvement in semantic segmentation; the effectiveness of our MVNet baseline has also been verified. © 2023 IEEE.",Scene analysis and understanding
Scopus,"Thirumurugan, S.; Pradeepan, M.L.; Sundhar, K.; Dhanapal, R.",Literature Review of Drowsiness Identification and Alert System for Real Time Network,,2023,,,,10.1109/ICICT57646.2023.10134489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163533810&doi=10.1109%2fICICT57646.2023.10134489&partnerID=40&md5=4c7057987ef3f2f848630f84c9f87566,"Road mishaps are more likely to be caused by tired drivers. Driving for extended periods of time, driving while sleep deprived, and driving while under the influence of drugs, alcohol, or medication are just a few of the factors that contribute to driver fatigue. In developing nations, where the death rate from traffic injuries is above 85% and the disability-adjusted life expectancy is above 90%, traffic injuries and fatalities are a major public health concern. One of the safeguards to avoid accidents is drowsiness detection. Based on the parameters each method uses, drowsiness detection techniques created for defence purposes are divided into four groups. These groups are: Subjective-based, Vehicle-based, Behavioral or Visual-based, Mouth Tracking, Physiological or Non visual-based. The remote healthcare monitoring and drowsiness alert system is a network of interconnected vehicles that communicate with each other and with the surrounding environment using advanced communication technologies. It leverages this network to collect and transmit vital health data from drivers to healthcare professionals who can then use the information's to monitor their health status and provide timely medical interventions when necessary. © 2023 IEEE.",Alert system; differential algorithm; Drowsiness Detection; Healthcare monitoring; Quality of Service; Health care; Vehicle to vehicle communications; Alert systems; Differential algorithms; Driver fatigue; Drowsiness detection; Healthcare monitoring; Literature reviews; Quality-of-service; Real time network; Time drivings; Traffic injuries; Quality of service
Scopus,"Choudhary, A.; Mishra, R.K.; Fatima, S.; Panigrahi, B.K.",Multi-input CNN based vibro-acoustic fusion for accurate fault diagnosis of induction motor,,2023,,,,10.1016/j.engappai.2023.105872,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147193484&doi=10.1016%2fj.engappai.2023.105872&partnerID=40&md5=23971dd79737e02a29853d04f1102ca9,"Induction motor (IM) is a highly efficient prime mover in industrial applications. To maintain an uninterrupted operation, accurate fault diagnosis system of IM is required. It can help to improve operational safety and prevent unexpected economic losses. The traditional diagnosis methods are less capable of dealing with real-time and varying working environments. This paper presents a vibro-acoustic fusion technique for an accurate fault diagnosis under varying working conditions. The suggested method fuses the features of vibration and acoustic signals using Multi Input-Convolutional Neural Network (MI-CNN) technique. At first, raw vibration and acoustic signals are acquired at varying speeds and converted into a time–frequency spectrum using the Constant Q-Non-Stationary Gabor Transform (CQ-NSGT). Thereafter, a MI-CNN-based vibro-acoustic fusion is adopted for the fusion of vibration and acoustic features. Six distinct motor conditions are utilized to compute the effectiveness of the suggested MI-CNN model. Further, two additional datasets, i.e., bearing and the gearbox datasets, are employed to validate the suggested approach. The experimental results demonstrate that the suggested methodology is accurate and reliable for IMs and other components of rotating machine. © 2023 Elsevier Ltd",CQ-NSGT; Fault detection; Induction Motor; MI-CNN; Vibro-acoustic fusion; Acoustic waves; Failure analysis; Induction motors; Losses; Constant Q-non-stationary gabor transform; Convolutional neural network; Faults detection; Gabor transform; Inductions motors; Multi input-convolutional neural network; Multiinput; Nonstationary; Vibro-acoustic fusion; Vibroacoustics; Fault detection
Scopus,"Kurian, M.Z.",Methodological Insights Towards Leveraging Performance in Video Object Tracking and Detection,,2023,,,,10.14569/IJACSA.2023.0140851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170647212&doi=10.14569%2fIJACSA.2023.0140851&partnerID=40&md5=4120aed07d5cefd594a6a349ab5d6de0,"Video Object Detection and Tracking (VODT), one of its integral operations of surveillance system in present time, mechanizes a way to identify and track the target object autonomously and seamlessly within its visual field. However, the challenges associated with video feeding are immensely high, and the scene context is out of human control, posing an impediment to a successful model of VODT. The presented work has discussed about effectiveness of existing VODT approaches considering its identified taxonomies viz. satellite based, remote sensing-based, unmanned-based, Real-time Tracking based, behavioral analysis and event detection based, integration of multiple data sources, and privacy and ethics. Further, research trend associated with cumulative publications and evolving methods to realize the frequently used methodologies in VODT. Further, the results of review showcase that there is prominent research gap of manifold attributes that demands to be addressed for improving performance of VODT. © (2023), (Science and Information Organization). All Rights Reserved.",Object detection; object tracking; surveillance system; video; video feed; visual field; Object recognition; Remote sensing; Security systems; Tracking (position); Vision; Object detection and tracking; Object Tracking; Objects detection; Performance; Surveillance systems; Video; Video feed; Video object detections; Video object tracking; Visual fields; Object detection
Scopus,"Iftikhar, S.; Asim, M.; Zhang, Z.; Muthanna, A.; Chen, J.; El-Affendi, M.; Sedik, A.; Abd El-Latif, A.A.",Target Detection and Recognition for Traffic Congestion in Smart Cities Using Deep Learning-Enabled UAVs: A Review and Analysis,,2023,,,,10.3390/app13063995,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152262021&doi=10.3390%2fapp13063995&partnerID=40&md5=e5e48d87855d72be72b9d060fd7b64c8,"In smart cities, target detection is one of the major issues in order to avoid traffic congestion. It is also one of the key topics for military, traffic, civilian, sports, and numerous other applications. In daily life, target detection is one of the challenging and serious tasks in traffic congestion due to various factors such as background motion, small recipient size, unclear object characteristics, and drastic occlusion. For target examination, unmanned aerial vehicles (UAVs) are becoming an engaging solution due to their mobility, low cost, wide field of view, accessibility of trained manipulators, a low threat to people’s lives, and ease to use. Because of these benefits along with good tracking effectiveness and resolution, UAVs have received much attention in transportation technology for tracking and analyzing targets. However, objects in UAV images are usually small, so after a neural estimation, a large quantity of detailed knowledge about the objects may be missed, which results in a deficient performance of actual recognition models. To tackle these issues, many deep learning (DL)-based approaches have been proposed. In this review paper, we study an end-to-end target detection paradigm based on different DL approaches, which includes one-stage and two-stage detectors from UAV images to observe the target in traffic congestion under complex circumstances. Moreover, we also analyze the evaluation work to enhance the accuracy, reduce the computational cost, and optimize the design. Furthermore, we also provided the comparison and differences of various technologies for target detection followed by future research trends. © 2023 by the authors.",cascade R-CNN; deep learning; faster R-CNN; target detection; traffic congestion; unmanned aerial vehicles; YOLO versions
Scopus,"Doly, M.; Al-Khowarizmi, N.; Rahmat, R.F.; Lubis, A.R.; Lubis, M.",The Role of Faster R-CNN Algorithm in the Internet of Things to Detect Mask Wearing: The Endemic Preparations,,2023,,,,10.24425/ijet.2023.147689,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179004846&doi=10.24425%2fijet.2023.147689&partnerID=40&md5=8ddb131edf3c17cd26703fe2ca972e30,"Faster R-CNN is an algorithm development that continuously starts from CNN then R-CNN and Faster R-CNN. The development of the algorithm is needed to test whether the heuristic algorithm has optimal provisions. Broadly speaking, faster R-CNN is included in algorithms that are able to solve neural network and machine learning problems to detect a moving object. One of the moving objects in the current phenomenon is the use of masks. Where various countries in the world have issued endemic orations after the Covid 19 pandemic occurred. Detection tool has been prepared that has been tested at the mandatory mask door, namely for mask users. In this paper, the role of the Faster R-CNN algorithm has been carried out to detect masks poured on Internet of Thinks (IoT) devices to automatically open doors for standard mask users. From the results received that testing on the detection of moving mask objects when used reaches 100% optimal at a distance of 0.5 to 1 meter and 95% at a distance of 1.5 to 2 meters so that the process of sending detection signals to IoT devices can be carried out at a distance of 1 meter at the position mask users to automatic doors. © 2023 Polish Academy of Sciences. All rights reserved.",Endemic; Faster R-CNN; IoT; Mask; Heuristic algorithms; Internet of things; Machine learning; Object detection; Wear of materials; 'current; Algorithms development; Detection tools; Endemic; Fast R-CNN; Heuristics algorithm; Internet of think; Machine learning problem; Moving objects; Neural network learning; Optimization
Scopus,"Lai, H.; Liu, B.; Kan, H.Y.; Lam, C.-T.; Im, S.K.",YOLOv8-lite: An Interpretable Lightweight Object Detector for Real-Time UAV Detection,,2023,,,,10.1109/ICCC59590.2023.10507293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192976083&doi=10.1109%2fICCC59590.2023.10507293&partnerID=40&md5=b40b0f39e16c18901a826be516f5f9e5,"UAV detection is an important problem in sensitive areas involving security and privacy. This paper proposes an interpretable lightweight model designed explicitly for the real-time detection of UAVs, called YOLOv8-lite. By employing a high-speed YOLOv8 model and Depthwise convolution, the model performs better than the original YOLOv8 with fewer parameters in the Det-fly dataset. The proposed YOLOv8-lite achieves impressive results with 0.98 AP50 and 0.68 AP0.5:0.95 on the test set, using only 2 million parameters. Meanwhile, YOLOv8-lite shows good results in solving the challenges of detecting UAVs against various environmental backgrounds. In addition, interpretability methods are applied to illustrate the factors contributing to the effectiveness and generalization capability of the model. The code for the model is available: https://github.com/hawkinglai/uav-det.  © 2023 IEEE.",Depthwise convolution; Interpretable machine learning; Object detection; UAV detection; YOLOv8; Aircraft detection; Machine learning; Object detection; Object recognition; Unmanned aerial vehicles (UAV); Depthwise convolution; Interpretable machine learning; Machine-learning; Object detectors; Objects detection; Real- time; Security and privacy; Sensitive area; UAV detection; YOLOv8; Convolution
Scopus,"Mahaur, B.; Kumar, A.",Designing an Efficient Object Detection Model for Autonomous Driving Applications,,2023,,,,10.1109/SPIN57001.2023.10117387,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160006276&doi=10.1109%2fSPIN57001.2023.10117387&partnerID=40&md5=fa8fb962a1fa589d1bdca7342f59fba7,"The detection of road objects plays an essential role in the development of autonomous vehicles in intelligent transportation systems, which has become an emerging field in deep learning. Several objects on the road, like vehicles, pedestrians, etc., are necessary to be accurately identified, which guarantees the safety of other people and vehicles in the surroundings. In this article, we aim to design an efficient object detection model for autonomous driving systems. To achieve this, we investigate the recently developed YOLOv7 and optimize the same for improving the detection performance to satisfy the realtime safety requirements of autonomous vehicles. We perform extensive experimentation and demonstrate the effectiveness of our method on the BDDIOOK dataset. Experimental results show that our proposed method increases the detection accuracy to 82.6% and inference speed to 97.2 FPS compared to the baselines, with no additional increase in model complexity.  © 2023 IEEE.",Architectural Design; Autonomous Vehicles; Efficient Model; Object Detection; YOLOv7; Autonomous vehicles; Deep learning; Intelligent systems; Intelligent vehicle highway systems; Object recognition; Pedestrian safety; Roads and streets; Autonomous driving; Autonomous Vehicles; Detection models; Detection performance; Driving systems; Efficient model; Efficient object detections; Intelligent transportation systems; Objects detection; YOLOv7; Object detection
Scopus,"Oliskevich, M.; Danchuk, V.; Bakulich, O.",Information System for Energy-Saving Vehicle Traffic Control on the Highway,,2023,,,,10.1007/978-3-031-46874-2_31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180622824&doi=10.1007%2f978-3-031-46874-2_31&partnerID=40&md5=76dc3d863ae9e2287e491a21fb738167,"The paper is devoted to the problem of information provision of energy-efficient automatic control of vehicles on highways. Comprehensive measures are proposed for the transmission of data flow to on-board controllers of vehicles, which makes it possible to avoid radio signal interference. The main technical means of the proposed system are unmanned aerial vehicles equipped with emitters and direction finders of radio signals, which are able to communicate with motor vehicles, with each other, as well as with immovable roadside infrastructure objects. These objects play the role of markers for the automatic flight of drones along the highway and means of saving fragments of the data stream. Every vehicle on the highway has access to the 3D trajectory and road conditions at least 4 km ahead on the chosen route, thanks to the use of markers with a constant distance between them and drones that fly precisely along the highway, against the direction of traffic. Simulation modeling of information system functioning with different numbers of markers was carried out. The deviation of the average cruising speed forecast from the value obtained as a result of simulation was determined. It was established that the accuracy of forecasting significantly depends on the horizon and the number of markers. Reducing the horizon to less than 1 km and the number of markers to less than 25 is impractical, as the error can exceed 50%. At the same time, the highest prediction accuracy was achieved at the level of 2.8%. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Autonomous vehicles; Drones; Energy saving; Radars; Antennas; Autonomous vehicles; Energy efficiency; Forecasting; Information systems; Information use; Radio transmission; Traffic control; Vehicle transmissions; Aerial vehicle; Autonomous Vehicles; Dataflow; Energy  savings; Energy efficient; Energy-savings; Information provision; Radio signals; Transmission of data; Vehicle traffic; Drones
Scopus,"Khan, M.U.; Dil, M.; Alam, M.Z.; Orakazi, F.A.; Almasoud, A.M.; Kaleem, Z.; Yuen, C.",SafeSpace MFNet: Precise and Efficient MultiFeature Drone Detection Network,,2023,,,,10.1109/TVT.2023.3323313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174846836&doi=10.1109%2fTVT.2023.3323313&partnerID=40&md5=7a46286625299b9048e50a2cfb6d45ed,"The increasing prevalence of unmanned aerial vehicles (UAVs), commonly known as drones, has generated a demand for reliable detection systems. The inappropriate use of drones presents potential security and privacy hazards, particularly concerning sensitive facilities. Consequently, a critical necessity revolves around the development of a proficient system with the capability to precisely identify UAVs and other flying objects even in challenging scenarios. Although advancements have been made in deep learning models, obstacles such as computational intricacies, precision limitations, and scalability issues persist. To overcome those obstacles, we proposed the concept of MultiFeatureNet (MFNet), a solution that enhances feature representation by capturing the most concentrated feature maps. Additionally, we present MultiFeatureNet-Feature Attention (MFNet-FA), a technique that adaptively weights different channels of the input feature maps. To meet the requirements of multi-scale detection, we presented the versions of MFNet and MFNet-FA, namely the small (S), medium (M), and large (L). The outcomes reveal notable performance enhancements. For optimal bird detection, MFNet-M (Ablation study 2) achieves an impressive precision of 99.8%, while for UAV detection, MFNet-L (Ablation study 2) achieves a precision score of 97.2%. Among the options, MFNet-FA-S (Ablation study 3) emerges as the most resource-efficient alternative, considering its small feature map size, computational demands (GFLOPs), and operational efficiency (in frame per second). This makes it particularly suitable for deployment on hardware with limited capabilities. Additionally, MFNet-FA-S (Ablation study 3) stands out for its swift real-time inference and multiple-object detection due to the incorporation of the FA module. The proposed MFNet-L with the focus module (Ablation study 2) demonstrates the most remarkable classification outcomes, boasting an average precision of 98.4%, average recall of 96.6%, average mean average precision (mAP) of 98.3%, and average intersection over union (IoU) of 72.8%. © 2023 IEEE.",Birds; feature attention; multi-scale detection; multifeaturenet (MFNet); MultiFeatureNet-Feature Attention (MFNet-FA); unmanned aerial vehicle (UAV) detection; YOLOv5s; Ablation; Aircraft detection; Antennas; Autonomous vehicles; Deep learning; Drones; Feature extraction; Object detection; Aerial vehicle; Feature attention; Features extraction; Multi-scale detection; Multi-scales; Multifeaturenet; Multifeaturenet</italic>; Multifeaturenet</italic>-FA; Radiofrequencies; Security; Unmanned aerial vehicle detection; Vehicles detection; YOLOv5; Birds
Scopus,"Kaur, J.; Singh, W.","Tools, techniques, datasets and application areas for object detection in an image: a review",,2022,,,,10.1007/s11042-022-13153-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128750331&doi=10.1007%2fs11042-022-13153-y&partnerID=40&md5=e3b3a270fd6692be27bcd984d7dc91c9,"Object detection is one of the most fundamental and challenging tasks to locate objects in images and videos. Over the past, it has gained much attention to do more research on computer vision tasks such as object classification, counting of objects, and object monitoring. This study provides a detailed literature review focusing on object detection and discusses the object detection techniques. A systematic review has been followed to summarize the current research work’s findings and discuss seven research questions related to object detection. Our contribution to the current research work is (i) analysis of traditional, two-stage, one-stage object detection techniques, (ii) Dataset preparation and available standard dataset, (iii) Annotation tools, and (iv) performance evaluation metrics. In addition, a comparative analysis has been performed and analyzed that the proposed techniques are different in their architecture, optimization function, and training strategies. With the remarkable success of deep neural networks in object detection, the performance of the detectors has improved. Various research challenges and future directions for object detection also has been discussed in this research paper. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Computer vision; Dataset; Deep learning; Object detection; Deep neural networks; Object detection; Object recognition; 'current; Application area; Dataset; Deep learning; Literature reviews; Object classification; Object monitoring; Objects detection; Research questions; Systematic Review; Computer vision
Scopus,"Pang, F.; Chen, X.",MS-YOLOv5: a lightweight algorithm for strawberry ripeness detection based on deep learning,,2023,,,,10.1080/21642583.2023.2285292,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178227208&doi=10.1080%2f21642583.2023.2285292&partnerID=40&md5=49feafebc1c61dd02fd9e54d745a5c6a,"The existing ripeness detection algorithm for strawberries suffers from low detection accuracy and high detection error rate. Considering these problems, we propose an improvement method based on YOLOv5, named MS-YOLOv5. The first step is to reconfigure the feature extraction network of MS-YOLOv5 by replacing the standard convolution with the depth hybrid deformable convolution (Ms-MDconv). In the second step, a double cooperative attention mechanism (Bc-attention) is constructed and implemented in the CSP2 module to improve the feature representation in complex environments. Finally, the Neck section of MS-YOLOv5 has been enhanced to use the fast-weighted fusion of cross-scale feature pyramid networks (FW-FPN) to replace the CSP2 module. It not only integrates multi-scale target features but also significantly reduces the number of parameters. The method was tested on the strawberry ripeness dataset, the mAP reached 0.956, the FPS reached 76, and the model size was 7.44M. The mAP and FPS are 8.4 and 1.3 percentage higher than the baseline network, respectively. The model size is reduced by 6.28M. This method is superior to mainstream algorithms in detection speed and accuracy. The system can accurately identify the ripeness of strawberries in complex environments, which could provide technical support for automated picking robots. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",attention mechanism; Cross-scale feature fusion; deformable convolution; object detection; YOLO; Complex networks; Convolution; Deep learning; Feature extraction; Fruits; Attention mechanisms; Complex environments; Cross-scale feature fusion; Deformable convolution; Detection accuracy; Detection algorithm; Features fusions; Model size; Objects detection; YOLO; Object detection
Scopus,"Xiao, M.",Research on the Application of Improved SSD Algorithm in the Intelligent Recognition Model of Anime Characters,,2023,,,,10.1109/ACAIT60137.2023.10528481,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194197263&doi=10.1109%2fACAIT60137.2023.10528481&partnerID=40&md5=2c36f19e04225f7aa310f09fe885a634,"In order to apply object detection techniques with better detection performance to anime character detection, an improved Single Shot MulmtiBox Detector (SSD) algorithm is proposed. Based on the basic SSD algorithm, the research introduces a Feature Pyramid Network (FPN) upsampling module in feature layer prediction, and replaces the VGGNet network structure in the basic algorithm with ResNet network structure as the front-end network structure. And introduce weight parameters to adjust the proportion balance in the samples, optimize the training strategy of the algorithm, and obtain the final improved SSD algorithm, further improving the detection accuracy and speed of the algorithm. The study compared the performance of the improved SSD algorithm with other algorithms on the COCO dataset and the anime character dataset. The results showed that the improved SSD algorithm achieved an average accuracy (mAP) of 0.78, a positioning error of 0.04, a recall rate of 0.87, and a processing speed of 18 FPS on the COCO dataset, all of which were the best among the algorithms. The algorithm achieved a detection AP value of 0.82 for character 30. The average detection time of the improved SSD algorithm is 32ms, and the MAP reaches 0.74, which is the highest among all algorithms. At the same time, experiments were conducted on the value of the weight parameter. When the weight parameter value was 1, the detection mAP value remained stable at 0.77, achieving the best detection effect. The results show that the improved SSD algorithm proposed in the study has superior detection performance, and has further improved detection accuracy and speed, providing a certain experimental basis for the detection research of anime characters.  © 2023 IEEE.",anime; character; CNN; SSD; target detection; YOLO; Anime; Character; Detection performance; Detection speed; Network structures; Single shot mulmtibox detector; Single-shot; Targets detection; Weight parameters; YOLO; Object detection
Scopus,"Kukolj, D.; Marinović, I.; Nemet, S.",Road edge detection based on combined deep learning and spatial statistics of LiDAR data,,2023,,,,10.1080/14498596.2021.1960912,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112147929&doi=10.1080%2f14498596.2021.1960912&partnerID=40&md5=e0a82ba3ca529c3c6268b3e428d875ef,"Mobile laser scanning data can be used for effective extraction of road edge information, which is important in the domain of road maintenance and intelligent transportation. This paper proposes a road edge detection method that combines a deep learning and spatial statistics of point cloud data. Semantic segmentation using a deep neural network enables the effective extraction of point cloud fragments recognized as road. The process continues with the spatial statistical analysis of voxel features of data organized into a 3D voxel grid. Filtered voxels are clustered into spatially proximate clusters of similar shape, i.e. straight or curved edges. © 2021 Mapping Science Institute, Australia and Surveying and Spatial Science Institute.",LiDAR; machine learning; neural network; point cloud; road edge; artificial neural network; detection method; learning; lidar; machine learning; road; spatial analysis; statistical analysis
Scopus,"Barnawi, A.; Budhiraja, I.; Kumar, K.; Kumar, N.; Alzahrani, B.; Almansour, A.; Noor, A.",A comprehensive review on landmine detection using deep learning techniques in 5G environment: open issues and challenges,,2022,,,,10.1007/s00521-022-07819-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138329430&doi=10.1007%2fs00521-022-07819-9&partnerID=40&md5=b1aadf47f5d822fdc98fe9b474f7c600,"Detection of Landmines, especially anti-tank mines, bombs, and unexploded substances, is one of the major challenges facing humanity. The devastation and human tragedy associated with undetected explosives are self-evident in war-torn communities. To deal with this problem, we are only left with proactive measures that such substances must be detected and dealt with before the fallout. Most available solutions have major shortcomings, such as cost, efficiency, and accuracy, where the trade-offs among them are inversely related. On the other hand, advances in deep learning, unmanned aerial vehicle, and sensing are making their way as potential technologies to revolutionize the detection and removal of landmines. In this paper, we go through the literature reviewing the most recent work featuring computerized technologies to detect landmines. To our knowledge, no such study has taken place in this respect. Our aim is to find out how deep learning can be integrated with landmine detection. We identify open challenges toward viable automated solutions that enable deep learning to optimize performance effectively. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Applied artificial intelligence; Deep learning; Ground penetrating radar; Hyperspectral imaging; Landmine; Magnetometery; UAV; 5G mobile communication systems; Aircraft detection; Antennas; Bombs (ordnance); Deep learning; Economic and social effects; Explosives; Geological surveys; Ground penetrating radar systems; Hyperspectral imaging; Spectroscopy; Unmanned aerial vehicles (UAV); Anti-tank mines; Applied artificial intelligence; Cost-efficiency; Deep learning; Ground Penetrating Radar; Issues and challenges; Landmine; Learning techniques; Magnetometery; Proactive measures; Landmine detection
Scopus,"Dai, H.; Liu, B.; Wan, G.; Qi, J.; Wan, L.",Research on Lightweight Small Object Detection Method with Fusion Attention Mechanism,,2023,,,,10.1109/CAC59555.2023.10451036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189335689&doi=10.1109%2fCAC59555.2023.10451036&partnerID=40&md5=347cdf524554b13e3ccecaa59722c387,"In the task of small object detection, due to the relatively small size of the objects, complex situations such as occlusion and overlap may arise in high-density scenes. These factors can significantly increase the issues of missed detections and false detections in object detection tasks. To address the aforementioned issues, this paper proposes a lightweight small object detection model, YOLOv5s-MGC. Firstly, we introduce an improved Mobilenetv3 network to replace the backbone network of YOLOv5s, aiming to improve the network's capability for effective feature extraction. Secondly, we incorporate the GSConv module into the feature fusion network to reduce model parameters and strengthen the network's ability in fusing feature information. Finally, we have integrated Convolutional Block Attention Module (CBAM) at the forefront of the detection heads, achieving the fusion of fine-grained features. We assessed our approach on the openly accessible dataset 'SHWD' and contrasted the results with the baseline YOLOv5s and various SOTA object detection models. Precisely, our method attains an accuracy of 91.1%, exceeding the baseline module by 1.6%. Furthermore, the module size is reduced to 1.97MB and it achieves a 52.4 FPS. In comparison to other prevailing algorithms, YOLOv5s-MGC enables efficient and accurate small object detection.  © 2023 IEEE.",CBAM; GSConv; Mobilenetv3; Small object detection; YOLOv5s; Computer vision; Feature extraction; Object recognition; Attention mechanisms; Convolutional block attention module; Detection models; Gsconv; Missed detections; Mobilenetv3; Object detection method; Objects detection; Small object detection; YOLOv5; Object detection
Scopus,"Wang, Y.; Deng, J.; Li, Y.; Hu, J.; Liu, C.; Zhang, Y.; Ji, J.; Ouyang, W.; Zhang, Y.",Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection,,2023,,,,10.1109/CVPR52729.2023.01287,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215237226&doi=10.1109%2fCVPR52729.2023.01287&partnerID=40&md5=399d49f49f059d5c4a336b7a66fbd0e0,"LiDAR and Radar are two complementary sensing approaches in that LiDAR specializes in capturing an object's 3D shape while Radar provides longer detection ranges as well as velocity hints. Though seemingly natural, how to efficiently combine them for improved feature representation is still unclear. The main challenge arises from that Radar data are extremely sparse and lack height information. Therefore, directly integrating Radar features into LiDAR-centric detection networks is not optimal. In this work, we introduce a bi-directional LiDAR-Radar fusion framework, termed Bi-LRFusion, to tackle the challenges and improve 3D detection for dynamic objects. Technically, Bi-LRFusion involves two steps: first, it enriches Radar's local features by learning important details from the LiDAR branch to alleviate the problems caused by the absence of height information and extreme sparsity; second, it combines LiDAR features with the enhanced Radar features in a unified bird's-eye-view representation. We conduct extensive experiments on nuScenes and ORR datasets, and show that our Bi-LRFusion achieves state-of-the-art performance for detecting dynamic objects. Notably, Radar data in these two datasets have different formats, which demonstrates the generalizability of our method. Codes will be published.  © 2023 IEEE.",3D from multi-view and sensors; Human computer interaction; Image segmentation; Object detection; Object recognition; Object tracking; 3-D shape; 3d from multi-view and sensor; Bi-directional; Detection range; Dynamic objects; Feature representation; Multi sensor; Multi-views; Objects detection; Radar data; Optical radar
Scopus,"Li, Y.; Hoi, K.I.; Mok, K.M.; Yuen, K.V.",Air Quality Monitoring and Advanced Bayesian Modeling,,2023,,,,10.1016/C2020-0-03496-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150086501&doi=10.1016%2fC2020-0-03496-1&partnerID=40&md5=17487e20a0741d6cb9e5547c6cbad448,"Air Quality Monitoring and Advanced Bayesian Modeling introduces recent developments in urban air quality monitoring and forecasting. The book presents concepts, theories, and case studies related to monitoring methods of criteria air pollutants, advanced methods for real-time characterization of chemical composition of PM and VOCs, and emerging strategies for air quality monitoring. The book illustrates concepts and theories through case studies about the development of common statistical air quality forecasting models. Readers will also learn advanced topics such as the Bayesian model class selection, adaptive forecasting model development with Kalman filter, and the Bayesian model averaging of multiple adaptive forecasting models. © 2023 Elsevier Inc. All rights reserved.",
Scopus,"Mian, T.; Choudhary, A.; Fatima, S.",A sensor fusion based approach for bearing fault diagnosis of rotating machine,,2022,,,,10.1177/1748006X211044843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114441519&doi=10.1177%2f1748006X211044843&partnerID=40&md5=3d7858276e4f6ff4d469fa5a607301a3,"Fault diagnosis in rotating machines plays a vital role in various industries. Bearing is the essential element of rotating machines, and early fault detection can reduce the maintenance cost and enhance machine availability. In complex industrial machinery, a single sensor has a limitation to capture complete information about fault conditions. Hence, there is a need to involve multiple sensors to diagnose all possible fault conditions effectively. In such situations, an efficient fusion of information is required to develop a reliable fault diagnosis system. In this work, a feature fusion approach is implemented using two different sensors, that is, a contact type vibration sensor and a non-invasive thermal imaging camera. Hilbert transform is applied to decompose raw vibration and thermal image data, and subsequently, features are extracted and fused into a single feature vector. However, the features are fused in a concatenation manner, but this stage has high dimensionality. Neighborhood component analysis (NCA) is applied to reduce this high dimensionality of the feature vector, followed by a relief algorithm (RA) to compute the relevance level to find the optimal features. Finally, these optimal features are used as an input feature vector to the support vector machine (SVM) to classify the faults. The proposed approach resulted in considerably improved classification accuracy and detection quality than individual sensors. Also, the relevance of the proposed approach is proved by comparing its performance with other prevalent feature fusion techniques. © IMechE 2021.",fault classification; Hilbert transform; neighborhood component analysis; relief algorithm; Bearings (machine parts); Failure analysis; Infrared imaging; Mathematical transformations; Rotating machinery; Support vector machines; Vectors; Bearing fault diagnosis; Classification accuracy; Complete information; Fault diagnosis systems; Industrial machinery; Machine availability; Neighborhood component analysis; Thermal imaging cameras; Fault detection
Scopus,"Khan, M.; Raza, M.A.; Abbas, G.; Othmen, S.; Yousef, A.; Jumani, T.A.",Pothole detection for autonomous vehicles using deep learning: a robust and efficient solution,,2023,,,,10.3389/fbuil.2023.1323792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183611995&doi=10.3389%2ffbuil.2023.1323792&partnerID=40&md5=311ecedf382ee477b21e4fc8697e9bed,"Autonomous vehicles can transform the transportation sector by offering a safer and more effective means of travel. However, the success of self-driving cars depends on their ability to navigate complex road conditions, including the detection of potholes. Potholes pose a substantial risk to vehicles and passengers, leading to potential damage and safety hazards, making their detection a critical task for autonomous driving. In this work, we propose a robust and efficient solution for pothole detection using the “you look only once (YOLO) algorithm of version 8, the newest deep learning object detection algorithm.” Our proposed system employs a deep learning methodology to identify real-time potholes, enabling autonomous vehicles to avoid potential hazards and minimise accident risk. We assess the effectiveness of our system using publicly available datasets and show that it outperforms existing state-of-the-art approaches in terms of accuracy and efficiency. Additionally, we investigate different data augmentation methods to enhance the detection capabilities of our proposed system. Our results demonstrate that YOLO V8-based pothole detection is a promising solution for autonomous driving and can significantly improve the safety and reliability of self-driving vehicles on the road. The results of our study are also compared with the results of YOLO V5. Copyright © 2024 Khan, Raza, Abbas, Othmen, Yousef and Jumani.",autonomous vehicles; deep learning; image classification; intelligent technologies and cities; pothole detection; YOLO V8
Scopus,"Chen, A.; Wang, X.; Shi, K.; Zhu, S.; Fang, B.; Chen, Y.; Chen, J.; Huo, Y.; Ye, Q.",ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions,,2023,,,,10.1109/ICRA48891.2023.10161428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168693333&doi=10.1109%2fICRA48891.2023.10161428&partnerID=40&md5=6bf271fb0a7d31487c4d0914254fa8b6,"3D human reconstruction from RGB images achieves decent results in good weather conditions but degrades dramatically in rough weather. Complementary, mmWave radars have been employed to reconstruct 3D human joints and meshes in rough weather. However, combining RGB and mmWave signals for robust all-weather 3D human reconstruction is still an open challenge, given the sparse nature of mmWave and the vulnerability of RGB images. In this paper, we present ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies in all weather conditions robustly. Specifically, our ImmFusion consists of image and point backbones for token feature extraction and a Transformer module for token fusion. The image and point backbones refine global and local features from original data, and the Fusion Transformer Module aims for effective information fusion of two modalities by dynamically selecting informative tokens. Extensive experiments on a large-scale dataset, mmBody, captured in various environments demonstrate that ImmFusion can efficiently utilize the information of two modalities to achieve a robust 3D human body reconstruction in all weather conditions. In addition, our method's accuracy is significantly superior to that of state-of-the-art Transformer-based LiDAR-camera fusion methods. © 2023 IEEE.",Computer vision; Large dataset; Meteorology; Millimeter waves; 3D human body; Body reconstruction; Condition; Features extraction; Global feature; Human joints; Large-scale datasets; Local feature; Mm waves; RGB images; Image reconstruction
Scopus,"Naf’an, E.; Sulaiman, R.; Ali, N.M.",Optimization of Trash Identification on the House Compound Using a Convolutional Neural Network (CNN) and Sensor System,,2023,,,,10.3390/s23031499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147894666&doi=10.3390%2fs23031499&partnerID=40&md5=0983f2f48ab7854b1aa2b96bc63f3c63,"This study aims to optimize the object identification process, especially identifying trash in the house compound. Most object identification methods cannot distinguish whether the object is a real image (3D) or a photographic image on paper (2D). This is a problem if the detected object is moved from one place to another. If the object is 2D, the robot gripper only clamps empty objects. In this study, the Sequential_Camera_LiDAR (SCL) method is proposed. This method combines a Convolutional Neural Network (CNN) with LiDAR (Light Detection and Ranging), with an accuracy of ±2 mm. After testing 11 types of trash on four CNN architectures (AlexNet, VGG16, GoogleNet, and ResNet18), the accuracy results are 80.5%, 95.6%, 98.3%, and 97.5%. This result is perfect for object identification. However, it needs to be optimized using a LiDAR sensor to determine the object in 3D or 2D. Trash will be ignored if the fast scanning process with the LiDAR sensor detects non-real (2D) trash. If Real (3D), the trash object will be scanned in detail to determine the robot gripper position in lifting the trash object. The time efficiency generated by fast scanning is between 13.33% to 59.26% depending on the object’s size. The larger the object, the greater the time efficiency. In conclusion, optimization using the combination of a CNN and a LiDAR sensor can identify trash objects correctly and determine whether the object is real (3D) or not (2D), so a decision may be made to move the trash object from the detection location. © 2023 by the authors.",Convolutional Neural Network (CNN); identification; optimization; sensor; trash; Convolutional neural networks; Efficiency; Grippers; Object detection; Optical radar; Photography; Convolutional neural network; Detection sensors; Identification; Light detection and ranging; Object identification; Optimisations; Ranging sensors; Robot gripper; Trash; Convolution
Scopus,"Qi, L.; Gao, J.",Small Object Detection Based on Improved YOLOv7,,2023,,,,10.19678/j.issn.1000-3428.0065942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147041053&doi=10.19678%2fj.issn.1000-3428.0065942&partnerID=40&md5=6638c6e7f6d6742daedcdf6d24ed19d0,"Despite advancements in object detection technology, Small Object Detection (SOD) is still difficult to research.To address the challenge of easily missing detection in the process of object detection, this study proposes an improved YOLOv7 object detection model. Firstly, the MPConv module in the YOLOv7 model is improved by combining feature separation with merge, to reduce the feature loss caused by the process of network feature processing. The optimal position of the improved MPConv module is determined through experiments. Secondly, due to the phenomenon of missing detection in SOD, the algorithm uses the ACmix attention module to increase the sensitivity of the network to small-scale targets and reduce the influence caused by noise.Finally, SIoU is used to replace CIoU in the original YOLOv7 network model to optimize the loss function, reduce the freedom of the loss function, and improve the network robustness. Compared with the original network, the improved YOLOv7 network model can improve the missing detection situation of the images in the data set of the dense, small target, and ultra-small target by experimental comparison with the FloW-Img sub-dataset published by Okahublot. The results show that the mAP of the improved YOLOv7 network model can reach 71.1%, 4 percentage points higher than that of the baseline YOLOv7 network model, and the detection effect is better than that of the original network model and traditional classical target detection networks model. © 2023, Editorial Office of Computer Engineering. All rights reserved.",Attention module; Loss function; Object detection technology; Small Object Detection (SOD); YOLOv7 network model
Scopus,"Jiang, Y.; Zhu, J.; Jiang, C.; Xie, T.; Liu, R.; Wang, Y.",Adaptive Suppression Method of LiDAR Background Noise Based on Threshold Detection,,2023,,,,10.3390/app13063772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151527251&doi=10.3390%2fapp13063772&partnerID=40&md5=32e4cb4b8a0886f4b70c080ce3f765e3,"Background radiation in the LiDAR detection field of view is complex and variable, and the background noise generated can easily cause false alarms in the receiver, which affects the effective detection of the system. Through the analysis of the influence of background radiation noise of LiDAR on the detection performance, an adaptive suppression method of LiDAR background noise is proposed. This method realizes the rapid suppression of background noise in the instantaneous field of view through an adaptive threshold adjustment of current steering architecture with a back-end digital-to-analog converter (DAC) correction based on the principle of constant false alarm rate (CFAR) control. Aiming at the problem of accurate noise detection and quantification in a very short time, a dynamic comparator is used to replace the traditional continuous comparator. While detecting the number of noise pulses, the measurement of the pulse duration of noise is realized, which improves the accuracy of short-time noise detection. In order to verify the actual effect of the adaptive method, experiments were carried out based on the team’s self-developed LiDAR. The experimental results show that the measured noise ratio of the adaptive mode by using this method is the lowest. Even at 12 a.m., the noise ratio of the point cloud obtained by the adaptive mode is 0.012%, compared with 0.08% obtained by the traditional mode, which proves that this method has a good ability to suppress background noise. The proportion of noise reduction of the adaptive mode is more than 80% compared with the traditional mode. It achieves noise suppression through hardware at each detection, and each adjustment can be completed within a single period of pulse detection. Therefore, it has great advantages in real-time detection compared with the back-end software noise reduction processing method, and it is suitable for the application of LiDAR in the complex background environment. © 2023 by the authors.",background radiation; constant false alarm rate (CFAR); LiDAR; noise detection
Scopus,"Hu, D.; Tong, Q.; Chai, G.; Wang, K.; Mu, Y.; Su, S.",Two-Stage Progressive Image Deraining Algorithm for Vehicle Detection in Rainy Days,,2023,,,,10.3788/LOP231053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181700401&doi=10.3788%2fLOP231053&partnerID=40&md5=b4d43bd29b3fd57cf888ad3bd8f9ea2d,"This study proposes a two-stage progressive image deraining algorithm for vehicle detection in rainy days. The proposed algorithm aims to improve the accuracy of vehicle detection in rainy days and solve the problem of accuracy degradation caused by rain streak interference in the vehicle detection system of intelligent and connected vehicles. For the algorithm, a two-stage progressive deraining network was developed with light feature extraction and weighting block along with efficient feature transfer and fuse block as the core to realize the mining and capture of rain streak information and achieve accurate deraining. The deraining images were input to benchmark vehicle detector YOLOv5 for verifying the effectiveness of the proposed algorithm. Furthermore, a mix vehicle dataset was constructed based on the working environment of intelligent and connected vehicles. The gains of the proposed deraining algorithm on the precision, recall, and mAP@0. 5 of the benchmark vehicle detector YOLOv5 are 3. 0 percentage points, 8. 9 percentage points, and 7. 6 percentage points, respectively, under a rainy traffic scenario compared with other algorithms. The results prove that the proposed deraining algorithm considerably improves the accuracy of vehicle detection in rainy days and can be used in practice. © 2023 Universitat zu Koln. All rights reserved.",image deraining; image enhancing; image processing; two-stage progressive image deraining algorithm; vehicle detection
Scopus,"Cheng, Y.; Su, J.; Jiang, M.; Liu, Y.",A Novel Radar Point Cloud Generation Method for Robot Environment Perception,,2022,,,,10.1109/TRO.2022.3185831,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134272094&doi=10.1109%2fTRO.2022.3185831&partnerID=40&md5=afe77d5aa0f1b3a73a03fceaee7fb8bb,"Millimeter-wave (mmWave) radar has been widely used in autonomous driving due to its good performance under harsh weather conditions. In recent years, with the development of mmWave radar hardware performance, radar point clouds, as an important data format of mmWave radar, have been widely used in high-level perception tasks of mobile robots and autonomous driving. However, at present, compared to LiDAR point clouds, in common application scenes of mobile robots, mmWave radar point clouds have shortcomings such as sparsity and containing many 'ghost' targets. Therefore, in this article, we analyze the reasons that cause these problems and propose a new method for point cloud generation as well as a new evaluation metric. After building a new dataset and carrying out experiments in real-world scenes, our method shows better performance on the quality of radar point clouds compared to other methods. In addition, by evaluating the performance of applying the high-quality radar point clouds to object detection tasks as well as localization and mapping tasks, the result shows that radar point clouds generated using our method can significantly improve the environment perception ability of mobile robots.  © 2004-2012 IEEE.",Millimeter-wave (mmWave) radar; point cloud; radar detector; robot environment perception; Autonomous vehicles; Doppler radar; Millimeter waves; Object detection; Optical radar; Radar antennas; Tracking radar; Environment perceptions; Millimeter-wave radar; Millimetre-wave radar; Point cloud compression; Point-clouds; Radar applications; Radar detection; Radar detectors; Radars antennas; Robot environment; Robot environment perception; Mobile robots
Scopus,"Zhang, M.; Zhao, D.; Sheng, C.; Liu, Z.; Cai, W.",Long-Strip Target Detection and Tracking with Autonomous Surface Vehicle,,2023,,,,10.3390/jmse11010106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146785223&doi=10.3390%2fjmse11010106&partnerID=40&md5=c1ebacbf015020dfdf45c90cd38706ac,"As we all know, target detection and tracking are of great significance for marine exploration and protection. In this paper, we propose one Convolutional-Neural-Network-based target detection method named YOLO-Softer NMS for long-strip target detection on the water, which combines You Only Look Once (YOLO) and Softer NMS algorithms to improve detection accuracy. The traditional YOLO network structure is improved, the prediction scale is increased from threeto four, and a softer NMS strategy is used to select the original output of the original YOLO method. The performance improvement is compared totheFaster-RCNN algorithm and traditional YOLO methodin both mAP and speed, and the proposed YOLO–Softer NMS’s mAP reaches 97.09%while still maintaining the same speed as YOLOv3. In addition, the camera imaging model is used to obtain accurate target coordinate information for target tracking. Finally, using the dicyclic loop PID control diagram, the Autonomous Surface Vehicle is controlled to approach the long-strip target with near-optimal path design. The actual test results verify that our long-strip target detection and tracking method can achieve gratifying long-strip target detection and tracking results. © 2023 by the authors.",autonomous surface vehicle; softer NMS; targetdetection; YOLO
Scopus,"Qiao, Y.; Yin, J.; Wang, W.; Duarte, F.; Yang, J.; Ratti, C.",Survey of Deep Learning for Autonomous Surface Vehicles in Marine Environments,,2023,,,,10.1109/TITS.2023.3235911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147265791&doi=10.1109%2fTITS.2023.3235911&partnerID=40&md5=6ff9cfbb9c4e4b513131b5c8106363a0,"Within the next several years, there will be a high level of autonomous technology that will be available for widespread use, which will reduce labor costs, increase safety, save energy, enable difficult unmanned tasks in harsh environments, and eliminate human error. Compared to software development for other autonomous vehicles, maritime software development, especially in aging but still functional fleets, is described as being in a very early and emerging phase. This presents great challenges and opportunities for researchers and engineers to develop maritime autonomous systems. Recent progress in sensor and communication technology has introduced the use of autonomous surface vehicles (ASVs) in applications such as coastline surveillance, oceanographic observation, multi-vehicle cooperation, and search and rescue missions. Advanced artificial intelligence technology, especially deep learning (DL) methods that conduct nonlinear mapping with self-learning representations, has brought the concept of full autonomy one step closer to reality. This article reviews existing work on the implementation of DL methods in fields related to ASV. First, the scope of this work is described after reviewing surveys on ASV developments and technologies, which draws attention to the research gap between DL and maritime operations. Then, DL-based navigation, guidance, control (NGC) systems and cooperative operations are presented. Finally, this survey is completed by highlighting current challenges and future research directions.  © 2000-2011 IEEE.","Autonomous surface vehicle; deep learning; intelligent autonomous systems; neural network; NGC system; Autonomous vehicles; Control system synthesis; Deep learning; Job analysis; Software design; Surface waters; Vehicle to vehicle communications; Wages; Autonomous surface vehicles; Deep learning; Guidance control; Intelligent autonomous systems; Marine vehicles; Navigation guidance; Navigation, guidance, control system; Neural-networks; Sea surfaces; Sensor systems; Task analysis; Unmanned surface vehicles"
Scopus,"Wang, K.; Zhou, T.; Li, X.; Ren, F.",Performance and Challenges of 3D Object Detection Methods in Complex Scenes for Autonomous Driving,,2023,,,,10.1109/TIV.2022.3213796,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139850424&doi=10.1109%2fTIV.2022.3213796&partnerID=40&md5=b0fc26d4893ca8737fb5a61d55ddad83,"How to ensure robust and accurate 3D object detection under various environment is essential for autonomous driving (AD) environment perception. While, until now, most of the existing 3D object detection methods are based on the ordinary driving scenes provided by the mainstream dataset. The researches on actual complex scenes (adverse illumination, inclement weather, distant or small objects, etc.) have been ignored and there is still no comprehensive review of the recent progress in this field. Thence, this paper aims to gain a deep insight on the performance and challenges of 3D object detection methods under complex scenes for AD. Firstly, we discuss the complex driving environments in actual and the perception limitations of mainstream sensors (LIDAR and camera). Then we analyze the performance and challenges of single-modality 3D object detection methods. Therefore, in order to improve the accuracy and robustness of 3D object detection methods in some complex AD scenes, the fusion of L-C (LIDAR-camera) is recommended and systematically analyzed. Finally, some suitable datasets and potential directions are comparatively summarized to support the relative research in complex driving scenes. We hope that this review could facilitate people's research and look forward to more progress in this timely and crucial problem field. © 2016 IEEE.",3D object detection; autonomous driving; Complex scenes; datasets; multimodal fusion; Automobile drivers; Autonomous vehicles; Lighting; Object detection; Object recognition; Optical radar; Three dimensional displays; 3D object; 3d object detection; Autonomous driving; Autonomous Vehicles; Complex scenes; Dataset; Multi-modal fusion; Objects detection; Three-dimensional display; Cameras
Scopus,"Cao, L.; Zheng, X.; Fang, L.",The Semantic Segmentation of Standing Tree Images Based on the Yolo V7 Deep Learning Algorithm,,2023,,,,10.3390/electronics12040929,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149225665&doi=10.3390%2felectronics12040929&partnerID=40&md5=ca0de19873922dea4b75533fbcc8064a,"The existence of humans and the preservation of the natural ecological equilibrium depend greatly on trees. The semantic segmentation of trees is very important. It is crucial to learn how to properly and automatically extract a tree’s elements from photographic images. Problems with traditional tree image segmentation include low accuracy, a sluggish learning rate, and a large amount of manual intervention. This research suggests the use of a well-known network segmentation technique based on deep learning called Yolo v7 to successfully accomplish the accurate segmentation of tree images. Due to class imbalance in the dataset, we use the weighted loss function and apply various types of weights to each class to enhance the segmentation of the trees. Additionally, we use an attention method to efficiently gather feature data while reducing the production of irrelevant feature data. According to the experimental findings, the revised model algorithm’s evaluation index outperforms other widely used semantic segmentation techniques. In addition, the detection speed of the Yolo v7 model is much faster than other algorithms and performs well in tree segmentation in a variety of environments, demonstrating the effectiveness of this method in improving the segmentation performance of the model for trees in complex environments and providing a more effective solution to the tree segmentation issue. © 2023 by the authors.",deep learning; fast segmentation; semantic segmentation; tree segmentation; Yolo v7
Scopus,"Deng, K.; Zhao, D.; Han, Q.; Wang, S.; Zhang, Z.; Zhou, A.; Ma, H.",Geryon: Edge Assisted Real-time and Robust Object Detection on Drones via mmWave Radar and Camera Fusion,,2022,,,,10.1145/3550298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139166067&doi=10.1145%2f3550298&partnerID=40&md5=e00b14dd7569a8461bac6c43f4301e89,"Vision-based drone-view object detection suffers from severe performance degradation under adverse conditions (e.g., foggy weather, poor illumination). To remedy this, leveraging complementary mmWave radar has become a trend. However, existing fusion approaches seldom apply to drones due to i) the aggravated sparsity and noise of point clouds from low-cost commodity radars, and ii) explosive sensing data and intensive computations leading to high latency. To address these issues, we design Geryon, an edge assisted object detection system on drones, which utilizes a suit of approaches to fully exploit the complementary advantages of camera and mmWave radar on three levels: (i) a novel multi-frame compositing approach utilizes camera to assist radar to address the aggravated sparsity and noise of radar point clouds; (ii) a saliency area extraction and encoding approach utilizes radar to assist camera to reduce the bandwidth consumption and offloading latency; (iii) a parallel transmission and inference approach with a lightweight box enhancement scheme further reduces the offloading latency while ensuring the edge-side accuracy-latency trade-off by the parallelism and better camera-radar fusion. We implement and evaluate Geryon with four datasets we collect under foggy/rainy/snowy weather and poor illumination conditions, demonstrating its great advantages over other state-of-the-art approaches in terms of both accuracy and latency.  © 2022 ACM.",drone; edge network orchestration; mmWave radar sensing; multimodal fusion; real-time object detection; Aircraft detection; Cameras; Drones; Economic and social effects; Millimeter waves; Object recognition; Tracking radar; Edge network orchestration; EDGE Networks; Mm waves; Mmwave radar sensing; Multi-modal fusion; Objects detection; Point-clouds; Radar sensing; Real- time; Real-time object detection; Object detection
Scopus,"Zhang, Z.; Hanwen, G.; Wu, X.",Detection of pedestrians and vehicles in autonomous driving with selective kernel networks,,2023,,,,10.1049/ccs2.12078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149452252&doi=10.1049%2fccs2.12078&partnerID=40&md5=9e931703e8eee0f5b8b60a56b962877b,"Accurate detection of pedestrians and vehicles on the road is an important content in autonomous driving technology. In this article, a method to optimise the object detection network using the channel attention mechanism is proposed. In general, small object detection problems and difficult sample detection problems in object detection tasks can be solved by using feature pyramids. Different from building a feature pyramid, the authors did not make extensive changes to the network, but used the channel attention mechanism to dynamically adjust the output of a layer during the feature extraction process, allowing each neuron to adjust its receptive field size adaptively according to multiple scales of the input information, so that the network pays attention to the extraction of important features, especially the features of small objects and difficult samples. In order to evaluate the performance of the proposed method, experiments were conducted on standard benchmark data sets. It has been observed that the proposed method is superior to the original object detection network in terms of the detection accuracy of pedestrians and vehicles, especially the detection of small objects. © 2023 The Authors. Cognitive Computation and Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Shenzhen University.",artificial intelligence; computer vision; image processing; Autonomous vehicles; Benchmarking; Cognitive systems; Computer vision; Extraction; Feature extraction; Object recognition; Pedestrian safety; Attention mechanisms; Autonomous driving; Detection networks; Detection problems; Detection tasks; Feature pyramid; Images processing; Objects detection; Small object detection; Small objects; Object detection
Scopus,"Shin, H.; Yeom, T.; Lee, S.",Real-Time Lane/Pothole Detection and Collision Avoidance Algorithm for Low-cost Autonomous Driving Systems,,2023,,,,10.3795/KSME-A.2023.47.11.901,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179752991&doi=10.3795%2fKSME-A.2023.47.11.901&partnerID=40&md5=cfdef2f577da91604921bcd72b709f9b,"Lane detection and object detection represent pivotal technologies in modern autonomous driving systems. Therefore, this study introduces a lightweight model that encompasses both object detection and lane detection, tailored for utilization with Raspberry Pi and Arduino platforms. The proposed method incorporates an algorithm designed for lane-change maneuvers, which involves the real-time identification of potholes-localized depressions in the road surface-by processing camera data. Moreover, effective training was administered via data augmentation techniques to establish a brightness-robust object detection network. This approach facilitated successful pothole detection, even in low-light conditions. Experimental trials conducted on a test track validated the functionality of the model, demonstrating its effectiveness through a range of evaluation metrics. © 2023 The Korean Society of Mechanical Engineers.",Autonomous Driving; Computer Vision; Deep Learning; Lane Detection; Object Detection; Autonomous vehicles; Computer vision; Data handling; Deep learning; Landforms; Object recognition; Autonomous driving; Collisions avoidance; Deep learning; Driving systems; Lane change; Lane detection; Low-costs; Objects detection; Real- time; Real-time identification; Object detection
Scopus,"Alam, A.; Singh, L.; Jaffery, Z.A.; Verma, Y.K.; Diwakar, M.",Distance-based confidence generation and aggregation of classifier for unstructured road detection,,2022,,,,10.1016/j.jksuci.2021.09.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118785580&doi=10.1016%2fj.jksuci.2021.09.020&partnerID=40&md5=049f634a0a1700077db7f7444df14594,"Road region and non-road region separation in the unstructured road intends to be an important task for safe navigation and collision avoidance for autonomous driving vehicles. The road that connects rural areas and cities to the national highways are considered as unstructured roads. Absence of clear lane marking on these unstructured roads makes them more prone to accidents in comparison to highways, which have clear lane marking for indication of road and non-road regions. However, the unstructured roads have different color information from its background that paves an easy way for design and development of an efficient road detection system for recognition and classification of road and non-road regions. Hence, in this paper, we propose an efficient road detection system for the classification of unstructured roads into road and non-road regions using multiple nearest neighbors (NN) classifier and soft voting aggregation approach. The proposed system utilized the chromatic information (i.e. *a,*b, and Hue) to train the NN classifiers and aggregated their output using soft voting (SV) approach for final output response. The output results of multiple classifiers were aggregated using SV approach based on the confidence score obtained by each individual classifier. The performance of the proposed system is evaluated in terms of precision, recall, accuracy, intersection over union (IOU), true positive rate (TPR), and processing time and compared with current state of art methods reported in the literature. The proposed system achieved precision, recall, accuracy, IOU, and TPR of 96.79%, 96.92%, 97.8%, 96.08% and 96%, respectively with the processing time three times smaller than those of the existing state of art methods. The experimental results demonstrate that the proposed system can provide an effective guidance to the autonomous vehicles through recognition and classification of road and non-road regions in rural, urban, and city areas, wherein single unstructured roads connect the national highways. © 2021",Autonomous vehicle; L*a*b color space; Nearest neighbor classifier; Soft voting aggregation; Unstructured road detection
Scopus,"Brophy, T.; Mullins, D.; Parsi, A.; Horgan, J.; Ward, E.; Denny, P.; Eising, C.; Deegan, B.; Glavin, M.; Jones, E.",A Review of the Impact of Rain on Camera-Based Perception in Automated Driving Systems,,2023,,,,10.1109/ACCESS.2023.3290143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163495381&doi=10.1109%2fACCESS.2023.3290143&partnerID=40&md5=83e6ff2e39fec48a22a80e4adb7f9d9d,"Automated vehicles rely heavily on image data from visible spectrum cameras to perform a wide range of tasks from object detection, classification, and avoidance to path planning. The availability and reliability of these sensors in adverse weather is therefore of critical importance to the safe and continuous operation of an automated vehicle. This review paper presents a data communication-inspired Image Formation Framework that characterizes the data flow from object through channel to sensor, and subsequent processing of the data. This framework is used to explore the degree to which adverse weather conditions affect the cameras used in automated vehicles for sensing and perception. The effects of rain on each element of the model are reviewed. Furthermore, the prevalence of these rain-induced changes in publicly available open-source datasets is reviewed. The degree to which synthetic rain generation techniques can accurately capture these changes is also examined. Finally, this paper offers some suggestions on how future adverse weather automotive datasets should be collected.  © 2013 IEEE.",Adverse weather; automated vehicles; computer vision; rain; sensor availability; Computer vision; Data handling; Motion planning; Object detection; Rain; Vehicles; Adverse weather; Automated driving systems; Automated vehicles; Camera-based; Image color analysis; Image data; Road; Sensor availability; Sensor phenomenon and characterizations; Visible spectrums; Cameras
Scopus,"Krishna Rao Muvva, V.V.R.M.; Samal, K.; Bradley, J.; Wolf, M.",A Closed Loop Perception Subsystem for small Unmanned Aerial Systems,,2023,,,,10.2514/6.2023-2673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196498398&doi=10.2514%2f6.2023-2673&partnerID=40&md5=5ab91187001146f5645255e8adf45ff7,"The perception subsystem in modern autonomous systems use convolutional neural networks (CNNs) for their high accuracy. The structure of such networks is static and there is an inverse relationship between their accuracy and resource consumption, such as inference speed and energy utilized. This poses a challenge while designing the perception subsystem for agile autonomous systems such as small unmanned aerial systems which have limited resources while operating in real world dynamic scenarios. Existing approaches design the perception subsystems for the worst case scenario which can lead to inefficient resource utilization that can hamper the mission capabilities of such systems. At the same time it is difficult to predict the worst case condition during design time especially for systems that operate in stochastic and dynamic real world scenarios. Therefore, we have developed a closed-loop perception subsystem that can dynamically change the structure of its CNN such that the accuracy and latency adapt to the requirements of a given scenario. The proposed system was tested on an UAS-UAS tracking scenario. It was demonstrated that the chasing UAS with the proposed closed loop perception subsystem was able to successfully track the target UAS more accurately than the baseline approach, with static CNNs, while consuming less energy. Furthermore, the adaptive latency of the proposed method lets the chasing UAS track the target UAS at higher velocities compared to baseline methods. © 2023, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.",Air navigation; Antennas; Convolutional neural networks; Real time systems; Stochastic systems; Unmanned aerial vehicles (UAV); Closed-loop; Convolutional neural network; Energy; High-accuracy; Inverse relationship; Real-world; Resources consumption; System use; Unmanned aerial systems; World dynamics; Inverse problems
Scopus,"Di, J.; Feng, F.; Yang, Y.; Zhang, W.",UAV Image Object Detection Based on Improved YOLOv5s,,2023,,,,10.1117/12.3011970,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180127829&doi=10.1117%2f12.3011970&partnerID=40&md5=824c55fc8ca280bf892fd9b8df24b369,"As we all know, the objects in the images taken by unmanned aerial vehicle (UAV) are relatively small, while our naked eyes are able to extract the information almost instantly, even from far away, image resolution and computational resources limitations make detecting smaller objects a genuinely challenging task for machines. We propose an algorithm based on YOLOv5s with small computational resources and high accuracy, so as to be applied to edge detection devices such as unmanned aerial vehicles. By simplifying the depth of the feature extraction network and adjusting the size of the feature map of the detection head, the target in the image taken by UAV can be accurately identified. In the end, we reduced the number of parameters by 70% at the expense of a little accuracy, while improving accuracy by 15.25%, or 5.2 percentage points, over the baseline. © 2023 SPIE. All rights reserved.",Edge device object detection; Small object detection; UAV image; Aircraft detection; Antennas; Edge detection; Feature extraction; Image enhancement; Object detection; Object recognition; Unmanned aerial vehicles (UAV); Aerial vehicle; Computational resources; Edge device object detection; Image object detection; Naked-eye; Objects detection; Resource limitations; Small object detection; Unmanned aerial vehicle image; Vehicle images; Image resolution
Scopus,"Jiang, Y.; Huang, L.; Zhang, Z.; Nie, B.; Zhang, F.",Analysis of Scale Sensitivity of Ship Detection in an Anchor-Free Deep Learning Framework,,2023,,,,10.3390/electronics12010038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145845750&doi=10.3390%2felectronics12010038&partnerID=40&md5=133aaf58e1ec9fc9ac0936ad899282c4,"Ship detection is an important task in sea surveillance. In the past decade, deep learning-based methods have been proposed for ship detection from images and videos. Convolutional features are observed to be very effective in representing ship objects. However, the scales of convolution often lead to different capacities of feature representation. It is unclear how the scale influences the performance of deep learning methods in ship detection. To this end, this paper studies the scale sensitivity of ship detection in an anchor-free deep learning framework. Specifically, we employ the classical CenterNet as the base and analyze the influence of the size, the depth, and the fusion strategy of convolution features on multi-scale ship target detection. Experiments show that, for small targets, the features obtained from the top-down path fusion can improve the detection performance more significantly than that from the bottom-up path fusion; on the contrary, the bottom-up path fusion achieves better detection performance on larger targets. © 2022 by the authors.",convolutional neural network; multi-scale features; object detection; scale sensitivity; ship detection
Scopus,"Li, Q.; Ye, X.-M.; Feng, W.-B.",Vehicle detection in foggy weather combining millimeter wave rada and machine vision,,2023,,,,10.37188/CJLCD.2022-0412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176120584&doi=10.37188%2fCJLCD.2022-0412&partnerID=40&md5=a243d57817d8afea4946bb4d0430daf2,"Vehicle detection is very vital to the assisted driving system. Due to the serious degradation of the foggy road scene，the vehicle information in the image is not obvious，resulting in missed detection and false detection problems in vehicle detection. Aiming at the above problems，a vehicle detection method in foggy weather combining millimeter-wave radar and machine vision is proposed. First，the dark channel dehazing algorithm is used to preprocess the image to improve the salience of vehicle information in the image under foggy conditions. Then，the knowledge distillation is used to improve the YOLOv5s algorithm， and the knowledge distillation is introduced into the feature extraction network of YOLOv5s，which is used in the target positioning and classification stages to calculate the distillation loss and backpropagate the loss to train a small network model to improve the detection speed while ensuring the accuracy of visual detection. Finally，the distance matching algorithm based on the search of potential target detection areas is used to compare the visual detection results and the millimeter-wave radar detection results decision-making fusion. Based on the type and distance of the detected target，the interference information and erroneous information is filtered out，and the targets with high confidence after fusion in millimeter-wave radar detection and visual detection is retained. Thereby，the accuracy of vehicle detection is improved. The experimental results show that the method has the highest detection accuracy rate of 92. 8% and the recall rate of 90. 7% in foggy weather，which can realize the detection of vehicles in foggy weather. © 2023, Science Press. All rights reserved.",decision level fusion; defogging; distance matching; knowledge distillation; millimeter wave radar; vehicle detection
Scopus,"Hu, R.; Ting, R.",An Object Detection Method for Remote Sensing Images Based on DA-YOLO,,2023,,,,10.1007/978-981-99-0923-0_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152616921&doi=10.1007%2f978-981-99-0923-0_13&partnerID=40&md5=c6332991577d219be684cbeda04bb087,"Aiming at the difficulty of small-scale objects in high-resolution remote sensing images, this paper proposes a new detector DA-YOLO (dilation and attention YOLO) to locate objects quickly and accurately. Firstly, during the data preprocessing, the remote sensing images processed by “quadruple cropping” to adjust the original image size and enlarge the number of data instance. Then, the CSPDarknet53 backbone network is optimized: the dilated separable convolution (DSC) module is applied to enlarge the receptive range of feature maps without losing the resolution of feature maps. Then, the convolutional block attention module (CBAM) is introduced for feature enhancement, and finally, the last four stages of feature maps are used instead of three stages to obtain more contour details of small-scale objects. Extensive experiments show that DA-YOLO has good performance in DOTA, with a 2.36% increase in mAP compared to the original YOLOv4 without a significant decrease in detection speed. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Attention mechanism; Dilated separable convolution; Multi-scale; Object detection; Remote sensing image; Convolution; Object recognition; Remote sensing; Attention mechanisms; Dilated separable convolution; Feature map; High-resolution remote sensing images; Image-based; Multi-scales; Object detection method; Objects detection; Remote sensing images; Small scale; Object detection
Scopus,"Zhang, M.; Gao, F.; Yang, W.; Zhang, H.",Wildlife Object Detection Method Applying Segmentation Gradient Flow and Feature Dimensionality Reduction,,2023,,,,10.3390/electronics12020377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146810122&doi=10.3390%2felectronics12020377&partnerID=40&md5=ad289244ca243c7cbb56eadac28ccd13,"This work suggests an enhanced natural environment animal detection algorithm based on YOLOv5s to address the issues of low detection accuracy and sluggish detection speed when automatically detecting and classifying large animals in natural environments. To increase the detection speed of the model, the algorithm first enhances the SPP by switching the parallel connection of the original maximum pooling layer for a series connection. It then expands the model’s receptive field using the dataset from this paper to enhance the feature fusion network by stacking the feature pyramid network structure as a whole; secondly, it introduces the GSConv module, which combines standard convolution, depth-separable convolution, and hybrid channels to reduce network parameters and computation, making the model lightweight and easier to deploy to endpoints. At the same time, GS bottleneck is used to replace the Bottleneck module in C3, which divides the input feature map into two channels and assigns different weights to them. The two channels are combined and connected in accordance with the number of channels, which enhances the model’s ability to express non-linear functions and resolves the gradient disappearance issue. Wildlife images are obtained from the OpenImages public dataset and real-life shots. The experimental results show that the improved YOLOv5s algorithm proposed in this paper reduces the computational effort of the model compared to the original algorithm, while also providing an improvement in both detection accuracy and speed, and it can be well applied to the real-time detection of animals in natural environments. © 2023 by the authors.",animal recognition; feature fusion networks; GSConv; segmentation gradient flow; YOLOv5s
Scopus,"Fei, X.; Han, Y.; Wong, S.V.",An Overview of and Prospects for Research on Energy Savings in Wheel Loaders,,2023,,,,10.31603/ae.8759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159388951&doi=10.31603%2fae.8759&partnerID=40&md5=1510040731d3fd73b5b33ba4891baf5b,"Wheel loaders consume a large amount of energy, and research on energy savings in wheel loaders has been carried out for decades. This paper introduces several types of wheel loaders and compares their structures. The research progress of the energy savings of three different forms of wheel loaders is reviewed, including a diesel engine wheel loader, a hybrid wheel loader, and an electric wheel loader. In particular, the energy-saving control methods of an electric wheel loader in the working cycle are analyzed, as construction machinery electrification is an emerging trend. Based on the analysis of the driving features and the working process of a wheel loader, energy-saving control methods are introduced including the resistance reduction method, optimized control strategies, intelligent control, and unmanned WL research. Comparing various energy-saving research methods and the advantages of electric wheel loaders, the pure electric wheel loaders are advised to be researched at present and in the future. Controlling the torque distribution of the front and rear motors of electric wheel loaders and assistant drive control are proposed to be significant research prospects for energy savings in wheel loaders usage. © 2023, Universitas Muhammadiyah Magelang. All rights reserved.",Control strategy; Electric wheel loader; Energy savings; Wheel loader
Scopus,"Yao, Y.; Zhu, L.J.; Shi, W.",DICE: Dynamic In-Situ Control for Edge-Based Applications,,2023,,,,10.1145/3583740.3626817,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186117195&doi=10.1145%2f3583740.3626817&partnerID=40&md5=776f13598162c9cc5e23a1bc1960fd1d,"This paper focuses on addressing computational constraints and energy limitations prevalent in edge-based applications through an innovative approach, dynamic in-situ control for edge-based applications (DICE). DICE capitalizes on the burgeoning trend in vehicle sensor technologies, such as camera, Radar, and LiDAR, which are becoming increasingly powerful and capable of performing pre-processing computations. DICE introduces a concept of 'downstream offloading', which distinguishes it from traditional offloading approaches that typically offload computational tasks from edge devices to more powerful Edge Servers. In contrast, DICE offloads part of the computational tasks from the Edge Server to the sensor itself, thereby optimizing data processing at the source and reducing the volume of data transmission required. This approach not only addresses the latency bottleneck frequently encountered in energy-intensive neural networks but also enhances the efficiency of data processing by selectively filtering out non-critical frames based on event-triggering mechanisms. DICE leverages the unique strengths of portable devices such as smartwatches and smartphones, even with their inherent computational and power limitations. The framework consists of an adaptive control layer for dynamic task allocation and an application layer designed to deploy quantized models on System on Chips (SoCs) like TinyML, thereby improving the efficiency of AI-driven applications while conservatively utilizing energy. This system proposes a sustainable, energy-efficient pathway for future edge-based applications. © 2023 ACM.",dynamic control; edge collaboration; smart sensor; Adaptive control systems; Computation offloading; Computing power; Data handling; Optical radar; System-on-chip; Computational constraints; Computational task; Dynamic controls; Edge collaboration; Edge server; Edge-based; Energy; Energy limitations; In-situ control; Innovative approaches; Energy efficiency
Scopus,"Mungekar, K.; Marakarkandy, B.; Kelkar, S.; Gupta, P.",Design of an Aqua Drone for Automated Trash Collection from Swimming Pools Using a Deep Learning Framework,,2023,,,,10.1007/978-981-19-9225-4_41,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150965019&doi=10.1007%2f978-981-19-9225-4_41&partnerID=40&md5=b51873472d27a2666a57736c2886e152,"Water conservation is of prime importance to sustain life. An ample amount of water is used in swimming pools. Material contamination of water in a swimming pool occurs from various sources, viz., leaves of trees and plants from the surrounding area, plastic bottles, wearables for protection of eyes, ears, and hair left by people who swim in the pools. Removal of this trash is a challenging task. The existing trash collector boats being huge in size are primarily designed for cleaning rivers and seas; moreover, manual intervention is needed for its operation. These boats are not suitable to be used in swimming pools. This paper presents an automatic robotic trash boat for floating trash detection, collection, and finally accumulation of the floating trash using a conveyor machine. The system is portable, user-friendly, environmentally pleasant, and facilitates remote control operation. The system uses a deep learning model based on a modified YOLO framework. The prototype has superior performance compared to existing systems with respect to accuracy and time. Performance indicators, viz., accuracy, error rate, precision, recall F1 score and the quadratic weighted kappa were used to evaluate the prototype. The final trained network obtained 92% accuracy, 0.0167 error rate, 0.23 logloss, 92% precision, 92% recall, 92% F1 score, and 0.9665 kappa quadratic weighted value. The system would help in cleaning swimming pools thus preventing contamination of water due to this water replenishment cycle can be reduced. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Aqua drone; CNN; Trash collection; Boats; Deep learning; Drones; Plants (botany); Remote control; Swimming pools; Water pollution; Aqua drone; Error rate; F1 scores; Learning frameworks; Manual intervention; Material contamination; Remote control operations; System use; Trash collection; User friendly; Lakes
Scopus,"Talsma, D.",The Psychology of Cognition: An Introduction to Cognitive Neuroscience,,2023,,,,10.4324/9781003319344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171005735&doi=10.4324%2f9781003319344&partnerID=40&md5=c3a9edade2faaedf89d1b4da318de92d,"This comprehensive, cutting-edge textbook offers a layered approach to the study of cognitive neuroscience and psychology. It embraces multiple exciting and influential theoretical approaches such as embodied cognition and predictive coding, and explaining new topics such as motor cognition, cognitive control, consciousness, and social cognition. Durk Talsma offers foundational knowledge which he expands and enhances with coverage of complex topics, explaining their interrelatedness and presenting them together with classic experiments and approaches in a historic context. Providing broad coverage of world-class international research this richly illustrated textbook covers key topics including: Action control and cognitive control Consciousness and attention Perception Multisensory processing and perception-action integration Motivation and reward processing Emotion and cognition Learning and memory Language processing Reasoning Numerical cognition and categorisation Judgement, decision making, and problem solving Social cognition Applied cognitive psychology With pedagogical features that include highlights of relevant methods and historical notes to spark student interest, this essential text will be invaluable reading for all students of cognitive psychology and cognitive neuroscience. © 2023 Durk Talsma.",
Scopus,"Xu, X.; Pan, H.; Wang, H.; Cao, Y.",Object Detection Algorithm for Railway Scenes Based on Infrared and RGB Image Fusion,,2023,,,,10.1109/PRMVIA58252.2023.00015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163865123&doi=10.1109%2fPRMVIA58252.2023.00015&partnerID=40&md5=bdf3ff5f8119e28e4c4191b5143d195e,"The driver-assistance system tends to fuse multi-modal sensor data, for instance, the infrared and RGB sensors, to detect intrusion objects to enhance driving safety. However, the semantic misalignment dilemma and the spectral imb-alance between infrared and RGB images make it hard to exp-loit the advantages of multi-sensors in the end-to-end learning system. To solve these problems, we employ the widely used affine transformation on our railway dataset to solve the se-mantic-misalignment issue, in addition, we propose a fusion module, DMF, to fuse the well-aligned features, which can bri-dge the domain gap among different sensors. To this end, we propose an efficient railway invasive object detection network, YOLOv5s-DMF. Compared with the state-of-the-art metho-ds, the YOLOv5s-DMF substantially reduces the MR by 14.23% by employing the well-established decouple head. And our YOLOv5s-DMF further increases the mAP@0.5 by 5.7% and the mAP@0.5:0.95by4.1%.  © 2023 IEEE.",component; deep learning; multi-modal fusion; object detection; Alignment; Automobile drivers; Deep learning; Image fusion; Learning systems; Object recognition; Railroads; Semantics; Component; Deep learning; Driver-assistance systems; Multi-modal fusion; Multimodal sensor; Object detection algorithms; Objects detection; RGB images; Scene-based; Sensors data; Object detection
Scopus,"Boulch, A.; Sautier, C.; Michele, B.; Puy, G.; Marlet, R.",ALSO: Automotive Lidar Self-Supervision by Occupancy Estimation,,2023,,,,10.1109/CVPR52729.2023.01293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171390587&doi=10.1109%2fCVPR52729.2023.01293&partnerID=40&md5=42a231526d2891e361120f22aaae1629,"We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information, that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches. The code is available at github.com/valeoai/ALSO © 2023 IEEE.",Self-supervised or unsupervised representation learning
Scopus,,Compilation of references,,2022,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161594795&partnerID=40&md5=6edd375a425d893f6cbf417d574d7d9a,,
Scopus,"Yi, J.; Chen, G.; Ru, Q.; Li, M.",Lightweight semantic segmentation for digital workshop objects,,2023,,,,10.13196/j.cims.2023.03.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163179956&doi=10.13196%2fj.cims.2023.03.021&partnerID=40&md5=ef49c521a752c2a55dbbee1f0fabd633,"To meet the real-time demand of manufacturing in the digital workshop, a lightweight semantic segmentation network named Multi Pyramid Pooling Segmentation Network (MPPSNet) that satisfied both accuracy and real-time was proposed, which realized the semantic segmentation for the goal of digital workshop. The improved MobileNetv2 was used as the encoder of semantic segmentation network in MPPSNet, which effectively reduced the amount of network parameters and improved the real-time performance of the overall network operation; the multi-pyramid pooling network was used as the decoder of the segmentation network, which could integrate multiple layers of feature information and improve the accuracy of the network. Tests found that the semantic segmentation effect of MPPSNet was better than that of FCN8 and BiSeNet in VOC20 12data set. In the self-building object semantic segmentation data set of the digital workshop, the Mean Intersection over Unions ( MIoU) of segmenting human, machine tool, and mobile robot of workshop objects reached 7 1.8% in MPPSNet and the parameter amount of the entire network was 2.55M, which could meet the accurate and real-time requirements of segmenting workshop objects. © 2023 CIMS. All rights reserved.",digital workshop; lightweight; MobileNetv2; real-time; semantic segmentation; Machine tools; Semantic Segmentation; Digital workshop; Lightweight; Lightweight semantics; Mobilenetv2; Network operations; Network parameters; Overall networks; Real time performance; Real- time; Semantic segmentation; Semantics
Scopus,"Zong, C.; Meng, K.; Sun, J.; Zhou, Q.",Real Time Object Recognition Based on YOLO Model,,2023,,,,10.1109/EIECS59936.2023.10435392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187003667&doi=10.1109%2fEIECS59936.2023.10435392&partnerID=40&md5=1c20584431ef0de396853fc199dc8f01,"Real-time object recognition is a fundamental task in computer vision with many applications. This paper presents a comprehensive survey of the evolution of the You Only Look Once (YOLO) object detection models, spanning from YOLOv1 to YOLOv8. Each iteration of the YOLO model is examined in detail, elucidating architectural advancements and innovations that have propelled real-time object recognition performance. The survey delves into the strengths and limitations of each YOLO version, highlighting their respective contributions to the field. Additionally, two prominent practical implementations of YOLO models are elucidated, exemplifying the models' efficacy in complex scenarios. The first case study explores YOLO's role in enabling real-time object detection for autonomous driving systems, enhancing safety and situational awareness. The second case study investigates the integration of YOLO models in unmanned aerial vehicles, showcasing their utility in aerial surveillance and reconnaissance. By providing an in-depth exploration of YOLO models and their evolution, this paper equips researchers and practitioners with a comprehensive understanding of real-time object recognition techniques. Furthermore, the analysis of practical applications underscores the tangible impact of YOLO models on cutting-edge technologies. Looking forward, this survey sets the stage for future advancements in real-time object recognition, addressing challenges and opportunities for refining performance in dynamic and complex environments.  © 2023 IEEE.",autonomous driving; computer vision; real-time object recognition; survey; unmanned aerial vehicles; YOLO models; Antennas; Autonomous vehicles; Iterative methods; Object detection; Object recognition; Real time systems; Unmanned aerial vehicles (UAV); Aerial vehicle; Autonomous driving; Case-studies; Detection models; Objects detection; Performance; Real- time; Real-time object recognition; Unmanned aerial vehicle; You only look once model; Computer vision
Scopus,"Shi, H.; Zhao, D.",License Plate Recognition System Based on Improved YOLOv5 and GRU,,2023,,,,10.1109/ACCESS.2023.3240439,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148333620&doi=10.1109%2fACCESS.2023.3240439&partnerID=40&md5=f1bab0371813ef2fd1a3756389fa0bf0,"Aiming at the problem that the traditional license plate recognition method lacking of accuracy and speed, an end-to-end deep learning model for license plate location and recognition in natural scenarios was proposed. First, we added an improved channel attention mechanism to the down-sampling process of the You only look once(YOLOv5). Additionally, a location information is added in the ones to minimize the information loss from sampling, which can improve the feature extraction ability of the model. Then we reduce the number of parameters on the input side and set only one class in the YOLO layer, which improves the efficiency and accuracy of the detector for locating license plates. Finally, Gated recurrent units(GRU) + Connectionist temporal classification(CTC) was used to build the recognition network to complete the character segmentation-free recognition task of the license plate, significantly shortened the training time and improved the convergence speed and recognition accuracy of the network. The experimental results show that the average recognition precision of the license plate recognition model proposed in this paper reaches 98.98%, which is significantly better than the traditional recognition algorithm, and the recognition effect is good in complex environment with good stability and robustness.  © 2013 IEEE.",Deep learning; GRU; license plate recognition; target detection; YOLOv5; Deep learning; Learning systems; License plates (automobile); Optical character recognition; Deep learning; End to end; Gated recurrent unit; Learning models; License plate location; License plate recognition systems; Licenses plate recognition; Recognition methods; Targets detection; YOLOv5; Location
Scopus,"Zhang, Y.; Zhu, G.; Shi, T.; Zhang, K.; Yan, J.",Small Object Detection in Remote Sensing Images Based on Feature Fusion and Attention,,2022,,,,10.3788/AOS202242.2415001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144635190&doi=10.3788%2fAOS202242.2415001&partnerID=40&md5=a4e59b16d3ddba261697c57f4da16d4c,"To deal with issues such as less feature information and difficult positioning raised by small object detection in remote sensing images, this paper proposes a remote sensing image small-target detection algorithm FFAM-YOLO (Feature Fusion and Attention Mechanism YOLO) based on feature fusion and attention mechanism. Firstly, in terms of inadequate effective information in backbone network feature extraction and weak information representation in feature maps, the algorithm constructs a feature enhancement module (FEM) to fuse multiple receptive field features in lower-level feature maps and improve the network′s ability in extracting object features. Secondly, with low-level and high-level feature maps obtained by the backbone network, the algorithm′s low-level and high-level feature fusion structures are rebuilt, and a feature fusion module (FFM) is implemented to enhance the feature information of small targets. Thirdly, small object features are accurately captured by cascade attention mechanism (ESM) consisting of enhanced-efficient channel attention (E-ECA) and spatial attention module (SAM). Finally, the small object is detected in the output dualbranch feature maps, and results are delivered. The experimental results show that with the USOD (Unicorn Small Object Dataset), based on the constructed remote sensing images, the proposed algorithm achieves a precision of 91. 9% and a recall of 83. 5%, with an average precision AP of 89% for intersection ratio threshold (IoU) between the prediction box and the ground truth box of 0. 5 and an AP of 32. 6% for IoU of 0. 5∶ 0. 95, respectively, and the detection rate reaches 120 frame/s. The algorithm is with robustness and real-time performance. © 2022 Chinese Optical Society. All rights reserved.",attention mechanism; feature enhancement; feature fusion; machine vision; remote sensing image; small object detection; Feature extraction; Image enhancement; Image fusion; Object detection; Object recognition; Remote sensing; Attention mechanisms; Feature enhancement; Feature information; Feature map; Features fusions; Fusion mechanism; Machine-vision; Remote sensing images; Small object detection; Small objects; Computer vision
Scopus,"El Mazgualdi, C.; Masrour, T.; Barka, N.; El Hassani, I.",A Learning-Based Decision Tool towards Smart Energy Optimization in the Manufacturing Process,,2022,,,,10.3390/systems10050180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140607359&doi=10.3390%2fsystems10050180&partnerID=40&md5=1017e5db1ed847347dba0d41a18b9564,"We developed a self-optimizing decision system that dynamically minimizes the overall energy consumption of an industrial process. Our model is based on a deep reinforcement learning (DRL) framework, adopting three reinforcement learning methods, namely: deep Q-network (DQN), proximal policy optimization (PPO), and advantage actor–critic (A2C) algorithms, combined with a self-predicting random forest model. This smart decision system is a physics-informed DRL that sets the key industrial input parameters to optimize energy consumption while ensuring the product quality based on desired output parameters. The system is self-improving and can increase its performances without further human assistance. We applied the approach to the process of heating tempered glass. Indeed, the identification and control of tempered glass parameters is a challenging task requiring expertise. In addition, optimizing energy consumption while dealing with this issue is of great value-added. The evaluation of the decision system under the three configurations has been performed and consequently, outcomes and conclusions have been explained in this paper. Our intelligent decision system provides an optimized set of parameters for the heating process within the acceptance limits while minimizing overall energy consumption. This work provides the necessary foundations to address energy optimization issues related to process parameterization from theory to practice and providing real industrial application; further research opens a new horizon towards intelligent and sustainable manufacturing. © 2022 by the authors.",autonomous process control; deep reinforcement learning; dual-optimization problem; energy self-optimization; glass tempering process; Industry 4.0; intelligent manufacturing; process parameters self-configuration
Scopus,"Liu, G.; Li, J.; Yan, S.; Liu, R.",A Novel Small Target Detection Strategy: Location Feature Extraction in the Case of Self-Knowledge Distillation,,2023,,,,10.3390/app13063683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151551267&doi=10.3390%2fapp13063683&partnerID=40&md5=12f8b0a2b5963c1237b7e74044ae3709,"Small target detection has always been a hot and difficult point in the field of target detection. The existing detection network has a good effect on conventional targets but a poor effect on small target detection. The main challenge is that small targets have few pixels and are widely distributed in the image, so it is difficult to extract effective features, especially in the deeper neural network. A novel plug-in to extract location features of the small target in the deep network was proposed. Because the deep network has a larger receptive field and richer global information, it is easier to establish global spatial context mapping. The plug-in named location feature extraction establishes the spatial context mapping in the deep network to obtain the global information of scattered small targets in the deep feature map. Additionally, the attention mechanism can be used to strengthen attention to the spatial information. The comprehensive effect of the above two can be utilized to realize location feature extraction in the deep network. In order to improve the generalization of the network, a new self-distillation algorithm was designed for pre-training that could work under self-supervision. The experiment was conducted on the public datasets (Pascal VOC and Printed Circuit Board Defect dataset) and the self-made dedicated small target detection dataset, respectively. According to the diagnosis of the false-positive error distribution, the location error was significantly reduced, which proved the effectiveness of the plug-in proposed for location feature extraction. The mAP results can prove that the detection effect of the network applying the location feature extraction strategy is much better than the original network. © 2023 by the authors.",attention mechanism; location feature extraction; self-knowledge distillation; small target detection
Scopus,"Zhang, Y.; Zhang, S.; Xin, D.; Chen, D.",A Small Target Pedestrian Detection Model Based on Autonomous Driving,,2023,,,,10.1155/2023/5349965,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149415458&doi=10.1155%2f2023%2f5349965&partnerID=40&md5=1fb258d4a28bc40284127cb58e191634,"Since small-target pedestrians account for a small proportion of pixels in images and lack texture features, the feature information of small-target pedestrians is often ignored in the feature extraction process, leading to reduced accuracy and poor robustness. To improve the accuracy of small-target pedestrian detection and the anti-interference ability of the model, a small-target pedestrian detection model that fuses residual networks and feature pyramids is proposed. First, a residual block with a discard layer is constructed to replace the standard residual block in the residual network structure to reduce the complexity of the model computation process and solve the problems of gradient disappearance and explosion in the deep network. Then, feature selection and feature alignment modules are added to the lateral connection part of the feature pyramid to enhance important pedestrian features in the input image, and the multiscale feature fusion capability of the model is enhanced for small-target pedestrians, thereby improving the detection accuracy of small-target pedestrians and solving the problems of feature misalignment and ignored multiscale features in the feature pyramid network. Finally, a cascaded autofocus query module is proposed to increase the inference speed of the feature pyramid network through focusing and querying, thus improving the performance and efficiency of small-target pedestrian detection. The experimental results show that the proposed model achieves better detection results than previous models.  © 2023 Yang Zhang et al.",Autonomous vehicles; Feature Selection; Textures; Autonomous driving; Feature information; Feature pyramid; Model-based OPC; Multi-scale features; Pedestrian detection; Pedestrian detection models; Pyramid network; Small targets; Texture features; Image enhancement
Scopus,"Bomantara, Y.A.; Mustafa, H.; Bartholomeus, H.; Kooistra, L.",Detection of Artificial Seed-like Objects from UAV Imagery,,2023,,,,10.3390/rs15061637,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151475481&doi=10.3390%2frs15061637&partnerID=40&md5=175da9d210ca1e74e9df522988b6f26f,"In the last two decades, unmanned aerial vehicle (UAV) technology has been widely utilized as an aerial survey method. Recently, a unique system of self-deployable and biodegradable microrobots akin to winged achene seeds was introduced to monitor environmental parameters in the air above the soil interface, which requires geo-localization. This research focuses on detecting these artificial seed-like objects from UAV RGB images in real-time scenarios, employing the object detection algorithm YOLO (You Only Look Once). Three environmental parameters, namely, daylight condition, background type, and flying altitude, were investigated to encompass varying data acquisition situations and their influence on detection accuracy. Artificial seeds were detected using four variants of the YOLO version 5 (YOLOv5) algorithm, which were compared in terms of accuracy and speed. The most accurate model variant was used in combination with slice-aided hyper inference (SAHI) on full resolution images to evaluate the model’s performance. It was found that the YOLOv5n variant had the highest accuracy and fastest inference speed. After model training, the best conditions for detecting artificial seed-like objects were found at a flight altitude of 4 m, on an overcast day, and against a concrete background, obtaining accuracies of 0.91, 0.90, and 0.99, respectively. YOLOv5n outperformed the other models by achieving a mAP0.5 score of 84.6% on the validation set and 83.2% on the test set. This study can be used as a baseline for detecting seed-like objects under the tested conditions in future studies. © 2023 by the authors.",background type; deep learning; flying height; light conditions; object detection; unmanned aerial vehicles; Aircraft detection; Antennas; Data acquisition; Deep learning; Object recognition; Unmanned aerial vehicles (UAV); Aerial vehicle; Artificial seeds; Background type; Condition; Deep learning; Environmental parameter; Flying heights; Light conditions; Objects detection; Unmanned aerial vehicle; Object detection
Scopus,"Hasan, M.; Hanawa, J.; Goto, R.; Suzuki, R.; Fukuda, H.; Kuno, Y.; Kobayashi, Y.","LiDAR-based detection, tracking, and property estimation: A contemporary review",,2022,,,,10.1016/j.neucom.2022.07.087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136156443&doi=10.1016%2fj.neucom.2022.07.087&partnerID=40&md5=a1a7a7fdaba21c60be60d6a4b4d441f5,"Object detection, Person tracking, and Person property estimation (PPE) are identical innovation areas trying to improve their accuracy in different parameters to fit various real applications. For many years, so much research has been done in these fields. Many scientists also used many more techniques and algorithms. But most of the innovations were deeply based on image-based analysis, where cameras were the critical components of data acquisition. Over the years, new technologies arrived, and different types of research are happening. Rather than cameras, some other sensors, like infrared, depth cameras, and very recently LiDAR sensors, are used to estimate person properties, track them, as well as to detect them. Especially, height, age, gender, region, etc., parameters can be measured as person property. Eventually, 3D object detection by LiDAR will be a state-of-the-art research field with the advent of autonomous driving initiations. We studied many articles and found enthusiastic outcomes with these sensor setups to understand contemporary technology and its efficacy. We categorized these research articles into video camera-based studies and other sensor-based studies. So many surveys have been done on video-based analysis, even with deep learning techniques. Another sensor-based research is very recent, and we do not get enough study on it. We thought to summarize these studies in a survey article, especially LiDAR-based analysis. This article covered most of the recent possible sensor-based studies of detection, person tracking and property estimation except cameras (all, RGB, RGB-D, etc.) based learning. © 2022 Elsevier B.V.",LiDAR Sensor; Person Property Estimation; Person Recognition; Person Tracking; Sensor-based Sensing; Data acquisition; Deep learning; Object recognition; Optical radar; Surveys; Video cameras; Detection estimation; LiDAR sensor; Objects detection; Person property estimation; Person recognition; Person tracking; Property; Property estimation; Real applications; Sensor-based sensing; analytic method; Article; automated pattern recognition; deep learning; human; image analysis; three-dimensional imaging; videorecording; Object detection
Scopus,"Xia, Z.; Shi, Y.",Small object in the dark light scene using dual-branch channel and spatial,,2023,,,,10.1117/1.JEI.32.2.023037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159405390&doi=10.1117%2f1.JEI.32.2.023037&partnerID=40&md5=58fce970634d30a99211445b0ab872e5,"Object detection is an enabling technology of computer vision for locating instances and categorizing classes of objects in images or videos. It has made enormous strides with significant growing of deep learning research over the last decade. However, there are severe challenges on small object detection in complex scenes as they appear with low resolution and have not enough contrast from the background information. This disturbance may cause missed detection and detection accuracy decline for small objects. We propose a small object detection network in the dark light scene based on improved YOLOv5. The network takes YOLOv5 as the baseline and incorporates a channel and spatial dual-branch backbone module, which enhances the details by fusing the features of the two branches. We also introduce a densely linked feature fusion network before detection layers with receptive field block. The fusion network integrates deep features with shallow ones across different scales. A data augmentation module is used to enhance the brightness and limit the contrast for small object detection in the dark light scene. Experiments are carried out on the Dark Face and darkened Vis Drone dataset. The results show that the evaluation index of the proposed method is better than that of the comparison methods. From the detected images, it is obvious that the undetected frame of the small object in the dark light scene decreases and the detection accuracy improves. All of the results show that our model has better performance than some existing methods for small object detection in the dark light scene.  © 2023 SPIE and IS&T.",attention mechanism; dark light scene; data augmentation; small object; Deep learning; Image enhancement; Network layers; Object recognition; Attention mechanisms; Complex scenes; Dark light scene; Data augmentation; Detection accuracy; Enabling technologies; Lower resolution; Objects detection; Small object detection; Small objects; Object detection
Scopus,"Rakhmonov, A.A.U.; Subramanian, B.; Kim, T.; Kim, J.",Airy YOLOv5 for Disabled Sign Detection,,2023,,,,10.1109/ICUFN57995.2023.10200853,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169295016&doi=10.1109%2fICUFN57995.2023.10200853&partnerID=40&md5=ade4c5a58df6740d603ca13b2a9e18e3,"Designated parking spaces for individuals with disabilities are only meant to be used by vehicles with proper handicapped signage. Real-time monitoring is necessary to ensure that only authorized vehicles are parked in these spaces and to prevent unauthorized vehicles from using them. First, this research proposes to replace the backbone of a baseline YOLOv5 model which has 9 blocks with 6 EfficientNet blocks with less parameters but still have a higher accuracy in detecting disabled signs among other signages on the windshield of cars. Second, to compensate for the loss of blocks we have included an attention mechanism before detection part in our architecture which allows us to focus on the important regions needed for the task. Additionally, we propose to use a better optimizer AdamW to prevent overfitting. Based on these improvements, we have created a new object detector named Airy YOLOv5. To evaluate the effectiveness of our proposed method, a dataset containing images of cars with disabled signage on their windshields will be gathered and labeled. Experiments using this dataset show that our model achieves a better F1 score of 0.67 with 5 percent less parameters compared to the baseline model.  © 2023 IEEE.",depthwise separable convolution; disabled signage; small object detection; supervised learning; Machine learning; Object recognition; Traffic signs; Windshields; Attention mechanisms; Depthwise separable convolution; Disabled signage; High-accuracy; Optimizers; Overfitting; Parking spaces; Real time monitoring; Sign detection; Small object detection; Object detection
Scopus,"Musunuri, Y.R.; Kwon, O.-S.; Kung, S.-Y.",SRODNet: Object Detection Network Based on Super Resolution for Autonomous Vehicles,,2022,,,,10.3390/rs14246270,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144622410&doi=10.3390%2frs14246270&partnerID=40&md5=6ccfa3680334d84c9b14bd507b2322c1,"Object detection methods have been applied in several aerial and traffic surveillance applications. However, object detection accuracy decreases in low-resolution (LR) images owing to feature loss. To address this problem, we propose a single network, SRODNet, that incorporates both super-resolution (SR) and object detection (OD). First, a modified residual block (MRB) is proposed in the SR to recover the feature information of LR images, and this network was jointly optimized with YOLOv5 to benefit from hierarchical features for small object detection. Moreover, the proposed model focuses on minimizing the computational cost of network optimization. We evaluated the proposed model using standard datasets such as VEDAI-VISIBLE, VEDAI-IR, DOTA, and Korean highway traffic (KoHT), both quantitatively and qualitatively. The experimental results show that the proposed method improves the accuracy of vehicular detection better than other conventional methods. © 2022 by the authors.",autonomous vehicles; modified residual block; object detection network; remote sensing data; super-resolution; Antennas; Autonomous vehicles; Feature extraction; Object recognition; Optical resolving power; Remote sensing; Autonomous Vehicles; Detection networks; Low resolution images; Modified residual block; Network-based; Object detection method; Object detection network; Objects detection; Remote sensing data; Superresolution; Object detection
Scopus,"Lin, Z.-R.; Wang, H.-C.",Efficient Optimization and Compression of Autonomous Vehicle Target Detection Models,,2023,,,,10.1109/SSIM59263.2023.10469478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190376060&doi=10.1109%2fSSIM59263.2023.10469478&partnerID=40&md5=075cf29fa848bff438fe5d54a130e9a1,"Target detection is a crucial element of computer vision and often requires complex models and extensive computing resources. This inherent complexity poses significant challenges for use especially in the demanding field of autonomous vehicles. Thus, for the urgent need to reduce resource consumption while preserving the accuracy of target detection, we optimized the YOLO model by utilizing depthwise separable convolution. The model reduced resource usage without compromising detection performance. Additionally, quantitative compression techniques were incorporated to decrease the model size for easy deployment. The optimization strategy better-tailored target detection solutions to the specific requirements of autonomous vehicle applications.  © 2023 IEEE.",autonomous vehicles; model optimization; resource efficiency; YOLOv3-tiny; Autonomous Vehicles; Complex model; Computing resource; Detection models; Efficient optimisation; Model optimization; Resource efficiencies; Targets detection; Vehicle targets; YOLOv3-tiny; Autonomous vehicles
Scopus,"Wei, J.; Wang, Q.; Zhao, Z.",Interactive Transformer for Small Object Detection,,2023,,,,10.32604/cmc.2023.044284,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179132222&doi=10.32604%2fcmc.2023.044284&partnerID=40&md5=67edca15d0000e1f47a52a2fe87c8ef5,"The detection of large-scale objects has achieved high accuracy, but due to the low peak signal to noise ratio (PSNR), fewer distinguishing features, and ease of being occluded by the surroundings, the detection of small objects, however, does not enjoy similar success. Endeavor to solve the problem, this paper proposes an attention mechanism based on cross-Key values. Based on the traditional transformer, this paper first improves the feature processing with the convolution module, effectively maintaining the local semantic context in the middle layer, and significantly reducing the number of parameters of the model. Then, to enhance the effectiveness of the attention mask, two Key values are calculated simultaneously along Query and Value by using the method of dual-branch parallel processing, which is used to strengthen the attention acquisition mode and improve the coupling of key information. Finally, focusing on the feature maps of different channels, the multi-head attention mechanism is applied to the channel attention mask to improve the feature utilization effect of the middle layer. By comparing three small object datasets, the plug-and-play interactive transformer (IT-transformer) module designed by us effectively improves the detection results of the baseline. © 2023 Tech Science Press. All rights reserved.",attention; plug-and-play; Small object detection; transformer; Object recognition; Semantics; Signal to noise ratio; Attention; Attention mechanisms; High-accuracy; Key values; Large-scale objects; Middle layer; Plug-and-play; Small object detection; Small objects; Transformer; Object detection
Scopus,"Tarun, R.; Priya Esther, B.",Traffic Anomaly Alert Model to Assist ADAS Feature based on Road Sign Detection in Edge Devices,,2023,,,,10.1109/ICESC57686.2023.10193442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168314654&doi=10.1109%2fICESC57686.2023.10193442&partnerID=40&md5=93dd2043dd3c65c48f14e65418556747,"Gazing into the future of driver assistance systems such as Advance Driving Assistance System (ADAS), Radar, Automatic Emergency Braking (AEB), Lane-Keeping Assist (LKA), Traffic Jam Assist (TJA), and much more systems in affordable prices, which are to increase and improve the safety and security of the driver and the passengers, yet still they cause certain new problems due to some of the short comings of the current systems, when there is anomaly such as renovation work in the freeway. Therefore to overcome those situations, an system with low latency that must be able in to run in edge devices with less computational power and be highly precise, and affordable must be developed to alert the driver if the road has some anomaly when the vehicle is in ADAS mode. In this paper, a system that works in complimentary with ADAS will be able to detect road sign using the real-time capturing of he connected webcam and alert the driver using buzzer and red LED light to take control of the vehicle. To facilitate the road sign detection in edge devices with low latency, less size and high precision EfficientDet-Lite2 model architecture and TensorFlow Lite Model Maker is used to train the dataset, which uses transfer learning to reduce the amount of data for training, combining all that with Raspberry Pi 4 model b as the processor, the system is able to reliably detection and produce a output with up-to 90% average precision and low latency.  © 2023 IEEE.",Advanced driver-assistance systems; Deep Learning; edge device; Raspberry Pi; TensorFlow; Automobile drivers; Deep learning; Learning systems; Roads and streets; Traffic congestion; Traffic signs; Deep learning; Driving assistance systems; Edge device; Feature-based; Low latency; Raspberry pi; Road sign detection; System features; Tensorflow; Traffic anomalies; Advanced driver assistance systems
Scopus,"Neves, F.S.; Claro, R.M.; Pinto, A.M.",End-to-End Detection of a Landing Platform for Offshore UAVs Based on a Multimodal Early Fusion Approach,,2023,,,,10.3390/s23052434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149745238&doi=10.3390%2fs23052434&partnerID=40&md5=c2be59dfe67447284abecb218fc81b2d,"A perception module is a vital component of a modern robotic system. Vision, radar, thermal, and LiDAR are the most common choices of sensors for environmental awareness. Relying on singular sources of information is prone to be affected by specific environmental conditions (e.g., visual cameras are affected by glary or dark environments). Thus, relying on different sensors is an essential step to introduce robustness against various environmental conditions. Hence, a perception system with sensor fusion capabilities produces the desired redundant and reliable awareness critical for real-world systems. This paper proposes a novel early fusion module that is reliable against individual cases of sensor failure when detecting an offshore maritime platform for UAV landing. The model explores the early fusion of a still unexplored combination of visual, infrared, and LiDAR modalities. The contribution is described by suggesting a simple methodology that intends to facilitate the training and inference of a lightweight state-of-the-art object detector. The early fusion based detector achieves solid detection recalls up to 99% for all cases of sensor failure and extreme weather conditions such as glary, dark, and foggy scenarios in fair real-time inference duration below 6 ms. © 2023 by the authors.",3D LiDAR; computer vision; early-fusion; object detection; RGB camera; sensor fusion; thermal camera; Aircraft detection; Cameras; Object recognition; Offshore oil well production; Optical radar; Robot vision; Unmanned aerial vehicles (UAV); 3d LiDAR; Early fusion; End to end; Landing platforms; Objects detection; Offshores; RGB cameras; Sensor fusion; Sensors' failures; Thermal camera; Object detection
Scopus,"Chaturvedi, R.P.; Ghose, U.",A review of small object and movement detection based loss function and optimized technique,,2023,,,,10.1515/jisys-2022-0324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156095014&doi=10.1515%2fjisys-2022-0324&partnerID=40&md5=6170fefa93ca175f927ddcbcc580e171,"The objective of this study is to supply an overview of research work based on video-based networks and tiny object identification. The identification of tiny items and video objects, as well as research on current technologies, are discussed first. The detection, loss function, and optimization techniques are classified and described in the form of a comparison table. These comparison tables are designed to help you identify differences in research utility, accuracy, and calculations. Finally, it highlights some future trends in video and small object detection (people, cars, animals, etc.), loss functions, and optimization techniques for solving new problems.  © 2023 the author(s), published by De Gruyter.",detection of small objects; detection of video objects; loss functions; optimization; Object recognition; Detection of small object; Detection of video object; Loss functions; Losses optimisation; Movement detection; Optimisations; Optimization techniques; Small object detection; Small objects; Video objects; Object detection
Scopus,"Li, H.; Todd, Z.; Bielski, N.","Equirectangular Image Data Detection, Segmentation and Classification of Varying Sized Traffic Signs: A Comparison of Deep Learning Methods",,2023,,,,10.3390/s23073381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152333384&doi=10.3390%2fs23073381&partnerID=40&md5=d488ef00d835b5207e9de2aadeb926d9,"There are known limitations in mobile omnidirectional camera systems with an equirectangular projection in the wild, such as momentum-caused object distortion within images, partial occlusion and the effects of environmental settings. The localization, instance segmentation and classification of traffic signs from image data is of significant importance to applications such as Traffic Sign Detection and Recognition (TSDR) and Advanced Driver Assistance Systems (ADAS). Works show the efficacy of using state-of-the-art deep pixel-wise methods for this task yet rely on the input of classical landscape image data, automatic camera focus and collection in ideal weather settings, which does not accurately represent the application of technologies in the wild. We present a new processing pipeline for extracting objects within omnidirectional images in the wild, with included demonstration in a Traffic Sign Detection and Recognition (TDSR) system. We compare Mask RCNN, Cascade RCNN, and Hybrid Task Cascade (HTC) methods, while testing RsNeXt 101, Swin-S and HRNetV2p backbones, with transfer learning for localization and instance segmentation. The results from our multinomial classification experiment show that using our proposed pipeline, given that a traffic sign is detected, there is above a 95% chance that it is classified correctly between 12 classes despite the limitations mentioned. Our results on the projected images should provide a path to use omnidirectional images with image processing to enable the full surrounding awareness from one image source. © 2023 by the authors.",deep learning; omnidirectional camera imaging; small object detection; traffic sign detection; Advanced driver assistance systems; Automobile drivers; Classification (of information); Deep learning; Image classification; Image segmentation; Learning systems; Object detection; Pipelines; Video cameras; Data-detection; Deep learning; Image data; Localisation; Omnidirectional camera imaging; Omnidirectional cameras; Omnidirectional image; Small object detection; Traffic sign detection; Traffic sign detection and recognition; Traffic signs
Scopus,"Nigam, N.; Singh, D.P.; Choudhary, J.",A Review of Different Components of the Intelligent Traffic Management System (ITMS),,2023,,,,10.3390/sym15030583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151661781&doi=10.3390%2fsym15030583&partnerID=40&md5=cb5ab3853670a129ae4885e96619d632,"Traffic congestion is a serious challenge in urban areas. So, to address this challenge, the intelligent traffic management system (ITMS) is used to manage traffic on road networks. Managing traffic helps to focus on environmental impacts as well as emergency situations. However, the ITMS system has many challenges in analyzing scenes of complex traffic. New technologies such as computer vision (CV) and artificial intelligence (AI) are being used to solve these challenges. As a result, these technologies have made a distinct identity in the surveillance industry, particularly when it comes to keeping a constant eye on traffic scenes. There are many vehicle attributes and existing approaches that are being used in the development of ITMS, along with imaging technologies. In this paper, we reviewed the ITMS-based components that describe existing imaging technologies and existing approaches on the basis of their need for developing ITMS. The first component describes the traffic scene and imaging technologies. The second component talks about vehicle attributes and their utilization in existing vehicle-based approaches. The third component explains the vehicle’s behavior on the basis of the second component’s outcome. The fourth component explains how traffic-related applications can assist in the management and monitoring of traffic flow, as well as in the reduction of congestion and the enhancement of road safety. The fifth component describes the different types of ITMS applications. The sixth component discusses the existing methods of traffic signal control systems (TSCSs). Aside from these components, we also discuss existing vehicle-related tools such as simulators that work to create realistic traffic scenes. In the last section named discussion, we discuss the future development of ITMS and draw some conclusions. The main objective of this paper is to discuss the possible solutions to different problems during the development of ITMS in one place, with the help of components that would play an important role for an ITMS developer to achieve the goal of developing efficient ITMS. © 2023 by the authors.",intelligent traffic management system (ITMS); simulators; traffic signal control systems (TSCSs); vehicle detection; vehicle tracking
Scopus,"Wang, K.; Zhou, T.; Zhang, Z.; Chen, T.; Chen, J.",PVF-DectNet: Multi-modal 3D detection network based on Perspective-Voxel fusion,,2023,,,,10.1016/j.engappai.2023.105951,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147541158&doi=10.1016%2fj.engappai.2023.105951&partnerID=40&md5=c660354a3e5abf03a7c7022f2b327717,"The detection of small objects such as pedestrians still poses challenges to the LiDAR-based 3D object detection due to the sparseness and disorder of point clouds. Conversely, images from cameras can provide rich semantic information, which makes these small-sized objects easy to be detected. To take use of the advantages of both devices to achieve better 3D object detection, research on the fusion of LiDAR and camera information is now being conducted. The existing fusion methods between point clouds and image are normally weighed more on the point clouds. Hence the semantic information of images is not fully utilized. We propose a new fusion method named PVFusion to try to fuse more image features. We first divide each point into a separate perspective voxel and project the voxel onto the image feature maps. Then the semantic feature of the perspective voxel is fused with the geometric feature of the point. A 3D object detection model (PVF-DectNet) is designed using PVFusion. During training we employ the ground truth paste (GT-Paste) data augmentation and solve the occlusion problem caused by newly added object. The KITTI validation set is used to validate the PVF-DectNet, which shows 3.6% AP improvement over the other feature fusion methods in pedestrian detection. On the KITTI test set, the PVF-DectNet outperforms the other multi-modal SOTA methods by 2.2% AP in pedestrian detection. And PVFusion shows better detection performance for sparse point clouds than PointFusion in both car and pedestrian categories. As for 32 beams LiDAR scene, there are 4.2% AP increment in moderate difficulty car category and 5.2% mAP improvement in pedestrian category. © 2023 Elsevier Ltd",3D object detection; Deep learning; LiDAR-camera-based detector; Sensor fusion; Small objects; 3D modeling; Cameras; Deep learning; Image fusion; Object recognition; Optical radar; Semantics; 3D object; 3d object detection; Camera-based; Deep learning; LiDAR-camera-based detector; Multi-modal; Objects detection; Point-clouds; Sensor fusion; Small objects; Object detection
Scopus,"Gao, Z.; Wang, H.; Sun, Y.; Wang, F.",A Review of Object Detection Techniques for Automobile Assisted Driving9,,2023,,,,10.23919/CCC58697.2023.10240672,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175544203&doi=10.23919%2fCCC58697.2023.10240672&partnerID=40&md5=360b78b2bd7160f1befece655ed55204,"In this paper, the development of current target detection technology and its advantages and disadvantages are described in the context of foreign literature, mainly focusing on the advantages and disadvantages of traditional visual detection algorithms and deep learning detection methods in image target detection algorithms are analyzed and compared. Radar target detection algorithms: traditional point cloud features and deep learning point cloud features target detection algorithms are analyzed and compared. Finally in the analysis of multi-sensor fusion algorithms. Random model fusion methods and artificial intelligence fusion methods are described, respectively. The characteristics and research status of each detection method and fusion method are analyzed together with the literature, and the future development trend of assisted driving target detection methods is prospected: detection methods are lightweight to meet in-vehicle needs, three or more sensors are fused to achieve the purpose of improving detection accuracy, and multi-sensor fusion is the future development trend for target detection of harsh road conditions and small and distant targets. © 2023 Technical Committee on Control Theory, Chinese Association of Automation.",aided driving; artificial intelligence; camera; data fusion; radar; target detection; Learning algorithms; Learning systems; Object detection; Signal detection; Tracking radar; 'current; Aided driving; Detection methods; Development trends; Fusion methods; Multi-sensor fusion; Objects detection; Point-clouds; Target detection algorithm; Targets detection; Deep learning
Scopus,"Williams, K.C.; O'toole, M.D.; Peyton, A.J.",Scrap Metal Classification Using Magnetic Induction Spectroscopy and Machine Vision,,2023,,,,10.1109/TIM.2023.3284930,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162630454&doi=10.1109%2fTIM.2023.3284930&partnerID=40&md5=518e473a56d75149401e64d117547ac3,"The need to recover and recycle material toward building a circular economy is increasingly a global imperative. Nonferrous metals in particular are highly recyclable and can be extracted using processes such as eddy current separation. However, their further separation into recyclable groups based on metal or alloy continues to pose a challenge. Recently, we proposed a new technique to discriminate between nonferrous metals: magnetic induction spectroscopy (MIS) measures how a metal fragment scatters an excitation magnetic field over different frequencies. MIS is related to conductivity, which can be used to classify the fragment according to this property. In this article, we demonstrate for the first time the use of MIS with machine learning to classify nonferrous scrap metals drawn from commercial waste streams. Two approaches are explored: 1) MIS over a bandwidth from 3 to 90 kHz and 2) the combination of MIS with the physical color of the metal samples. We show that MIS alone can obtain purity and recovery rates >80% for most metal groups and waste streams, rising to >93% for stainless steel. The exception was the Zorba waste stream where the mix of aluminum alloys within the sample set led to poor conductivity contrasts. The introduction of color substantially improved results in this case, increasing purity and recovery rates by 20%-35% points. Of the machine-learning models tested, we found that random forest (RF), extra trees, and support vector machine (SVM) algorithms consistently achieved the highest performance. © 1963-2012 IEEE.",Classification algorithms; electromagnetic induction; machine vision; recycling; waste recovery; Computer vision; Copper; Electromagnetic induction; Learning algorithms; Learning systems; Magnetic separation; Recycling; Scrap metal reprocessing; Classification algorithm; Conductivity; Machine-vision; Non-ferrous metals; Recovery rate; Recyclables; Recycle materials; Recycling; Waste recoveries; Waste stream; Magnetic resonance imaging
Scopus,"Lin, T.; Zhang, Y.",The application of improved Yolov5s in anomaly detection of circuit lines for small targets,,2023,,,,10.1109/ICIIBMS60103.2023.10347895,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182274727&doi=10.1109%2fICIIBMS60103.2023.10347895&partnerID=40&md5=7ca17ee1847200637c444e60b2b02163,"The purpose of this study is to address the challenges and limitations of detecting small anomalies in circuit lines by modifying Yolov5s model. We optimized the model to improve its performance in detecting small abnormal targets. Taking the detection of exposed copper wires in meter wiring as an example, we conducted a series of experiments to validate the performance of the improved Yolov5s model. The experimental results showed that our improved model achieved a significant performance improvement on the self-made dataset for exposed copper wires in meter wiring. Compared to the baseline method, our improved model achieved a 4.5 percentage point increase in MAP [IoU=0.5:0.95], while maintaining accurate identification and precise localization of small abnormal targets. This provides a more reliable and efficient solution for circuit anomaly detection, with potential applications in the field. © 2023 IEEE.",anomaly detection; circuit lines; improved Yolov5s; small targets; Copper; Timing circuits; Wire; Anomaly detection; Baseline methods; Circuit line; Copper wires; Improved yolov5s; Localisation; Percentage points; Performance; Small targets; Anomaly detection
Scopus,"Wang, G.-Q.; Zhang, C.-Z.; Chen, M.-S.; Lin, Y.-C.; Tan, X.-H.; Liang, P.; Kang, Y.-X.; Zeng, W.-D.; Wang, Q.",Yolo-MSAPF: Multiscale Alignment Fusion with Parallel Feature Filtering Model for High Accuracy Weld Defect Detection,,2023,,,,10.1109/TIM.2023.3302372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167808991&doi=10.1109%2fTIM.2023.3302372&partnerID=40&md5=dd5010f30c486c6332fc8a61ef08e681,"This work aims to improve the low accuracy caused by interference information during real-time weld surface detection. First, a weld surface dataset with 7580 pictures containing eight types of defects was established. Second, an improved detection model named Yolo-MSAPF was designed based on Yolo-v5 model and verified by the self-established database. Finally, a real-time detection system was built to analyze the performance of the detection model in an industrial environment. The design principle of the Yolo-MSAPF is to eliminate interference information but enhance necessary features in each scale as much as possible by multiscale alignment fusion (MSAF) with parallel features filtering (PFF) modules (i.e., MSAPF strategy). For the MSAF, not only the accuracy but the richness of fused features is guaranteed by aligning features at one level to fuse all other scales. After that, the fused features in each scale were individually filtered out in parallel spatial and channel in the PFF module. The results show that rate for the images with missing detection for eight types of defects sharply drops from 21.47% to 1.68% when the MSAPF strategy is used. Moreover, it is worth noting that the mAP@0.5 of the Yolo-MSAPF model reaches 95.3%, which is similar to the Yolo-v7 model, while the number of parameters is reduced by 30.1%, compared with the baseline model. Additionally, the great ability to screen out unqualified weld is also verified in the industrial environment. Soon, code and dataset will be available at https://github.com/Luckycat518/Yolo-MSAPF. © 1963-2012 IEEE.",Detection system; multiscale fusion; parallel enhanced attention; weld defect detection; Yolo-v5; Information filtering; Welds; Detection models; Detection system; Feature filtering; Industrial environments; Muli-scale fusion; Multi-scales; Parallel enhanced attention; Weld defects detections; Weld surfaces; Yolo-v5; Defects
Scopus,"Marconato, E.; Bontempo, G.; Ficarra, E.; Calderara, S.; Passerini, A.; Teso, S.","Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal",,2023,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174387101&partnerID=40&md5=66863975b5c8e72aabbe6c4875f67146,"We introduce Neuro-Symbolic Continual Learning, where a model has to solve a sequence of neuro-symbolic tasks, that is, it has to map sub-symbolic inputs to high-level concepts and compute predictions by reasoning consistently with prior knowledge. Our key observation is that neuro-symbolic tasks, although different, often share concepts whose semantics remains stable over time. Traditional approaches fall short: existing continual strategies ignore knowledge altogether, while stock neuro-symbolic architectures suffer from catastrophic forgetting. We show that leveraging prior knowledge by combining neurosymbolic architectures with continual strategies does help avoid catastrophic forgetting, but also that doing so can yield models affected by reasoning shortcuts. These undermine the semantics of the acquired concepts, even when detailed prior knowledge is provided upfront and inference is exact, and in turn continual performance. To overcome these issues, we introduce COOL, a COncept-level cOntinual Learning strategy tailored for neuro-symbolic continual problems that acquires high-quality concepts and remembers them over time. Our experiments on three novel benchmarks highlights how COOL attains sustained high performance on neuro-symbolic continual learning tasks in which other strategies fail. © 2023 Proceedings of Machine Learning Research. All rights reserved.",Benchmarking; Knowledge management; Machine learning; Catastrophic forgetting; Concept levels; Continual learning; Knowledge reasoning; Learning strategy; Performance; Prior-knowledge; Sub-symbolic; Traditional approaches; Yield models; Semantics
Scopus,"Wang, J.; Yuan, Y.; Luo, Z.; Xie, K.; Lin, D.; Iqbal, U.; Fidler, S.; Khamis, S.",Learning Human Dynamics in Autonomous Driving Scenarios,,2023,,,,10.1109/ICCV51070.2023.01901,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181564134&doi=10.1109%2fICCV51070.2023.01901&partnerID=40&md5=0ea8ed361c8bcb9f1e8df7a5bb7e6dfb,"Simulation has emerged as an indispensable tool for scaling and accelerating the development of self-driving systems. A critical aspect of this is simulating realistic and diverse human behavior and intent. In this work, we propose a holistic framework for learning physically plausible human dynamics from real driving scenarios, narrowing the gap between real and simulated human behavior in safety-critical applications. We show that state-of-the-art methods underperform in driving scenarios where video data is recorded from moving vehicles, and humans are frequently partially or fully occluded. Furthermore, existing methods often disregard the global scene where humans are situated, resulting in various motion artifacts like foot sliding, floating, or ground penetration. To address this challenge, we propose an approach that incorporates physics with a reinforcement learning-based motion controller to learn human dynamics for driving scenarios. Our framework can simulate physically plausible human dynamics that accurately match observed human motions and infill motions for occluded body parts, while improving the physical plausibility of the entire motion sequence. Experiments on the challenging Waymo Open Dataset show that our method outperforms state-of-the-art motion capture approaches significantly in recovering high-quality, physically plausible, and scene-aware human dynamics. © 2023 IEEE.",Automobile bodies; Autonomous vehicles; Dynamics; Reinforcement learning; Safety engineering; Autonomous driving; Driving systems; Holistic frameworks; Human behaviors; Human dynamics; Indispensable tools; Real drivings; Safety critical applications; Scalings; Self drivings; Behavioral research
Scopus,"Ma, C.; Fu, Y.; Wang, D.; Guo, R.; Zhao, X.; Fang, J.",YOLO-UAV: Object Detection Method of Unmanned Aerial Vehicle Imagery Based on Efficient Multi-Scale Feature Fusion,,2023,,,,10.1109/ACCESS.2023.3329713,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177566813&doi=10.1109%2fACCESS.2023.3329713&partnerID=40&md5=a3950013a20ef88f072be073ab8f0fc9,"As Unmanned Aerial Vehicle (UAV) remote sensing technology progresses, the utilization of deep learning in UAV imagery object detection has become more prevalent. However, detecting small targets in complex backgrounds and distinguishing dense targets remains a major challenge. To address these issues and improve object detection efficiency, this study proposes an UAV imagery object detection method called YOLO-UAV by optimizing YOLOv5. YOLO-UAV first reconstructs the backbone and feature fusion networks by simplifying the network structure and reducing computational burden. The employment of a Dense-CSPDarknet53 backbone network, fashioned via the incorporation of dense connections, facilitates the extraction of latent image information through the recurrent utilization of features. In the Neck structure, an efficient feature fusion block with structural re-parameterization and ELAN strategies is integrated to effectively reduce interference from complex background noise while extracting more accurate and rich features. In addition, by proposing GS-Decoupled Head, this approach diminishes the parameter count of the decoupled head without compromising accuracy. It also separates classification tasks from regression tasks to lessen the influence of task disparities on prediction bias. To tackle the discrepancy between positive and negative samples in bounding box regression tasks, this study introduces a new loss function, Focal-ECIoU, capable of expediting network convergence and improve model positioning ability. Experimental findings from the public VisDrone2019 dataset indicate that YOLO-UAV outperforms other advanced object detection methods in comprehensive performance. Compared with the baseline model YOLOv5s, YOLO-UAV increased mAP0.5 from 35.1% to 46.7%, while mAP0.5:0.95 increased from 19.1% to 27.4%. For small-scale targets, APsmall increased from 10.2% to 17.3%. The experiment proves that YOLO-UAV performs well in improving object detection accuracy and has strong generalization ability, satisfying the practical requirements of UAV imagery object detection tasks.  © 2013 IEEE.",object detection; UAV imagery; VisDrone2019; YOLO-UAV; Aircraft detection; Antennas; Complex networks; Deep learning; Image enhancement; Object recognition; Remote sensing; Unmanned aerial vehicles (UAV); Aerial vehicle; Complex background; Features fusions; Multi-scale features; Object detection method; Objects detection; Unmanned aerial vehicle imagery; Visdrone2019; YOLO-unmanned aerial vehicle; Object detection
Scopus,"Niu, X.; Wang, Q.; Liu, B.; Zhang, J.",An Automatic Chinaware Fragments Reassembly Method Framework Based on Linear Feature of Fracture Surface Contour,,2022,,,,10.1145/3569091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156103357&doi=10.1145%2f3569091&partnerID=40&md5=6401d8f522a773c40410ee933c87c7ab,"For Chinaware fragments, it is difficult to assemble them directly without considering the wonderful patterns painted on them. Given the simplicity of the Chinaware designs, each object contains similar textures and patterns. Compared to the oddly diverse appliance modeling, the difference between fragments of different Chinaware is small. The fragments are small and the fracture is flat, and there are many potential matching objects. For the Chinaware fragments' reassembly, most of the work is still done manually. There is little available fully automatic reassembly work, and these approaches are for the reassembly of a single Chinaware. But what reality demands is the reassembly of the multi-Chinaware. Therefore, this article proposes an automatic method, whose strategy is analogous to the manual assembly, to accomplish this complex task. First, segment the contours of fracture surfaces of the fragments by the geometric feature of fracture edge; then, the contours are matched using our proposed multi-scale linear feature descriptor; given the massive fragments of multiple objects, we use the inaccurate matching strategy to build a matching set for each fracture surface contour and perform contour matching in this set. In this article, the fracture surface is segmented using the 2D slope information on the fragment edge. The descriptor proposed in the article uses distance triangle towers and chained angles to describe the ""undulations""of the fracture surface. Moreover, the article uses a strict absolute-advantage-principle to reject false matching. In addition, after the initial reconstruction, we will iteratively adjust fragments according to the gap between the fragments to achieve better multi-fragment matching results. In this article, 18 porcelains, a total of 103 pieces, have been tested. Experiments are also carried out for special cases, including fragment reassembly with missing fragments and fragment reassembly with redundant fragments. The experimental results showed the effectiveness of method. However, the types of experimental data we currently have are relatively simple, and there is no way to reassemble the fragments in minutes. We hope to enable faster reassembly of more fragments in the future.  © 2022 Association for Computing Machinery.",contour matching; contour segmentation; Fragment reassembly; linear feature descriptor; Contour measurement; Fracture; Textures; Contour matching; Contour segmentation; Feature descriptors; Fracture surfaces; Fragment reassembly; Linear feature; Linear feature descriptor; Matchings; Reassembly; Surface contour; Iterative methods
Scopus,"Yi, W.; Wang, B.",Research on Underwater Small Target Detection Algorithm Based on Improved YOLOv7,,2023,,,,10.1109/ACCESS.2023.3290903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163568086&doi=10.1109%2fACCESS.2023.3290903&partnerID=40&md5=a0c9946cc7d1109c7237762d61e275c3,"Target detection research has always been difficult when it comes to small target detection in underwater situations. To address the issues of a high miss detection rate and poor underwater scene recognition in underwater small target detection tasks, an improved underwater small target detection technique utilizing YOLOv7 is proposed. To achieve the accuracy rate while considering the high detection speed, the YOLOv7 network is used as the basic network. The network concentrates more crucial feature information of small targets to increase detection accuracy while reducing model complexity by merging the SENet attention mechanism, enhancing the FPN network topology, and incorporating the EIoU loss function. Through simulation tests, the mAP, P, and R metrics are confirmed on the test set and contrasted with other conventional target detection techniques. The outcomes demonstrate that the enhanced algorithm outperforms competing networks and successfully raises detection accuracy on the test set.  © 2013 IEEE.",attention mechanism; EIoU; FPN; underwater small target detection; YOLOv7; Attention mechanisms; Detection accuracy; EIoU; FPN; Small target detection; Target detection algorithm; Targets detection; Test sets; Underwater small target detection; YOLOv7; Network topology
Scopus,"Kumar, S.; Sharma, S.C.; Kumar, R.",Wireless Sensor Network Based Real-Time Pedestrian Detection and Classification for Intelligent Transportation System,,2023,,,,10.33889/IJMEMS.2023.8.1.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147885102&doi=10.33889%2fIJMEMS.2023.8.1.012&partnerID=40&md5=a1863e8541dfb70952c4793c342dde41,"Pedestrian safety has become a critical consideration in developing society especially road traffic, an intelligent transportation need of the hour is the solution left. India tops the world with 11% of global road accidents. With this data, we have moved in the direction of computer vision applications for efficient and accurate pedestrian detection for intelligent transportation systems (ITS). The important application of this research is robot development, traffic management and control, unmanned vehicle driving (UVD), intelligent monitoring and surveillance system, and automatic pedestrian detection system. Much research has focused on pedestrian detection, but sustainable solution-driven research must still be required to overcome road accidents. We have proposed a wireless sensor network-based pedestrian detection system that classifies the real-time set of pedestrian activity and samples the reciprocally received signal strength (RSS) from the sensor node. We applied a histogram of oriented gradient (HOG) descriptor algorithm K-nearest neighbor, decision tree and linear support vector machine to measure the performance and prediction of the target. Also, these algorithms have performed a comparative analysis under different aspects. The linear support vector machine algorithm was trained with 481 samples. The performance achieves the accuracy of 98.90%and has accomplished superior results with a maximum precision of 0.99, recall of 0.98, and F-score of 0.95 with 2% error rate. The model's prediction indicates that it can be used in the intelligent transportation system. Finally, the limitation and the challenges discussed to provide an outlook for future research direction to perform effective pedestrian detection. © 2023 International Journal of Mathematical, Engineering and Management Sciences. All rights reserved.",Computer vision; Intelligent transportation system; Machine learning; Pedestrian detection; Unmanned vehicle driving; Accidents; Computer vision; Intelligent systems; Intelligent vehicle highway systems; Learning systems; Motor transportation; Nearest neighbor search; Pedestrian safety; Roads and streets; Sensor nodes; Support vector machines; Intelligent transportation systems; Linear Support Vector Machines; Machine-learning; Network-based; Pedestrian detection; Pedestrian detection and classifications; Pedestrian detection system; Performance; Real- time; Unmanned vehicle driving; Decision trees
Scopus,"Natarajan, B.; Elakkiya, R.; Bhuvaneswari, R.; Saleem, K.; Chaudhary, D.; Samsudeen, S.H.",Creating Alert Messages Based on Wild Animal Activity Detection Using Hybrid Deep Neural Networks,,2023,,,,10.1109/ACCESS.2023.3289586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163548013&doi=10.1109%2fACCESS.2023.3289586&partnerID=40&md5=822d42b1bf2e2b22efaab77346e36dda,"The issue of animal attacks is increasingly concerning for rural populations and forestry workers. To track the movement of wild animals, surveillance cameras and drones are often employed. However, an efficient model is required to detect the animal type, monitor its locomotion and provide its location information. Alert messages can then be sent to ensure the safety of people and foresters. While computer vision and machine learning-based approaches are frequently used for animal detection, they are often expensive and complex, making it difficult to achieve satisfactory results. This paper presents a Hybrid Visual Geometry Group (VGG)-19+ Bidirectional Long Short-Term Memory (Bi-LSTM) network to detect animals and generate alerts based on their activity. These alerts are sent to the local forest office as a Short Message Service (SMS) to allow for immediate response. The proposed model exhibits great improvements in model performance, with an average classification accuracy of 98%, a mean Average Precision (mAP) of 77.2%, and a Frame Per Second (FPS) of 170. The model was tested both qualitatively and quantitatively using 40,000 images from three different benchmark datasets with 25 classes and achieved a mean accuracy and precision of above 98%. This model is a reliable solution for providing accurate animal-based information and protecting human lives.  © 2013 IEEE.",activity recognition; alert system; Animal detection; Bi-LSTM; convolutional neural network; VGG-Net; video surveillance; wild animal monitoring; Alarm systems; Animals; Cameras; Convolution; Deep neural networks; Feature extraction; Activity recognition; Alert systems; Animal detection; Bidirectional long short-term memory; Convolutional neural network; Deep learning; Features extraction; Task analysis; Video surveillance; Visual geometry group-net; Wild animal monitoring; Wild animals; Long short-term memory
Scopus,"Huangfu, P.; Dang, L.",A multi-scale pyramid feature fusion-based object detection method for remote sensing images,,2023,,,,10.1080/01431161.2023.2288947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179549519&doi=10.1080%2f01431161.2023.2288947&partnerID=40&md5=21284fa43ced3b490a73963afb354f1d,"Object detection is a basic and challenging task in remote sensing image analysis that has received extensive attention in recent years. Feature fusion is one of the key steps in object detection. Most existing methods of feature fusion first complete the preliminary fusion of feature maps of different scales through ‘add’ or ‘concat’ operations, followed by using a single-scale convolution to further improve the fusion effect. However, due to the fact that multi-level features exhibit multi-scale representations, the fusion effect of existing methods is limited. To improve the efficiency of feature fusion, we propose a multi-scale pyramid feature fusion network, which performs multi-scale learning through multi-scale convolution kernels to complete multi-level feature fusion more effectively. Then we propose a lightweight decoupled head, which alleviates the conflict between the classification task and the localization task. We conducted experiments on the dataset of object detection in aerial images (DOTA) dataset and the HRSC2016 dataset to verify our proposed methods. The results show that the performance of our proposed methods is better than other existing methods, with an mAP of 73.3%, 67.6%, 65.0%, and 96.7% on the DOTA1.0, DOTA1.5, DOTA2.0, and HRSC2016 datasets, respectively. Meanwhile, the parameter quantity of the proposed model is 10.3 M, and the inference time is 5.1 ms, which meets the requirement of lightweight and ensures the timeliness of detection. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",feature fusion; neural network; Object detection; remote sensing image; YOLO; Antennas; Convolution; Feature extraction; Image fusion; Object recognition; Remote sensing; Features fusions; Image-analysis; Multi-Scale pyramids; Multi-scales; Multilevels; Neural-networks; Object detection method; Objects detection; Remote sensing images; YOLO; artificial neural network; detection method; image resolution; remote sensing; satellite data; satellite imagery; Object detection
Scopus,"Ando, S.; Kindo, T.",Direct Imaging of Stabilized Optical Flow and Possible Anomalies from Moving Vehicle,,2022,,,,10.1109/TITS.2022.3199203,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137539903&doi=10.1109%2fTITS.2022.3199203&partnerID=40&md5=99a7191579e5c4cc0bb3c473719805b2,"Machine perception of dynamic scenes becomes more and more important for autonomous vehicles and vision-based driver-assistance systems. Even with other 3D ranging devices, dense, detailed and instantaneous detection of optical flow is essential for early distinguishing small moving objects in the 3D environment from the moving vehicle. To overcome the limited performance in the immediacy, resolution, accuracy and acuity of existing methods, we provide an optical flow detection scheme based on a three-phase correlation image sensor (3PCIS) that is capable of Fourier-coefficient imaging combined with an exact and direct algorithm derived from the weighted integral method of identifying the differential equation model from a short-duration observation. To utilize inherent performances of the detection scheme by removing the large and rapid disturbances induced by the rotational fluctuations of the platform, we introduce a software operation of gaze in which the image coordinates are fixed on and smoothly pursue a forward stable object so that the optical flow field is relative to the moving coordinate system. In it, the gaze subsystem continuously provides angular velocity and pose between the camera and gaze target, while the imaging subsystem instantaneously obtains two optical flow distributions by cancelling the ego-rotation components and then removing the outwardly diverging components derived mainly from stationary 3D environments. Possible anomalies captured in each frame instantaneously provide candidates of hazardous objects that should be tracked and further investigated. We examine the performance of optical flow stabilization and anomaly detection using image sequences of monocular 3PCIS mounted on a moving vehicle on town roads and a highway.  © 2000-2011 IEEE.",autonomous vehicle; correlation image sensor; ego-motion; gaze; Optical flow; weighted integral method; Cameras; Differential equations; Fourier analysis; Image sensors; Optical correlation; Stereo image processing; Three dimensional computer graphics; Three dimensional displays; 3-D environments; Autonomous Vehicles; Correlation image sensor; Ego-motion; Gaze; Moving vehicles; Performance; Road; Three-dimensional display; Weighted integral method; Optical flows
Scopus,"Zhang, X.; Zhao, Z.; Sun, W.; Cui, Q.",3D Object Detection with Attention: Shell-Based Modeling,,2023,,,,10.32604/csse.2023.034230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147452198&doi=10.32604%2fcsse.2023.034230&partnerID=40&md5=66e822355bf0fa67992e6e9ce2b75d8e,"LIDAR point cloud-based 3D object detection aims to sense the surrounding environment by anchoring objects with the Bounding Box (BBox). However, under the three-dimensional space of autonomous driving scenes, the previous object detection methods, due to the pre-processing of the original LIDAR point cloud into voxels or pillars, lose the coordinate information of the original point cloud, slow detection speed, and gain inaccurate bounding box positioning. To address the issues above, this study proposes a new two-stage network structure to extract point cloud features directly by PointNet++, which effectively preserves the original point cloud coordinate information. To improve the detection accuracy, a shell-based modeling method is proposed. It roughly determines which spherical shell the coordinates belong to. Then, the results are refined to ground truth, thereby narrowing the localization range and improving the detection accuracy. To improve the recall of 3D object detection with bounding boxes, this paper designs a self-attention module for 3D object detection with a skip connection structure. Some of these features are highlighted by weighting them on the feature dimensions. After training, it makes the feature weights that are favorable for object detection get larger. Thus, the extracted features are more adapted to the object detection task. Extensive comparison experiments and ablation experiments conducted on the KITTI dataset verify the effectiveness of our proposed method in improving recall and precision. © 2023 CRL Publishing. All rights reserved.",3D object detection; autonomous driving; point cloud; self-attention mechanism; shell-based modeling; 3D modeling; Autonomous vehicles; Object recognition; Optical radar; Shells (structures); 3D object; 3d object detection; Attention mechanisms; Autonomous driving; Based modelling; Bounding-box; Objects detection; Point-clouds; Self-attention mechanism; Shell-based modeling; Object detection
Scopus,"Huang, X.",Moving object detection in low-luminance images,,2023,,,,10.1007/s00371-021-02320-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118855217&doi=10.1007%2fs00371-021-02320-1&partnerID=40&md5=8055c71b1f9989f30f0d4fc45d639a8b,"Moving object detection in low-luminance Images is one of the most fundamental and difficult issues in machine vision. Therefore, in this paper, deep self-adaptive network (DSA-Net) is proposed to effectively detect moving objects in low-luminance scenes. Particularly, (1) three mechanisms are developed in this joint learning framework: graph-based unsupervised feature selection, feature representations ranking, and multiple-way feature interaction. (2) Both anti-occlusion and multi-object handling module are explored simultaneously in the unified DSA-Net model. (3) A weakly fine-tuning strategy is presented, including the easiness and group curriculum term. It leverages helpful prior-knowledge to guide the learner to select confident training samples. The experimental results show that DSA-Net outperforms the state-of-the-art methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Deep learning; Low luminance; Moving object detection; Deep learning; Feature extraction; Graphic methods; Luminance; Object detection; Adaptive networks; Deep learning; Graph-based; Joint learning; Learning frameworks; Low luminance; Luminance images; Machine-vision; Moving objects; Moving-object detection; Object recognition
Scopus,"Patel, S.",Marigold Flower Blooming Stage Detection in Complex Scene Environment using Faster RCNN with Data Augmentation,,2023,,,,10.14569/IJACSA.2023.0140379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151795869&doi=10.14569%2fIJACSA.2023.0140379&partnerID=40&md5=2acba3381ad3d25a8fd85331d8189c03,"In recent years, flower growing has developed into a lucrative agricultural sector that provides employment and business opportunities for small and marginal growers in both urban and rural locations in India. One of the most often cultivated flowers for landscaping design is the Marigold flower. It is also widely used to create garlands for ceremonial and social occasions using loose flowers. Understanding the appropriate stage of harvesting for each plant species is essential to ensuring the quality of the flowers after they have been picked. It has been demonstrated that human assessors consistently used a category scoring system to evaluate various flowering stages. Deep learning and convolutional neural networks have the potential to revolutionize agriculture by enabling efficient analysis of large-scale data. In order to address the problem of Marigold flower stages detection and classification in complex real-time field scenarios, this study proposes a fine-tuned Faster RCNN with ResNet50 network coupled with data augmentation. Faster RCNN is a popular deep learning framework for object detection that uses a region proposal network to efficiently identify object locations and features in an image. The Marigold flower dataset was collected from three different Marigold fields in the Anand District of Gujarat State, India. The collection includes of photos that were taken outdoors in natural light at various heights, angles, and distances. We have developed and fine-tuned a Faster RCNN detection and classification model to be particularly sensitive to Marigold flowers, and we have compared the generated method's performance to that of other cutting-edge models to determine its accuracy and effectiveness © 2023, International Journal of Advanced Computer Science and Applications.All Rights Reserved.",convolutional neural networks; Deep learning; marigold flower blooming stage detection; object detection; Complex networks; Convolution; Convolutional neural networks; Cultivation; Deep learning; Object recognition; Agricultural sector; Business opportunities; Complex scenes; Convolutional neural network; Data augmentation; Deep learning; Employment opportunities; Marigold flower blooming stage detection; Marigold flowers; Objects detection; Object detection
Scopus,"Zou, T.; Chen, G.; Li, Z.; He, W.; Qu, S.; Gu, S.; Knoll, A.",KAM-Net: Keypoint-Aware and Keypoint-Matching Network for Vehicle Detection from 2-D Point Cloud,,2022,,,,10.1109/TAI.2021.3112945,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132955393&doi=10.1109%2fTAI.2021.3112945&partnerID=40&md5=1aa3a6a3aa0eed0f9056f0511cddba22,"Two-dimesional (2-D) LiDAR is an efficient alternative sensor for vehicle detection, which is one of the most critical tasks in autonomous driving. Compared to the fully developed 3-D LiDAR vehicle detection, 2-D LiDAR vehicle detection has much room to improve. Most existing state-of-the-art works represent 2-D point clouds as pseudo-images and then perform detection with traditional object detectors on 2-D images. However, they ignore the sparse representation and geometric information of vehicles in the 2-D cloud points. To address these issues, in this article, we present a novel keypoint-aware and keypoint-matching network termed as KAM-Net, which focuses on better detecting the vehicles by explicitly capturing and extracting the sparse information of L-shape in 2-D LiDAR point clouds. The whole framework consists of two stages - namely, keypoint-aware stage and keypoint-matching stage. The keypoint-aware stage utilizes the heatmap and edge extraction module to simultaneously predict the position of L-shaped keypoints and inflection offset of L-shaped endpoints. The keypoint-matching stage is followed to group the keypoints and produce the oriented bounding boxes with axis by utilizing the endpoint-matching and L-shaped-matching methods. Further, we conduct extensive experiments on a recently released public dataset to evaluate the effectiveness of our approach. The results show that our KAM-Net achieves a new state-of-the-art performance. The source code is available at https://github.com/ispc-lab/KAM-Net.  © 2020 IEEE.",Artificial intelligence algorithmic design and analysis; artificial intelligence in transportation; deep learning; supervised learning; Computer vision; Deep learning; Optical radar; Vehicles; Algorithmic analysis; Algorithmic design; Artificial intelligence algorithmic design and analyse; Artificial intelligence in transportation; Deep learning; Design and analysis; Key point matching; Keypoints; Point-clouds; Vehicles detection; Object detection
Scopus,"Woodruff, J.; Armengol-Estapé, J.; Ainsworth, S.; O'Boyle, M.F.P.",Bind the gap: compiling real software to hardware FFT accelerators,,2022,,,,10.1145/3519939.3523439,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132244166&doi=10.1145%2f3519939.3523439&partnerID=40&md5=1a1ef4eacec0ee237eaba1a0861e0abd,"Specialized hardware accelerators continue to be a source of performance improvement. However, such specialization comes at a programming price. The fundamental issue is that of a mismatch between the diversity of user code and the functionality of fixed hardware, limiting its wider uptake. Here we focus on a particular set of accelerators: those for Fast Fourier Transforms. We present FACC (Fourier ACcelerator Compiler), a novel approach to automatically map legacy code to Fourier Transform accelerators. It automatically generates drop-in replacement adapters using Input-Output (IO)-based program synthesis that bridge the gap between user code and accelerators. We apply FACC to unmodified GitHub C programs of varying complexity and compare against two existing approaches. We target FACC to a high-performance library, FFTW, and two hardware accelerators, the NXP PowerQuad and the Analog Devices FFTA, and demonstrate mean speedups of 9x, 17x and 27x respectively © 2022 ACM.",FFT; hardware accelerator; program synthesis; Acceleration; C (programming language); Program compilers; Fourier; Hardware accelerators; Input-output; Legacy code; Performance; Program synthesis; Real softwares; Specialisation; Specialized hardware; User codes; Fast Fourier transforms
Scopus,"Nigar, N.; Muhammad Faisal, H.; Kashif Shahzad, M.; Islam, S.; Oki, O.",An Offline Image Auditing System for Legacy Meter Reading Systems in Developing Countries: A Machine Learning Approach,,2022,,,,10.1155/2022/4543530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143389980&doi=10.1155%2f2022%2f4543530&partnerID=40&md5=9a264e6deb818fd753f985a21afc4abd,"The developing countries are challenged with overbilling and underbilling, due to manual meter reading, which results in consumer dissatisfaction and loss of revenue. The existing automated meter reading (AMR) solutions are expensive; hence, sample-based manual snap auditing systems are introduced to control such meter reading inaccuracies. In these systems, the meter reader, besides reading, also collects meter images, which are used to manually audit the meter's accuracy. Although such systems are inexpensive, they are limited in their ability to be sustainable and ensure 100% accurate meter readings. In this paper, a novel offline optical character recognition (OCR) system-based Snap Audit system is proposed and tested for its efficient and real-time 100% accurate meter reading capabilities. The experimental results on 5,000 real-world instances show that the proposed approach processes an image in 0.05 seconds with 94% accuracy. Moreover, the developed approach is evaluated with four state-of-the-art algorithms: region convolution neural network (RCNN), nanonets, Fast-OCR, and PyTesseract. The results provide evidence that our new system design along with novel approach is more robust and efficient as compared to existing algorithms by 43.6%. © 2022 Natasha Nigar et al.",Deep learning; Legacy systems; Optical character recognition; Audit systems; Auditing systems; Automated meter readings; Machine learning approaches; Meter accuracy; Meter readers; Meter reading systems; Meter readings; Offline; Optical character recognition system; Developing countries
Scopus,"Yuan, Z.; Song, X.; Bai, L.; Wang, Z.; Ouyang, W.",Temporal-Channel Transformer for 3D Lidar-Based Video Object Detection for Autonomous Driving,,2022,,,,10.1109/TCSVT.2021.3082763,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107203510&doi=10.1109%2fTCSVT.2021.3082763&partnerID=40&md5=75ca7bf2a42273e2bc9e9a49e0429c83,"The strong demand of autonomous driving in the industry has led to vigorous interest in 3D object detection and resulted in many excellent 3D object detection algorithms. However, the vast majority of algorithms only model single-frame data, ignoring the temporal clue in video sequence. In this work, we propose a new transformer, called Temporal-Channel Transformer (TCTR), to model the temporal-channel domain and spatial-wise relationships for video object detecting from Lidar data. As the special design of this transformer, the information encoded in the encoder is different from that in the decoder. The encoder encodes temporal-channel information of multiple frames while the decoder decodes the spatial-wise information for the current frame in a voxel-wise manner. Specifically, the temporal-channel encoder of the transformer is designed to encode the information of different channels and frames by utilizing the correlation among features from different channels and frames. On the other hand, the spatial decoder of the transformer decodes the information for each location of the current frame. Before conducting the object detection with detection head, a gate mechanism is further deployed for re-calibrating the features of current frame, which filters out the object-irrelevant information by repetitively refining the representation of target frame along with the up-sampling process. Experimental results reveal that TCTR achieves the state-of-the-art performance in grid voxel-based 3D object detection on the nuScenes benchmark. © 1991-2012 IEEE.",3D object detection; Lidar-based video; temporal-channel attention; transformer; Autonomous vehicles; Benchmarking; Channel coding; Decoding; Encoding (symbols); Motion compensation; Object recognition; Optical radar; Signal encoding; Autonomous driving; Channel encoder; Channel information; Multiple-frame; Single frames; State-of-the-art performance; Video object detections; Video sequences; Object detection
Scopus,"Humayun, M.; Ashfaq, F.; Jhanjhi, N.Z.; Alsadun, M.K.",Traffic Management: Multi-Scale Vehicle Detection in Varying Weather Conditions Using YOLOv4 and Spatial Pyramid Pooling Network,,2022,,,,10.3390/electronics11172748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137845925&doi=10.3390%2felectronics11172748&partnerID=40&md5=0a3f4dfd9f582cbb805ce6b258a93d6d,"Detecting and counting on road vehicles is a key task in intelligent transport management and surveillance systems. The applicability lies both in urban and highway traffic monitoring and control, particularly in difficult weather and traffic conditions. In the past, the task has been performed through data acquired from sensors and conventional image processing toolbox. However, with the advent of emerging deep learning based smart computer vision systems the task has become computationally efficient and reliable. The data acquired from road mounted surveillance cameras can be used to train models which can detect and track on road vehicles for smart traffic analysis and handling problems such as traffic congestion particularly in harsh weather conditions where there are poor visibility issues because of low illumination and blurring. Different vehicle detection algorithms focusing the same issue deal only with on or two specific conditions. In this research, we address detecting vehicles in a scene in multiple weather scenarios including haze, dust and sandstorms, snowy and rainy weather both in day and nighttime. The proposed architecture uses CSPDarknet53 as baseline architecture modified with spatial pyramid pooling (SPP-NET) layer and reduced Batch Normalization layers. We also augment the DAWN Dataset with different techniques including Hue, Saturation, Exposure, Brightness, Darkness, Blur and Noise. This not only increases the size of the dataset but also make the detection more challenging. The model obtained mean average precision of 81% during training and detected smallest vehicle present in the image. © 2022 by the authors.",artificial intelligence; deep learning; intelligent traffic monitoring; traffic surveillance; urban and highway traffic analysis; vehicle detection
Scopus,"Tuteja, S.; Poddar, S.; Agrawal, D.; Karar, V.",PredictV: A Vehicle Prediction Scheme to Circumvent Occluded Frames,,2022,,,,10.1109/ACCESS.2022.3151973,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124850165&doi=10.1109%2fACCESS.2022.3151973&partnerID=40&md5=3e1f4668c7a29b53815296d9bace2446,"There are many methods to overcome traffic congestion on the roads, but occlusion is still there in most methods, so it is a dire need of the time that researchers have to look into this matter. In the rural and urban areas, heavy congestion on the roads has become the leading cause of occlusion. The PredictV method works on the prediction principle based on existing values and is proved one of the naval approaches for this problem. This scheme uses blob detection for the first frame detection and predicts other vehicles based on percentage increment in the different parameters to identify the vehicles. For improving the quality of the sample, data cleaning has been included in this work. With the help of this approach, congestion has been avoided, and this method prefers to use predicted points to detect vehicles on the frame. The suggested approach is implemented via a MATLAB simulator. It is tested on a large dataset which includes 7152 frames of 6 different videos from the Urban Tracker and KoPer dataset. In total, there are 46876 vehicles present on the frame at first, and the existing methods have detected only 58% and 73% of vehicles, whereas the detection rate is 82% with this suggested approach. The occlusion rate is only 17% on average, which has been reduced via this proposed approach which was 24% and 43% earlier than the existing one. The performance of this suggested method has been tested based on performance and results, which are pretty impressive that is almost 99.74%. In short, the outcome of this prediction technique has been improved now, and the effectiveness has been observed too.  © 2013 IEEE.",occlusion; prediction; Traffic monitoring; vehicle detection; Forecasting; MATLAB; Traffic congestion; Vehicles; Blob detection; Features extraction; Occlusion; Performance; Prediction algorithms; Prediction schemes; Rural and urban; Traffic monitoring; Urban areas; Vehicles detection; Feature extraction
Scopus,"Kitayama, A.; Kuriyama, A.; Nagaishi, H.; Kuroda, H.",High-density implementation techniques for long-range radar using horn and lens antennas,,2021,,,,10.1587/transele.2021MMP0006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116395288&doi=10.1587%2ftransele.2021MMP0006&partnerID=40&md5=90149dad623806d29bc55a2ed33b60c1,"Long-range radars (LRRs) for higher level autonomous driving (AD) will require more antennas than simple driving assistance. The point at issue here is 50–60% of the LRR module area is used for antennas. To miniaturize LRR modules, we use horn and lens antenna with highly efficient gain. In this paper, we propose two high-density implementation techniques for radio-frequency (RF) front-end using horn and lens antennas. In the first technique, the gap between antennas was eliminated by taking advantage of the high isolation performance of horn and lens antennas. In the second technique, the RF front-end including microstrip-lines, monolithic microwave integrated circuits, and peripheral parts is placed in the valley area of each horn. We fabricated a prototype LRR operating at 77 GHz with only one printed circuit board (PCB). To detect vehicles horizontally and vertically, this LRR has a minimum antenna configuration of one Tx antenna and four Rx antennas placed in 2×2 array, and 30 mm thickness. Evaluation results revealed that vehicles could be detected up to 320 m away and that the horizontal and vertical angle error was less than +/− 0.2 degrees, which is equivalent to the vehicle width over 280 m. Thus, horn and lens antennas implemented using the proposed techniques are very suitable for higher level AD LRRs. © 2021 Institute of Electronics, Information and Communication, Engineers, IEICE. All rights reserved.",77 GHz long-range radar; Antenna isolation; Compact implementation; Direction of arrival estimation; Horn and lens antenna; Antenna arrays; Direction of arrival; Horn antennas; Printed circuit boards; Radar; Radar antennas; Vehicles; 77 GHz long-range radar; Antenna isolation; Autonomous driving; Compact implementation; Direction of arrival estimation; Driving assistance; Implementation techniques; Long-range radars; Radio frequency front end; Simple++; Lens antennas
Scopus,"An, Y.; Liu, W.; Cui, Y.; Wang, J.; Li, X.; Hu, H.",Multilevel Ground Segmentation for 3-D Point Clouds of Outdoor Scenes Based on Shape Analysis,,2022,,,,10.1109/TIM.2022.3154835,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125351377&doi=10.1109%2fTIM.2022.3154835&partnerID=40&md5=dd8284aace139eea1857c409e2f671ac,"Ground segmentation of 3-D point clouds acquired by laser sensors plays a crucial role in many applications, such as environment perception, scene understanding, and environment modeling. This article proposes a novel multilevel framework of the ground segmentation for 3-D point clouds of outdoor scenes based on shape analysis. The local shape of the 3-D point cloud of an outdoor scene is captured by principal component analysis. Then, the 3-D point cloud is classified into scattered points, linear points, and surface points. The unit normal vectors of the surface points are calculated and mapped into a unit ball. On the normal ball, the normal vectors are clustered, which segments the surface points into some surface regions correspondingly. Each surface region is further divided into several surface fragments according to point positions. The surface fragment that meets the ground conditions is regarded as a part of the initial ground. Finally, the ground is obtained by using the 2-D Gaussian process regression. The proposed method explores both local shapes and multilevel structures of outdoor scenes and constructs a probabilistic ground model in order to improve the accuracy and adaptivity of ground segmentation. Experiment results demonstrate that the proposed method has good performance. © 2022 IEEE.",Ground segmentation; laser rangefinder (LRF); normal clustering; point cloud; shape analysis; Principal component analysis; Robotics; Three dimensional computer graphics; 3D point cloud; Ground segmentation; Laser range finders; Multilevels; Normal Clustering; Outdoor scenes; Point-clouds; Scene-based; Shape-analysis; Surface points; Range finders
Scopus,"Chen, G.; Wang, F.; Qu, S.; Chen, K.; Yu, J.; Liu, X.; Xiong, L.; Knoll, A.",Pseudo-Image and Sparse Points: Vehicle Detection with 2D LiDAR Revisited by Deep Learning-Based Methods,,2021,,,,10.1109/TITS.2020.3007631,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120441579&doi=10.1109%2fTITS.2020.3007631&partnerID=40&md5=d3a8a43d738cc1b7b32b75c5bc2e5eb2,"Detecting and locating surrounding vehicles robustly and efficiently are essential capabilities for autonomous vehicles. Existing solutions often rely on vision-based methods or 3D LiDAR-based methods. These methods are either too expensive in both sensor pricing (3D LiDAR) and computation (camera and 3D LiDAR) or less robust in resisting harsh environment changes (camera). In this work, we revisit the LiDAR based approaches for vehicle detection with a less expensive 2D LiDAR by utilizing modern deep learning approaches. We aim at filling in the gap as few previous works conclude an efficient and robust vehicle detection solution in a deep learning way in 2D. To this end, we propose a learning based method with the input of pseudo-images, named Cascade Pyramid Region Proposal Convolution Neural Network (Cascade Pyramid RCNN), and a hybrid learning method with the input of sparse points, named Hybrid Resnet Lite. Experiments are conducted with our newly 2D LiDAR vehicle dataset recorded in complex traffic environments. Results demonstrate that the Cascade Pyramid RCNN outperforms state-of-the-art methods in accuracy while the proposed Hybrid Resnet Lite provides superior performance of the speed and lightweight model by hybridizing learning based and non-learning based modules. As few previous works conclude an efficient and robust vehicle detection solution with 2D LiDAR, our research fills in this gap and illustrates that even with limited sensing source from a 2D LiDAR, detecting obstacles like vehicles efficiently and robustly is still achievable.  © 2000-2011 IEEE.",2D LiDAR; autonomous driving; deep learning; intelligent transportation system; Vehicle detection; Autonomous vehicles; Deep learning; Intelligent systems; Intelligent vehicle highway systems; Optical radar; 2d LiDAR; Autonomous driving; Autonomous Vehicles; Convolution neural network; Deep learning; Harsh environment; Intelligent transportation systems; Learning-based methods; Vehicles detection; Vision-based methods; Cameras
Scopus,"Kalita, R.; Talukdar, A.K.; Kumar Sarma, K.",Real-Time Human Detection with Thermal Camera Feed using YOLOv3,,2020,,,,10.1109/INDICON49873.2020.9342089,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101540968&doi=10.1109%2fINDICON49873.2020.9342089&partnerID=40&md5=1c69894f78f5ca235daec86ea5532c9e,"Human detection is needed for various applications such as advanced driver assistance systems and autonomous driving, security and surveillance etc. Thermal imaging is often adopted for night time because of its capability of capturing the energy emitted from human body where visible light camera fails. In this paper, we employ YOLOv3 for an accurate real-time human detection using thermal images. We modified the network parameters according to the characteristics of the human, making this method more suitable for detecting human. Subset of Korea Advanced Institute of Science and Technology (KAIST) multispectral dataset consisting of 47650 thermal images is used for training and testing of YOLOv3. During experimentation, it is observed that humans are detected at 17 millisecond which is much faster than a local machine detection. Our test result also shows improved performance of the detector with thermal image with average precision of 95.5% and miss rate of 4.7%. © 2020 IEEE.",Deep learning; Real-time; Thermal image; YOLOv3; Automobile drivers; Cameras; Image enhancement; Infrared imaging; Security systems; Statistical tests; Autonomous driving; Human detection; Korea advanced institute of science and technologies; Network parameters; Security and surveillances; Thermal camera; Thermal images; Training and testing; Advanced driver assistance systems
Scopus,"Li, B.; Song, Z.; Guo, X.",Monocular Camera Ranging based on Vehicle Attitude Estimation,,2022,,,,10.1109/ISCTech58360.2022.00122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161876145&doi=10.1109%2fISCTech58360.2022.00122&partnerID=40&md5=192e70dd905f9a4d66cd18cf987f65d0,"Assisted and autonomous driving technologies are gaining popularity. A key component of autonomous driving technology is affected by the measurement of the distance in front of the vehicle. Distance prediction using monocular cameras is an efficient and low-cost ranging method. There are some limitations to existing monocular camera-based vehicle distance estimation methods. If known object features are used as the calculation standard, the calibration information is easily lost in real-world road conditions. In this paper, we proposed a method for estimating vehicle distances based on vehicle pose estimation and RGB images based on known vehicle metrics without using external references. This method estimates the distance of the vehicle by using the vehicle's characteristics as the calibration value. Good results are obtained after testing on the KITTI dataset. © 2022 IEEE.",autonomous driving; Mask-RCNN; measurement of distance; monocular cameras; YoloV5; Calibration; Cameras; Statistical tests; Assisted drivings; Attitude estimation; Autonomous driving; Efficient costs; Low-costs; Mask-RCNN; Measurement of distance; Measurements of; Monocular cameras; Yolov5; Autonomous vehicles
Scopus,"Li, W.; Yuan, Q.; Chen, L.; Zheng, L.; Tang, X.",Human Target Detection Method Based on Fusion of Radar and Image Data,,2021,,,,10.16337/j.1004-9037.2021.02.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104522325&doi=10.16337%2fj.1004-9037.2021.02.014&partnerID=40&md5=4c676ac37df5d62d432ff09356934287,"Three-dimensional (3-D) human target detection has important application value in intelligent security, robot, automatic driving and other fields. At present, the 3-D human target detection method based on radar and image data fusion mainly adopts two-stage network structure, which respectively completes the selection of candidate boundary boxes with high target probability and the target classification/regression of target candidate boxes. Although the preselection of target candidate bounding box enables the two-stage network structure to achieve higher detection accuracy and positioning accuracy, the complexity of the network structure leads to the limitation of the operation speed, which cannot be applied in scenarios with high real-time requirements. In order to solve the above problem, this paper studies a real-time detection method of 3-D human targets based on improved RetinaNet. The backbone network and feature pyramid network are combined for point cloud and image feature extraction, and the fused feature anchors are input into the functional network to output the 3-D boundary boxes and target category information. By using the one-stage network structure, the method directly regresses the category probability and position coordinates of the targets, solving the imbalance problem of positive and negative samples in the process of one-stage network training by introducing focal loss function. Experiments on KITTI dataset show that the proposed method outperforms the contrast algorithms in terms of average accuracy and time-consuming, and can effectively balance the accuracy and real-time performance of target detection. © 2021 by Journal of Data Acquisition and Processing.",3-D human target detection; Deep learning; Focal loss function; Improved RetinaNet; Multi-sensor information fusion
Scopus,"Changfeng, X.; Wang, C.; Yu, L.; Chao, L.; Xiufeng, Z.",A Review of Lane Line Detection Technology Based on Machine Vision,,2021,,,,10.1007/978-981-33-6318-2_88,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101832713&doi=10.1007%2f978-981-33-6318-2_88&partnerID=40&md5=2ca542ee8af70e8b6ddcea7af99a84c3,"In recent years, lane line detection is the first step of autopilot technology. In this paper, the development of lane line detection technology based on machine vision is studied, and lane line technology is subdivided. The development of image processing technology, region of interest extraction and lane line fitting in recent years is classified. At the same time, the traditional algorithm and deep learning algorithm are analyzed. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Area of interest; Deep learning; Image pre-processing; Lane detection; Machine vision; Target detection; Deep learning; Image segmentation; Learning algorithms; Manufacture; Image processing technology; Line detection; Line fitting; On-machines; Region of interest; Computer vision
Scopus,"Zarei, N.; Moallem, P.; Shams, M.",Fast-Yolo-Rec: Incorporating Yolo-Base Detection and Recurrent-Base Prediction Networks for Fast Vehicle Detection in Consecutive Images,,2022,,,,10.1109/ACCESS.2022.3221942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142780442&doi=10.1109%2fACCESS.2022.3221942&partnerID=40&md5=ddeacc8c7e054b662642d57c147a3270,"Despite significant advances and innovations in deep network-based vehicle detection methods, finding a balance between detector accuracy and speed remains a significant challenge. This study aims to present an algorithm that can manage the speed and accuracy of the detector in real-time vehicle detection while increasing detector speed with accuracy comparable to high-speed detectors. To this end, the Fast-Yolo-Rec algorithm is proposed. The proposed method includes a new Yolo-based detection network and LSTM-based position prediction networks. The proposed semantic attention mechanism in the spatial semantic attention module (SSAM) significantly impacts accuracy and speed on par with the most recent fast detectors. Recurrent position prediction networks, on the other hand, improve the detection speed by estimating the current vehicle position using vehicle position history. The vehicle trajectories are classified, and the LSTM network for the specified trajectory predicts the vehicle positions. The Fast-Yolo-Rec algorithm not only determines the position of the vehicle faster than high-speed detectors but also allows for the speed control of the detection network with acceptable accuracy. The evaluation results on a large Highway dataset show that the proposed scheme outperforms the baseline methods.  © 2013 IEEE.",attention mechanism; recurrent prediction network; Yolo-based detection network; Feature extraction; Long short-term memory; Semantic Web; Semantics; Speed; Vehicles; Attention mechanisms; Detection networks; Features extraction; Hardware; High speed detectors; Prediction algorithms; Recurrent prediction network; Vehicle position; Vehicles detection; Yolo-based detection network; Forecasting
Scopus,"Fang, Z.; Lin, T.; Li, Z.; Yao, Y.; Zhang, C.; Ma, R.; Chen, Q.; Fu, S.; Ren, H.",Automatic Walking Method of Construction Machinery Based on Binocular Camera Environment Perception,,2022,,,,10.3390/mi13050671,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129904666&doi=10.3390%2fmi13050671&partnerID=40&md5=df94aaa7f825dfed533b111da3fc3bf7,"In this paper, we propose an end-to-end automatic walking system for construction machin-ery, which uses binocular cameras to capture images of construction machinery for environmental perception, detects target information in binocular images, estimates the relative distance between the current target and cameras, and predicts the real-time control signal of construction machinery. This system consists of two parts: the binocular recognition ranging model and the control model. Objects within 5 m can be quickly detected by the recognition ranging model, and at the same time, the distance of the object can be accurately ranged to ensure the full perception of the surrounding environment of the construction machinery. The distance information of the object, the feature information of the binocular image, and the control signal of the previous stage are sent to the control model; then, the prediction of the control signal of the construction machinery can be output in the next stage. In this way, the automatic walking experiment of the construction machinery in a specific scenario is completed, which proves that the model can control the machinery to complete the walking task smoothly and safely. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",binocular detection; construction machinery; end-to-end; ranging; unmanned driving; Binoculars; Construction equipment; Machinery; Binocular camera; Binocular detection; Construction machinery; Control model; Control signal; End to end; Environment perceptions; Methods of constructions; Unmanned drivings; Walking systems; Cameras
Scopus,"Chen, B.; Yang, X.",Small obstacles image detection and classification for driver assistance,,2022,,,,10.1007/s11042-022-12706-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127671179&doi=10.1007%2fs11042-022-12706-5&partnerID=40&md5=3ca669ab8525f4ff82cd792974a20d7c,"Small obstacles can cause big accidents, even if the vehicle is equipped with an intelligent auxiliary system. In order to detect four kinds of small obstacles quickly and accurately, this paper proposes an optimized neural network algorithm based on YOLOv3. K-Means+ is used to determine the prior box and enhance the adaptability of the YOLO scale. For the data samples imbalance, loss function of YOLO is improved to increase the precision of the prediction box. In addition, a special classification and counting algorithm is proposed to get results quickly and visually. The experimental results show that the our method can classify and locate four kinds of small obstacles more accurately and faster. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Classification; Intelligent auxiliary system; Object detection; Small obstacle; YOLO; Abstracting; Accidents; Automobile drivers; Auxiliary equipment; Image classification; K-means clustering; Auxiliary systems; Data sample; Driver assistance; Image detection; Images classification; Intelligent auxiliary system; K-means; Neural networks algorithms; Small obstacle; YOLO; Object detection
Scopus,"Yu, X.-J.; Huai, Y.-H.; Yao, Z.-W.; Sun, Z.-C.; Yu, A.",Key technologies in autonomous vehicle for engineering,,2021,,,,10.13229/j.cnki.jdxbgxb20210038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110116601&doi=10.13229%2fj.cnki.jdxbgxb20210038&partnerID=40&md5=a6effdd74249d21f71119e366e2910dd,"As the society emphasizes on the life safety of operators and the standard of machinery performance requirements for construction, engineering vehicles are developing in the direction of autonomy, efficiency and reliability. In order to realize the automatic transfer and operation of unmanned engineering vehicles, this paper systematically summarizes the relevant technologies at home and abroad, and analyzes the research progress of key technologies of unmanned engineering vehicles in detail in terms of environment perception, motion planning, engineering operation and condition monitoring, etc. It points out that the technologies of unstructured environment identification, path planning and trajectory tracking of vehicles with variable body structure and automated operation still need to be broken through, and proposes the adoption of mechanism/structure optimization design, advanced communication means, machine learning and digital twin, etc., which is conducive to promoting the development of key technologies of unmanned engineering vehicles. © 2021 Editorial Board of Journal of Jilin University (Engineering and Technology Edition). All right reserved.",Digital twin; Engineering vehicles; Environment perception; Motion planning; Unmanned driving; Automatic vehicle identification; Automobile frames; Autonomous vehicles; Condition monitoring; Digital twin; Machinery; Motion tracking; Automated operations; Efficiency and reliability; Engineering operation; Engineering vehicles; Environment perceptions; Machinery performance; Trajectory tracking; Unstructured environments; Engineering education
Scopus,"Liu, C.K.; Negrut, D.",The Role of Physics-Based Simulators in Robotics,,2021,,,,10.1146/annurev-control-072220-093055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102671355&doi=10.1146%2fannurev-control-072220-093055&partnerID=40&md5=d18b0aa57e78a3736bd05f64e7eace45,"Physics-based simulation provides an accelerated and safe avenue for developing, verifying, and testing robotic control algorithms and prototype designs. In the quest to leverage machine learning for developing AI-enabled robots, physics-based simulation can generate a wealth of labeled training data in a short amount of time. Physics-based simulation also creates an ideal proving ground for developing intelligent robots that can both learn from their mistakes and be verifiable. This article provides an overview of the use of simulation in robotics, emphasizing how robots (with sensing and actuation components), the environment they operate in, and the humans they interact with are simulated in practice. It concludes with an overview of existing tools for simulation in robotics and a short discussion of aspects that limit the role that simulation plays today in intelligent robot design. © 2021 by Annual Reviews. All rights reserve.",robot simulation; testing through simulation; virtual prototyping of robots
Scopus,"Piao, C.; Ding, X.; He, J.; Jang, S.; Liu, M.",Implementation of Image Transmission Based on Vehicle-to-Vehicle Communication,,2022,,,,10.3745/JIPS.03.0176,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129917939&doi=10.3745%2fJIPS.03.0176&partnerID=40&md5=690ec68146822613ccfa1637beaaaac1,"Weak over-the-horizon perception and blind spot are the main problems in intelligent connected vehicles (ICVs). In this paper, a V2V image transmission-based road condition warning method is proposed to solve them. The encoded road emergency images which are collected by the ICV are transmitted to the on-board unit (OBU) through Ethernet. The OBU broadcasts the fragmented image information including location and clock of the vehicle to other OBUs. To satisfy the channel quality of the V2X communication in different times, the optimal fragment length is selected by the OBU to process the image information. Then, according to the position and clock information of the remote vehicles, OBU of the receiver selects valid messages to decode the image information which will help the receiver to extend the perceptual field. The experimental results show that our method has an average packet loss rate of 0.5%. The transmission delay is about 51.59 ms in low-speed driving scenarios, which can provide drivers with timely and reliable warnings of the road conditions. © 2022. KIPS",Internet of vehicles; Real-time image transmission; Road condition warning; V2x; Clocks; Vehicle to Everything; Vehicle to vehicle communications; Vehicle transmissions; Blind spots; Image information; Internet of vehicle; On-board units; Over the horizons; Real-time image transmissions; Road condition; Road condition warning; V2x; Vehicle-to-vehicle communication; Roads and streets
Scopus,"Bangalore Ramaiah, N.K.; Kundu, S.K.",Stereo Vision-Based Road Debris Detection System for Advanced Driver Assistance Systems,,2021,,,,10.4271/09-10-01-0003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122004994&doi=10.4271%2f09-10-01-0003&partnerID=40&md5=d1fdd5686f5549543f57a8ae3b5e65c3,"Reliable detection of obstacles around an autonomous vehicle is essential to avoid potential collision and ensure safe driving. However, a vast majority of existing systems are mainly focused on detecting large obstacles such as vehicles, pedestrians, and so on. Detection of small obstacles such as road debris, which pose a serious potential threat are often overlooked. In this article, a novel stereo vision-based road debris detection algorithm is proposed that detects debris on the road surfaces and estimates their height accurately. Moreover, a collision warning system that could warn the driver of an imminent crash by using 3D information of detected debris has been studied. A novel feature-based classifier that uses a combination of strong and weak features has been developed for the proposed algorithm, which identifies debris from selected candidates and calculates its height. 3D information of detected debris and vehicle's speed are used in the collision warning system to warn the driver to safely maneuver the vehicle. The performance of the proposed algorithm has been evaluated by implementing it on a passenger vehicle. Experimental results confirm that the proposed algorithm can successfully detect debris of ≥5 cm height for up to a 22 m distance with an accuracy of 90%. Moreover, the debris detection algorithm runs at 20 Hz in a commercially available stereo camera making it suitable for real-time applications in commercial vehicles. © 2022 SAE International.",Classification; Collision Warning System; Debris detection; Feature extraction; Stereo vision; Accidents; Advanced driver assistance systems; Automobile drivers; Classification (of information); Commercial vehicles; Debris; Roads and streets; Signal detection; Stereo image processing; Stereo vision; 3D information; Autonomous Vehicles; Collision warning system; Debris detection; Detection algorithm; Detection system; Features extraction; Reliable detection; Safe driving; Vision based; Feature extraction
Scopus,"Ponnaganti, V.; Moh, M.; Moh, T.-S.",Utilizing CNNs for Object Detection with LiDAR Data for Autonomous Driving,,2021,,,,10.1109/IMCOM51814.2021.9377361,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103740940&doi=10.1109%2fIMCOM51814.2021.9377361&partnerID=40&md5=eedc851bc446f20e5289d88f3c902356,"This project evaluates the feasibility of utilizing popular Convolutional Neural Networks (CNNs) to detect objects present in LiDAR (Light Detection And Ranging) data, and the resulting neural network's performance. This work aims to further existing experimentation using raw LiDAR data that is analyzed and represented in a two-dimensional frame. Using this method, hundreds of frames were generated to create a dataset that was used for neural network training and validation on an existing CNN architecture. The LiDAR dataset was used to train YOLOv3, a popular CNN model, to detect vehicles. This research aims to test a smaller version of the network, YOLOv3-tiny, to measure the change in accuracy between using YOLOv3 and YOLOv3-tiny on the LiDAR dataset. The results are then compared to the loss typically found when going from YOLOv3 to YOLOV3-tiny on camera-based images. In prior experimentation, a preprocessing method was also introduced to attempt to isolate target objects in the frame. The method will be evaluated in this paper to measure its effect on the final accuracy metric of the network. Lastly, the runtime performance of these networks will be evaluated on two embedded platforms to understand if the frame rate that the networks perform on is usable for real-world applications, based on the frame rate the sensor is capable of outputting and the inference speed of the network on the embedded platforms. © 2021 IEEE.",Artificial Intelligence; Convolutional Neural Network; LiDAR; Object Detection; Convolutional neural networks; Information management; Lithium compounds; Object detection; Statistical tests; Autonomous driving; Embedded platforms; LIDAR (light detection and ranging); Neural network training; Pre-processing method; Raw lidar data; Run-time performance; Target object; Optical radar
Scopus,"Zuzanna, K.; Tomasz, U.; Michal, G.; Robert, P.","How High-Tech Solutions Support the Fight Against IUU and Ghost Fishing: A Review of Innovative Approaches, Methods, and Trends",,2022,,,,10.1109/ACCESS.2022.3212384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139839520&doi=10.1109%2fACCESS.2022.3212384&partnerID=40&md5=fddf04845e10db2bb5d465bb88596f79,"Illegal, Unreported, and Unregulated fishing is a major threat to human food supply and marine ecosystem health. Not only is it a cause of significant economic loss but also its effects have serious long-term environmental implications, such as overfishing and ocean pollution. The beginning of the fight against this problem dates since the early 2000s. From that time, a number of approaches and methods have been developed and reported. A key role in this topic is played by machine learning algorithms which exploit data provided by classical and high-tech sensors, devices and systems such as for example: CCTV, on-board cameras placed on autonomous vehicles, Global Positioning Systems, radars, Automatic Identification Systems, Vessel Monitoring Systems, or Coastal Surveillance Systems. The main objective of this paper is to provide the reader with knowledge about the scale of this phenomenon, methods to tackle the issue, and the current state of research on the subject. This has been achieved through a review of existing approaches that deal with these harmful phenomena by using dedicated artificial intelligence and machine learning tools, as well as the accompanying equipment and devices. In addition, flaws and gaps in current methods, and future directions are disscussed.  © 2013 IEEE.",Artificial intelligence; autonomous systems; boat tracking; ghost fishing; intelligent vehicles; IUU; machine learning; Artificial intelligence; Automation; Ecosystems; Fisheries; Food supply; Learning algorithms; Learning systems; Losses; Marine pollution; Security systems; 'current; Autonomous system; Boat tracking; Ghost fishing; High tech; Innovative approaches; Innovative method; Innovative trends; IUU; Machine-learning; Health risks
Scopus,"Panahi, M.; Khosravi, K.; Ahmad, S.; Panahi, S.; Heddam, S.; Melesse, A.M.; Omidvar, E.; Lee, C.-W.",Cumulative infiltration and infiltration rate prediction using optimized deep learning algorithms: A study in Western Iran,,2021,,,,10.1016/j.ejrh.2021.100825,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105350635&doi=10.1016%2fj.ejrh.2021.100825&partnerID=40&md5=8ad0039c00eb039b19cfbf33a3d951f5,"Study region: Sixteen different sites from two provinces (Lorestan and Illam) in the western part of Iran were considered for the field data measurement of cumulative infiltration, infiltration rate, and other effective variables that affect infiltration process. Study focus: Soil infiltration is recognized as a fundamental process of the hydrologic cycle affecting surface runoff, soil erosion, and groundwater recharge. Hence, accurate prediction of the infiltration process is one of the most important tasks in hydrological science. As direct measurement is difficult and costly, and empirical models are inaccurate, the current study proposed a standalone, and optimized deep learning algorithm of a convolutional neural network (CNN) using gray wolf optimization (GWO), a genetic algorithm (GA), and an independent component analysis (ICA) for cumulative infiltration and infiltration rate prediction. First, 154 raw datasets were collected including the time of measuring; sand, clay, and silt percent; bulk density; soil moisture percent; infiltration rate; and cumulative infiltration using field survey. Next, 70 % of the dataset were used for model building and the remaining 30 % was used for model validation. Then, based on the correlation coefficient between input variables and outputs, different input combinations were constructed. Finally, the prediction power of each developed algorithm was evaluated using different visually-based (scatter plot, box plot and Taylor diagram) and quantitatively-based [root mean square error (RMSE), mean absolute error (MAE), the Nash-Sutcliffe efficiency (NSE), and percentage of bias (PBIAS)] metrics. New Hydrological Insights for the Region: Finding revealed that the time of measurement is more important for cumulative infiltration, while soil characteristics (i.e. silt content) are more significant in infiltration rate prediction. This shows that in the study area, silt parameter, which is the dominant constituent parameter, can control infiltration process more effectively. Effectiveness of the variables in the present study, in the order of importance are time, silt, clay, moisture content, sand, and bulk density. This can be related to the fact that most of study area is rangeland and thus, overgrazing leads to compaction of the silt soil that can lead to a slow infiltration process. Soil moisture content and bulk density are not highly effective in our study because these two factors do not significantly change across the study area. Findings demonstrated that the optimum input variable combination, is the one in which all input variables are considered. The results illustrated that CNN algorithms have a very high performance, while a metaheuristic algorithm enhanced the performance of a standalone CNN algorithm (from 7% to 28 %). The results also showed that a CNN-GWO algorithm outperformed the other algorithms, followed by CNN-ICA, CNN-GA, and CNN for both cumulative infiltration and infiltration rate prediction. All developed algorithms underestimated cumulative infiltration, while overestimating infiltration rates. © 2021 The Authors",CNN; Cumulative infiltration; Deep learning; Infiltration rate; Iran; Metaheuristic
Scopus,"Cao, J.; Pang, Y.; Xie, J.; Khan, F.S.; Shao, L.",From Handcrafted to Deep Features for Pedestrian Detection: A Survey,,2022,,,,10.1109/TPAMI.2021.3076733,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105102179&doi=10.1109%2fTPAMI.2021.3076733&partnerID=40&md5=56ba6774facccab7d4b7c71957740c74,"Pedestrian detection is an important but challenging problem in computer vision, especially in human-centric tasks. Over the past decade, significant improvement has been witnessed with the help of handcrafted features and deep features. Here we present a comprehensive survey on recent advances in pedestrian detection. First, we provide a detailed review of single-spectral pedestrian detection that includes handcrafted features based methods and deep features based approaches. For handcrafted features based methods, we present an extensive review of approaches and find that handcrafted features with large freedom degrees in shape and space have better performance. In the case of deep features based approaches, we split them into pure CNN based methods and those employing both handcrafted and CNN based features. We give the statistical analysis and tendency of these methods, where feature enhanced, part-Aware, and post-processing methods have attracted main attention. In addition to single-spectral pedestrian detection, we also review multi-spectral pedestrian detection, which provides more robust features for illumination variance. Furthermore, we introduce some related datasets and evaluation metrics, and a deep experimental analysis. We conclude this survey by emphasizing open problems that need to be addressed and highlighting various future directions. Researchers can track an up-To-date list at https://github.com/JialeCao001/PedSurvey. © 1979-2012 IEEE.","deep features based methods; handcrafted features based methods; multi-spectral pedestrian detection; Pedestrian detection; Algorithms; Humans; Image Processing, Computer-Assisted; Lighting; Neural Networks, Computer; Pedestrians; Surveys; Evaluation metrics; Experimental analysis; Freedom degrees; Human-centric; Multi-spectral; Pedestrian detection; Postprocessing methods; algorithm; human; illumination; image processing; pedestrian; procedures; Feature extraction"
Scopus,"Guoqiang, C.; Huailong, Y.; Zhuangzhuang, M.",Vehicle and Pedestrian Detection Based on Multi-Level Feature Fusion in Autonomous Driving,,2021,,,,10.2174/2666255813666200304123323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122536990&doi=10.2174%2f2666255813666200304123323&partnerID=40&md5=46316be0ae326b338e329af9325d300c,"Aims: The factors including light, weather, dynamic objects, seasonal effects and struc-tures bring great challenges for the autonomous driving algorithm in the real world. Autonomous vehicles can detect different object obstacles in complex scenes to ensure safe driving. Background: The ability to detect vehicles and pedestrians is critical to the safe driving of autonomous vehicles. Automated vehicle vision systems must handle extremely wide and challenging sce-narios. Objective: The goal of the work is to design a robust detector to detect vehicles and pedestrians. The main contribution is that the Multi-level Feature Fusion Block (MFFB) and the Detector Cascade Block (DCB) are designed. The multi-level feature fusion and multi-step prediction are used which greatly improve the detection object precision. Methods: The paper proposes a vehicle and pedestrian object detector, which is an end-to-end deep convolutional neural network. The key parts of the paper are to design the Multi-level Feature Fusion Block (MFFB) and Detector Cascade Block (DCB). The former combines inherent multi-level features by combining contextual information with useful multi-level features that combine high resolution but low semantics and low resolution but high semantic features. The latter uses multi-step prediction, cascades a series of detectors, and combines predictions of multiple feature maps to handle objects of different sizes. Results: The experiments on the RobotCar dataset and the KITTI dataset show that our algorithm can achieve high precision results through real-time detection. The algorithm achieves 84.61% mAP on the RobotCar dataset and is evaluated on the well-known KITTI benchmark dataset, achieving 81.54% mAP. In particular, the detection accuracy of a single-category vehicle reaches 90.02%. Conclusion: The experimental results show that the proposed algorithm has a good trade-off between detection accuracy and detection speed, which is beyond the current state-of-the-art Refine-Det algorithm. The 2D object detector is proposed in the paper, which can solve the problem of vehicle and pedestrian detection and improve the accuracy, robustness and generalization ability in autonomous driving. © 2021 Bentham Science Publishers.",Algorithm; Autonomous driving; Convolutional neural network; Deep learning; Multi-level feature fusion; Vehicle and pedestrian detection; Convolution; Convolutional neural networks; Deep neural networks; Economic and social effects; Feature extraction; Forecasting; Object detection; Pedestrian safety; Semantics; Autonomous driving; Autonomous Vehicles; Convolutional neural network; Deep learning; Features fusions; Multi-level feature fusion; Multilevels; Pedestrian detection; Safe driving; Vehicles detection; Autonomous vehicles
Scopus,"Rao, Z.; He, M.; Dai, Y.; Shen, Z.",Patch attention network with generative adversarial model for semi-supervised binocular disparity prediction,,2022,,,,10.1007/s00371-020-02001-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096025370&doi=10.1007%2fs00371-020-02001-5&partnerID=40&md5=25dc8d65ef9882cdddb71b3c3c46aa7f,"In this paper, we address the challenging points of binocular disparity estimation: (1) unsatisfactory results in the occluded region when utilizing warping function in unsupervised learning; (2) inefficiency in running time and the number of parameters as adopting a lot of 3D convolutions in the feature matching module. To solve these drawbacks, we propose a patch attention network for semi-supervised stereo matching learning. First, we employ a channel-attention mechanism to aggregate the cost volume by selecting its different surfaces for reducing a large number of 3D convolution, called the patch attention network (PA-Net). Second, we use our proposed PA-Net as a generator and then combine it, traditional unsupervised learning loss, and the adversarial learning model to construct a semi-supervised learning framework for improving performance in the occluded areas. We have trained our PA-Net in supervised learning, semi-supervised learning, and unsupervised learning manners. Extensive experiments show that (1) our semi-supervised learning framework can overcome the drawbacks of unsupervised learning and significantly improve the performance in the ill-posed region by using only a few or inaccurate ground truths; (2) our PA-Net can outperform other state-of-the-art approaches in supervised learning and use fewer parameters. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Binocular disparity estimation; Generative adversarial model; Patch attention mechanism; Semi-supervised learning; Binoculars; Convolution; Unsupervised learning; Adversarial learning; Attention mechanisms; Binocular disparity; Feature matching; Improving performance; Semi-supervised; State-of-the-art approach; Warping function; Semi-supervised learning
Scopus,"Carranza-García, M.; Lara-Benítez, P.; García-Gutiérrez, J.; Riquelme, J.C.",Enhancing object detection for autonomous driving by optimizing anchor generation and addressing class imbalance,,2021,,,,10.1016/j.neucom.2021.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104482306&doi=10.1016%2fj.neucom.2021.04.001&partnerID=40&md5=30661692e9442238a28a7633d5c8ea49,"Object detection has been one of the most active topics in computer vision for the past years. Recent works have mainly focused on pushing the state-of-the-art in the general-purpose COCO benchmark. However, the use of such detection frameworks in specific applications such as autonomous driving is yet an area to be addressed. This study presents an enhanced 2D object detector based on Faster R-CNN that is better suited for the context of autonomous vehicles. Two main aspects are improved: the anchor generation procedure and the performance drop in minority classes. The default uniform anchor configuration is not suitable in this scenario due to the perspective projection of the vehicle cameras. Therefore, we propose a perspective-aware methodology that divides the image into key regions via clustering and uses evolutionary algorithms to optimize the base anchors for each of them. Furthermore, we add a module that enhances the precision of the second-stage header network by including the spatial information of the candidate regions proposed in the first stage. We also explore different re-weighting strategies to address the foreground-foreground class imbalance, showing that the use of a reduced version of focal loss can significantly improve the detection of difficult and underrepresented objects in two-stage detectors. Finally, we design an ensemble model to combine the strengths of the different learning strategies. Our proposal is evaluated with the Waymo Open Dataset, which is the most extensive and diverse up to date. The results demonstrate an average accuracy improvement of 6.13% mAP when using the best single model, and of 9.69% mAP with the ensemble. The proposed modifications over the Faster R-CNN do not increase computational cost and can easily be extended to optimize other anchor-based detection frameworks. © 2021 Elsevier B.V.",Anchor optimization; Autonomous vehicles; Class imbalance; Convolutional neural networks; Deep learning; Object detection; Autonomous vehicles; Clustering algorithms; Deep neural networks; Object recognition; 2D objects; Anchor optimization; Autonomous driving; Autonomous Vehicles; Class imbalance; Convolutional neural network; Deep learning; Detection framework; Objects detection; State of the art; Object detection
Scopus,"Ahlberg, C.; León, M.; Ekstrand, F.; Ekström, M.",The genetic algorithm census transform: evaluation of census windows of different size and level of sparseness through hardware in-the-loop training,,2021,,,,10.1007/s11554-020-00993-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087560320&doi=10.1007%2fs11554-020-00993-w&partnerID=40&md5=1b489719934554744bb86110077cdf96,"Stereo correspondence is a well-established research topic and has spawned categories of algorithms combining several processing steps and strategies. One core part to stereo correspondence is to determine matching cost between the two images, or patches from the two images. Over the years several different cost metrics have been proposed, one being the Census Transform (CT). The CT is well proven for its robust matching, especially along object boundaries, with respect to outliers and radiometric differences. The CT also comes at a low computational cost and is suitable for hardware implementation. Two key developments to the CT are non-centric and sparse comparison schemas, to increase matching performance and/or save computational resources. Recent CT algorithms share both traits but are handcrafted, bounded with respect to symmetry, edge lengths and defined for a specific window size. To overcome this, a Genetic Algorithm (GA) was applied to the CT, proposing the Genetic Algorithm Census Transform (GACT), to automatically derive comparison schemas from example data. In this paper, FPGA-based hardware acceleration of GACT, has enabled evaluation of census windows of different size and shape, by significantly reducing processing time associated with training. The experiments show that lateral GACT windows produce better matching accuracy and require less resources when compared to square windows. © 2020, The Author(s).",Census transform; FPGA; Genetic algorithm; Matching cost metric; Real time; SoC; Stereo correspondence; VHDL; Computerized tomography; Genetic algorithms; Hardware-in-the-loop simulation; Image segmentation; Surveys; Computational costs; Computational resources; Hard-ware-in-the-loop; Hardware acceleration; Hardware implementations; Matching performance; Object boundaries; Stereo correspondences; Stereo image processing
Scopus,"Song, J.; Qian, J.; Li, Y.; Liu, Z.; Chen, Y.; Chen, J.",Automatic Extraction of Power Lines from Aerial Images of Unmanned Aerial Vehicles,,2022,,,,10.3390/s22176431,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137566186&doi=10.3390%2fs22176431&partnerID=40&md5=70d59c7974fa958ec81eb5e52ab5cf3d,"Automatic power line extraction from aerial images of unmanned aerial vehicles is one of the key technologies of power line inspection. However, the faint power line targets and complex image backgrounds make the extraction of power lines a greater challenge. In this paper, a new power line extraction method is proposed, which has two innovative points. Innovation point one, based on the introduction of the Mask RCNN network algorithm, proposes a block extraction strategy to realize the preliminary extraction of power lines with the idea of “part first and then the whole”. This strategy globally reduces the anchor frame size, increases the proportion of power lines in the feature map, and reduces the accuracy degradation caused by the original negative anchor frames being misclassified as positive anchor frames. Innovation point two, the proposed connected domain group fitting algorithm solves the problem of broken and mis-extracted power lines even after the initial extraction and solves the problem of incomplete extraction of power lines by background texture interference. Through experiments on 60 images covering different complex image backgrounds, the performance of the proposed method far exceeds that of commonly used methods such as LSD, Yolact++, and Mask RCNN. DSCPL, TPR, precision, and accuracy are as high as 73.95, 81.75, 69.28, and 99.15, respectively, while FDR is only 30.72. The experimental results show that the proposed algorithm has good performance and can accomplish the task of power line extraction under complex image backgrounds. The algorithm in this paper solves the main problems of power line extraction and proves the feasibility of the algorithm in other scenarios. In the future, the dataset will be expanded to improve the performance of the algorithm in different scenarios. © 2022 by the authors.",aerial images; connected domain grouping; deep learning; Mask RCNN; power line segmentation; UAV; Antennas; Complex networks; Deep learning; Image segmentation; Textures; Unmanned aerial vehicles (UAV); Aerial images; Complex image; Connected domain grouping; Connected domains; Deep learning; Line extraction; Line segmentation; Mask RCNN; Power line segmentation; Power lines; Extraction
Scopus,"Wang, W.; Wang, S.; Guo, Y.; Zhao, Y.",Obstacle detection method of unmanned electric locomotive in coal mine based on YOLOv3-4L,,2022,,,,10.1117/1.JEI.31.2.023032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129819242&doi=10.1117%2f1.JEI.31.2.023032&partnerID=40&md5=460e3deb463673f820c9b3b6d343276e,"It is one of the most critical technologies for unmanned electric locomotives to detect the obstacles in front of their operation quickly and accurately, which is of great significance for the safe operation of electric locomotives. Aiming at the problems of current computer vision detection methods, such as error warning, low detection accuracy, and slow detection speed, an obstacle intelligent detection method for unmanned electric locomotives based on an improved YOLOv3 (YOLOv3-4L) algorithm is proposed. The obstacle image data set of the electric locomotive running area is constructed to provide a testing environment for various obstacle detection algorithms. In the network structure, the darknet-53 feature extraction network is simplified, and the four-scale detection structure is formed by adding the shallow layer detection scale to the detection layer, which can improve the detection speed and accuracy of the algorithm for obstacles in front of the locomotive. Distance intersection over union loss function and Focal loss function are adopted to redesign the loss function of the target detector to further improve the detection accuracy of the algorithm. Traditional computer vision techniques such as perspective transformation, sliding window, and least square cubic polynomial are used to detect the track lines. By finding the area where the track was located and extending a certain distance to the outside of the track, the dangerous area of electric locomotive running is obtained. The improved YOLOv3 algorithm is utilized to detect obstacles, and only the types and positions of obstacles coincident with dangerous areas are output. The experimental results show that the traditional computer vision techniques such as perspective transformation, sliding window, and least square cubic polynomial can detect not only straight track but also curved track, which makes up for the shortcomings of the Hough transforms in detecting curved tracks. Compared with the original YOLOv3 algorithm, the YOLOv3-4L algorithm improves the mean average precision by 5.1%, and the detection speed increases by 7 fps. YOLOv3-4L detection model has high detection accuracy and speed, which can meet the actual working conditions and provide technical reference for unmanned driving of electric locomotives in underground coal mines.  © 2022 SPIE and IST.",computer vision; dangerous area; feature extraction network; feature scale; loss function; Computer vision; Engines; Extraction; Hough transforms; Statistical tests; Dangerous area; Detection accuracy; Detection methods; Detection speed; Feature extraction network; Feature scale; Features extraction; Loss functions; Obstacles detection; Traditional computers; Feature extraction
Scopus,"Perumal, P.S.; Sujasree, M.; Chavhan, S.; Gupta, D.; Mukthineni, V.; Shimgekar, S.R.; Khanna, A.; Fortino, G.","An insight into crash avoidance and overtaking advice systems for Autonomous Vehicles: A review, challenges and solutions",,2021,,,,10.1016/j.engappai.2021.104406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111931318&doi=10.1016%2fj.engappai.2021.104406&partnerID=40&md5=7023d1048354b81d2cd60718cabe443c,"Emergence of communication technologies made the automotive industries across the globe to embrace Advanced Driver Assistance Systems (ADAS) by considerable investments to ensure accident-free travel, reduction of pollution, fuel conservation. ADAS achieves its goals by integrating complex subsystems such as obstacle avoidance, overtaking advice, lane changing assistance, planning shortest routes, parking assistance, automatic gear shifting, etc., using the emerging technologies. This article emphasizes the road safety aspect of the ADAS by exploring Crash Avoidance and Overtaking Advice (CAOA) subsystems. Existing studies have a noticeable lack of connectivity between various aspects of CAOA subsystems. This review deeply explores and connects CAOA subsystems like road geometries, road debris, obstacle avoidance algorithms powered by Artificial Intelligence (AI), overtaking advice systems, perception challenges of human drivers in various light and weather conditions, driver inattention and misjudgments, vehicle blind-spots, vehicle parameter analysis, performance of vision sensors, in-vehicle computers, driver–vehicle interactions, Vehicle to Infrastructure (V2I) technologies. This article emphasizes the three primary performance metrics of the ADAS, namely accuracy, response time and robustness. Finally, this article discusses a typical functional architecture and gaps identified in existing studies. This article is structured to assist like-minded researchers, who work on CAOA systems for road safety. © 2021 Elsevier Ltd",ADAS; Communication network; Connected autonomous vehicles; Crash avoidance; Deep learning models; Driver assistance; In-vehicle computers; Primary vision sensors; V2X; Accident prevention; Accidents; Advanced driver assistance systems; Automotive industry; Autonomous vehicles; Collision avoidance; Deep learning; Investments; Motor transportation; Road vehicles; Roads and streets; Vehicle to vehicle communications; Advanced driver assistances; Communications networks; Connected autonomous vehicle; Crash avoidance; Deep learning model; Driver assistance; Driver-assistance systems; In-vehicle computer; Primary vision sensor; V2X; Automobile drivers
Scopus,"Rakotoniaina, Z.A.T.; Chelbi, N.E.; Gingras, D.; Faulconnier, F.","LiDAR and Radar Sensor Fusion for Detection and Tracking of Dynamic Objects in Autonomous Vehicles using Probabilistic Occupancy Grid, YOLO and DeepSORT",,2022,,,,10.1109/ITSC55140.2022.9922271,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141838120&doi=10.1109%2fITSC55140.2022.9922271&partnerID=40&md5=d5a897942b92229b28270bf1d1fc521d,"Detection and tracking of on-road moving objects are crucial for vehicle perception. Data Fusion of perception sensors like cameras, lidar, and radar, allows the automotive perception to achieve a certain level of robustness and performance. The efficiency of vehicle perception determines its accuracy to take better decisions related to road safety. In this article, we propose two approaches for the detection and tracking of on-road moving objects by combining information from lidar and radar sensors. In this work, we propose to use a probabilistic occupancy grid for sensor fusion of radar and lidar point cloud, and a modified YOLO DeepSORT approach to detect, classify, and track the moving objects. Using both traditional and deep learning approaches, data fusion allows us to improve the robustness of the automotive perception pipeline. We finally used the nuScenes dataset to evaluate and compare the accuracy of our approaches with state-of-the-art trackers and detectors. We achieved an AMOTA score of 68.3% and a mAP score of 68.7% for the tracking and detection tasks respectively.  © 2022 IEEE.",Autonomous vehicles; Deep learning; Intelligent vehicle highway systems; Motor transportation; Object detection; Roads and streets; Sensor data fusion; Tracking radar; Automotives; Autonomous Vehicles; Detection and tracking; Dynamic objects; Moving objects; Occupancy grids; Performance; Probabilistics; Radar sensors; Sensor fusion; Optical radar
Scopus,"Sellat, Q.; Bisoy, S.; Priyadarshini, R.; Vidyarthi, A.; Kautish, S.; Barik, R.K.",Intelligent Semantic Segmentation for Self-Driving Vehicles Using Deep Learning,,2022,,,,10.1155/2022/6390260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123815278&doi=10.1155%2f2022%2f6390260&partnerID=40&md5=b00f9cb65a26237a4e377623f7a14ed7,"Understanding the situation is a critical component of any self-driving system. Accurate real-time visual signal processing to create pixelwise classed pictures, also known as semantic segmentation, is critical for scenario comprehension and subsequent acceptance of this new technology. Due to the intricate interaction between pixels in each frame of the received camera data, such efficiency in terms of processing time and accuracy could not be achieved prior to recent advances in deep learning algorithms. We present an effective approach for semantic segmentation for self-driving automobiles in this study. We combine deep learning architectures like convolutional neural networks and autoencoders, as well as cutting-edge approaches like feature pyramid networks and bottleneck residual blocks, to develop our model. The CamVid dataset, which has undergone considerable data augmentation, is utilised to train and test our model. To validate the suggested model, we compare the acquired findings to various baseline models reported in the literature.  © 2022 Qusay Sellat et al.","Autonomous Vehicles; Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics; Convolutional neural networks; Deep learning; Learning algorithms; Semantics; Statistical tests; Critical component; Driving systems; Effective approaches; Learning architectures; Processing accuracies; Processing time; Real- time; Self drivings; Semantic segmentation; Visual signal processing; image processing; semantics; Semantic Segmentation"
Scopus,"Cheng, Y.; Zhu, J.; Jiang, M.; Fu, J.; Pang, C.; Wang, P.; Sankaran, K.; Onabola, O.; Liu, Y.; Liu, D.; Bengio, Y.",FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters,,2021,,,,10.1109/ICCV48922.2021.01077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124784920&doi=10.1109%2fICCV48922.2021.01077&partnerID=40&md5=de7ab02048c2142350b280214555d22f,"Marine debris is severely threatening the marine lives and causing sustained pollution to the whole ecosystem. To prevent the wastes from getting into the ocean, it is helpful to clean up the floating wastes in inland waters using the autonomous cleaning devices like unmanned surface vehicles. The cleaning efficiency relies on a high-accurate and robust object detection system. However, the small size of the target, the strong light reflection over water surface, and the reflection of other objects on bank-side all bring challenges to the vision-based object detection system. To promote the practical application for autonomous floating wastes cleaning, we present FloW†, the first dataset for floating waste detection in inland water areas. The dataset consists of an image sub-dataset FloW-Img and a multimodal sub-dataset FloW-RI which contains synchronized millimeter wave radar data and images. Accurate annotations for images and radar data are provided, supporting floating waste detection strategies based on image, radar data, and the fusion of two sensors. We perform several baseline experiments on our dataset, including vision-based and radar-based detection methods. The results show that, the detection accuracy is relatively low and floating waste detection still remains a challenging task. © 2021 IEEE",Cleaning; Light reflection; Marine pollution; Millimeter waves; Object detection; Oil spills; Tracking radar; Autonomous cleaning; Cleaning devices; Cleaning efficiency; Inland waters; Marine debris; Marine life; Object detection systems; Radar data; Robust object detection; Vision based; Object recognition
Scopus,"Luo, Q.; Wang, J.; Gao, M.; He, Z.; Yang, Y.; Zhou, H.",Multiple Mechanisms to Strengthen the Ability of YOLOv5s for Real-Time Identification of Vehicle Type,,2022,,,,10.3390/electronics11162586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137393119&doi=10.3390%2felectronics11162586&partnerID=40&md5=dc23cffcd250ee53855a6f44f2d226c1,"Identifying the type of vehicle on the road is a challenging task, especially in the natural environment with all its complexities, such that the traditional architecture for object detection requires an excessively large amount of computation. Such lightweight networks as MobileNet are fast but cannot satisfy the performance-related requirements of this task. Improving the detection-related performance of small networks is, thus, an outstanding challenge. In this paper, we use YOLOv5s as the backbone network to propose a large-scale convolutional fusion module called the ghost cross-stage partial network (G_CSP), which can integrate large-scale information from different feature maps to identify vehicles on the road. We use the convolutional triplet attention network (C_TA) module to extract attention-based information from different dimensions. We also optimize the original spatial pyramid pooling fast (SPPF) module and use the dilated convolution to increase the capability of the network to extract information. The optimized module is called the DSPPF. The results of extensive experiments on the bdd100K, VOC2012 + 2007, and VOC2019 datasets showed that the improved YOLOv5s network performs well and can be used on mobile devices in real time. © 2022 by the authors.",C_TA; DSPPF; G_CSP; object detection; vehicle type detection
Scopus,"Cao, X.; Zhang, Z.; Sun, Y.; Wang, P.; Xu, S.; Liu, F.; Wang, C.; Peng, F.; Mu, S.; Liu, W.; Yang, Y.",The review of image processing and edge computing for intelligent transportation system,,2022,,,,10.11834/jig.211266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131901853&doi=10.11834%2fjig.211266&partnerID=40&md5=3c437780aa6042e458dd79b4c30d110e,"Current intelligent transportation system (ITS) issue is challenged to improve conventional transportation engineering solutions nowadays. ITS has integrated advanced information and communication technologies (ICTs) into the holistic transportation system for a safe, efficient, comfortable and environment friendly transport ecological construction. A variety of ITS applications and services have emerged in the intelligent traffic management system, autonomous driving system, and cooperative vehicle-infrastructure system (CVIS). To realise the intelligentisation of the traffic management, the intelligent traffic management system is mainly supported by optimised smart traffic facilities. Autonomous driving is mainly based on vehicle intelligence, relying on the cooperation of visual perception, radar perception, positioning system, on-board computing and artificial intelligence. The CVIS depends on the collaborative work of intelligent vehicles, roadside equipment and cloud platforms in support of the internet of vehicles (IOV) to fully implement dynamic real-time information interaction between vehicles and related traffic issues to achieve the vehicle-road collaborative management and traffic safety. Image processing is one of the core technologies that can support the deployment of a large number of ITS applications. It is based on computer algorithms application to extract useful visual sensor data derived information, including image enhancement and restoration, feature extraction and classification, as well as the semantic and instance segmentation. Image enhancement and restoration refers to improve the visual performance of the image and render more suitable image for human or machine analysis to deal with the image quality reduction issues of the challenging traffic system scenarios. Image feature extraction and classification technology is the core of object detection for accurate images or videos derived traffic objects locating and identifying, which is the basis for solving more complex higher-level tasks such as segmentation, scene understanding, object tracking, image description, event detection and activity recognition. To achieve higher-precision environmental perception in ITS, the semantic and instance segmentation realises pixel-level image classification based on scene semantic information in terms of the provision of different labels for instances of the same category. These image processing technologies can illustrate crucial references and information to enhance the capabilities of the perception, recognition, object detection, tracking, and path planning modules for more ITS applications. The multifaceted scenarios integration provides important technical support for intelligent traffic management, autonomous driving and the CVIS. In addition, the wide deployment of sensing devices places tremendous demands on data transmission and processing. As the widely recognised data processing technology, the centralised cloud computing is challenged to meet the real-time requirements of most massive data applications in ITS, which leads to uncertainty and barriers in the transmission process. Differentiated from the centralised cloud computing, the multi-access edge computing (MEC) technology deploys sensing, computation, and storage resources close to the network edge and provide a low-latency response-based platform for ITS, high bandwidth, and real-time applications and services access to network information. Our research reviews the current development status and typical applications of ITS. We focus on current image processing and MEC technologies for ITS. Future research direction of ITS and its related image processing and MEC technologies are predicted. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.",Autonomous driving; Cooperative vehicle-infrastructure system(CVIS); Deep learning; Edge computing; Image processing; Intelligent transportation system(ITS)
Scopus,"Junaid, M.; Szalay, Z.; Török, Á.",Evaluation of non-classical decision-making methods in self driving cars: Pedestrian detection testing on cluster of images with different luminance conditions,,2021,,,,10.3390/en14217172,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118554181&doi=10.3390%2fen14217172&partnerID=40&md5=a94bac53c5cc2906715977ec2669aa4f,"Self-driving cars, i.e., fully automated cars, will spread in the upcoming two decades, according to the representatives of automotive industries; owing to technological breakthroughs in the fourth industrial revolution, as the introduction of deep learning has completely changed the concept of automation. There is considerable research being conducted regarding object detection systems, for instance, lane, pedestrian, or signal detection. This paper specifically focuses on pedestrian detection while the car is moving on the road, where speed and environmental conditions affect visibility. To explore the environmental conditions, a pedestrian custom dataset based on Common Object in Context (COCO) is used. The images are manipulated with the inverse gamma correction method, in which pixel values are changed to make a sequence of bright and dark images. The gamma correction method is directly related to luminance intensity. This paper presents a flexible, simple detection system called Mask R-CNN, which works on top of the Faster R-CNN (Region Based Convolutional Neural Network) model. Mask R-CNN uses one extra feature instance segmentation in addition to two available features in the Faster R-CNN, called object recognition. The performance of the Mask R-CNN models is checked by using different Convolu-tional Neural Network (CNN) models as a backbone. This approach might help future work, especially when dealing with different lighting conditions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Illumination; Instance segmentation; Inverse gamma correction; Mask R-CNN; Pedestrian custom dataset; Transfer learning; Automotive industry; Convolution; Convolutional neural networks; Decision making; Deep learning; Inverse problems; Luminance; Object detection; Object recognition; Statistical tests; Convolutional neural network; Correction method; Environmental conditions; Inverse gamma correction; Mask region based convolutional neural network; Neural network model; Pedestrian custom dataset; Pedestrian detection; Region-based; Transfer learning; Instance Segmentation
Scopus,"Blachut, K.; Danilowicz, M.; Szolc, H.; Wasala, M.; Kryjak, T.; Komorkiewicz, M.",Automotive Perception System Evaluation with Reference Data from a UAV’s Camera Using ArUco Markers and DCNN,,2022,,,,10.1007/s11265-021-01734-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914056&doi=10.1007%2fs11265-021-01734-3&partnerID=40&md5=be00ba7ba8217131d116c6ecfe04e333,"Testing and evaluation of an automotive perception system is a complicated task which requires special equipment and infrastructure. To compute key performance indicators and compare the results with real-world situation, some additional sensors and manual data labelling are often required. In this article, we propose a different approach, which is based on a UAV equipped with a 4K camera flying above a test track. Two computer vision methods are used to precisely determine the positions of the objects around the car – one based on ArUco markers and the other on a DCNN (we provide the algorithms used on GitHub). The detections are then correlated with the perception system readings. For the static and dynamic experiments, the differences between various systems are mostly below 0.5 m. The results of the experiments performed indicate that this approach could be an interesting alternative to existing evaluation solutions. © 2021, The Author(s).",ADAS; ArUco markers; Automatic labelling; Automotive; DCNN; Drone; Evaluation; LiDAR; Perception systems; Testing; UAV; Benchmarking; Drones; ADAS; Aruco marker; Automatic labelling; Automotives; DCNN; Drone; Evaluation; LiDAR; Perception systems; System evaluation; Cameras
Scopus,"Kumawat, H.; Mukhopadhyay, S.",Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection,,2022,,,,10.1109/IJCNN55064.2022.9892184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140709378&doi=10.1109%2fIJCNN55064.2022.9892184&partnerID=40&md5=53fc48cb51881e32b0330c35e1a90adf,"An autonomous system's perception engine must provide an accurate understanding of the environment for it to make decisions. Deep learning based object detection networks experience degradation in the performance and robustness for small and far away objects due to a reduction in object's feature map as we move to higher layers of the network. In this work, we propose a novel radar-guided spatial attention for RGB images to improve the perception quality of autonomous vehicles operating in a dynamic environment. In particular, our method improves the perception of small and long range objects, which are often not detected by the object detectors in RGB mode. The proposed method consists of two RGB object detectors, namely the Primary detector and a lightweight Secondary detector. The primary detector takes a full RGB image and generates primary detections. Next, the radar proposal framework creates regions of interest (ROIs) for object proposals by projecting the radar point cloud onto the 2D RGB image. These ROIs are cropped and fed to the secondary detector to generate secondary detections which are then fused with the primary detections via non-maximum suppression. This method helps in recovering the small objects by preserving the object's spatial features through an increase in their receptive field. We evaluate our fusion method on the challenging nuScenes dataset and show that our fusion method with SSD-lite as primary and secondary detector improves the baseline primary yolov3 detector's recall by 14 % while requiring three times fewer computational resources. © 2022 IEEE.",Autonomous Systems; Deep Learning; Object Detection; Radar; RGB Camera; Sensor Fusion; Behavioral research; Deep learning; Image enhancement; Network layers; Object recognition; Radar imaging; Tracking radar; Autonomous system; Deep learning; Fusion methods; Object detectors; Objects detection; Region-of-interest; Regions of interest; RGB cameras; RGB images; Sensor fusion; Object detection
Scopus,"Xue, J.; Cheng, F.; Li, Y.; Song, Y.; Mao, T.",Detection of Farmland Obstacles Based on an Improved YOLOv5s Algorithm by Using CIoU and Anchor Box Scale Clustering,,2022,,,,10.3390/s22051790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125045718&doi=10.3390%2fs22051790&partnerID=40&md5=5a7f947fcbf1efeb6857e85d5bdfe02f,"It is necessary to detect multi‐type farmland obstacles in real time and accurately for un-manned agricultural vehicles. An improved YOLOv5s algorithm based on the K‐Means clustering algorithm and CIoU Loss function was proposed to improve detection precision and speed up real-time detection. The K‐Means clustering algorithm was used in order to generate anchor box scales to accelerate the convergence speed of model training. The CIoU Loss function, combining the three geometric measures of overlap area, center distance and aspect ratio, was adopted to reduce the occurrence of missed and false detection and improve detection precision. The experimental results showed that the inference time of a single image was reduced by 75% with the improved YOLOv5s algorithm; compared with that of the Faster R‐CNN algorithm, real‐time performance was effec-tively improved. Furthermore, the mAP value of the improved algorithm was increased by 5.80% compared with that of the original YOLOv5s, which indicates that using the CIoU Loss function had an obvious effect on reducing the missed detection and false detection of the original YOLOv5s. Moreover, the detection of small target obstacles of the improved algorithm was better than that of the Faster R‐CNN. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Deep learning; Farmland obstacles; Target detection; YOLOv5s; Algorithms; Cluster Analysis; Farms; Aspect ratio; Clustering algorithms; Deep learning; Image enhancement; Inference engines; Anchor-box; Deep learning; Detection precision; False detections; Farmland obstacle; K-means clustering algorithms; Loss functions; Missed detections; Targets detection; YOLOv5; agricultural land; algorithm; cluster analysis; Farms
Scopus,"Zhou, Y.; Liu, L.; Zhao, H.; López-Benítez, M.; Yu, L.; Yue, Y.","Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges",,2022,,,,10.3390/s22114208,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131703088&doi=10.3390%2fs22114208&partnerID=40&md5=8e2f428493ac9745a882311b903d038b,"With recent developments, the performance of automotive radar has improved significantly. The next generation of 4D radar can achieve imaging capability in the form of high-resolution point clouds. In this context, we believe that the era of deep learning for radar perception has arrived. However, studies on radar deep learning are spread across different tasks, and a holistic overview is lacking. This review paper attempts to provide a big picture of the deep radar perception stack, including signal processing, datasets, labelling, data augmentation, and downstream tasks such as depth and velocity estimation, object detection, and sensor fusion. For these tasks, we focus on explaining how the network structure is adapted to radar domain knowledge. In particular, we summarise three overlooked challenges in deep radar perception, including multi-path effects, uncertainty problems, and adverse weather effects, and present some attempts to solve them. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","automotive radars; autonomous driving; deep learning; multi-sensor fusion; object detection; radar signal processing; Perception; Radar; Signal Processing, Computer-Assisted; Weather; Automotive radar; Deep learning; Object detection; Object recognition; Sensor data fusion; Tracking radar; Automotive radar; Autonomous driving; Deep learning; High resolution point clouds; Imaging capabilities; Labelings; Multi-sensor fusion; Performance; Review papers; Signal-processing; perception; signal processing; telecommunication; weather; Autonomous vehicles"
Scopus,"Juyal, A.; Sharma, S.; Matta, P.",Multiclass Objects Localization Using Deep Learning Technique in Autonomous Vehicle,,2022,,,,10.1109/CSITSS57437.2022.10026411,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147846523&doi=10.1109%2fCSITSS57437.2022.10026411&partnerID=40&md5=9b7ef12216f83254c5e5089ac98b61c5,"In a conventional automobile, the vehicle is controlled by a human brain. The eyes, which gather visual data from surrounding traffic, are the most important human sense. This visual input is then sent to the brain through the eyes. The eyes then communicate this visual information to the brain, which is in charge of operating the car by applying brakes, accelerating, or turning left or right. Autonomous vehicles or self-driving cars, on the other hand, rely on sensors, LiDAR, RADAR, and cameras to gather traffic data. This data from multiple sensors and cameras must be processed for the car to travel smoothly and securely. Intelligent software that can simulate a human brain, classify, and localize numerous sorts of traffic objects in real- time is required for autonomous vehicles. The image is the major input of an autonomous vehicle, and the program must detect the object from the input image. Object localization, or predicting the true position of a recognized object in an image, is a critical problem. The software can only predict the next step or decide what distance to keep between the car and other traffic objects, as well as whether to brake and accelerate, after exact object localization. We are proposing an object localization method using YOLOv5 as an object detector and LiDAR to calculate the distance. We created a custom dataset of 6000 images of five different classes and achieved 89.2% mAP value and at a.95 IoU threshold, our model achieved 52.1 mAP. We have also compared several conceivable computer vision challenges and real-time deep learning models in object localization in this work.  © 2022 IEEE.",autonomous; Deep learning; Faster RCNN; Object localization; Self-driving car; SSD; YOLOv5; Brain; Brakes; Cameras; Deep learning; Learning systems; Object detection; Object recognition; Optical radar; Autonomous; Autonomous Vehicles; Deep learning; Fast RCNN; Human brain; Object localization; Real- time; SSD; Traffic objects; YOLOv5; Autonomous vehicles
Scopus,"Zhou, Z.; Sun, J.; Yu, J.; Liu, K.; Duan, J.; Chen, L.; Chen, C.L.P.",An Image-Based Benchmark Dataset and a Novel Object Detector for Water Surface Object Detection,,2021,,,,10.3389/fnbot.2021.723336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116895167&doi=10.3389%2ffnbot.2021.723336&partnerID=40&md5=217ac554f4b1baa515fdc31370995f18,"Water surface object detection is one of the most significant tasks in autonomous driving and water surface vision applications. To date, existing public large-scale datasets collected from websites do not focus on specific scenarios. As a characteristic of these datasets, the quantity of the images and instances is also still at a low level. To accelerate the development of water surface autonomous driving, this paper proposes a large-scale, high-quality annotated benchmark dataset, named Water Surface Object Detection Dataset (WSODD), to benchmark different water surface object detection algorithms. The proposed dataset consists of 7,467 water surface images in different water environments, climate conditions, and shooting times. In addition, the dataset comprises a total of 14 common object categories and 21,911 instances. Simultaneously, more specific scenarios are focused on in WSODD. In order to find a straightforward architecture to provide good performance on WSODD, a new object detector, named CRB-Net, is proposed to serve as a baseline. In experiments, CRB-Net was compared with 16 state-of-the-art object detection methods and outperformed all of them in terms of detection precision. In this paper, we further discuss the effect of the dataset diversity (e.g., instance size, lighting conditions), training set size, and dataset details (e.g., method of categorization). Cross-dataset validation shows that WSODD significantly outperforms other relevant datasets and that the adaptability of CRB-Net is excellent. © Copyright © 2021 Zhou, Sun, Yu, Liu, Duan, Chen and Chen.",baseline; cross-dataset validation; dataset; detector; surface object detection; Autonomous vehicles; Benchmarking; Large dataset; Object recognition; water; Autonomous driving; Baseline; Benchmark datasets; Cross-dataset validation; Dataset; Image-based; Object detectors; Objects detection; Surface object detection; Water surface; algorithm; aquatic environment; Article; benchmarking; climate; data base; illumination; imaging; measurement precision; validation process; water surface object detection dataset; Object detection
Scopus,"Cheng, Y.; Xu, H.; Liu, Y.",Robust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar,,2021,,,,10.1109/ICCV48922.2021.01498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127081552&doi=10.1109%2fICCV48922.2021.01498&partnerID=40&md5=a2bc207bd921cf0bf005d261e2817458,"In recent years, unmanned surface vehicles (USVs) have been experiencing growth in various applications. With the expansion of USVs' application scenes from the typical marine areas to inland waters, new challenges arise for the object detection task, which is an essential part of the perception system of USVs. In our work, we focus on a relatively unexplored task for USVs in inland waters: small object detection on water surfaces, which is of vital importance for safe autonomous navigation and USVs' certain missions such as floating waste cleaning. Considering the limitations of vision-based object detection, we propose a novel radar-vision fusion based method for robust small object detection on water surfaces. By using a novel representation format of millimeter wave radar point clouds and applying a deep-level multi-scale fusion of RGB images and radar data, the proposed method can efficiently utilize the characteristics of radar data and improve the accuracy and robustness for small object detection on water surfaces. We test the method on the real-world floating bottle dataset that we collected and released. The result shows that, our method improves the average detection accuracy significantly compared to the vision-based methods and achieves state-of-the-art performance. Besides, the proposed method performs robustly when single sensor degrades. © 2021 IEEE",Bottles; Image enhancement; Millimeter waves; Object recognition; Radar imaging; Statistical tests; Synthetic aperture radar; Unmanned surface vehicles; Autonomous navigation; Inland waters; Marine areas; Millimeter-wave radar; Millimetre-wave radar; Perception systems; Radar data; Small object detection; Vehicle applications; Water surface; Object detection
Scopus,"Dazlee, N.M.A.A.; Khalil, S.A.; Abdul-Rahman, S.; Mutalib, S.",Object Detection for Autonomous Vehicles with Sensor-based Technology Using YOLO,,2022,,,,10.18201/ijisae.2022.276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128167980&doi=10.18201%2fijisae.2022.276&partnerID=40&md5=15b2021d6550f4a80f9474611ec8897b,"The year 2020 has been a tough year with the global pandemic situation, and the utmost priority is to live in a clean, green, and safe environment. One of the areas that the governments are emphasizing for the readiness of our ecosystem is autonomous and contactless environments in adapting to the new norm. Thus, Autonomous Vehicle (AV) is a promising technology to bring forward. One of the critical aspects of Autonomous Navigation is object detection. Most AV use multiple sensors to detect objects, such as cameras, radar and Light Detection and Ranging sensor (LiDAR). Nowadays, the LiDAR sensor is widely implemented due to the ability to detect objects in the form of pulsed lasers, benefiting in low-light object detection. However, even with advanced technology, poor programming can affect the performance of object detection system. Thus, the study explores the state-of-the-art of You Only Look Once (YOLO) algorithms namely Tiny-YOLO and Complex-YOLO for object detection on KITTI dataset. Their performances were compared based on accuracy, precision, and recall metrics. The results showed that the Complex-YOLO has better performance as the mean average precision is higher than the Tiny-YOLO model when tested with equal parameters. © 2022, Ismail Saritas. All rights reserved.",Autonomous Vehicle; KITTI; LiDAR; Object Detection; Sensor; YOLO
Scopus,"Nex, F.; Armenakis, C.; Cramer, M.; Cucci, D.A.; Gerke, M.; Honkavaara, E.; Kukko, A.; Persello, C.; Skaloud, J.",UAV in the advent of the twenties: Where we stand and what is next,,2022,,,,10.1016/j.isprsjprs.2021.12.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122819494&doi=10.1016%2fj.isprsjprs.2021.12.006&partnerID=40&md5=7741605ac38cb3758131594caf76dabe,"The use of Unmanned Aerial Vehicles (UAVs) has surged in the last two decades, making them popular instruments for a wide range of applications, and leading to a remarkable number of scientific contributions in geoscience, remote sensing and engineering. However, the development of best practices for high quality of UAV mapping are often overlooked representing a drawback for their wider adoption. UAV solutions then require an inter-disciplinary research, integrating different expertise and combining several hardware and software components on the same platform. Despite the high number of peer-reviewed papers on UAVs, little attention has been given to the interaction between research topics from different domains (such as robotics and computer vision) that impact the use of UAV in remote sensing. The aim of this paper is to (i) review best practices for the use of UAVs for remote sensing and mapping applications and (ii) report on current trends - including adjacent domains - for UAV use and discuss their future impact in photogrammetry and remote sensing. Hardware developments, navigation and acquisition strategies, and emerging solutions for data processing in innovative applications are considered in this analysis. As the number and the heterogeneity of debated topics are large, the paper is organized according to very specific questions considered most relevant by the authors. © 2021 The Authors",Data processing; Deep learning; Hyperspectral; LiDAR; Navigation; Photogrammetry; Remote sensing; Sensors; UAV; Antennas; Computer hardware; Computer vision; Data handling; Deep learning; Mapping; Photogrammetry; Remote sensing; Robotics; Robots; Best practices; Deep learning; Geosciences; High quality; HyperSpectral; Inter-disciplinary researches; LiDAR; Remote engineering; Remote-sensing; Scientific contributions; best management practice; data processing; hardware; instrumentation; interdisciplinary approach; photogrammetry; remote sensing; software; unmanned vehicle; Unmanned aerial vehicles (UAV)
Scopus,"Roszyk, K.; Nowicki, M.R.; Skrzypczyński, P.",Adopting the YOLOv4 Architecture for Low-Latency Multispectral Pedestrian Detection in Autonomous Driving,,2022,,,,10.3390/s22031082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123549403&doi=10.3390%2fs22031082&partnerID=40&md5=38a2e662d5e7593952c8accf016e56a0,"Detecting pedestrians in autonomous driving is a safety-critical task, and the decision to avoid a a person has to be made with minimal latency. Multispectral approaches that combine RGB and thermal images are researched extensively, as they make it possible to gain robustness under varying illumination and weather conditions. State-of-the-art solutions employing deep neural networks offer high accuracy of pedestrian detection. However, the literature is short of works that evaluate multispectral pedestrian detection with respect to its feasibility in obstacle avoidance scenarios, taking into account the motion of the vehicle. Therefore, we investigated the real-time neural network detector architecture You Only Look Once, the latest version (YOLOv4), and demonstrate that this detector can be adapted to multispectral pedestrian detection. It can achieve accuracy on par with the state-of-the-art while being highly computationally efficient, thereby supporting low-latency decision making. The results achieved on the KAIST dataset were evaluated from the perspective of automotive applications, where low latency and a low number of false negatives are critical parameters. The middle fusion approach to YOLOv4 in its Tiny variant achieved the best accuracy to computational efficiency trade-off among the evaluated architectures. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Deep learning; Multispectral fusion; Pedestrian detection; Real-time; You Only Look Once; Computational efficiency; Decision making; Deep neural networks; Economic and social effects; Network architecture; Pedestrian safety; Autonomous driving; Critical tasks; Deep learning; Low latency; Multi-spectral; Multispectral fusion; Pedestrian detection; Real- time; State of the art; You only look once; Autonomous vehicles
Scopus,"Chen, G.; Wang, F.; Li, W.; Hong, L.; Conradt, J.; Chen, J.; Zhang, Z.; Lu, Y.; Knoll, A.",NeuroIV: Neuromorphic Vision Meets Intelligent Vehicle Towards Safe Driving with a New Database and Baseline Evaluations,,2022,,,,10.1109/TITS.2020.3022921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104099888&doi=10.1109%2fTITS.2020.3022921&partnerID=40&md5=6a6079237a2d6affe1441f8743cd729f,"Neuromorphic vision sensors such as the Dynamic and Active-pixel Vision Sensor (DAVIS) using silicon retina are inspired by biological vision, they generate streams of asynchronous events to indicate local log-intensity brightness changes. Their properties of high temporal resolution, low-bandwidth, lightweight computation, and low-latency make them a good fit for many applications of motion perception in the intelligent vehicle. However, as a younger and smaller research field compared to classical computer vision, neuromorphic vision is rarely connected with the intelligent vehicle. For this purpose, we present three novel datasets recorded with DAVIS sensors and depth sensor for the distracted driving research and focus on driver drowsiness detection, driver gaze-zone recognition, and driver hand-gesture recognition. To facilitate the comparison with classical computer vision, we record the RGB, depth and infrared data with a depth sensor simultaneously. The total volume of this dataset has 27360 samples. To unlock the potential of neuromorphic vision on the intelligent vehicle, we utilize three popular event-encoding methods to convert asynchronous event slices to event-frames and adapt state-of-the-art convolutional architectures to extensively evaluate their performances on this dataset. Together with qualitative and quantitative results, this work provides a new database and baseline evaluations named NeuroIV in cross-cutting areas of neuromorphic vision and intelligent vehicle.  © 2000-2011 IEEE.",advanced driver assistance system; database and baseline evaluations; deep learning; distracted driving; event encoding; Neuromorphic vision; Advanced driver assistance systems; Automobile drivers; Computer vision; Database systems; Deep learning; Encoding (symbols); Vehicles; Active pixel; Asynchronous event; Database and baseline evaluation; Deep learning; Depth sensors; Distracted driving; Event encoding; Neuromorphic visions; Safe driving; Vision sensors; Signal encoding
Scopus,"Luo, H.; Wang, P.; Chen, H.; Xu, M.",Object Detection Method Based on Shallow Feature Fusion and Semantic Information Enhancement,,2021,,,,10.1109/JSEN.2021.3103612,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116513288&doi=10.1109%2fJSEN.2021.3103612&partnerID=40&md5=aa93ad0cc9e74800905167163a725996,"In recent years, object detection algorithm based on deep learning has made great progress, but the detection effect is not ideal for small objects detection. Some methods use high-resolution features or enhance shallow features to improve the detection accuracy of small objects. However, using high-resolution features for detection needs higher computational cost, and enhancing shallow features by propagating semantic information from high-level into low-level may bring information aliasing. To address this issue, we propose a novel object detection method based on shallow feature fusion and semantic information enhancement (FFSI). The high-level semantic information is injected into low-level features to guide the enhancement of specific detail information. In order to reduce the information aliasing in shallow features and enhance the receptive field of shallow features, we design two parallel modules: context information enhancement module (CIE) and receptive field enhancement module (RFE). CIE highlights the location of objects by establishing the relationship between local and global context information. RFE enhances the receptive field of shallow features by using dilated convolution to adapt to object detection of different scales, especially small objects. The proposed model is evaluated extensively on PASCAL VOC, and COCO datasets. The experimental results demonstrate that the proposed FFSI model has competitive performance. More importantly, this study reveals that FFSI outperforms the state-of-the-art methods in detecting small objects.  © 2001-2012 IEEE.",context information enhancement; Object detection; receptive field enhancement; shallow feature enhancement; Deep learning; Feature extraction; Object recognition; Semantics; Context information; Context information enhancement; Feature enhancement; Features fusions; Field enhancement; Receptive field enhancement; Receptive fields; Semantics Information; Shallow feature enhancement; Small objects; Object detection
Scopus,"Castells-Rufas, D.; Ngo, V.; Borrego-Carazo, J.; Codina, M.; Sanchez, C.; Gil, D.; Carrabina, J.",A Survey of FPGA-Based Vision Systems for Autonomous Cars,,2022,,,,10.1109/ACCESS.2022.3230282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144769258&doi=10.1109%2fACCESS.2022.3230282&partnerID=40&md5=4ad4b5917f90b53518d2ab4c034d3690,"On the road to making self-driving cars a reality, academic and industrial researchers are working hard to continue to increase safety while meeting technical and regulatory constraints Understanding the surrounding environment is a fundamental task in self-driving cars. It requires combining complex computer vision algorithms. Although state-of-the-art algorithms achieve good accuracy, their implementations often require powerful computing platforms with high power consumption. In some cases, the processing speed does not meet real-time constraints. FPGA platforms are often used to implement a category of latency-critical algorithms that demand maximum performance and energy efficiency. Since self-driving car computer vision functions fall into this category, one could expect to see a wide adoption of FPGAs in autonomous cars. In this paper, we survey the computer vision FPGA-based works from the literature targeting automotive applications over the last decade. Based on the survey, we identify the strengths and weaknesses of FPGAs in this domain and future research opportunities and challenges.  © 2013 IEEE.",Autonomous automobile; computer vision; field programmable gate arrays; reconfigurable architectures; Accident prevention; Computer vision; Electric power utilization; Energy efficiency; Feature extraction; Field programmable gate arrays (FPGA); Image classification; Object detection; Object recognition; Autonomous car; Computer vision algorithms; Features extraction; Field programmable gate array; Field programmables; Images classification; Objects detection; Programmable gate array; Surrounding environment; Vision systems; Reconfigurable architectures
Scopus,"Han, X.",Modified Cascade RCNN Based on Contextual Information for Vehicle Detection,,2021,,,,10.1007/s11220-021-00342-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104096133&doi=10.1007%2fs11220-021-00342-6&partnerID=40&md5=86203ad48ff9f29b7640f8686464a393,"In the process of traditional vehicle detection, there are some problems such as the fault detection and missing detection for small objects and shielded objects. Therefore, we propose a modified Cascade region-based convolutional neural network (RCNN) based on contextual information for vehicle detection. Firstly, the feature pyramid is improved to integrate the shallow information into the deep network layer by layer to enhance the features of small objects and occlusion objects. In here, we introduce the predictive optimization module and combine the context information of the region of interest (ROI), which makes the feature information have stronger robustness. Meanwhile, the multi-scale and multi-stage prediction is realized through the multi-threshold prediction network of internal cascade. Under the premise that the network parameters are basically unchanged, the accuracy rate is improved. Secondly, the multi-branch dilated convolution is introduced to reduce the feature loss during the down-sampling process. Finally, the region of interest and context information are fused to enhance the object feature expression. Experimental results show that the new Cascade RCNN method can better detect small and shielded vehicles compared with other state-of-the-art vehicle detection methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Cascade RCNN; Contextual information; Feature pyramid; ROI; Vehicle detection; Convolution; Convolutional neural networks; Fault detection; Image segmentation; Network layers; Vehicles; Context information; Contextual information; Feature expression; Feature information; Network parameters; Optimization module; Region of interest; The region of interest (ROI); Object detection
Scopus,"Kim, S.; Park, S.-Y.",Expandable spherical projection and feature fusion methods for object detection from fisheye images,,2021,,,,10.23919/MVA51890.2021.9511379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113987286&doi=10.23919%2fMVA51890.2021.9511379&partnerID=40&md5=4cf295a7f08ca8065344b527840f73ae,"One of the key requirements for enhanced autonomous driving systems is accurate detection of the objects from a wide range of view. Large-angle images from a fisheye lens camera can be an effective solution for automotive applications. However, it comes with the cost of strong radial distortions. In particular, the fisheye camera has a photographic effect of exaggerating the size of objects in central regions of the image, while making objects near the marginal area appear smaller. Therefore, we propose the Expandable Spherical Projection that expands center or margin regions to produce straight edges of de-warped objects with less unwanted background in the bounding boxes. In addition to this, we analyze the influence of multi-scale feature fusion in a real-time object detector, which learns to extract more meaningful information for small objects. We present three different types of concatenated YOLOv3-SPP architectures. Moreover, we demonstrate the effectiveness of our proposed projection and feature-fusion using multiple fisheye lens datasets, which shows up to 4.7% AP improvement compared to fisheye images and baseline model.  © 2021 MVA Organization.",Cameras; Computer vision; Feature extraction; Image enhancement; Image fusion; Automotive applications; Autonomous driving; Effective solution; Feature fusion method; Multi-scale features; Object detectors; Radial distortions; Spherical projection; Object detection
Scopus,"Sharma, V.K.; Mir, R.N.",A comprehensive and systematic look up into deep learning based object detection techniques: A review,,2020,,,,10.1016/j.cosrev.2020.100301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097498691&doi=10.1016%2fj.cosrev.2020.100301&partnerID=40&md5=3fe81e0533dc670e9746d057afde6cd7,"Object detection can be regarded as one of the most fundamental and challenging visual recognition task in computer vision and it has received great attention over the past few decades. Object detection techniques find their application in almost all the spheres of life, most prominent ones being surveillance, autonomous driving, pedestrian detection and so on. The primary focus of visual object detection is to detect objects belonging to certain class targets with absolute localization in a realistic scene or an input image and also to assign each detected instance of an object a predefined class label. Owing to rapid development of deep neural networks, the performance of object detectors has rapidly improved and as a result of this deep learning based detection techniques have been actively studied over the past several years. In this paper we provide a comprehensive survey of latest advances in deep learning based visual object detection. Firstly we have reviewed a large body of recent works in literature and using that we have analyzed traditional and current object detectors. Afterwards and primarily we provide a rigorous overview of backbone architectures for object detection followed by a systematic cover up of current learning strategies. Some popular datasets and metrics used for object detection are analyzed as well. Finally we discuss applications of object detection and provide several future directions to facilitate future research for visual object detection with deep learning. © 2020 Elsevier Inc.",Classification; Convolutional neural networks; Deep learning; Localization; Object detection; Segmentation; Convolutional neural networks; Deep neural networks; Learning systems; Object recognition; Transfer learning; 'current; Autonomous driving; Convolutional neural network; Deep learning; Localisation; Object detectors; Objects detection; Segmentation; Visual objects; Visual recognition; Object detection
Scopus,"Fursa, I.; Fandi, E.; Musat, V.; Culley, J.; Gil, E.; Teeti, I.; Bilous, L.; Vander Sluis, I.; Rast, A.; Bradley, A.",Worsening Perception: Real-Time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions,,2022,,,,10.4271/12-05-01-0008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124040624&doi=10.4271%2f12-05-01-0008&partnerID=40&md5=b97dee0adbf4bfcf9ea06ddaa54053f1,"Autonomous vehicles (AVs) rely heavily upon their perception subsystems to ""see""the environment in which they operate. Unfortunately, the effect of variable weather conditions presents a significant challenge to object detection algorithms, and thus, it is imperative to test the vehicle extensively in all conditions which it may experience. However, the development of robust AV subsystems requires repeatable, controlled testing - while real weather is unpredictable and cannot be scheduled. Real-world testing in adverse conditions is an expensive and time-consuming task, often requiring access to specialist facilities. Simulation is commonly relied upon as a substitute, with increasingly visually realistic representations of the real world being developed. In the context of the complete AV control pipeline, subsystems downstream of perception need to be tested with accurate recreations of the perception system output, rather than focusing on subjective visual realism of the input - whether in simulation or the real world. This study develops the untapped potential of a lightweight weather augmentation method in an autonomous racing vehicle - focusing not on visual accuracy but rather the effect upon perception subsystem performance in real time. With minimal adjustment, the prototype developed in this study can replicate the effects of water droplets on the camera lens and fading light conditions. This approach introduces a latency of less than 8 ms using computer hardware well suited to being carried in the vehicle - rendering it ideal for real-time implementation that can be run during experiments in simulation and augmented reality testing in the real world. © 2022 SAE International.",Augmented reality testing; Autonomous driving; Autonomous racing; Autonomous vehicle simulation; Sensor modelling; Weather augmentation; Autonomous vehicles; Computer hardware; Meteorology; Object detection; Real time control; Vehicle performance; Augmented reality testing; Autonomous driving; Autonomous racing; Autonomous vehicle simulation; Autonomous Vehicles; Real- time; Real-world; Sensors models; Vehicle simulation; Weather augmentation; Augmented reality
Scopus,"Lin, L.; Gong, S.; Peeta, S.; Wu, X.",Long short-term memory-based human-driven vehicle longitudinal trajectory prediction in a connected and autonomous vehicle environment,,2021,,,,10.1177/0361198121993471,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108263300&doi=10.1177%2f0361198121993471&partnerID=40&md5=f7ef46e61da4aa33025c6d5fd97a586e,"The advent of connected and autonomous vehicles (CAVs) will change driving behavior and travel environment, and provide opportunities for safer, smoother, and smarter road transportation. During the transition from the current human-driven vehicles (HDVs) to a fully CAV traffic environment, the road traffic will consist of a ‘‘mixed’’ traffic flow of HDVs and CAVs. Equipped with multiple sensors and vehicle-to-vehicle communications, a CAV can track surrounding HDVs and receive trajectory data of other CAVs in communication range. These trajectory data can be leveraged with recent advances in deep learning methods to potentially predict the trajectories of a target HDV. Based on these predictions, CAVs can react to circumvent or mitigate traffic flow oscillations and accidents. This study develops attention-based long short-term memory (LSTM) models for HDV longitudinal trajectory prediction in a mixed flow environment. The model and a few other LSTM variants are tested on the Next Generation Simulation US 101 dataset with different CAV market penetration rates (MPRs). Results illustrate that LSTM models that utilize historical trajectories from surrounding CAVs perform much better than those that ignore information even when the MPR is as low as 0.2. The attention-based LSTM models can provide more accurate multi-step longitudinal trajectory predictions. Further, grid-level average attention weight analysis is conducted and the CAVs with higher impact on the target HDV’s future trajectories are identified. © National Academy of Sciences: Transportation Research Board 2021.",Autonomous vehicles; Brain; Forecasting; Learning systems; Roads and streets; Trajectories; Vehicle to vehicle communications; 'current; Autonomous Vehicles; Driving behaviour; Market penetration; Memory modeling; Penetration rates; Road transportation; Trajectories datum; Trajectory prediction; Vehicle traffic; Long short-term memory
Scopus,"Alabdulkreem, E.; Alzahrani, J.S.; Nemri, N.; Alharbi, O.; Mohamed, A.; Marzouk, R.; Hilal, A.M.",Computational Intelligence with Wild Horse Optimization Based Object Recognition and Classification Model for Autonomous Driving Systems,,2022,,,,10.3390/app12126249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132916314&doi=10.3390%2fapp12126249&partnerID=40&md5=49ae7876db005b3de0b3388fa0276a74,"Presently, autonomous systems have gained considerable attention in several fields such as transportation, healthcare, autonomous driving, logistics, etc. It is highly needed to ensure the safe operations of the autonomous system before launching it to the general public. Since the design of a completely autonomous system is a challenging process, perception and decision-making act as vital parts. The effective detection of objects on the road under varying scenarios can considerably enhance the safety of autonomous driving. The recently developed computational intelligence (CI) and deep learning models help to effectively design the object detection algorithms for environment perception depending upon the camera system that exists in the autonomous driving systems. With this motivation, this study designed a novel computational intelligence with a wild horse optimization-based object recognition and classification (CIWHO-ORC) model for autonomous driving systems. The proposed CIWHO-ORC technique intends to effectively identify the presence of multiple static and dynamic objects such as vehicles, pedestrians, signboards, etc. Additionally, the CIWHO-ORC technique involves the design of a krill herd (KH) algorithm with a multi-scale Faster RCNN model for the detection of objects. In addition, a wild horse optimizer (WHO) with an online sequential ridge regression (OSRR) model was applied for the classification of recognized objects. The experimental analysis of the CIWHO-ORC technique is validated using benchmark datasets, and the obtained results demonstrate the promising outcome of the CIWHO-ORC technique in terms of several measures. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",autonomous systems; classification; computational intelligence; decision support; deep learning; object detection
Scopus,"Machkour, Z.; Ortiz-Arroyo, D.; Durdevic, P.",Classical and Deep Learning based Visual Servoing Systems: a Survey on State of the Art,,2022,,,,10.1007/s10846-021-01540-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121693466&doi=10.1007%2fs10846-021-01540-w&partnerID=40&md5=ed2d40ee7ab4c15b47376a4d8ac940f1,"Computer vision, together with bayesian estimation algorithms, sensors, and actuators, are used in robotics to solve a variety of critical tasks such as localization, obstacle avoidance, and navigation. Classical approaches in visual servoing systems relied on extracting features from images to control robot movements. Now, state of the art computer vision systems use deep neural networks in tasks such as object recognition, detection, segmentation, and tracking. These networks and specialized controllers play a predominant role in the design and implementation of modern visual servoing systems due to their accuracy, flexibility, and adaptability. Recent research in direct systems for visual servoing has created robotic systems capable of relying only on the information contained in the whole image. Furthermore, end-to-end systems learn the control laws during training, eliminating entirely the controller. This paper presents a comprehensive survey on the state of the art in visual servoing systems, discussing the latest classical methods not included in other surveys but emphasizing the new approaches based on deep neural networks and their applications in a broad variety of applications within robotics. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.",Computer vision; Deep neural networks; Robotics; Visual servoing; Bayesian networks; Computer vision; Controllers; Object detection; Object recognition; Robotics; Robots; Surveys; Visual servoing; Bayesian estimations; Critical tasks; Estimation algorithm; Localisation; Obstacles avoidance; On state; Sensors and actuators; Servoing systems; State of the art; Visual-servoing; Deep neural networks
Scopus,"Cervera-Uribe, A.A.; Méndez-Monroy, P.E.",U19-Net: a deep learning approach for obstacle detection in self-driving cars,,2022,,,,10.1007/s00500-022-06980-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127645556&doi=10.1007%2fs00500-022-06980-6&partnerID=40&md5=8879530c341e179e8eec3bc529cb8678,"Development of self-driving cars aims to drive safely from one point to another in a coordinated system where an on-board system should react and possibly alert drivers about the driving environments and possible collisions that may arise between drivers and obstacles. There are many deep learning approaches available for obstacle detection especially convolutional neural networks (CNNs) with improvement accuracy, and encoder–decoder networks are CNNs with a current attraction for researchers mainly because these models provide better results than classical statistical models for image segmentation and object classification tasks. This work proposes U19-Net an encoder–decoder deep model that explores the deep layers of a VGG19 model as an encoder following a symmetrical approach with an U-Net decoder designed for pixel-wise classifications. The U19-Net has end-to-end learning successfully effectiveness for the vehicle and pedestrian detection within the open-source Udacity dataset showing an IoU score of 87.08 and 78.18%, respectively. The proposed U19-Net is compared with five recent CNN networks using the AP metric, obtaining near results (less than 5%) for the faster R-CNN, one of the most commonly used networks for object detection. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Convolutional neural networks; Deep learning; Object detection; Self-driving cars; Advanced driver assistance systems; Automobile drivers; Convolution; Convolutional neural networks; Decoding; Deep learning; Image enhancement; Image segmentation; Object recognition; Obstacle detectors; Signal encoding; 'current; Convolutional neural network; Coordinated system; Deep learning; Driving environment; Encoder-decoder; Learning approach; Obstacles detection; On-board systems; Statistic modeling; Object detection
Scopus,"Liang, T.; Bao, H.; Pan, W.; Pan, F.",ALODAD: An Anchor-Free Lightweight Object Detector for Autonomous Driving,,2022,,,,10.1109/ACCESS.2022.3166923,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128324587&doi=10.1109%2fACCESS.2022.3166923&partnerID=40&md5=915b546b35faf0e1b66d190874894799,"Vision-based object detection is an essential component of autonomous driving. Because vehicles typically have limited on-board computing resources, a small-sized detection model is required. Simultaneously, high object detection accuracy and real-time inference detection speeds are required to ensure safety while driving. In this paper, an anchor-free lightweight object detector for autonomous driving called ALODAD is proposed. ALODAD incorporates an attention scheme into the lightweight neural network GhostNet and builds an anchor-free detection framework to achieve lower computational costs and provide parameters with high detection accuracy. Specifically, the lightweight backbone neural network integrates a convolutional block attention model that analyzes the valuable features from traffic scene images to generate an accurate bounding box, and then constructs feature pyramids for multi-scale object detection. The proposed method adds an intersection over union (IoU) branch to the decoupled detector to rank the vast number of candidate detections accurately. To increase the data diversity, data augmentation was used during training. Extensive experiments based on benchmarks demonstrate that the proposed method offers improved performance compared to the baseline. The proposed method can achieve an increased detection accuracy while meeting the real-time requirements of autonomous driving. The proposed method was compared with the YOLOv5 and RetinaNet models and 98.7% and 94.5% were obtained for the average precision metrics AP50 and AP75, respectively, on the BCTSDB dataset.  © 2013 IEEE.",Autonomous driving; deep learning; lightweight; object detection; Benchmarking; Deep learning; Feature extraction; Object detection; Object recognition; Anchor-free; Autonomous driving; Autonomous Vehicles; Computational modelling; Deep learning; Features extraction; Lightweight; Location awareness; Neural-networks; Objects detection; Convolution
Scopus,"Lee, K.-F.; Chen, X.-Z.; Yu, C.-W.; Chin, K.-Y.; Wang, Y.-C.; Hsiao, C.-Y.; Chen, Y.-L.",An Intelligent Driving Assistance System Based on Lightweight Deep Learning Models,,2022,,,,10.1109/ACCESS.2022.3213328,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139833241&doi=10.1109%2fACCESS.2022.3213328&partnerID=40&md5=46525957a2f98f9e913c9b4f7c5b97ba,"An intelligent driver assistance system is developed in this study, which is able to remind the drivers to turn on the head lights or wipers through situation recognition method when driving at night or on rainy days. Furthermore, the object detection results from multiple perspective views are integrated, and the surrounding object detection results are produced for collision avoidance. The system is able to alarm the drivers based on the lightweight deep learning model and the distance estimation method when surrounding vehicles are too close. Experimental results show that the proposed methods and the chosen lightweight model in our proposed system obtain reliable performance and sufficient computational efficiency under limited computing resource. In conclude, our proposed system obtains high probability to be adopted for the development of advanced driver assistance systems (ADAS). The proposed system can not only assist the driver in determining the vision ahead, but also provide an instant overview of the vehicle's surrounding conditions to enhance driving safety.  © 2013 IEEE.",Advanced driver assistance systems; distance estimation method; lightweight deep learning model; situation recognition method; vehicle detection method; Advanced driver assistance systems; Automobile drivers; Computational efficiency; Deep learning; Feature extraction; Object detection; Object recognition; Vehicles; Deep learning; Detection methods; Distance estimation; Distance estimation method; Estimation methods; Features extraction; Learning models; Lightweight deep learning model; Objects detection; Recognition methods; Situation recognition; Situation recognition method; Vehicle detection method; Vehicles detection; Computer architecture
Scopus,"Zhou, Y.; Chen, S.; Zhao, J.; Yao, R.; Xue, Y.; Saddik, A.E.",CLT-Det: Correlation Learning Based on Transformer for Detecting Dense Objects in Remote Sensing Images,,2022,,,,10.1109/TGRS.2022.3204770,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137869636&doi=10.1109%2fTGRS.2022.3204770&partnerID=40&md5=4f3b3490f71d746fed30fc95b6d0728a,"Challenges still exist in the task of object detection in remote sensing images with densely distributed objects due to large variation in scale and neglect of the relative position and correlation. To address these issues, a correlation learning detector based on transformer (CLT-Det) is proposed for detecting dense objects in remote sensing images. A transformer attention module (TAM) is designed to improve the densely packed objects' model representation ability by learning pixelwise attention with a transformer. To alleviate the semantic gap caused by the variations in scale, a feature refinement module (FRM) is proposed by improving the multiscale feature pyramid. A correlation transformer module (CTM) is proposed to extract correlation information and it encodes position information of dense objects' features on the classification branch for fully using the position information and correlation among objects. Extensive experiments compared with several state-of-art methods on two challenging remote sensing datasets, namely, dataset for object detection in aerial images (DOTA) and HRSC2016, demonstrate that the proposed CLT-Det achieves promising and competitive performance.  © 1980-2012 IEEE.",Correlation learning; deep learning; object detection; remote sensing images; vision transformer (ViT); Classification (of information); Deep learning; Feature extraction; Object detection; Object recognition; Remote sensing; Correlation; Correlation learning; Deep learning; Features extraction; Objects detection; Remote sensing images; Remote-sensing; Transformer; Vision transformer; algorithm; correlation; detection method; image analysis; satellite imagery; Semantics
Scopus,"Alo, U.R.; Nkwo, F.O.; Nweke, H.F.; Achi, I.I.; Okemiri, H.A.","Non-pharmaceutical interventions against covid-19 pandemic: Review of contact tracing and social distancing technologies, protocols, apps, security and open research directions",,2022,,,,10.3390/s22010280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121964814&doi=10.3390%2fs22010280&partnerID=40&md5=25f45575a247690b5fc2987cae899def,"The COVID-19 Pandemic has punched a devastating blow on the majority of the world’s population. Millions of people have been infected while hundreds of thousands have died of the disease throwing many families into mourning and other psychological torments. It has also crip-pled the economy of many countries of the world leading to job losses, high inflation, and dwindling Gross Domestic Product (GDP). The duo of social distancing and contact tracing are the major tech-nological-based non-pharmaceutical public health intervention strategies adopted for combating the dreaded disease. These technologies have been deployed by different countries around the world to achieve effective and efficient means of maintaining appropriate distance and tracking the transmission pattern of the diseases or identifying those at high risk of infecting others. This paper aims to synthesize the research efforts on contact tracing and social distancing to minimize the spread of COVID-19. The paper critically and comprehensively reviews contact tracing technolo-gies, protocols, and mobile applications (apps) that were recently developed and deployed against the coronavirus disease. Furthermore, the paper discusses social distancing technologies, appropriate methods to maintain distances, regulations, isolation/quarantine, and interaction strategies. In addition, the paper highlights different security/privacy vulnerabilities identified in contact tracing and social distancing technologies and solutions against these vulnerabilities. We also x-rayed the strengths and weaknesses of the various technologies concerning their application in contact tracing and social distancing. Finally, the paper proposed insightful recommendations and open research directions in contact tracing and social distancing that could assist researchers, developers, and gov-ernments in implementing new technological methods to combat the menace of COVID-19. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Artificial intelligence; Contact tracing; COVID-19; Internet of things; Review; Sensor technologies; Social distancing; Contact Tracing; COVID-19; Humans; Pandemics; Physical Distancing; SARS-CoV-2; Coronavirus; Employment; Application security; Contact tracing; COVID-19; Gross domestic products; Health interventions; Intervention strategy; Job loss; Non-pharmaceutical interventions; Sensor technologies; Social distancing; contact examination; human; pandemic; Internet of things
Scopus,"Xu, X.; Chen, X.; Wu, B.; Wang, Z.; Zhen, J.",Exploiting high-fidelity kinematic information from port surveillance videos via a YOLO-based framework,,2022,,,,10.1016/j.ocecoaman.2022.106117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126681274&doi=10.1016%2fj.ocecoaman.2022.106117&partnerID=40&md5=ab6305aab6ac72ed28595bfa9acfc97a,"Port surveillance videos provide rich and intuitive temporal and spatial information and motion information, which is conducive to path planning, obstacle avoidance and subsequent risk prediction of automated guided vehicle (AGV) in automated container terminal (ACT). Extracting high-fidelity kinematic traffic relevant data from port surveillance videos become an active yet important research task in the ACT community. To that aim, the study proposes a deep-learning based framework to exploit spatial-temporal information from port videos with steps of object detection, object moving displacement mapping and speed estimation. First, the proposed framework detects both AGVs and people in the video with you only look once (YOLO) based model (i.e., YOLO V5 model). Second, we map the AGV (and people) moving distance from videos into physical world with the help of pinhole imaging rule. Third, we estimate AGV and people moving speed in the video with the help of linear regression model. The experiment results suggested that the proposed framework obtains satisfied performance considering that the average object moving displacement error is 0.19 m and the speed error is 0.08 m/s. The research findings help ACT management departments and regulations with high-fidelity traffic relevant data to further port safety and management efficiency. © 2022 Elsevier Ltd",Automated container terminal; Automated guided vehicle; Kinematic information exploitation; Linear regression model; YOLO detection Model; Automatic guided vehicles; Automation; Containers; Kinematics; Monitoring; Motion planning; Neural networks; Port terminals; Security systems; Automated container terminals; Automated guided vehicles; Detection models; High-fidelity; Information exploitation; Kinematic information; Kinematic information exploitation; Linear regression modelling; Surveillance video; You only look once detection model; container terminal; displacement; exploitation; kinematics; mapping method; videography; Object detection
Scopus,"Lopes, A.; Souza, R.; Pedrini, H.",A survey on RGB-D datasets,,2022,,,,10.1016/j.cviu.2022.103489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134589068&doi=10.1016%2fj.cviu.2022.103489&partnerID=40&md5=5df8a4118f61fea898fba903c26ebd6e,"RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field. © 2022 The Author(s)",Computer vision; Depth datasets; Monocular depth estimation; RGB-D data; Antennas; Depth dataset; Depth Estimation; Depth information; Image datasets; Indoor/outdoor; Machine learning models; Monocular depth estimation; RGB-D data; Scene object; Three categories; Computer vision
Scopus,"Xie, Z.; Tsuzaki, M.; Lu, H.; Serikawa, S.",Improved Point-Voxel Region Convolutional Neural Network for Small Object Detection,,2022,,,,10.1117/12.2657106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146716974&doi=10.1117%2f12.2657106&partnerID=40&md5=03ca81ba5745b7c9d3b8db04ad07c53e,"With the widespread use of LiDAR sensors, 3D object detection through 3D point cloud data processing has become a research target in robotics and autonomous driving. However, the disorder and sparsity of point cloud data are the problems in traditional point cloud data processing. It is challenging to detect objects using a large amount of point cloud data. Conventional 3D object detectors have mainly grid-based methods and point-based methods. PV-RCNN proposed a framework that combines voxel-based and point-based techniques, and object features are extracted using 3D voxel CNNs. However, the resolution reduction caused by the CNN affects the localization of objects. This study aims to improve the detection accuracy of more minor things by feeding not only a single output of the voxel CNN but also multiple outputs, including high-resolution outputs, to the RPN. We came out with a new network that introduces the Multi-Scale Region Proposal Network to reduce the effect of resolution degradation. Our network has better recognition accuracy for small objects like bicycles than the original PV-RCNN. In extensive experiments, we demonstrate that our model has a 5% improvement for small things, such as cyclists training on the KITTI dataset. © 2022 SPIE.",3D detection; Autonomous Driving; Machine Learning; Convolutional neural networks; Data handling; Machine learning; Object detection; Object recognition; Optical radar; 3d detection; 3D object; 3D point cloud; Autonomous driving; Cloud data processing; Convolutional neural network; Machine-learning; Objects detection; Point cloud data; Small object detection; Autonomous vehicles
Scopus,"Marwala, T.",Rational Machines and Artificial Intelligence,,2021,,,,10.1016/B978-0-12-820676-8.09990-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127631243&doi=10.1016%2fB978-0-12-820676-8.09990-7&partnerID=40&md5=88fc1007e099eca581b16217ef3a0b7f,"Intelligent machines are populating our social, economic and political spaces. These intelligent machines are powered by Artificial Intelligence technologies such as deep learning. They are used in decision making. One element of decision making is the issue of rationality. Regulations such as the General Data Protection Regulation (GDPR) require that decisions that are made by these intelligent machines are explainable. Rational Machines and Artificial Intelligence proposes that explainable decisions are good but the explanation must be rational to prevent these decisions from being challenged. Noted author Tshilidzi Marwala studies the concept of machine rationality and compares this to the rationality bounds prescribed by Nobel Laureate Herbert Simon and rationality bounds derived from the work of Nobel Laureates Richard Thaler and Daniel Kahneman. Rational Machines and Artificial Intelligence describes why machine rationality is flexibly bounded due to advances in technology. This effectively means that optimally designed machines are more rational than human beings. Readers will also learn whether machine rationality can be quantified and identify how this can be achieved. Furthermore, the author discusses whether machine rationality is subjective. Finally, the author examines whether a population of intelligent machines collectively make more rational decisions than individual machines. Examples in biomedical engineering, social sciences and the financial sectors are used to illustrate these concepts. © 2021 Elsevier Inc. All rights reserved.",
Scopus,"Wang, Y.; Jing, Z.; Ji, Z.; Wang, L.; Zhou, G.; Gao, Q.; Zhao, W.; Dai, S.",Lane Detection Based on Two-Stage Noise Features Filtering and Clustering,,2022,,,,10.1109/JSEN.2022.3187997,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135800123&doi=10.1109%2fJSEN.2022.3187997&partnerID=40&md5=f6ce48b121d5de79f604e3ecd99a4ff3,"Lane detection is essential for autonomous vehicles, and vision-based lane detection is widely used in the field of intelligent driving cars because of its low cost. Aiming to solve the problem of false detection caused by surrounding vehicles during lane detection, a new multi-lane detection method with strong anti-interference ability is proposed. The main contribution of this work is the utilization of object detection neural network which is applied to remove vehicles from road images as much as possible. Compared with the traditional feature-based detection algorithms, this method can preserve lane line features more accurately and effectively. Firstly, binarization of global optimal threshold method is utilized to extract the basic features of lanes. In order to further remove non-lane line noises and extract lane features as clear as possible, YOLOv4-tiny object detection network is introduced to detect and filter vehicles on the road. Then the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm with denoising function is applied to cluster the different lane features. Finally, an improved RANSAC (Random Sample Consensus) method is used to perform quadratic curve fitting on the clustered features. The experimental results on dataset show that consumed time of a single frame image is only 0.119 second and the precision of the proposed method can reach 94.4%. Compared with other detection algorithms, the proposed algorithm has good real-time performance, stability and robustness in a variety of complex scenarios.  © 2001-2012 IEEE.",clustering; DBSCAN; Lane detection; YOLOv4-tiny; Clustering algorithms; Curve fitting; Feature extraction; Object recognition; Roads and streets; Signal detection; Vehicles; Autonomous Vehicles; Clusterings; Density-based spatial clustering of applications with noise; Detection algorithm; Feature clustering; Feature filtering; Lane detection; Noise features; Objects detection; YOLOv4-tiny; Object detection
Scopus,"Callegaro, D.; Levorato, M.; Restuccia, F.",SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection,,2022,,,,10.1109/WoWMoM54355.2022.00034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137075043&doi=10.1109%2fWoWMoM54355.2022.00034&partnerID=40&md5=82ec1dc7ab8f05c8c13308241fc81107,"Mobile devices such as drones and autonomous vehicles increasingly rely on object detection (OD) through deep neural networks (DNNs) to perform critical tasks such as navigation, target-tracking and surveillance, just to name a few. Due to their high complexity, the execution of these DNNs requires excessive time and energy. Low-complexity object tracking (OT) is thus used along with OD, where the latter is periodically applied to generate ""fresh""references for tracking. However, the frames processed with OD incur large delays, which does not comply with real-time applications requirements. Offloading OD to edge servers can mitigate this issue, but existing work focuses on the optimization of the offloading process in systems where the wireless channel has a very large capacity. Herein, we consider systems with constrained and erratic channel capacity, and establish parallel OT (at the mobile device) and OD (at the edge server) processes that are resilient to large OD latency. We propose Katch-Up, a novel tracking mechanism that improves the system resilience to excessive OD delay. We show that this technique greatly improves the quality of the reference available to tracking, and boosts performance up to 33%. However, while Katch-Up significantly improves performance, it also increases the computing load of the mobile device. Hence, we design SmartDet, a low-complexity controller based on deep reinforcement learning (DRL) that learns to achieve the right trade-off between resource utilization and OD performance. SmartDet takes as input highly-heterogeneous context-related information related to the current video content and the current network conditions to optimize frequency and type of OD offloading, as well as Katch-Up utilization. We extensively evaluate SmartDet on a real-world testbed composed by a JetSon Nano as mobile device and a GTX 980 Ti as edge server, connected through a Wi-Fi link, to collect several network-related traces, as well as energy measurements. We consider a state-of-the-art video dataset (ILSVRC 2015 - VID) and state-of-the-art OD models (EfficientDet 0, 2 and 4). Experimental results show that SmartDet achieves an optimal balance between tracking performance - mean Average Recall (mAR) and resource usage. With respect to a baseline with full Katch-Up usage and maximum channel usage, we still increase mAR by 4% while using 50% less of the channel and 30% power resources associated with Katch-Up. With respect to a fixed strategy using minimal resources, we increase mAR by 20% while using Katch-Up on 1/3 of the frames. © 2022 IEEE.",deep reinforcement learning; Edge Computing; object detection; object tracking; Aircraft detection; Complex networks; Deep neural networks; Economic and social effects; Large dataset; Mobile edge computing; Mobile telecommunication systems; Object detection; Object recognition; Wi-Fi; 'current; Context-Aware; Deep reinforcement learning; Edge computing; Edge server; Lower complexity; Object Tracking; Objects detection; Reinforcement learnings; State of the art; Reinforcement learning
Scopus,"Zhou, X.; Zhang, Z.; Lu, Y.; Wang, Q.; Wang, K.",SAR image recognition based on improved R-FCN,,2022,,,,10.12305/j.issn.1001-506X.2022.04.17,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128320702&doi=10.12305%2fj.issn.1001-506X.2022.04.17&partnerID=40&md5=65dc2222b5ecab18d0c542e2888d1eba,"With remarkable achievements in target recognition, deep learning provides new ideas for improving the accuracy and speed of target recognition in synthetic aperture radar (SAR) images. In this paper, region-based fully convolutional networks (R-FCN) are applied to SAR image target recognition, and good results have been achieved. In order to solve the problem of small data set and high data similarity, the R-FCN model based on transfer learning is proposed for target recognition in SAR images. The faster region convolutional neural networks (Faster R-CNN) and the R-FCN models are trained and optimized, and the experimental results are compared with the improved R-FCN model based on transfer learning proposed in this paper. The results show that the proposed method has better recognition effects and faster recognition speed for SAR images. © 2022, Editorial Office of Systems Engineering and Electronics. All right reserved.",Fully convolutional network (FCN); Machine vision; Migration study; Synthetic aperture radar(SAR); Target recognition; Automatic target recognition; Computer vision; Convolution; Convolutional neural networks; Deep learning; Image enhancement; Radar imaging; Radar target recognition; Convolutional networks; Fully convolutional network; Machine-vision; Migration study; Model-based OPC; Network models; Region-based; Synthetic aperture radar images; Target recognition; Transfer learning; Synthetic aperture radar
Scopus,"Yang, J.; Wang, C.; Jiang, B.; Song, H.; Meng, Q.","Visual Perception Enabled Industry Intelligence: State of the Art, Challenges and Prospects",,2021,,,,10.1109/TII.2020.2998818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090965411&doi=10.1109%2fTII.2020.2998818&partnerID=40&md5=c0dcda3047d66f4f81070515f701bc7c,"Visual perception refers to the process of organizing, identifying, and interpreting visual information in environmental awareness and understanding. With the rapid progress of multimedia acquisition technology, research on visual perception has been a hot topic in the academical field and industrial applications. Especially after the introduction of artificial intelligence theory, intelligent visual perception has been widely used to promote the development of industrial production towards intelligence. In this article, we review the previous research and application of visual perception in different industrial fields such as product surface defect detection, intelligent agricultural production, intelligent driving, image synthesis, and event reconstruction. The applications basically cover most of the intelligent visual perception processing technologies. Through this survey, it will provide a comprehensive reference for research on this direction. Finally, this article also summarizes the current challenges of visual perception and predicts its future development trends.  © 2005-2012 IEEE.",Artificial intelligence; industrial application; visual perception; Agricultural robots; Agriculture; Artificial intelligence; Industrial research; Surface defects; Technology transfer; Agricultural productions; Development trends; Environmental awareness; Event reconstruction; Industrial production; Processing technologies; Research and application; Visual information; Vision
Scopus,"Juyal, A.; Sharma, S.; Matta, P.",Deep Learning Methods for Object Detection in Autonomous Vehicles,,2021,,,,10.1109/ICOEI51242.2021.9452932,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113452357&doi=10.1109%2fICOEI51242.2021.9452932&partnerID=40&md5=ba401e15ce31a9ae5c0cd0ef294ee788,"The automotive industry and researchers have recently shown an interest in autonomous vehicles. In an autonomous vehicle, different technologies may be used. Radio Detection and Ranging Technology (RADAR), Light Image Detection and Ranging Technology (LiDAR) and computer vision are commonly recognized techniques. Computer vision is a method of extracting significant features from a digital image that allows the computer to perceive the features of objects and interpret the image. In recent research, it has been found that methods of deep learning can detect objects in real time. In the current review, approaches focused on CNN are discussed and their performances are compared.  © 2021 IEEE.",autonomous vehicle; CNN; Fast R-CNN; Faster R-CNN; R-CNN; SSD; YOLO; Autonomous vehicles; Computer vision; Learning systems; Object detection; Optical radar; Digital image; Image detection; Learning methods; Radio detection and ranging technologies; Real time; Recent researches; Deep learning
Scopus,"Kegyes, T.; Süle, Z.; Abonyi, J.",The Applicability of Reinforcement Learning Methods in the Development of Industry 4.0 Applications,,2021,,,,10.1155/2021/7179374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121602568&doi=10.1155%2f2021%2f7179374&partnerID=40&md5=48dd6dda6a10c61da94827ca3c4a5c2f,"Reinforcement learning (RL) methods can successfully solve complex optimization problems. Our article gives a systematic overview of major types of RL methods, their applications at the field of Industry 4.0 solutions, and it provides methodological guidelines to determine the right approach that can be fitted better to the different problems, and moreover, it can be a point of reference for R&D projects and further researches.  © 2021 Tamás Kegyes et al.",Reinforcement learning; Complex optimization problems; Methodological guidelines; Reinforcement learning method; Industry 4.0
Scopus,"Khosravian, A.; Amirkhani, A.; Masih-Tehrani, M.",Enhancing the robustness of the convolutional neural networks for traffic sign detection,,2022,,,,10.1177/09544070211042961,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113667503&doi=10.1177%2f09544070211042961&partnerID=40&md5=9cda32a282365cc1bc935be27ebb88b5,"The detection of traffic signs in clean and noise-free images has been investigated by numerous researchers; however, very few of these works have focused on noisy environments. While in the real world, for different reasons (e.g. the speed and acceleration of a vehicle and the roughness around it), the input images of the convolutional neural networks (CNNs) could be extremely noisy. Contrary to other research works, in this paper, we investigate the robustness of the deep learning models against the synthetically modeled noises in the detection of small objects. To this end, the state-of-the-art architectures of Faster-RCNN Resnet101, R-FCN Resnet101, and Faster-RCNN Inception Resnet V2 are trained by means of the Tsinghua-Tencent 100K database, and the performances of the trained models on noisy data are evaluated. After verifying the robustness of these models, different training scenarios (1 – Modeling various climatic conditions, 2 – Style randomization, and 3 – Augmix augmentation) are used to enhance the model robustness. The findings indicate that these scenarios result in up to 13.09%, 12%, and 13.61% gains in the mentioned three networks by means of the mPC metric. They also result in 11.74%, 8.89%, and 7.27% gains in the rPC metric, demonstrating that improvement in robustness does not lead to performance drop on the clean data. © IMechE 2021.",convolutional neural network; image distortions; model robustness; small object detection; Traffic sign; Convolution; Deep learning; Object detection; Traffic signs; Climatic conditions; Learning models; Model robustness; Noisy environment; State of the art; Three networks; Traffic sign detection; Training scenario; Convolutional neural networks
Scopus,"Shankar, U.",Machine and Deep Learning Algorithms and Applications Uday Shankar Shanthamallu,,2021,,,,10.2200/S01135ED1V01Y202109SPR022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122127802&doi=10.2200%2fS01135ED1V01Y202109SPR022&partnerID=40&md5=397cfc3609142ed547504d9ac41f5b56,"This book introduces basic machine learning concepts and applications for a broad audience that includes students, faculty, and industry practitioners. We begin by describing how machine learning provides capabilities to computers and embedded systems to learn from data. A typical machine learning algorithm involves training, and generally the performance of a machine learning model improves with more training data. Deep learning is a sub-area of machine learning that involves extensive use of layers of artificial neural networks typically trained on massive amounts of data. Machine and deep learning methods are often used in contemporary data science tasks to address the growing data sets and detect, cluster, and classify data patterns. Although machine learning commercial interest has grown relatively recently, the roots of machine learning go back to decades ago. We note that nearly all organizations, including industry, government, defense, and health, are using machine learning to address a variety of needs and applications.The machine learning paradigms presented can be broadly divided into the following three categories: supervised learning, unsupervised learning, and semi-supervised learning. Supervised learning algorithms focus on learning a mapping function, and they are trained with supervision on labeled data. Supervised learning is further sub-divided into classification and regression algorithms. Unsupervised learning typically does not have access to ground truth, and often the goal is to learn or uncover the hidden pattern in the data. Through semi-supervised learning, one can effectively utilize a large volume of unlabeled data and a limited amount of labeled data to improve machine learning model performances. Deep learning and neural networks are also covered in this book. Deep neural networks have attracted a lot of interest during the last ten years due to the availability of graphics processing units (GPU) computational power, big data, and new software platforms. They have strong capabilities in terms of learning complex mapping functions for different types of data. We organize the book as follows. The book starts by introducing concepts in supervised, unsupervised, and semi-supervised learning. Several algorithms and their inner workings are presented within these three categories. We then continue with a brief introduction to artificial neural network algorithms and their properties. In addition, we cover an array of applications and provide extensive bibliography. The book ends with a summary of the key machine learning concepts. Table of Contents:Preface / Acknowledgments / Introduction to Machine Learning / Supervised Learning / Unsupervised Learning / Semi-Supervised Learning / Neural Networks and Deep Learning / Machine and Deep Learning Applications / Conclusion and Future Directions / Bibliography / Authors' Biographies  Copyright © 2021 by Morgan & Claypool.",artificial intelligence; big data; deep learning; Internet of things; machine learning; neural networks; signal processing; supervised learning; unsupervised learning; Big data; Classification (of information); Computer graphics; Deep neural networks; Embedded systems; Graphics processing unit; Learning algorithms; Mapping; Multilayer neural networks; Program processors; Signal processing; Supervised learning; Deep learning; Embedded-system; Labeled data; Learn+; Machine learning algorithms; Machine learning models; Mapping functions; Neural-networks; Signal-processing; Three categories; Internet of things
Scopus,"Choi, K.; Oh, B.-S.; Yu, S.",Memory access minimization for mean-shift tracking in mobile devices,,2021,,,,10.1007/s11042-020-09364-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088828559&doi=10.1007%2fs11042-020-09364-w&partnerID=40&md5=341c589bcad54222361ca67ea384ae46,"Due to the development of artificial intelligence and computer vision technology, many autonomous drones have been studied. However, computer vision technology requires high performance CPU due to its high complexity, and battery consumption is so high that drones are constrained to fly for a long time. Therefore, low-power mobile devices require tracking algorithms that minimize battery consumption. In this paper, we propose a mean-shift based tracking algorithm that minimizes memory access to reduce battery consumption. To accomplish this, we minimize the number of memory accesses by using an algorithm that divides the direction of the mean-shift vector into eight, and calculates the sum of the density maps only for the new area without calculating the sum of the density maps for the already calculated area. It is possible to increase the calculation efficiency by lowering the memory access cost. Experimental results show that the proposed method is more efficient than the existing method. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Mean shift; Mobile device; Object tracking; Artificial intelligence; Computer vision; Drones; Electric batteries; Tracking (position); Battery consumption; Calculation efficiency; Computer vision technology; High complexity; Mean shift tracking; Mean shift vector; Memory access; Tracking algorithm; Memory architecture
Scopus,"Kwak, D.-H.; Son, G.-J.; Park, M.-K.; Kim, Y.-D.",Rapid foreign object detection system on seaweed using vnir hyperspectral imaging,,2021,,,,10.3390/s21165279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111777131&doi=10.3390%2fs21165279&partnerID=40&md5=66a586e80cce9c1dceb31661af537a33,"The consumption of seaweed is increasing year by year worldwide. Therefore, the foreign object inspection of seaweed is becoming increasingly important. Seaweed is mixed with various materials such as laver and sargassum fusiforme. So it has various colors even in the same seaweed. In addition, the surface is uneven and greasy, causing diffuse reflections frequently. For these reasons, it is difficult to detect foreign objects in seaweed, so the accuracy of conventional foreign object detectors used in real manufacturing sites is less than 80%. Supporting real‐time inspection should also be considered when inspecting foreign objects. Since seaweed requires mass production, rapid inspection is essential. However, hyperspectral imaging techniques are generally not suitable for high‐speed inspection. In this study, we overcome this limitation by using dimensionality reduction and using simplified operations. For accuracy improvement, the proposed algorithm is carried out in 2 stages. Firstly, the subtraction method is used to clearly distinguish seaweed and conveyor belts, and also detect some relatively easy to detect foreign objects. Secondly, a standardization inspection is performed based on the result of the subtraction method. During this process, the proposed scheme adopts simplified and burdenless calculations such as subtraction, division, and one‐by‐one matching, which achieves both accuracy and low latency performance. In the experiment to evaluate the performance, 60 normal seaweeds and 60 seaweeds containing foreign objects were used, and the accuracy of the proposed algorithm is 95%. Finally, by implementing the proposed algorithm as a foreign object detection platform, it was confirmed that real‐time operation in rapid inspection was possible, and the possibility of deployment in real manufacturing sites was confirmed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Foreign object detection; Hyperspectral imaging; Seaweed; Signal processing; Spectroscopy; Visible and near‐infrared; Algorithms; Foreign Bodies; Hyperspectral Imaging; Seaweed; Vegetables; Belt conveyors; Dimensionality reduction; Hyperspectral imaging; Inspection; Object recognition; Seaweed; Spectroscopy; Accuracy Improvement; Conveyor belts; Diffuse reflection; Foreign object; Manufacturing sites; Mass production; Sargassum fusiforme; Subtraction method; algorithm; foreign body; seaweed; vegetable; Object detection
Scopus,"Zhang, J.; Zhang, L.; Liu, T.; Wang, Y.",YOLSO: You Only Look Small Object,,2021,,,,10.1016/j.jvcir.2021.103348,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117947649&doi=10.1016%2fj.jvcir.2021.103348&partnerID=40&md5=a3b3c1c0e7af637130e7cd0c657d2674,"Small object detection is challenging and far from satisfactory. Most general object detectors suffer from two critical issues with small objects: (1) Feature extractor based on classification network cannot express the characteristics of small objects reasonably due to insufficient appearance information of targets and a large amount of background interference around them. (2) The detector requires a much higher location accuracy for small objects than for general objects. This paper proposes an effective and efficient small object detector YOLSO to address the above problems. For feature representation, we analyze the drawbacks in previous backbones and present a Half-Space Shortcut(HSSC) module to build a background-aware backbone. Furthermore, a coarse-to-fine Feature Pyramid Enhancement(FPE) module is introduced for layer-wise aggregation at a granular level to enhance the semantic discriminability. For loss function, we propose an exponential L1 loss to promote the convergence of regression, and a focal IOU loss to focus on prime samples with high classification confidence and high IOU. Both of them significantly improves the location accuracy of small objects. The proposed YOLSO sets state-of-the-art results on two typical small object datasets, MOCOD and VeDAI, at a speed of over 200 FPS. In the meantime, it also outperforms the baseline YOLOv3 by a wide margin on the common COCO dataset. © 2021 Elsevier Inc.",Accurate location; Background-aware; Granular feature aggregation; High speed; Small object detection; Classification (of information); Feature extraction; Geometry; Location; Object recognition; Semantics; Accurate location; Background-aware; Detector suffers; Feature aggregation; Granular feature aggregation; High Speed; Location accuracy; Object detectors; Small object detection; Small objects; Object detection
Scopus,"Bhowmick, A.; Saharia, S.; Hazarika, S.M.",Non-parametric scene parsing: Label transfer methods and datasets,,2022,,,,10.1016/j.cviu.2022.103418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128685236&doi=10.1016%2fj.cviu.2022.103418&partnerID=40&md5=85e79a08d2a0f11e4dd45421b07c41a4,"Scene parsing is the problem of densely labeling every pixel in an image with a meaningful class label. Driven by powerful methods, remarkable progress has been achieved in scene parsing over a short period of time. With growing data, non-parametric scene parsing or label transfer approach has emerged as an exciting and rapidly growing research area within Computer Vision. This paper constitutes a first survey examining label transfer methods through the lens of non-parametric, data-driven philosophy. We provide insights on non-parametric system design and its working stages, i.e. algorithmic components such as scene retrieval, scene correspondence, contextual smoothing, etc. We propose a synthetic categorization of all the major existing methods, discuss the necessary background, the design choices, followed by an overview of the shortcomings and challenges for a better understanding of label transfer. In addition, we introduce the existing standard benchmark datasets, the evaluation metrics, and the comparisons of model-based and data-driven methods. Finally, we provide our recommendations and discuss the current challenges and promising research directions in the field. © 2022 Elsevier Inc.",Label transfer; Non-parametric; Scene parsing; Class labels; Data driven; Label transfer; Labelings; Nonparametrics; Research areas; Scene parsing; Short periods; Through the lens; Transfer method; Search engines
Scopus,"Camara, F.; Bellotto, N.; Cosar, S.; Nathanael, D.; Althoff, M.; Wu, J.; Ruenz, J.; Dietrich, A.; Fox, C.W.","Pedestrian Models for Autonomous Driving Part I: Low-Level Models, from Sensing to Tracking",,2021,,,,10.1109/TITS.2020.3006768,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116912007&doi=10.1109%2fTITS.2020.3006768&partnerID=40&md5=ef52a5dd15686c425173ebb9bfe59ae4,"Autonomous vehicles (AVs) must share space with pedestrians, both in carriageway cases such as cars at pedestrian crossings and off-carriageway cases such as delivery vehicles navigating through crowds on pedestrianized high-streets. Unlike static obstacles, pedestrians are active agents with complex, interactive motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behavior as well as detecting and tracking them. This narrative review article is Part I of a pair, together surveying the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low-level image detection to high-level psychology models, from the perspective of an AV designer. This self-contained Part I covers the lower levels of this stack, from sensing, through detection and recognition, up to tracking of pedestrians. Technologies at these levels are found to be mature and available as foundations for use in high-level systems, such as behavior modelling, prediction and interaction control.  © 2000-2011 IEEE.",autonomous vehicles; datasets; detection; eHMI; game-theoretic models; microscopic and macroscopic behavior models; pedestrian interaction; pedestrians; Review; sensing; signaling models; survey; tracking; trajectory prediction; Autonomous vehicles; Game theory; Pedestrian safety; Autonomous Vehicles; Behaviour models; Dataset; Detection; EHMI; Game-theoretic model; Macroscopic behaviors; Microscopic and macroscopic behavior model; Microscopic behavior; Pedestrian; Pedestrian interaction; Signaling model; Tracking; Trajectory prediction; Surveys
Scopus,"Fernandes, D.; Silva, A.; Névoa, R.; Simões, C.; Gonzalez, D.; Guevara, M.; Novais, P.; Monteiro, J.; Melo-Pinto, P.",Point-cloud based 3D object detection and classification methods for self-driving applications: A survey and taxonomy,,2021,,,,10.1016/j.inffus.2020.11.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097174674&doi=10.1016%2fj.inffus.2020.11.002&partnerID=40&md5=2edc0aee0430be2aefa746c09cdd7ade,"Autonomous vehicles are becoming central for the future of mobility, supported by advances in deep learning techniques. The performance of aself-driving system is highly dependent on the quality of the perception task. Developments in sensor technologies have led to an increased availability of 3D scanners such as LiDAR, allowing for a more accurate representation of the vehicle's surroundings, leading to safer systems. The rapid development and consequent rise of research studies around self-driving systems since early 2010, resulted in a tremendous increase in the number and novelty of object detection methods. After the first wave of works that essentially tried to expand known techniques from object detection in images, more recently there has been a notable development in newer and more adapted to LiDAR data works. This paper addresses the existing literature on object detection using LiDAR data within the scope of self-driving and brings a systematic way for analysing it. Unlike general object detection surveys, we will focus on point-cloud data, which presents specific challenges, notably its high-dimensional and sparse nature. This work introduces a common object detection pipeline and taxonomy to facilitate a thorough comparison between different techniques and, departing from it, this work will critically examine the representation of data (critical for complexity reduction), feature extraction and finally the object detection models. A comparison between performance results of the different models is included, alongside with some future research challenges. © 2020 Elsevier B.V.",3D object detection models; Autonomous vehicles; Computer vision; Deep learning; LiDAR; Perception; Deep learning; Feature extraction; Image classification; Object recognition; Optical radar; Surveys; Taxonomies; Classification methods; Complexity reduction; High-dimensional; Learning techniques; Object detection method; Point cloud data; Research challenges; Sensor technologies; Object detection
Scopus,"You, S.; Ji, Y.; Liu, S.; Mei, C.; Yao, X.; Feng, Y.",A Thermal Infrared Pedestrian-Detection Method for Edge Computing Devices,,2022,,,,10.3390/s22176710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137570075&doi=10.3390%2fs22176710&partnerID=40&md5=edff602f4c4b7748cc69f8fbb43f60a8,"The thermal imaging pedestrian-detection system has excellent performance in different lighting scenarios, but there are problems regarding weak texture, object occlusion, and small objects. Meanwhile, large high-performance models have higher latency on edge devices with limited computing power. To solve the above problems, in this paper, we propose a real-time thermal imaging pedestrian-detection method for edge computing devices. Firstly, we utilize multi-scale mosaic data augmentation to enhance the diversity and texture of objects, which alleviates the impact of complex environments. Then, the parameter-free attention mechanism is introduced into the network to enhance features, which barely increases the computing cost of the network. Finally, we accelerate multi-channel video detection through quantization and multi-threading techniques on edge computing devices. Additionally, we create a high-quality thermal infrared dataset to facilitate the research. The comparative experiments on the self-built dataset, YDTIP, and three public datasets, with other methods show that our method also has certain advantages. © 2022 by the authors.",attention mechanism; data augmentation; pedestrian detection; real-time; thermal infrared images; Infrared imaging; Infrared radiation; Textures; Attention mechanisms; Computing devices; Data augmentation; Detection methods; Edge computing; Pedestrian detection; Real- time; Thermal infrared images; Thermal-imaging; Thermal-infrared; article; attention; controlled study; human; pedestrian; quantization; thermography; Edge computing
Scopus,"Billah, A.Md.; Faruque, I.A.",Bioinspired visuomotor feedback in a multiagent group/swarm context,,2021,,,,10.1109/TRO.2020.3033703,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098758879&doi=10.1109%2fTRO.2020.3033703&partnerID=40&md5=3bb3ce1bb925cb309fd672beafae52b1,"Practical aerial swarm applications are limited by the need to provide fast, robust feedback. To consider how biological agents incorporate visual feedback in multiagent environments, this study extends a well-developed model of bioinspired visual feedback for individual agent to feedback in multiagent dynamic environments. A multiagent visual model is developed and analyzed for an individual agent operating in a group of other agents. The theoretical model indicates a stable equilibrium trajectory within the neighboring agents. When a minority of agents conducting bioinspired visual feedback are introduced into a planar swarm simulation, results show robustness of agents to perturbations from reference condition. The bioinspired feedback rule is implemented experimentally using ground microbots for two cases identified by a theoretical observability condition. The experimental performance of the single agent in an established swarm is consistent with the theoretical stability and observability predictions. The results indicate insect visuomotor response can help participate in established swarms, and indicate the importance of optic flow input in maintaining group motion. © 2020 IEEE.",Aerospace control; Autonomous vehicle navigation; Biologically-inspired robots; Feedback; Multi-agent systems; Optical control; Swarms; Visual servoing; Agricultural robots; Antennas; Multi agent systems; Observability; Visual communication; Biological agents; Developed model; Dynamic environments; Individual agent; Multi-agent environment; Reference condition; SWARM simulation; Theoretical modeling; Feedback
Scopus,"Mirugwe, A.; Nyirenda, J.; Dufourq, E.",Automating Bird Detection Based on Webcam Captured Images using Deep Learning,,2022,,,,10.29007/9fr5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135805420&doi=10.29007%2f9fr5&partnerID=40&md5=caa3fd2bfcc53761a656ea008cdf028a,"One of the most challenging problems faced by ecologists and other biological researchers today is to analyze the massive amounts of data being collected by advanced monitoring systems like camera traps, wireless sensor networks, high-frequency radio trackers, global positioning systems, and satellite tracking systems being used today. It has become expensive, laborious, and time-consuming to analyze this huge data using manual and traditional statistical techniques. Recent developments in the deep learning field are showing promising results towards automating the analysis of these extremely large datasets. The primary objective of this study was to test the capabilities of the state-of-the-art deep learning architectures to detect birds in the webcam captured images. A total of 10592 images were collected for this study from the Cornell Lab of Ornithology live stream feeds situated in six unique locations in United States, Ecuador, New Zealand, and Panama. To achieve the main objective of the study, we studied and evaluated two convolutional neural network object detection meta-architectures, single-shot detector (SSD) and Faster R-CNN in combination with MobileNet-V2, ResNet50, ResNet101, ResNet152, and Inception ResNet-V2 feature extractors. Through transfer learning, all the models were initialized using weights pre-trained on the MS COCO (Microsoft Common Objects in Context) dataset provided by TensorFlow 2 object detection API. The Faster R-CNN model coupled with ResNet152 outperformed all other models with a mean average precision of 92.3%. However, the SSD model with the MobileNet-V2 feature extraction network achieved the lowest inference time (110ms) and the smallest memory capacity (30.5MB) compared to its counterparts. The outstanding results achieved in this study confirm that deep learning-based algorithms are capable of detecting birds of different sizes in different environments and the best model could potentially help ecologists in monitoring and identifying birds from other species. © 2022, EasyChair. All rights reserved.",Faster R-CNN; ResNets Deep Learning; Single-Shot Detector; Transfer Learning; Birds; Convolution; Convolutional neural networks; Deep learning; Feature extraction; Large dataset; Multilayer neural networks; Network architecture; Transfer learning; Wireless sensor networks; Advanced monitoring; Bird detection; Fast R-CNN; Monitoring system; Objects detection; Resnet deep learning; Single-shot; Single-shot detector; Transfer learning; WebCams; Object detection
Scopus,"Vierhauser, M.; Bayley, S.; Wyngaard, J.; Xiong, W.; Cheng, J.; Huseman, J.; Lutz, R.; Cleland-Huang, J.",Interlocking Safety Cases for Unmanned Autonomous Systems in Shared Airspaces,,2021,,,,10.1109/TSE.2019.2907595,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106064142&doi=10.1109%2fTSE.2019.2907595&partnerID=40&md5=cc7a8990a00f1123df18a86b3475479e,"The growing adoption of unmanned aerial vehicles (UAVs) for tasks such as eCommerce, aerial surveillance, and environmental monitoring introduces the need for new safety mechanisms in an increasingly cluttered airspace. In our work we thus emphasize safety issues that emerge at the intersection of infrastructures responsible for controlling the airspace, and the diverse UAVs operating in their space. We build on safety assurance cases (SAC) - a state-of-the-art solution for reasoning about safety - and propose a novel approach based on interlocking SACs. The infrastructure safety case (ISAC) specifies assumptions upon UAV behavior, while each UAV demonstrates compliance to the ISAC by presenting its own (pluggable) safety case (pSAC) which connects to the ISAC through a set of interlock points. To collect information on each UAV we enforce a 'trust but monitor' policy, supported by runtime monitoring and an underlying reputation model. We evaluate our approach in three ways: first by developing ISACs for two UAV infrastructures, second by running simulations to evaluate end-to-end effectiveness, and finally via an outdoor field-study with physical UAVs. The results show that interlocking SACs can be effective for identifying, specifying, and monitoring safety-related constraints upon UAVs flying in a controlled airspace.  © 1976-2012 IEEE.",monitoring; safety assurance cases; UAV; unmanned autonomous systems; Antennas; Safety engineering; Aerial surveillance; Autonomous systems; Environmental Monitoring; Reputation modeling; Running simulations; Runtime Monitoring; Safety mechanisms; State of the art; Unmanned aerial vehicles (UAV)
Scopus,"Ristea, N.-C.; Anghel, A.; Ionescu, R.T.",Estimating the Magnitude and Phase of Automotive Radar Signals under Multiple Interference Sources with Fully Convolutional Networks,,2021,,,,10.1109/ACCESS.2021.3128151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534731&doi=10.1109%2fACCESS.2021.3128151&partnerID=40&md5=2f70ad174cf6416c9404b6aba762b54d,"Radar sensors are gradually becoming a wide-spread equipment for road vehicles, playing a crucial role in autonomous driving and road safety. The broad adoption of radar sensors increases the chance of interference among sensors from different vehicles, generating corrupted range profiles and range-Doppler maps. In order to extract distance and velocity of multiple targets from range-Doppler maps, the interference affecting each range profile needs to be mitigated. In this paper, we propose a fully convolutional neural network for automotive radar interference mitigation. In order to train our network in a real-world scenario, we introduce a new data set of realistic automotive radar signals with multiple targets and multiple interferers. To our knowledge, we are the first to apply weight pruning in the automotive radar domain, obtaining superior results compared to the widely-used dropout. While most previous works successfully estimated the magnitude of automotive radar signals, we propose a deep learning model that can accurately estimate the phase. For instance, our novel approach reduces the phase estimation error with respect to the commonly-adopted zeroing technique by half, from 12.55 degrees to 6.58 degrees. Considering the lack of databases for automotive radar interference mitigation, we release as open source our large-scale data set that closely replicates the real-world automotive scenario for multiple interference cases, allowing others to objectively compare their future work in this domain. Our data set is available for download at: http://github.com/ristea/arim-v2.  © 2013 IEEE.",automotive radar; Autonomous driving; deep learning; fully convolutional networks; interference mitigation; phase estimation; Automotive radar; Autonomous vehicles; Convolutional neural networks; Deep learning; Motor transportation; Radar interference; Radar measurement; Roads and streets; Automotive radar; Autonomous driving; Convolutional networks; Deep learning; Fully convolutional network; Interference mitigation; Phase-estimation; Radar sensors; Radar signals; Range-profiles; Convolution
Scopus,"Taghizadeh, M.; Chalechale, A.",A comprehensive and systematic review on classical and deep learning based region proposal algorithms,,2022,,,,10.1016/j.eswa.2021.116105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118832919&doi=10.1016%2fj.eswa.2021.116105&partnerID=40&md5=7dd9eb4bbf0a1a5bd0157f4b1ecec30f,"Development of region proposal algorithms has rapidly become one of the most critical research areas over recent years. The perfect accuracy of region-based recognition techniques has led to the use of proposal algorithms as an imperative core in various recognition problems. The main purpose of these algorithms is to extract effective regions of an image with an appropriate number that will reduce the search space and increase detection accuracy. The early development of these algorithms was based on a set of hand-crafted features. Recently, with advances in deep learning techniques, they have been widely and successfully applied to the region proposals. This paper reviews region proposal algorithms, theory, and evaluation metrics and also addresses the existing challenges. In addition, we present a classification for generating proposals, including classical and advanced methods based on hand-crafted features and deep learning, respectively. Both categories are described in details, and an extensive review of recent works is presented. The proposal improvement methods, including ranking algorithms, are also described. In total, more than 60 different algorithms have been studied and classified, and we also point out several applications based on region proposals. © 2021 Elsevier Ltd",Deep learning; Ranking; Region proposal; Region proposal network; Segmentation; Image segmentation; Critical researches; Deep learning; Proposal algorithm; Ranking; Region proposal; Region proposal network; Region-based; Research areas; Segmentation; Systematic Review; Deep learning
Scopus,"Reyes‐muñoz, A.; Guerrero‐ibáñez, J.",Vulnerable Road Users and Connected Autonomous Vehicles Interaction: A Survey,,2022,,,,10.3390/s22124614,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132119538&doi=10.3390%2fs22124614&partnerID=40&md5=549b0d5f8003ccf9a108204cd61a3be6,"There is a group of users within the vehicular traffic ecosystem known as Vulnerable Road Users (VRUs). VRUs include pedestrians, cyclists, motorcyclists, among others. On the other hand, connected autonomous vehicles (CAVs) are a set of technologies that combines, on the one hand, communication technologies to stay always ubiquitous connected, and on the other hand, automated technologies to assist or replace the human driver during the driving process. Autonomous vehicles are being visualized as a viable alternative to solve road accidents providing a general safe environment for all the users on the road specifically to the most vulnerable. One of the problems facing autonomous vehicles is to generate mechanisms that facilitate their integration not only within the mobility environment, but also into the road society in a safe and efficient way. In this paper, we analyze and discuss how this integration can take place, reviewing the work that has been developed in recent years in each of the stages of the vehicle‐human interaction, analyzing the challenges of vulnerable users and proposing solutions that contribute to solving these challenges. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","automated vehicles; connected vehicles; deep learning; machine learning; pedestrians; Accidents, Traffic; Autonomous Vehicles; Bicycling; Ecosystem; Humans; Pedestrians; Accidents; Deep learning; Motor transportation; Pedestrian safety; Roads and streets; Surveys; Vehicle to vehicle communications; Automated technology; Automated vehicles; Autonomous Vehicles; Communicationtechnology; Connected vehicle; Deep learning; Machine-learning; Pedestrian; Road users; Vehicle interactions; cycling; ecosystem; human; pedestrian; prevention and control; traffic accident; Autonomous vehicles"
Scopus,"Song, Y.; Xie, Z.; Wang, X.; Zou, Y.",MS-YOLO: Object Detection Based on YOLOv5 Optimized Fusion Millimeter-Wave Radar and Machine Vision,,2022,,,,10.1109/JSEN.2022.3167251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128676237&doi=10.1109%2fJSEN.2022.3167251&partnerID=40&md5=2956271dac5e77a63786f3278c3786c8,"Millimeter-wave radar and machine vision are both important means for intelligent vehicles to perceive the surrounding environment. Aiming at the problem of multi-sensor fusion, this paper proposes the object detection method of millimeter-wave radar and vision fusion. Radar and camera complement each other, and radar data fusion in machine vision network can effectively reduce the rate of missed detection under insufficient light conditions, and improve the accuracy of remote small object detection. The radar information is processed by mapping transformation neural network to obtain the mask map, so that radar information and visual information in the same scale. A multi-data source deep learning object detection network (MS-YOLO) based on millimeter-wave radar and vision fusion was proposed. Homemade datasets were used for training and testing. This maximized the use of sensor information and improved the detection accuracy under the premise of ensuring the detection speed. Compared with the original YOLOv5 (the fifth version of the You Only Look Once) network, the results show that the MS-YOLO network meets the accuracy requirements better. Among the models, the large model of MS-YOLO has the highest accuracy with an mAP reaching 0.888. The small model of MS-YOLO has good accuracy and speed, and the mAP reaches 0.841 while maintaining a high frame rate of 65 fps.  © 2001-2012 IEEE.",deep learning; MS-YOLO; multi-sensor fusion; object detection; Computer vision; Deep learning; Millimeter waves; Object recognition; Radar equipment; Sensor data fusion; Tracking radar; Deep learning; Intelligent sensors; Machine-vision; Millimeter-wave radar; Millimetre-wave radar; MS-YOLO; Multi-sensor fusion; Radar detection; Sensor phenomenon and characterizations; Wave machine; Object detection
Scopus,"Akshatha, K.R.; Karunakar, A.K.; Satish Shenoy, B.",Human Classification in Aerial Images Using Convolutional Neural Networks,,2022,,,,10.1007/978-981-16-7996-4_39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125288277&doi=10.1007%2f978-981-16-7996-4_39&partnerID=40&md5=8e6548230c18e100a4fb1ee7ba14e4b7,"Automatic detection of people in aerial images has potential applications in traffic monitoring, surveillance, human behavior analysis, etc. However, developing an algorithm for detection of human locations in aerial images is challenging because of the small target size, cluttered background, and varying appearance of humans. Deep learning-based object detections frameworks internally use the standard convolutional neural network (CNN) based classifiers for feature extraction and classification. Though these pre-trained classifiers perform image classification tasks with very good accuracy, they are computationally complex and hence require huge computation time. In this work, we custom-designed CNN-based classifiers to perform the human classification in aerial images and compared the performance with the standard VGG-16 based human classifier. Custom-designed classifier with fewer number of layers achieved a reduced computation time while maintaining good accuracy. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Aerial images; Convolutional neural network; Human classification; Antennas; Behavioral research; Classification (of information); Convolutional neural networks; Deep learning; Feature extraction; Image classification; Object detection; Aerial images; Automatic Detection; Computation time; Convolutional neural network; Human behavior analysis; Human classification; Network-based; Small targets; Target size; Traffic monitoring; Convolution
Scopus,"Markert, T.; Matich, S.; Neykov, D.; Muenig, M.; Theissler, A.; Atzmueller, M.",Visual Detection of Tiny and Transparent Objects for Autonomous Robotic Pick-and-Place Operations,,2022,,,,10.1109/ETFA52439.2022.9921555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141395344&doi=10.1109%2fETFA52439.2022.9921555&partnerID=40&md5=024a91785ef7e5759328a7774e96cf35,"For the manufacturing of miniature force/torque sensors, extreme accuracy is required due to the tiny size of the strain gauges inside the sensors (2×2.5 mm). The current method of manually assembling them by hand is difficult, time-intensive, and error-prone. To improve this, a system to pick up the tiny objects from a plate and place them on elementary cells is being devised using a 6-axis robot arm with custom end-effector and a camera with magnification lens. This paper focuses on the perception module by evaluating methods for detecting tiny and transparent objects and obtaining spatial information from 2D images. Additionally, it considers aspects of the camera-to-robot calibration process, which are necessary to transfer the accuracy of image recognition into the real world. An approach using image segmentation and blob detection is taken, precluding the need for machine learning models. This is possible due to the superb image quality achieved by the sufficiently advanced camera and lighting setup. As a conclusion, we propose a perception module, which is capable of pinpointing strain gauge positions within ±0.1 mm and can also recognize different types of components based on physical dimensions. Our end-to-end approach for automatic pick-and-place operations integrates the perception module, camera-to-robot calibration, and a last-minute correction routine, which ultimately leads to an overall positioning accuracy of ± 0.3 mm.  © 2022 IEEE.",factory automation; machine vision; micro assembly; pick-and-place systems; transparent object detection; Computer vision; End effectors; Factory automation; Image recognition; Image segmentation; Intelligent robots; Object detection; Strain gages; Machine-vision; Microassemblies; Objects detection; Pick and place; Pick-and-place system; Robot calibration; Strain-gages; Transparent object detection; Transparent objects; Visual detection; Cameras
Scopus,"Patel, H.; Prajapati, K.; Sarvaiya, A.; Upla, K.; Raja, K.; Ramachandra, R.; Busch, C.",Depthwise Convolution For Compact Object Detector In nighttime Images,,2022,,,,10.1109/CVPRW56347.2022.00053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137754353&doi=10.1109%2fCVPRW56347.2022.00053&partnerID=40&md5=01af5f1aeb1506933b8e4399609f7afd,"Despite thermal imaging primarily used for nighttime surveillance, uniform temperature of object and background makes it difficult to acquire details in the scene being observed and thereby object detection. Further, thermal images collected over long distances degrade the spatial resolution of the acquired objects and so do the moving objects leading to noisy features. We present a computationally efficient object detection approach using Depthwise Deep Convolutional Neural Network (DDCNN) for detecting and classifying objects in nighttime images under low resolution. The Depthwise Convolution (DC) employed in the proposed approach minimises the network's computational complexity resulting in the lowest number of training parameters (i.e., 3M) as compared to the other existing state-of-the-art methods such as FRCNN (52M), SSD (24M) and YOLO-v3 (61M) parameters. Further, by introducing novel Tversky and Intersection over Union (IoU) loss functions into the compact architectural design, we improve nighttime object detection accuracy. The validity of the proposed model is assessed on numerous datasets such as FLIR, KAIST, MS, and our internal dataset having multiple objects in each image. The experimental results from the proposed method indicate both quantitative and qualitative improvements over the recent state-of-the-art methods for nighttime imaging. The proposed approach achieves a mean Average Precision (mAP) of 52.39% and a highest individual object detection accuracy of 72.70% accuracy for cars in nigh-time situations suggesting applications in real-time use cases. © 2022 IEEE.",Computer vision; Convolution; Convolutional neural networks; Deep neural networks; Infrared imaging; Object recognition; Compact objects; Detection accuracy; Moving objects; Object detectors; Objects detection; Spatial resolution; State-of-the-art methods; Thermal images; Thermal-imaging; Uniform temperature; Object detection
Scopus,"Xiao, W.; Liu, M.; Chen, X.",Research Status and Development Trend of Underground Intelligent Load-Haul-Dump Vehicle—A Comprehensive Review,,2022,,,,10.3390/app12189290,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138600675&doi=10.3390%2fapp12189290&partnerID=40&md5=9d249a626542228e7b583999c7d7d426,"The underground intelligent load-haul-dump vehicle (LHD) is a product of the deep integration of traditional LHD with information network technology, automatic controlling and artificial intelligence technology. It gathers the functions of environmental perception, autonomous driving and fault diagnosis in one machine and exhibits higher safety and greater efficiency than traditional LHD. Hence, it is a particularly important piece of underground mining equipment for building green, safe and smart mines. Taking the studies about intelligent LHD collected by CNKI and WOS databases from 1980 to 2022 as a sample data source, employing Citespace visual analysis software for key feature extraction from the documents, statistical analysis was conducted to clarify the current research progress and the frontier topics of the intelligent LHD academia in the past 40 years, in relation to the future development trends. The development history and application status of underground intelligent LHD was expounded in this article, summarizing the research status at home and abroad from four aspects: ore heap perception and modeling technology, trajectory planning method of bucket shoveling, autonomous navigation technology, real-time monitoring and intelligent fault diagnosis technology. The demerits and merits of the technologies were reviewed as well, with future developing and researching trends of the underground intelligent LHD concluded. © 2022 by the authors.",autonomous navigation; CiteSpace; fault diagnosis; heap perception; real-time monitoring; trajectory planning; underground intelligent LHD
Scopus,"Jabłoński, P.; Iwaniec, J.; Zabierowski, W.","Comparison of Pedestrian Detectors for LiDAR Sensor Trained on Custom Synthetic, Real and Mixed Datasets",,2022,,,,10.3390/s22187014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138405256&doi=10.3390%2fs22187014&partnerID=40&md5=032ec2535e663829b5803c9d33805eaa,"Deep learning algorithms for object detection used in autonomous vehicles require a huge amount of labeled data. Data collecting and labeling is time consuming and, most importantly, in most cases useful only for a single specific sensor application. Therefore, in the course of the research which is presented in this paper, the LiDAR pedestrian detection algorithm was trained on synthetically generated data and mixed (real and synthetic) datasets. The road environment was simulated with the application of the 3D rendering Carla engine, while the data for analysis were obtained from the LiDAR sensor model. In the proposed approach, the data generated by the simulator are automatically labeled, reshaped into range images and used as training data for a deep learning algorithm. Real data from Waymo open dataset are used to validate the performance of detectors trained on synthetic, real and mixed datasets. YOLOv4 neural network architecture is used for pedestrian detection from the LiDAR data. The goal of this paper is to verify if the synthetically generated data can improve the detector’s performance. Presented results prove that the YOLOv4 model trained on a custom mixed dataset achieved an increase in precision and recall of a few percent, giving an F1-score of 0.84. © 2022 by the authors.","ADAS; artificial data; Carla simulator; deep learning; LiDAR; object detection; Waymo open dataset; YOLOv4; Algorithms; Humans; Neural Networks, Computer; Pedestrians; Deep learning; Learning algorithms; Network architecture; Object recognition; Optical radar; Three dimensional computer graphics; ADAS; Artificial data; Carlum simulator; Deep learning; LiDAR; Objects detection; Pedestrian detection; Performance; Waymo open dataset; YOLOv4; algorithm; human; pedestrian; Object detection"
Scopus,"Bratulescu, R.-A.; Vatasoiu, R.-I.; Suciu, G.; Mitroi, S.-A.; Vochin, M.-C.; Sachian, M.-A.",Object Detection in Autonomous Vehicles,,2022,,,,10.1109/WPMC55625.2022.10014804,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147040169&doi=10.1109%2fWPMC55625.2022.10014804&partnerID=40&md5=d58b79aab344608ed7a621e1b105b236,"In the coming years, autonomous driving will be the primary focus of the automobile industry. The great majority of accidents are caused by human mistakes, and autonomous cars can help to lower this number significantly, thus improving road safety. Object identification plays a critical part in autonomous vehicle driving, and deep learning techniques are used to implement it. YOLO is one of the most common methods for recognizing and identifying things that emerge on the road. Its popularity has developed as a result of its superior performance in terms of speed, high accuracy, and learning capabilities when compared to other object recognition approaches such as Retina-Net, fast R- CNN, and Single-Shot MultiBox Detection (SSD).  © 2022 IEEE.",autonomous vehicles; object detection; R-CNN; Retina-Net; SSD; YOLO; Accident prevention; Accidents; Automotive industry; Autonomous vehicles; Deep learning; Learning systems; Motor transportation; Object recognition; Roads and streets; Autonomous car; Autonomous driving; Autonomous Vehicles; Objects detection; R-CNN; Retina-net; Road safety; Single-shot; Single-shot multibox detection; YOLO; Object detection
Scopus,"Liang, T.; Bao, H.; Pan, W.; Fan, X.; Li, H.",DetectFormer: Category-Assisted Transformer for Traffic Scene Object Detection,,2022,,,,10.3390/s22134833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132843302&doi=10.3390%2fs22134833&partnerID=40&md5=b4aff40dfc69308200543c2d5f2a433c,"Object detection plays a vital role in autonomous driving systems, and the accurate detection of surrounding objects can ensure the safe driving of vehicles. This paper proposes a category-assisted transformer object detector called DetectFormer for autonomous driving. The proposed object detector can achieve better accuracy compared with the baseline. Specifically, ClassDecoder is assisted by proposal categories and global information from the Global Extract Encoder (GEE) to improve the category sensitivity and detection performance. This fits the distribution of object categories in specific scene backgrounds and the connection between objects and the image context. Data augmentation is used to improve robustness and attention mechanism added in backbone network to extract channel-wise spatial features and direction information. The results obtained by benchmark experiment reveal that the proposed method can achieve higher real-time detection performance in traffic scenes compared with RetinaNet and FCOS. The proposed method achieved a detection performance of 97.6% and 91.4% in AP50 and AP75 on the BCTSDB dataset, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",autonomous driving; deep learning; object detection; transformer; Autonomous vehicles; Benchmarking; Deep learning; Object recognition; Autonomous driving; Deep learning; Detection performance; Driving systems; Object detectors; Objects detection; Safe driving; Scene object; Traffic scene; Transformer; article; attention; deep learning; Object detection
Scopus,"Nafea, M.M.; Tan, S.Y.; Jubair, M.A.; Abd, M.T.",A Review of Lightweight Object Detection Algorithms for Mobile Augmented Reality,,2022,,,,10.14569/IJACSA.2022.0131162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143865177&doi=10.14569%2fIJACSA.2022.0131162&partnerID=40&md5=056fa56818219188fbc74ec37285978a,"Augmented Reality (AR) has led to several technologies being at the forefront of innovation and change in every sector and industry. Accelerated advances in Computer Vision (CV), AR, and object detection refined the process of analyzing and comprehending the environment. Object detection has recently drawn a lot of attention as one of the most fundamental and difficult computer vision topics. The traditional object detection techniques are fully computer-based and typically need massive Graphics Processing Unit (GPU) power, while they aren't usually real-time. However, an AR application required real-time superimposed digital data to enable users to improve their field of view. This paper provides a comprehensive review of most of the recent lightweight object detection algorithms that are suitable to be used in AR applications. Four sources including Web of Science, Scopus, IEEE Xplore, and ScienceDirect were included in this review study. A total of ten papers were discussed and analyzed from four perspectives: accuracy, speed, small object detection, and model size. Several interesting challenges are discussed as recommendations for future work in the object detection field. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.",Augmented reality (ar); Computer vision (cv); Non-graphics processing unit (non-gpu); Object detection; Real time; Augmented reality; Computer graphics equipment; Computer vision; Object detection; Object recognition; Program processors; Signal detection; Augmented reality; Augmented reality applications; Computer vision; Digital datas; Mobile augmented reality; Non-graphic processing unit; Object detection algorithms; Objects detection; Power; Real- time; Graphics processing unit
Scopus,"Rampriya, R.S.; Suganya, R.; Nathan, S.; Perumal, P.S.",A Comparative Assessment of Deep Neural Network Models for Detecting Obstacles in the Real Time Aerial Railway Track Images,,2022,,,,10.1080/08839514.2021.2018184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122411221&doi=10.1080%2f08839514.2021.2018184&partnerID=40&md5=af1ccac851d028d146458200a19c9b90,"Obstacles on the railway track leading to derailment accidents that cause significant damages to the railway in terms of killed and injuries over the years. Count of accident is increasing day by day due to its causes such as boulders on track, trees falling on the gauge, etc. Monitoring these events has been possible with humans working in railways. But when it comes to the real-time scenario, it turns to fatal work and requires more workers, particularly in a dangerous area. Also, this manual monitoring is not adequate to halt derailment accidents. In this perspective, railroad obstacle detection from aerial images has been growing as a trending research topic under artificial intelligence. Also, this mandates the assessment of familiar and latest deep neural network models such as CenterNet Hourglass, EfficientDet, Faster RCNN, SSD Mobile Net, SSD ResNet, and YOLO that detects the violator of accidents with the aid of our own developed Rail Obstacle Detection Dataset (RODD). These detectors were implemented on real-time aerial railway track images captured by Unmanned Aerial Vehicle (UAV) in India. Initially, the input images in the collected datasets were undergone to data preprocessing after that; the above mentioned deep neural models were trained individually. After that, the experiment is analyzed based on training, time, and performance metrics. At last, the results are visualized, evaluated, and compared; hence based on the performance, some effective deep neural network models have identified for detecting obstacles. The result shows that SSD Mobile Net and Faster RCNN can be used for railroad obstacle detection even in the different lighting conditions in railway with the accuracy of 96.75% and 84.75%, respectively. © 2022 The Author(s). Published with license by Taylor & Francis Group, LLC.",Aircraft detection; Antennas; Deep neural networks; Derailments; Neural network models; Obstacle detectors; Railroad tracks; Railroad transportation; Rails; Unmanned aerial vehicles (UAV); Aerial images; Comparative assessment; Dangerous area; Manual monitoring; Mobile nets; Neural network model; Obstacles detection; Railway track; Real- time; Workers'; Railroads
Scopus,"Carranza-García, M.; Galán-Sales, F.J.; Luna-Romera, J.M.; Riquelme, J.C.",Object detection using depth completion and camera-LiDAR fusion for autonomous driving,,2022,,,,10.3233/ICA-220681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133274406&doi=10.3233%2fICA-220681&partnerID=40&md5=f5ea75aaa5f2aa9ab57e445accd3fd2b,"Autonomous vehicles are equipped with complimentary sensors to perceive the environment accurately. Deep learning models have proven to be the most effective approach for computer vision problems. Therefore, in autonomous driving, it is essential to design reliable networks to fuse data from different sensors. In this work, we develop a novel data fusion architecture using camera and LiDAR data for object detection in autonomous driving. Given the sparsity of LiDAR data, developing multi-modal fusion models is a challenging task. Our proposal integrates an efficient LiDAR sparse-to-dense completion network into the pipeline of object detection models, achieving a more robust performance at different times of the day. The Waymo Open Dataset has been used for the experimental study, which is the most diverse detection benchmark in terms of weather and lighting conditions. The depth completion network is trained with the KITTI depth dataset, and transfer learning is used to obtain dense maps on Waymo. With the enhanced LiDAR data and the camera images, we explore early and middle fusion approaches using popular object detection models. The proposed data fusion network provides a significant improvement compared to single-modal detection at all times of the day, and outperforms previous approaches that upsample depth maps with classical image processing algorithms. Our multi-modal and multi-source approach achieves a 1.5, 7.5, and 2.1 mean AP increase at day, night, and dawn/dusk, respectively, using four different object detection meta-architectures.  © 2022 - IOS Press. All rights reserved.",Autonomous driving; data fusion; deep learning; object detection; transfer learning; Autonomous vehicles; Cameras; Convolutional neural networks; Data fusion; Deep learning; Image enhancement; Network architecture; Object recognition; Optical radar; Transfer learning; Autonomous driving; Autonomous Vehicles; Computer vision problems; Deep learning; Detection models; Effective approaches; Learning models; Objects detection; Reliable Networks; Transfer learning; Object detection
Scopus,"Nikolenko, S.I.",Synthetic data for deep learning,,2021,,,,10.1007/978-3-030-75178-4_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111984929&doi=10.1007%2f978-3-030-75178-4_1&partnerID=40&md5=d9bc097816634c367422da230a697d49,,
Scopus,"Wang, J.; Chai, W.; Venkatachalapathy, A.; Tan, K.L.; Haghighat, A.; Velipasalar, S.; Adu-Gyamfi, Y.; Sharma, A.",A Survey on Driver Behavior Analysis From In-Vehicle Cameras,,2022,,,,10.1109/TITS.2021.3126231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136201736&doi=10.1109%2fTITS.2021.3126231&partnerID=40&md5=0d5924b070155c24cef0ed878023fafb,"Distracted or drowsy driving is unsafe driving behavior responsible for thousands of crashes every year. Studying driver behavior has challenges associated with observing drivers in their natural environment. The naturalistic driving study (NDS) has become the most sought-after approach, since it eliminates the bias of a controlled setup, allowing researchers to understand drivers' behavior in real-world scenarios. Video recordings collected in NDS research are incredibly insightful in identifying driver errors. Computer vision techniques have been used to autonomously analyze video data and classify drivers' behavior. While computer vision scientists focus on image analytics, NDS researchers are interested in the factors impacting driver behavior. This survey paper makes a concerted effort to serve both communities by comprehensively reviewing studies, describing their data collection, computer vision techniques implemented, and performance in classifying driver behavior. The scope is limited to studies employing at least one camera observing the driver inside a vehicle. Based on their objective, papers have been classified as detecting low-level (e.g. head orientation) or high-level (e.g. distraction detection) driver information. Papers have been further classified based on the datasets they employ. In addition to twelve public datasets, many private datasets have also been identified, and their data collection design is discussed to highlight any impact on model performance. Across each task, algorithms employed and their performance are discussed to establish a baseline. A comparison of different frameworks for NDS video data analytics throws light on the existing gaps in the state-of-the-art that can be addressed by future computer vision research.  © 2000-2011 IEEE.",Driver behavior analysis; Driver distraction; Drowsiness; Face detection; Gaze; Head orientation; Lane change; Survey; Behavioral research; Cameras; Computer vision; Data acquisition; Data Analytics; Face recognition; Video recording; Computer vision techniques; Driver behaviour analysis; Driver distractions; Driver's behavior; Drowsiness; Faces detection; Gaze; Head orientation; Lane change; Naturalistic driving studies; Surveys
Scopus,"Rosenfeld, A.",Image Analysis and Computer Vision: 1996,,1997,,,,10.1006/cviu.1997.0602,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031116899&doi=10.1006%2fcviu.1997.0602&partnerID=40&md5=6f45c82cc2a82cdcfe61f88e4bb6caa5,"This paper presents a bibliography of nearly 2150 references related to computer vision and image analysis, arranged by subject matter. The topics covered include computational techniques; feature detection and segmentation; image and scene analysis; two-dimensional shape; pattern; color and texture; matching and stereo; three-dimensional recovery and analysis; three-dimensional shape; and motion. A few references are also given on related topics, including geometry and graphics, compression and processing, sensors and optics, visual perception, neural networks, artificial intelligence and pattern recognition, as well as on applications. © 1997 Academic Press.",Color image processing; Computational geometry; Computational methods; Feature extraction; Image analysis; Image compression; Image quality; Image reconstruction; Image segmentation; Neural networks; Stereo vision; Three dimensional computer graphics; Feature detection; Computer vision
Scopus,"Zhang, X.; Liu, H.; Li, X.",Target tracking for mobile robot platforms via object matching and background anti-matching,,2010,,,,10.1016/j.robot.2010.08.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957275313&doi=10.1016%2fj.robot.2010.08.002&partnerID=40&md5=5b281f3f06c648c457481629e89e93a9,"This study proposes a novel method for target tracking based on the combination of object matching and background anti-matching which take account of both the global property of covariance matching and local property of mean shift tracking synthetically. In the background anti-matching phrase, a certain number of background regions are extracted based on the feature of color orientation codes via an entropy filter, and the covariance matrix is adapted to match these regions to get the global motion of the background; further, the object matching is carried out by a mean-shift tracking algorithm. The proposed method is evaluated in various datasets in comparison with their counterpart algorithms; experimental results sufficiently demonstrate the effectiveness of the method proposed in this study. © 2010 Elsevier B.V. All rights reserved.",Background anti-matching; Color orientation codes; Covariance matching; Entropy filter; Motion compensation; Target tracking; Color; Covariance matrix; Entropy; Motion compensation; Target tracking; Tracking (position); Background anti-matching; Background region; Covariance matching; Data sets; Global motion; Global properties; Local property; Mean shift; Mean shift tracking; Mobile robot platforms; Novel methods; Object matching; Orientation codes; Tracking algorithm; Color matching
Scopus,"Kolb, A.; Barth, E.; Koch, R.; Larsen, R.",Time-of-flight cameras in computer graphics,,2010,,,,10.1111/j.1467-8659.2009.01583.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649089694&doi=10.1111%2fj.1467-8659.2009.01583.x&partnerID=40&md5=2fca592955ca398331ef69e33e16dadc,"A growing number of applications depend on accurate and fast 3D scene analysis. Examples are model and lightfield acquisition, collision prevention, mixed reality and gesture recognition. The estimation of a range map by image analysis or laser scan techniques is still a time-consuming and expensive part of such systems. A lower-priced, fast and robust alternative for distance measurements are time-of-flight (ToF) cameras. Recently, significant advances have been made in producing low-cost and compact ToF devices, which have the potential to revolutionize many fields of research, including computer graphics, computer vision and human machine interaction (HMI). These technologies are starting to have an impact on research and commercial applications. The upcoming generation of ToF sensors, however, will be even more powerful and will have the potential to become 'ubiquitous real-time geometry devices' for gaming, web-conferencing, and numerous other applications. This paper gives an account of recent developments in ToF technology and discusses the current state of the integration of this technology into various graphics-related applications. © 2010 The Eurographics Association and Blackwell Publishing Ltd.",3D time-of-flight cameras; Depth keying; Light fields; Range images; Scene analysis; Sensor fusion; Tracking; User interaction; Cameras; Computer graphics; Computer vision; Image analysis; Surface discharges; Tracking (position); Virtual reality; Depth keying; Light fields; Range images; Scene analysis; Sensor fusion; Time-of-flight cameras; User interaction; Human computer interaction
Scopus,"Cultrera, L.; Seidenari, L.; Becattini, F.; Pala, P.; Del Bimbo, A.",Explaining autonomous driving by learning end-to-end visual attention,,2020,,,,10.1109/CVPRW50498.2020.00178,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090162507&doi=10.1109%2fCVPRW50498.2020.00178&partnerID=40&md5=1e1ccc6606878c17f4e94a4a4595ae76,"Current deep learning based autonomous driving approaches yield impressive results also leading to inproduction deployment in certain controlled scenarios. One of the most popular and fascinating approaches relies on learning vehicle controls directly from data perceived by sensors. This end-to-end learning paradigm can be applied both in classical supervised settings and using reinforcement learning. Nonetheless the main drawback of this approach as also in other learning problems is the lack of ex- plainability. Indeed, a deep network will act as a black-box outputting predictions depending on previously seen driving patterns without giving any feedback on why such decisions were taken.While to obtain optimal performance it is not critical to obtain explainable outputs from a learned agent, especially in such a safety critical field, it is of paramount importance to understand how the network behaves. This is particularly relevant to interpret failures of such systems.In this work we propose to train an imitation learning based agent equipped with an attention model. The attention model allows us to understand what part of the image has been deemed most important. Interestingly, the use of attention also leads to superior performance in a standard benchmark using the CARLA driving simulator. © 2020 IEEE.",Autonomous vehicles; Behavioral research; Benchmarking; Computer vision; Control system synthesis; Reinforcement learning; Safety engineering; Attention model; Autonomous driving; Driving simulator; Imitation learning; Learning paradigms; Learning problem; Optimal performance; Visual Attention; Deep learning
Scopus,"Anderson, R.","Security Engineering: A Guide to Building Dependable Distributed Systems, Third Edition",,2020,,,,10.1002/9781119644682,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179271512&doi=10.1002%2f9781119644682&partnerID=40&md5=027098bf23c3a2131e3be49dae73f138,"In Security Engineering: A Guide to Building Dependable Distributed Systems, Third Edition Cambridge University professor Ross Anderson updates his classic textbook and teaches readers how to design, implement, and test systems to withstand both error and attack. This book became a best-seller in 2001 and helped establish the discipline of security engineering. By the second edition in 2008, underground dark markets had let the bad guys specialize and scale up; attacks were increasingly on users rather than on technology. The book repeated its success by showing how security engineers can focus on usability. Now the third edition brings it up to date for 2020. As people now go online from phones more than laptops, most servers are in the cloud, online advertising drives the Internet and social networks have taken over much human interaction, many patterns of crime and abuse are the same, but the methods have evolved. Ross Anderson explores what security engineering means in 2020, including: How the basic elements of cryptography, protocols, and access control translate to the new world of phones, cloud services, social media and the Internet of Things Who the attackers are - from nation states and business competitors through criminal gangs to stalkers and playground bullies, What they do - from phishing and carding through SIM swapping and software exploits to DDoS and fake news, Security psychology, from privacy through ease-of-use to deception, The economics of security and dependability - why companies build vulnerable systems and governments look the other way, How dozens of industries went online - well or badly, How to manage security and safety engineering in a world of agile development - from reliability engineering to DevSecOps, The third edition of Security Engineering ends with a grand challenge: sustainable security. As we build ever more software and connectivity into safety-critical durable goods like cars and medical devices, how do we design systems we can maintain and defend for decades? Or will everything in the world need monthly software upgrades, and become unsafe once they stop?. © 2020 by Ross Anderson Published by John Wiley & Sons, Inc.",
Scopus,"Liu, S.; Geng, K.; Yin, G.; Wu, C.; Ye, J.",Small objects detection with multi-layer laser radar based on projection dimensionality reduction,,2019,,,,10.23919/ChiCC.2019.8866188,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074436587&doi=10.23919%2fChiCC.2019.8866188&partnerID=40&md5=5dcb058b04f6b20bef9c86b725be908a,"Small objects on the road is extremely dangerous obstacles especially when driving at high speed. Detection of such small obstacles is crucial to the safety of autonomous car user. Therefore, a novel of small objects detection algorithm by using multi-layer laser radar is proposed in this paper. Firstly, the road edge point-clouds was extracted from the numerous raw point-clouds based on Hough Transform, and the drivable area and non-drivable area was separated by filtering the point-clouds outside the road edge. Secondly, projection-dimensionality reduction and Hough Transform was applied to recognize and filter the road point-clouds and the remaining point-clouds can be considered as the objects and some interference points. Then, the noise point-clouds was filtered by the outliers filtering algorithm. Finally, the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm was used to cluster the small objects. The proposed algorithm was tested on the platform of driverless formula one car and the experimental results show that the algorithm is efficient and robustness. © 2019 Technical Committee on Control Theory, Chinese Association of Automation.",DBSCAN; Hough transform; Laser radar; Projection dimensionality reduction; Small object detection
Scopus,"Ertler, C.; Mislej, J.; Ollmann, T.; Porzi, L.; Neuhold, G.; Kuang, Y.",The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale,,2020,,,,10.1007/978-3-030-58592-1_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097447491&doi=10.1007%2f978-3-030-58592-1_5&partnerID=40&md5=8d8dc56433cae56f6b3a6a3002e38c88,"Traffic signs are essential map features for smart cities and navigation. To develop accurate and robust algorithms for traffic sign detection and classification, a large-scale and diverse benchmark dataset is required. In this paper, we introduce a new traffic sign dataset of 105K street-level images around the world covering 400 manually annotated traffic sign classes in diverse scenes, wide range of geographical locations, and varying weather and lighting conditions. The dataset includes 52K fully annotated images. Additionally, we show how to augment the dataset with 53K semi-supervised, partially annotated images. This is the largest and the most diverse traffic sign dataset consisting of images from all over the world with fine-grained annotations of traffic sign classes. We run extensive experiments to establish strong baselines for both detection and classification tasks. In addition, we verify that the diversity of this dataset enables effective transfer learning for existing large-scale benchmark datasets on traffic sign detection and classification. The dataset is freely available for academic research (www.mapillary.com/dataset/trafficsign). © 2020, Springer Nature Switzerland AG.",Classification (of information); Computer vision; Large dataset; Transfer learning; Academic research; Benchmark datasets; Classification tasks; Geographical locations; Lighting conditions; Robust algorithm; Semi-supervised; Traffic sign detection; Traffic signs
Scopus,"Yang, B.; Dong, Z.",Progress and perspective of point cloud intelligence,,2019,,,,10.11947/j.AGCS.2019.20190465,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077467563&doi=10.11947%2fj.AGCS.2019.20190465&partnerID=40&md5=6b3898db02f3866b77c143a1e7882e68,"With the rapid development of the reality capture, such as laser scanning and oblique photogrammetry, point cloud has become the third important data source following vector maps and imagery, and also plays an increasingly important role in scientific research and engineering in the fields of earth science, spatial cognition, and smart city, and so on. However, how to acquire valid and accurate three-dimensional geospatial information from point clouds has become the scientific frontier and the urgent demand in the field of surveying and mapping as well as the geoscience applications. To address the challenges mentioned above, point cloud intelligence came into being. This paper summarizes the state-of-the art of point cloud intelligence in acquisition equipment, the intelligent processing, scientific research and the major engineering applications, focusing on its three important areas: the theoretical methods, the key techniques of intelligent processing and the major engineering applications. Finally, the promising development tendency of the point cloud intelligence is summarized. © 2019, Surveying and Mapping Press. All right reserved.",Deep learning; Point cloud big data; Point cloud intelligence; Semantic labeling; Structured modelling; Ubiquitous point cloud; Semantics; Acquisition equipments; Engineering applications; Geo-spatial informations; Geoscience applications; Intelligent processing; Point cloud; Scientific researches; Semantic labeling; computer simulation; data processing; Earth science; engineering geology; information management; laser method; machine learning; scanner; three-dimensional modeling; Deep learning
Scopus,"Murthy, C.B.; Hashmi, M.F.; Bokde, N.D.; Geem, Z.W.",Investigations of object detection in images/videos using various deep learning techniques and embedded platforms-A comprehensive review,,2020,,,,10.3390/app10093280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085064580&doi=10.3390%2fapp10093280&partnerID=40&md5=c090334d20eb5ec8665ee0a4325ff775,"In recent years there has been remarkable progress in one computer vision application area: object detection. One of the most challenging and fundamental problems in object detection is locating a specific object from the multiple objects present in a scene. Earlier traditional detection methods were used for detecting the objects with the introduction of convolutional neural networks. From 2012 onward, deep learning-based techniques were used for feature extraction, and that led to remarkable breakthroughs in this area. This paper shows a detailed survey on recent advancements and achievements in object detection using various deep learning techniques. Several topics have been included, such as Viola-Jones (VJ), histogram of oriented gradient (HOG), one-shot and two-shot detectors, benchmark datasets, evaluation metrics, speed-up techniques, and current state-of-art object detectors. Detailed discussions on some important applications in object detection areas, including pedestrian detection, crowd detection, and real-time object detection on Gpu-based embedded systems have been presented. At last, we conclude by identifying promising future directions. © 2020 by the authors.",Computer vision (CV); Convolutional neural network (CNN); Deep learning techniques; Graphics processing units (GPUs); Object detection
Scopus,"Adarsh, P.; Rathi, P.; Kumar, M.",YOLO v3-Tiny: Object Detection and Recognition using one stage improved model,,2020,,,,10.1109/ICACCS48705.2020.9074315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084648204&doi=10.1109%2fICACCS48705.2020.9074315&partnerID=40&md5=bd6a91de7af4e9515490ea817459f963,"Object detection has seen many changes in algorithms to improve performance both on speed and accuracy. By the continuous effort of so many researchers, deep learning algorithms are growing rapidly with an improved object detection performance. Various popular applications like pedestrian detection, medical imaging, robotics, self-driving cars, face detection, etc. reduces the efforts of humans in many areas. Due to the vast field and various state-of-the-art algorithms, it is a tedious task to cover all at once. This paper presents the fundamental overview of object detection methods by including two classes of object detectors. In two stage detector covered algorithms are RCNN, Fast RCNN, and Faster RCNN, whereas in one stage detector YOLO v1, v2, v3, and SSD are covered. Two stage detectors focus more on accuracy, whereas the primary concern of one stage detectors is speed. We will explain an improved YOLO version called YOLO v3-Tiny, and then its comparison with previous methods for detection and recognition of object is described graphically. © 2020 IEEE.",Computer vision; Convolutional Neural Networks; Deep learning; Faster RCNN; image processing; Object detection; YOLO v3; YOLO v3-Tiny; Deep learning; Face recognition; Learning algorithms; Medical imaging; Medical robotics; Object recognition; Detection performance; Improve performance; Object detection and recognition; Object detection method; Object detectors; Pedestrian detection; Recognition of objects; State-of-the-art algorithms; Object detection
Scopus,"Rumez, M.; Grimm, D.; Kriesten, R.; Sax, E.",An Overview of Automotive Service-Oriented Architectures and Implications for Security Countermeasures,,2020,,,,10.1109/ACCESS.2020.3043070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147696411&doi=10.1109%2fACCESS.2020.3043070&partnerID=40&md5=e5eb8dbc11975beb724c586f0a4eed95,"New requirements from the customers’ and manufacturers’ point of view such as adding new software functions during the product life cycle require a transformed architecture design for future vehicles. The paradigm of signal-oriented communication established for many years will increasingly be replaced by service-oriented approaches in order to increase the update and upgrade capability. In this article, we provide an overview of current protocols and communication patterns for automotive architectures based on the service-oriented architecture (SOA) paradigm and compare them with signal-oriented approaches. Resulting challenges and opportunities of SOAs with respect to information security are outlined and discussed. For this purpose, we explain different security countermeasures and present a state of the section of automotive approaches in the fields of firewalls, Intrusion Detection Systems (IDSs) and Identity and Access Management (IAM). Our final discussion is based on an exemplary hybrid architecture (signal- and service-oriented) and examines the adaptation of existing security measures as well as their specific security features. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",access control; Automotive SOA; connected vehicles; cybersecurity; firewall; intrusion detection system (IDS); service-oriented architectures
Scopus,"Marvin, S.",DICTIONARY OF SCIENTIFIC PRINCIPLES,,2012,,,,10.1002/9781118582121,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000058388&doi=10.1002%2f9781118582121&partnerID=40&md5=59ffb059270dbafc071dcf19243af00f,"Dictionary of Scientific Principles presents a unique and timeless collection of (almost) all known rules or laws commonly called principles, identified throughout the history of scientific development, their definition, and use. Exploring a broad range of disciplines, the book first lists more than 2, 000 principles organized in a standard alphabetical order, then provides a list of subject headings for which related principles are identified. A staple addition to every library, the dictionary will also be of interest to scientists and general readers. © 2011 by John Wiley & Sons, Inc. All rights reserved.",
Scopus,"Rosenfeld, A.",Image Analysis and Computer Vision: 1999,,2000,,,,10.1006/cviu.2000.0835,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000293529&doi=10.1006%2fcviu.2000.0835&partnerID=40&md5=419aca2a4754424115097d404e9ae5ce,"This paper presents a bibliography of nearly 1700 references related to computer vision and image analysis, arranged by subject matter. The topics covered include computational techniques; feature detection and segmentation; image and scene analysis; two-dimensional shape; pattern; color and texture; matching and stereo; 2[Formula presented]-dimensional recovery and analysis; three-dimensional shape; and motion. A few references are also given on related topics, including geometry and graphics, compression and processing, sensors and optics, visual perception, neural networks, artificial intelligence and pattern recognition, as well as on applications. © 2000 Academic Press",Color matching; Computer vision; Image analysis; Image segmentation; Color and textures; Computational technique; Feature detection; Scene analysis; Subject matters; Three-dimensional shape; Visual perception; Stereo image processing
Scopus,"Rosenfeld, A.",Image analysis and computer vision: 1990,,1991,,,,10.1016/1049-9660(91)90020-P,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025993254&doi=10.1016%2f1049-9660%2891%2990020-P&partnerID=40&md5=3444519eeda99360e60afac5cab41c66,"This paper presents a bibliography of over 1600 references related to computer vision and image analysis, arranged by subject matter. The topics covered include architectures; computational techniques; feature detection, segmentation, and image analysis; matching, stereo, and time-varying imagery; shape and pattern; color and texture; and three-dimensional scene analysis. A few references are also given on related topics, such as computational geometry, computer graphics, image input/output and coding, image processing, optical processing, visual perception, neural nets, pattern recognition, and artificial intelligence. © 1991.",bibliography; computer vision; image analysis
Scopus,"Ye, T.; Zhang, Z.; Zhang, X.; Zhou, F.",Autonomous Railway Traffic Object Detection Using Feature-Enhanced Single-Shot Detector,,2020,,,,10.1109/ACCESS.2020.3015251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089944513&doi=10.1109%2fACCESS.2020.3015251&partnerID=40&md5=ad00d0417c425e3a6218703f00210d41,"With the high growth rates of railway transportation, it is extremely important to detect railway obstacles ahead of the train to ensure safety. Manual and traditional feature-extraction methods have been utilized in this scenario. There are also deep learning-based railway object detection approaches. However, in the case of a complex railway scene, these object detection approaches are either inefficient or have insufficient accuracy, particularly for small objects. To address this issue, we propose a feature-enhanced single-shot detector (FE-SSD). The proposed method inherits a prior detection module of RON and a feature transfer block of FB-Net. It also employs a novel receptive field-enhancement module. Through the integration of these three modules, the feature discrimination and robustness are significantly enhanced. Experimental results for a railway traffic dataset built by our team indicated that the proposed approach is superior to other SSD-derived models, particularly for small-object detection, while achieving real-time performance close to that of the SSD. The proposed method achieved a mean average precision of 0.895 and a frame rate of 38 frames per second on a railway traffic dataset with an input size of 320×320 pixels. The experimental results indicate that the proposed method can be used for real-world railway object detection.  © 2013 IEEE.",Feature transfer block; prior detection module; railway object detection; receptive field-enhancement module; Deep learning; Feature extraction; Object recognition; Railroads; Detection approach; Detection modules; Feature discrimination; Feature extraction methods; Frames per seconds; Railway transportation; Real time performance; Small object detection; Object detection
Scopus,"Ranjan Kumar, H.S.; Bhat, D.",A novel method to recognize object in Images using Convolution Neural Networks,,2019,,,,10.1109/ICCS45141.2019.9065367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084088134&doi=10.1109%2fICCS45141.2019.9065367&partnerID=40&md5=5dca8de0becf7ed3144c7cae915e0052,"Detection and recognition of object is challenging task nowadays in the computer vision domain. To solve this problem, we have many techniques and algorithms that help us to detect objects in natural environment. Prime components of object detection are classification and feature extraction. In this paper we have used a method called Convolution Neural Network (CNN) a technique for training images for object recognition. Machine learning techniques are applied on computer vision domain. This research aims to propose a new design and a methodology to support a system in order to recognize the object and give us the pop-up message in case the object is considered as a weapon. Image processing techniques are used to develop the system. This experiment is conducted to access the following qualities, say, to verify whether the system is able to detect the object and to measure the accuracy of the proposed system. © 2019 IEEE.",Computer vision; Image processing; Inception v3; Machine learning technique; Object detection technique; Computer vision; Control systems; Convolution; Feature extraction; Intelligent computing; Learning systems; Object recognition; Convolution neural network; Image processing technique; Machine learning techniques; Natural environments; Recognition of objects; Training image; Object detection
Scopus,"Li, G.; Xie, H.; Yan, W.; Chang, Y.; Qu, X.",Detection of Road Objects with Small Appearance in Images for Autonomous Driving in Various Traffic Situations Using a Deep Learning Based Approach,,2020,,,,10.1109/ACCESS.2020.3036620,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097348264&doi=10.1109%2fACCESS.2020.3036620&partnerID=40&md5=6daaa4eb28b72ac2839a61bd0ae43102,"Effectively detecting road objects in various environments would significantly improve driving safety for autonomous vehicles. However, small objects, low illumination, and blurred outline in images strongly limit the performance of current road object detection methods. To solve these problems, this paper proposed a novel deep learning anchor-free approach based on CenterNet. The atrous spatial pyramid pooling (ASPP) was used to extract features from multiple scales to improve the detection performance while not increasing the computational cost and the number of parameters. The space to depth algorithm was then adopted in our proposed approach to optimize the traditional downsampling process. A large-scale naturalistic driving dataset (BDD100K) was used to examined the effectiveness of our proposed approach. The experimental results show that our proposed approach can effectively improve the detection performance on small objects in various traffic situations. © 2013 IEEE.",advanced driver assistance system (ADAS); Autonomous vehicle (AV); deep learning; driving safety; object detection; Autonomous vehicles; Large dataset; Object detection; Roads and streets; Autonomous driving; Computational costs; Detection performance; Learning-based approach; Low illuminations; Object detection method; Spatial pyramids; Traffic situations; Deep learning
Scopus,"Grauman, K.; Leibe, B.",Visual object recognition,,2011,,,,10.2200/S00332ED1V01Y201103AIM011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955445232&doi=10.2200%2fS00332ED1V01Y201103AIM011&partnerID=40&md5=572605e073b66d4aae4f94b74edc1425,"The visual recognition problem is central to computer vision research. From robotics to information retrieval, many desired applications demand the ability to identify and localize categories, places, and objects. This tutorial overviews computer vision algorithms for visual object recognition and image classification. We introduce primary representations and learning approaches, with an emphasis on recent advances in the field. The target audience consists of researchers or students working in AI, robotics, or vision who would like to understand what methods and representations are available for these problems. This lecture summarizes what is and isn't possible to do reliably today, and overviews key concepts that could be employed in systems requiring visual categorization. Copyright © 2011 by Morgan & Claypool.",dealing with outliers in correspondences; detection and description of local invariant features; detection via sliding windows; efficient algorithms for matching local features; Generalized distance transform; global representations versus local descriptors; histograms of oriented gradients and rectangular features; Hough voting; methods to verify geometric consistency according to parameterized geometric transformations; part-based models; pyramid match kernels; RANSAC and the Generalized Hough transform; star graph models and fully connected constellations; the Deformable Part-based Model; the Implicit Shape Model; tree-based and hashing-based search algorithms; visual vocabularies and bags-of-words; window-based descriptors; Computer vision; Content based retrieval; Deformation; Geometry; Graphic methods; Hough transforms; Image matching; Information retrieval; Learning algorithms; Object recognition; Robotics; Stars; Trees (mathematics); dealing with outliers in correspondences; detection and description of local invariant features; detection via sliding windows; efficient algorithms for matching local features; Generalized distance transform; global representations versus local descriptors; histograms of oriented gradients and rectangular features; Hough voting; methods to verify geometric consistency according to parameterized geometric transformations; part-based models; pyramid match kernels; RANSAC and the Generalized Hough transform; star graph models and fully connected constellations; the Deformable Part-based Model; the Implicit Shape Model; tree-based and hashing-based search algorithms; visual vocabularies and bags-of-words; window-based descriptors; Mathematical transformations
Scopus,"Sotak, M.",Coarse alignment algorithm for ADIS16405,,2010,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956034129&partnerID=40&md5=0f801159c0085af50bfefbe7b4245969,"The paper presents the coarse alignment algorithm for the sensor ADIS16405. The ADIS16405 is the new six-degree of freedom inertial measurement unit which is produced by Analog Devices. The main goal of the paper is to describe determination of the initial attitude of the sensor with respect to the navigation frame as a referenced frame. In this work the attitude is represented by the Euler angles (the roll, pitch and yaw angles). Developed coarse alignment algorithm is implemented for the real-time integrated navigation system that is in a progress nowadays.",Alignment; Euler angles; Inertial measurement unit; Low-cost navigation
Scopus,"Klinefelter, E.; Nanzer, J.A.",Interferometric microwave radar with a feedforward neural network for vehicle speed-over-ground estimation,,2020,,,,10.1109/LMWC.2020.2966191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081716685&doi=10.1109%2fLMWC.2020.2966191&partnerID=40&md5=3465a3086bee7b996caebff30bde4ff0,"A novel approach to measuring the ground speed of autonomous vehicles using interferometric radar is presented. Using a microwave radar with interferometric processing and a feedforward neural network with local regression, high-accuracy velocity estimation is achieved with a downward-facing radar that, unlike forward-looking Doppler radars, can be protected from road debris and is, furthermore, unaffected by wheel slip and requires no external inputs, such as Global Navigation Satellite Systems (GNSS). A 16.9-GHz active interferometric array generates a grating lobe pattern and as the ground passes through the pattern, the range of frequencies over which the response is distributed increases proportionally with the ground velocity. We implement a feedforward neural network to estimate the velocity based on the interferometer's frequency response. The estimator achieved a root-mean-squared error of 0.138 (m/s), equivalent to that of the forward-looking Doppler radars. © 2001-2012 IEEE.",Automotive radar; autonomous vehicles (AVs); ego-estimation; machine learning; neural networks; radar interferometry; velocity estimation; Automotive radar; Autonomous vehicles; Doppler radar; Feedforward neural networks; Frequency response; Global positioning system; Interferometry; Learning algorithms; Learning systems; Mean square error; Neural networks; Velocity; Global Navigation Satellite Systems; Interferometric array; Interferometric processing; Interferometric radars; Local regression; Radar interferometry; Root mean squared errors; Velocity estimation; Radar measurement
Scopus,"Seikavandi, M.J.; Nasrollahi, K.; Moeslund, T.B.",Deep car detection by fusing grayscale image and weighted upsampled LiDAR depth,,2020,,,,10.1117/12.2586908,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107836656&doi=10.1117%2f12.2586908&partnerID=40&md5=8b2b97ff41c514e31cacaccd6064c6b6,"Recent advances have shown sensor-fusion's vital role in accurate detection, especially for advanced driver assistance systems. We introduce a novel procedure for depth upsampling and sensor-fusion that together lead to an improved detection performance, compared to state-of-the-art results for detecting cars. Upsampling is generally based on combining data from an image to compensate for the low resolution of a LiDAR (Light Detector and Ranging). This paper, on the other hand, presents a framework to obtain dense depth map solely from a single LiDAR point cloud that makes it possible to use just one deep network for both LiDAR and image modalities. The produced full-depth map is added to the grayscale version of the image to produce a two-channel input for a deep neural network. The simple preprocessing structure is efficiently competent in filing cars' shapes, which helps the fusion framework to outperforms the state-of-the-art on the KITTI object detection for the Car class. © 2021 SPIE.",Autonomous Driving; Deep Learning; Depth Perception; LiDAR; Multimodal Fusion; Object Detection; Sensor Fusion; Advanced driver assistance systems; Automobile drivers; Chemical detection; Computer vision; Deep neural networks; Object detection; Signal sampling; Dense depth map; Depth upsampling; Detection performance; Gray-scale images; Image modality; Lidar point clouds; Low resolution; State of the art; Optical radar
Scopus,"Lu, Y.; Yang, M.; Wang, C.; Wang, B.",Pedestrian tracking based on laser and image data fusion,,2019,,,,10.1109/RCAR47638.2019.9044002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089136353&doi=10.1109%2fRCAR47638.2019.9044002&partnerID=40&md5=13150505c1e4c293454c25160ee4858b,"Pedestrian tracking in vehicle coordinates is not only a necessary part of the perception for autonomous cars but also a challenging task in computer vision. This paper presents a multi-sensor fusion model combining images and laser scanning data to track pedestrians in occupied grid maps. In our approach, the bounding boxes of pedestrians detected in images are used to generate regions of interest (ROI) in grids. A data association method based on multi-characteristic Mahalanobis distance (MMD) and sliding windows is proposed to establish correspondence between the detections in different frames. Then Sampling Importance Resampling Particle Filter (SIR PF) is used to update pedestrians' states with the fusion of images and grids. Experiments show our method has competitive efficiency and accuracy compared to conventional methods. Codes of this paper is released1,. © 2018 IEEE.",Agricultural robots; Importance sampling; Robotics; Sensor data fusion; Conventional methods; Laser scanning data; Mahalanobis distances; Multi characteristics; Multi-sensor fusion; Pedestrian tracking; Regions of interest; Sampling importance resampling; Image fusion
Scopus,"Poncela, A.; Urdiales, C.; de Trazegnies, C.; Sandoval, F.",A new sonar-based landmark for localization in indoor environments,,2007,,,,10.1007/s00500-006-0069-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750298587&doi=10.1007%2fs00500-006-0069-3&partnerID=40&md5=3356caf01af871c1ad7a74630d04c4d4,"This paper presents a new sonar based landmark to represent significant places in an environment for localization purposes. This landmark is based on extracting the contour free of obstacles around the robot from a local evidence grid. This contour is represented by its curvature, calculated by a noise-resistant function which adapts to the natural scale of the contour at each point. Then, curvature is reduced to a short feature vector by using Principal Component Analysis. The landmark calculation method has been successfully tested in a medium scale real environment using a Pioneer robot with Polaroid sonar sensors. © Springer-Verlag 2006.",
Scopus,"Mannan, M.S.","Lees' Loss Prevention in the Process Industries: Hazard Identification, Assessment and Control: Third Edition",,2004,,,,10.1016/B978-0-7506-7555-0.X5081-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041120899&doi=10.1016%2fB978-0-7506-7555-0.X5081-6&partnerID=40&md5=0f3f24c9690607fd0823046f6958522e,"Over the last three decades the process industries have grown very rapidly, with corresponding increases in the quantities of hazardous materials in process, storage or transport. Plants have become larger and are often situated in or close to densely populated areas. Increased hazard of loss of life or property is continually highlighted with incidents such as Flixborough, Bhopal, Chernobyl, Three Mile Island, the Phillips 66 incident, and Piper Alpha to name but a few.The field of Loss Prevention is, and continues to, be of supreme importance to countless companies, municipalities and governments around the world, because of the trend for processing plants to become larger and often be situated in or close to densely populated areas, thus increasing the hazard of loss of life or property. This book is a detailed guidebook to defending against these, and many other, hazards. It could without exaggeration be referred to as the ""bible"" for the process industries. This is THE standard reference work for chemical and process engineering safety professionals. For years, it has been the most complete collection of information on the theory, practice, design elements, equipment, regulations and laws covering the field of process safety. An entire library of alternative books (and cross-referencing systems) would be needed to replace or improve upon it, but everything of importance to safety professionals, engineers and managers can be found in this all-encompassing reference instead. Frank Lees' world renowned work has been fully revised and expanded by a team of leading chemical and process engineers working under the guidance of one of the worldâ??s chief experts in this field. Sam Mannan is professor of chemical engineering at Texas A and M University, and heads the Mary Kay Oâ??Connor Process Safety Center at Texas A and M. He received his MS and Ph.D. in chemical engineering from the University of Oklahoma, and joined the chemical engineering department at Texas A and M University as a professor in 1997. He has over 20 years of experience as an engineer, working both in industry and academiaNew detail is added to chapters on fire safety, engineering, explosion hazards, analysis and suppression, and new appendices feature more recent disasters. The many thousands of references have been updated along with standards and codes of practice issued by authorities in the US, UK/Europe and internationally. In addition to all this, more regulatory relevance and case studies have been included in this edition. Written in a clear and concise style, Loss Prevention in the Process Industries covers traditional areas of personal safety as well as the more technological aspects and thus provides balanced and in-depth coverage of the whole field of safety and loss prevention. © 2005 Elsevier Inc. All rights reserved.",
Scopus,"Li, Y.; Zhang, J.; Zhong, Y.; Wang, M.",An efficient stereo matching based on fragment matching,,2019,,,,10.1007/s00371-018-1491-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047263906&doi=10.1007%2fs00371-018-1491-0&partnerID=40&md5=44338ee7e1b1b799985c082a2c4f92cb,"We propose a stereo matching method based on image fragments. Unlike traditional pixel-based stereos matching methods, we use edge information in the reference image to divide it into small fragments, and we then use the segments to find the best matching fragments in another reference image from the horizontal and vertical directions. We obtain two disparity maps, and using the match confidence value for each disparity map, we can produce a more accurate disparity map. Next, we calculate the exact disparity value for each pixel within the fragment. Finally, the disparity map is filled and smoothed to obtain the final disparity result. Experiments demonstrated that the proposed method has low computation complexity, high matching accuracy, and the disparity of object edge is clear, and it achieved good performance with the Middlebury and KITTI benchmark. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Image edge; Same color region median filter; Segment matching; Stereo matching; Benchmarking; Color matching; Image segmentation; Median filters; Pixels; Computation complexity; Confidence values; Image edge; Region median filter; Segment matching; Stereo matching; Stereo matching method; Vertical direction; Stereo image processing
Scopus,"Chen, G.; Mao, Z.; Yi, H.; Li, X.; Bai, B.; Liu, M.; Zhou, H.",Pedestrian detection based on panoramic depth map transformed from 3d-lidar data,,2020,,,,10.3311/PPee.14960,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091931096&doi=10.3311%2fPPee.14960&partnerID=40&md5=881b37cc5665f91ca1c75d62eb9b2b65,"Object detection is a crucial task of autonomous driving. This paper addresses an effective algorithm for pedestrian detection of the panoramic depth map transformed from the 3D-LiDAR data. Firstly, the 3D point clouds are transformed into panoramic depth maps, and then the panoramic depth maps are enhanced. Secondly, the grounds of the 3D point clouds are removed. The remaining point clouds are clustered, filtered and projected onto the previously generated panoramic depth maps, and new panoramic depth maps are obtained. Finally, the new panoramic depth maps are jointed to generate depth maps with different sizes, which are used as input of the improved PVANET for pedestrian detection. The 2D image of the panoramic depth map applied to the proposed algorithm is transformed from 3D point cloud, effectively containing the panorama of the sensor, and is more suitable for the environment perception of autonomous driving. Compared with the detection algorithm based on RGB images, the proposed algorithm cannot be affected by light, and can maintain the normal average precision of pedestrian detection at night. In order to increase the robustness of detecting small objects like pedestrians, the network structure based on the original PVANET is modified in this paper. A new dataset is built by processing the 3D-LiDAR data and the model trained on the new dataset perform well. The experimental results show that the proposed algorithm achieves high accuracy and robustness in pedestrian detection under different illumination conditions. Furthermore, when trained on the new dataset, the model exhibits average precision improvements of 2.8-5.1 % over the original PVANET, making it more suitable for autonomous driving applications. © 2020 Budapest University of Technology and Economics. All rights reserved.",3D-LiDAR data; Improved PVANET; Panoramic depth map; Small object detection; Autonomous vehicles; Object recognition; Optical radar; Autonomous driving; Detection algorithm; Effective algorithms; Environment perceptions; Illumination conditions; Network structures; Pedestrian detection; Precision improvement; Object detection
Scopus,"Jiang, M.; Aoyama, T.; Takaki, T.; Ishii, I.",Pixel-level and robust vibration source sensing in high-frame-rate video analysis,,2016,,,,10.3390/s16111842,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994691341&doi=10.3390%2fs16111842&partnerID=40&md5=11270661961b238c9b73a675c4d6c0af,"We investigate the effect of appearance variations on the detectability of vibration feature extraction with pixel-level digital filters for high-frame-rate videos. In particular, we consider robust vibrating object tracking, which is clearly different from conventional appearance-based object tracking with spatial pattern recognition in a high-quality image region of a certain size. For 512 _ 512 videos of a rotating fan located at different positions and orientations and captured at 2000 frames per second with different lens settings, we verify how many pixels are extracted as vibrating regions with pixel-level digital filters. The effectiveness of dynamics-based vibration features is demonstrated by examining the robustness against changes in aperture size and the focal condition of the camera lens, the apparent size and orientation of the object being tracked, and its rotational frequency, as well as complexities and movements of background scenes. Tracking experiments for a flying multicopter with rotating propellers are also described to verify the robustness of localization under complex imaging conditions in outside scenarios. © 2016 by the authors; licensee MDPI, Basel, Switzerland.",Drone tracking; High-frame-rate video; Object tracking; Pixel-level digital filters; Vibration source localization; Aircraft detection; Computer graphics; Digital filters; Feature extraction; Image processing; Pixels; Frames per seconds; High frame rate; High quality images; Imaging conditions; Object Tracking; Pixel level; Rotational frequency; Vibration sources; Vibration analysis
Scopus,"Deruyttere, T.; Vandenhende, S.; Grujicic, D.; Liu, Y.; Van Gool, L.; Blaschko, M.; Tuytelaars, T.; Moens, M.-F.",Commands 4 Autonomous Vehicles (C4AV) Workshop Summary,,2020,,,,10.1007/978-3-030-66096-3_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101783520&doi=10.1007%2f978-3-030-66096-3_1&partnerID=40&md5=2781660150b366f388a0cd0758b8a5b2,"The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the Commands for Autonomous Vehicles (C4AV) challenge based on the recent Talk2Car dataset. This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work. © 2020, Springer Nature Switzerland AG.",Computer vision; Visual languages; Freeforms; Natural language queries; Natural languages; Potential failures; Spoken languages; State of the art; Autonomous vehicles
Scopus,"Tian, W.; Chen, L.; Zou, K.; Lauer, M.",Vehicle tracking at nighttime by kernelized experts with channel-wise and temporal reliability estimation,,2018,,,,10.1109/TITS.2017.2771410,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038361372&doi=10.1109%2fTITS.2017.2771410&partnerID=40&md5=35b7bb8d1da9c741c38ebfa243891b7d,"Despite the fact that in recent years, vision-based tracking approaches have made significant progress, the task of tracking vehicles at night still remains challenging. Visual information is strongly deteriorated or at least degraded due to poor illumination conditions. This reduces the perceptive ability of vision systems significantly and can even lead to target loss, resulting in false estimation and/or false prediction of object behavior. In this paper, we propose a novel online-learning method to track vehicles at night. Our method is based on the kernelized correlation filter and assembles different feature channels to kernelized experts. By estimating their reliabilities, we force the appearance model to focus on the most discriminative visual features to accomplish the classification. In addition, a temporal optimization step in conjunction with a memory model is used to remove outliers and keep the most reliable samples to train the tracker models. Experiments over various daytime and weather conditions show that our approach outperforms existing trackers at night and in case of bad weather while offering state-of-the-art performance in more favorable situations. As our tracker has only little computational cost, it is appropriate for use cases with real-time requirements like in automotive or industrial applications. © 2018 IEEE.",correlation filter; kernelized expert; Nighttime vehicle tracking; reliability estimation; weather robustness; Correlation methods; Flow visualization; Lighting; Personnel training; Reliability; Reliability analysis; Vehicles; Correlation filters; Illumination conditions; Image color analysis; kernelized expert; On-line learning methods; Real time requirement; Reliability estimation; State-of-the-art performance; Target tracking
Scopus,"Wang, L.; Cao, Z.; Cui, Z.; Cao, C.; Pi, Y.",Negative Latency Recognition Method for Fine-Grained Gestures Based on Terahertz Radar,,2020,,,,10.1109/TGRS.2020.2985421,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095856026&doi=10.1109%2fTGRS.2020.2985421&partnerID=40&md5=b8478f023bf895a74b2ea77012253dbf,"Noncontact gesture recognition is gradually being applied to emerging applications, such as smart cars and smart phones. Negative latency gesture recognition (recognition before a gesture is finished) is desirable due to the instantaneous feedback. However, it is difficult for existing methods to achieve a high precision and negative latency gesture recognition. A fragment can provide too few features to directly identify all gestures well. By observing a large number of existing gesture sets and people's daily operating habits, we found that some high frequency used gestures are similar. To the best of our knowledge, it is the first time to redivide the gestures into two subsets according to their movement physical states. We divided the gestures with different shapes or motion states into a parent-class subset, and further divided each pair of parent-class gestures to obtain a child-class subset. In order to achieve a better tradeoff between the high-precision and negative latency, an approach of motion pattern and behavior intention (MPBI) is proposed. Taking full advantage of the characteristics of each subset, MPBI includes two models. First, pattern model coarsely classify the parent-class gestures by a convolutional network, and then intention model further classifies child-class gestures according to their opposite motion direction. MPBI is evaluated on a 340-GHz terahertz radar. With the advantage of its accurate ranging, intention model can recognize child-class gestures directly without training. MPBI is evaluated on 12 gestures and achieves a recognition accuracy of 94.13%, which only needs a 0.033-s gesture fragment as an input sample.  © 1980-2012 IEEE.",Gesture recognition; high resolution range profile (HRRP); real-time; terahertz radar; Convolutional neural networks; Radar; Set theory; Smartphones; Convolutional networks; Emerging applications; High frequency HF; Intention modeling; Motion direction; Recognition accuracy; Recognition methods; Terahertz radars; algorithm; data processing; model validation; radar; recognition; remote sensing; satellite data; Gesture recognition
Scopus,"Smeulders, A.W.M.; Chu, D.M.; Cucchiara, R.; Calderara, S.; Dehghan, A.; Shah, M.",Visual tracking: An experimental survey,,2014,,,,10.1109/TPAMI.2013.230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903121415&doi=10.1109%2fTPAMI.2013.230&partnerID=40&md5=f0acbfd2f9f028f4f3a2837b51e6c412,"There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers. © 2014 IEEE.",Camera surveillance; Computer vision; Image processing; Object tracking; Tracking dataset; Tracking evaluation; Video understanding; Cameras; Computer vision; Image processing; Tracking (position); Camera surveillance; Experimental survey; Illumination changes; Object Tracking; Realistic scenario; Tracking evaluation; Video fragments; Video understanding; Security systems
Scopus,"Borrego-Carazo, J.; Castells-Rufas, D.; Biempica, E.; Carrabina, J.",Resource-Constrained Machine Learning for ADAS: A Systematic Review,,2020,,,,10.1109/ACCESS.2020.2976513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081676310&doi=10.1109%2fACCESS.2020.2976513&partnerID=40&md5=30daebe1e79874e3004a5ae8316936d5,"The advent of machine learning (ML) methods for the industry has opened new possibilities in the automotive domain, especially for Advanced Driver Assistance Systems (ADAS). These methods mainly focus on specific problems ranging from traffic sign and light recognition to pedestrian detection. In most cases, the computational resources and power budget found in ADAS systems are constrained while most machine learning methods are computationally intensive. The usual solution consists in adapting the ML models to comply with the memory and real-time (RT) requirements for inference. Some models are easily adapted to resource-constrained hardware, such as Support Vector Machines, while others, like Neural Networks, need more complex processes to fit into the desired hardware. The ADAS hardware (HW platforms) are diverse, from complex MPSoC CPUs down to classical MCUs, DPSs and application-specific FPGAs and ASICs or specific GPU platforms (such as the NVIDIA families Tegra or Jetson). Therefore, there is a tradeoff between the complexity of the ML model implemented and the selected platform that impacts the performance metrics: function results, energy consumption and speed (latency and throughput). In this paper, a survey in the form of systematic review is conducted to analyze the scope of the published research works that embed ML models into resource-constrained implementations for ADAS applications and what are the achievements regarding the ML performance, energy and speed trade-off. © 2013 IEEE.",ADAS; automotive engineering; embedded software; FPGA; GPU; Machine learning; Advanced driver assistance systems; Automobile drivers; Automotive engineering; Budget control; Complex networks; Economic and social effects; Embedded software; Energy utilization; Field programmable gate arrays (FPGA); Graphics processing unit; Program processors; Support vector machines; System-on-chip; ADAS; Application specific; Automotive domains; Computational resources; Machine learning methods; Pedestrian detection; Performance metrics; Specific problems; Learning systems
Scopus,"Firouzi, H.; Najjaran, H.",Robust decentralized multi-model adaptive template tracking,,2012,,,,10.1016/j.patcog.2012.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864277724&doi=10.1016%2fj.patcog.2012.05.005&partnerID=40&md5=835b922fe066d16fb7b75b4bb669be15,"In this paper, a robust and efficient visual tracking method through the fusion of several distributed adaptive templates is proposed. It is assumed that the target object is initially localized either manually or by an object detector at the first frame. The object region is then partitioned into several non-overlapping subregions. The new location of each subregion is found by an EM 1-like gradient-based optimization algorithm. The proposed localization algorithm is capable of simultaneously optimizing several possible solutions in a probabilistic framework. Each possible solution is an initializing point for the optimization algorithm which improves the accuracy and reliability of the proposed gradient-based localization method to the local extrema. Moreover, each subregion is defined by two adaptive templates named immediate and delayed templates to solve the drift problem. 2 The immediate template is updated by short-term appearance changes whereas the delayed template models the long-term appearance variations. Therefore, the combination of short-term and long-term appearance modeling can solve the template tracking drift problem. At each tracking step, the new location of an object is estimated by fusing the tracking result of each subregion. This fusion method is based on the local and global properties of the object motion to increase the robustness of the proposed tracking method against outliers, shape variations, scale changes. The accuracy and robustness of the proposed tracking method is verified by several experimental results. The results also show the superior efficiency of the proposed method by comparing it to several state-of-the-art trackers as well as the manually labeled ground truth data. © 2012 Elsevier Ltd.",Decentralized object localization; EM algorithm; Mixture of Gaussian; Non-rigid object; Robust fusion; Pattern recognition; Software engineering; EM algorithms; Mixture of Gaussians; Non-rigid objects; Object localization; Robust fusion; Algorithms
Scopus,"Wang, C.; Zhang, X.; Zang, X.; Liu, Y.; Ding, G.; Yin, W.; Zhao, J.",Feature sensing and robotic grasping of objects with uncertain information: A review,,2020,,,,10.3390/s20133707,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087464951&doi=10.3390%2fs20133707&partnerID=40&md5=6c016b46bddc7382309c39764285c70d,"As there come to be more applications of intelligent robots, their task object is becoming more varied. However, it is still a challenge for a robot to handle unfamiliar objects. We review the recent work on the feature sensing and robotic grasping of objects with uncertain information. In particular, we focus on how the robot perceives the features of an object, so as to reduce the uncertainty of objects, and how the robot completes object grasping through the learning-based approach when the traditional approach fails. The uncertain information is classified into geometric information and physical information. Based on the type of uncertain information, the object is further classified into three categories, which are geometric-uncertain objects, physical-uncertain objects, and unknown objects. Furthermore, the approaches to the feature sensing and robotic grasping of these objects are presented based on the varied characteristics of each type of object. Finally, we summarize the reviewed approaches for uncertain objects and provide some interesting issues to be more investigated in the future. It is found that the object’s features, such as material and compactness, are difficult to be sensed, and the object grasping approach based on learning networks plays a more important role when the unknown degree of the task object increases. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Feature sensing; Geometric uncertainty; Physical uncertainty; Robotic grasping; Uncertain objects; End effectors; Object recognition; Robotics; Geometric information; Learning network; Learning-based approach; Physical information; Robotic grasping; Three categories; Traditional approaches; Uncertain informations; learning; review; robotics; uncertainty; Intelligent robots
Scopus,"Nissan, E.","Computer Applications for Handling Legal Evidence, Police Investigation and Case Argumentation",,2012,,,,10.1007/978-90-481-8990-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145400546&doi=10.1007%2f978-90-481-8990-8&partnerID=40&md5=ec69acda7b39f0235302713548f42ab5,,Analitic Tools; Applications of Computing; Argumentation; Argumentation Methods; Argumentation Tools; Artificial Intelligence & Law; Cadaver Dogs; Computer Forensics; Computer Techniques; Crime Analysis; Crime Detection; Criminal Investigation; Criminal Trials; DNA; Environmental Forensics; Evidence; Evidentiary Value; Exoneration; Face Processing; Facial Reconstruction; Fingerprints; Forensic Archeaology; Forensic Disciplines; Forensic Engineering; Forensic Geology; Forensic Palynology; Forensic Science; Forensic Testing; Gas Soil Surveying; Historical Perspective; Identification Methods; Identity Parades; Intelligence Analysts; Juridic Culture; Law Enforcement; Legal Evidence; Legal Narratives; Legal Professionals; Litigation; Modelling of Reasoning on Legal Evidence; Narrow Evidence Domains; Odorology; Police Intelligence; Polygraph Tests; Prosecution; Questioned Documents Evidence; Reasoning; Reasoning of Jurors; Scent Detection; Self-Incriminating Confessions; Training Police Officers; Wigmore Chart
Scopus,"Xue, J.; Fang, J.; Li, T.; Zhang, B.; Zhang, P.; Ye, Z.; Dou, J.",BLVD: Building a large-scale 5D semantics benchmark for autonomous driving,,2019,,,,10.1109/ICRA.2019.8793523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071462075&doi=10.1109%2fICRA.2019.8793523&partnerID=40&md5=a49212cd595b15661108ebf48edf5e12,"In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/. © 2019 IEEE.",
Scopus,"Li, J.; Hou, Q.; Xing, J.",Multiobject detection algorithm based on adaptive default box mechanism,,2020,,,,10.1155/2020/5763476,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091973208&doi=10.1155%2f2020%2f5763476&partnerID=40&md5=b42f1a781e5d07170a985369a4c6b578,"Multiobject detection tasks in complex scenes have become an important research topic, which is the basis of other computer vision tasks. Considering the defects of the traditional single shot multibox detector (SSD) algorithm, such as poor small object detection effect, reliance on manual setting for default box generation, and insufficient semantic information of the low detection layer, the detection effect in complex scenes was not ideal. Aiming at the shortcomings of the SSD algorithm, an improved algorithm based on the adaptive default box mechanism (ADB) is proposed. The algorithm introduces the adaptive default box mechanism, which can improve the imbalance of positive and negative samples and avoid manually set default box super parameters. Experimental results show that, compared with the traditional SSD algorithm, the improved algorithm has a better detection effect and higher accuracy in complex scenes.  © 2020 Jinling Li et al.",Semantics; Signal detection; Complex scenes; Detection algorithm; Detection effect; Detection tasks; Improved * algorithm; Multiobject; Research topics; Semantics Information; Single-shot; Small object detection; Object detection
Scopus,"Hu, J.; Abubakar, S.; Liu, S.; Dai, X.; Yang, G.; Sha, H.",Near-infrared road-marking detection based on a modified faster regional convolutional neural network,,2019,,,,10.1155/2019/7174602,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077800319&doi=10.1155%2f2019%2f7174602&partnerID=40&md5=646dcb9ac7163787c88e958aaeb6e4ea,"Pedestrians, motorist, and cyclist remain the victims of poor vision and negligence of human drivers, especially in the night. Millions of people die or sustain physical injury yearly as a result of traffic accidents. Detection and recognition of road markings play a vital role in many applications such as traffic surveillance and autonomous driving. In this study, we have trained a nighttime road-marking detection model using NIR camera images. We have modified the VGG-16 base network of the state-of-the-art faster R-CNN algorithm by using a multilayer feature fusion technique. We have demonstrated another promising feature fusion technique of concatenating all the convolutional layers within a stage to extract image features. The modification boosts the overall detection performance of the model by utilizing the advantages of the shallow layers and the deep layers of the VGG-16 network. The training samples were augmented using random rotation and translation to enhance the heterogeneity of the detection algorithm. We have achieved a mean average precision (mAP) of 89.48% and 92.83% for the baseline faster R-CNN and our modified method, respectively. © 2019 Junping Hu et al.",Convolution; Highway markings; Infrared devices; Neural networks; Roads and streets; Autonomous driving; Convolutional neural network; Detection algorithm; Detection models; Detection performance; Physical injuries; State of the art; Traffic surveillance; Road and street markings
Scopus,"Long, Q.; Xie, Q.; Mita, S.; Ishimaru, K.; Shirai, N.",Small object detection based on stereo vision,,2016,,,,10.20485/jsaeijae.7.1_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963959026&doi=10.20485%2fjsaeijae.7.1_9&partnerID=40&md5=083c21ad38abb5e0924b143b3751d7c6,"Small size objects which dimensions are around 0.15m are one of the major security risks to driving vehicles in the highway. Lidar and radar are hard to detect this kind of objects due to the sparsity of their detecting signal. Vision based methods are possible to solve this problem because camera can generate dense information. We propose a new method to detection small objects in the highway based on stereo vision. This method uses Multi-Path-Viterbi algorithm to obtain dense depth information of stereo images. Based on the depth information, road surface can be detected. Objects on road can be mapped to the 3D space to determine their size and location, then small objects dangerous to the host Vehicle can be recognized and located. © 2016 Society of Automotive Engineers of Japan, Inc.",Obstacle detection; Road environment recognition; Safety; Stereo matching; Vehicle; Accident prevention; Object detection; Obstacle detectors; Optical radar; Roads and streets; Stereo image processing; Vehicles; Viterbi algorithm; Depth information; Obstacle detection; Road environment; Road surfaces; Security risks; Small object detection; Stereo matching; Vision-based methods; Stereo vision
Scopus,"Buch, N.; Velastin, S.A.; Orwell, J.",A review of computer vision techniques for the analysis of urban traffic,,2011,,,,10.1109/TITS.2011.2119372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052346510&doi=10.1109%2fTITS.2011.2119372&partnerID=40&md5=a343a7dc439772bcca05cb6934bb9621,"Automatic video analysis from urban surveillance cameras is a fast-emerging field based on computer vision techniques. We present here a comprehensive review of the state-of-the-art computer vision for traffic video with a critical analysis and an outlook to future research directions. This field is of increasing relevance for intelligent transport systems (ITSs). The decreasing hardware cost and, therefore, the increasing deployment of cameras have opened a wide application field for video analytics. Several monitoring objectives such as congestion, traffic rule violation, and vehicle interaction can be targeted using cameras that were typically originally installed for human operators. Systems for the detection and classification of vehicles on highways have successfully been using classical visual surveillance techniques such as background estimation and motion tracking for some time. The urban domain is more challenging with respect to traffic density, lower camera angles that lead to a high degree of occlusion, and the variety of road users. Methods from object categorization and 3-D modeling have inspired more advanced techniques to tackle these challenges. There is no commonly used data set or benchmark challenge, which makes the direct comparison of the proposed algorithms difficult. In addition, evaluation under challenging weather conditions (e.g., rain, fog, and darkness) would be desirable but is rarely performed. Future work should be directed toward robust combined detectors and classifiers for all road users, with a focus on realistic conditions during evaluation. © 2006 IEEE.",Closed-circuit television (CCTV); intersection monitoring; road user counting; road users; traffic analysis; urban traffic; vehicle classification; vehicle detection; visual surveillance; Algorithms; Cameras; Computer vision; Detectors; Mathematical operators; Monitoring; Motor transportation; Roads and streets; Security systems; Television networks; Three dimensional; Vehicles; Wireless telecommunication systems; Closed-circuit television (CCTV); Road users; traffic analysis; Urban traffic; vehicle classification; vehicle detection; Visual surveillance; Traffic congestion
Scopus,"Desanamukula, V.S.; Chilukuri, P.K.; Padala, P.; Padala, P.; Pvgd, P.R.",AMMDAS: Multi-modular generative masks processing architecture with adaptive wide field-of-view modeling strategy,,2020,,,,10.1109/ACCESS.2020.3033537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100327038&doi=10.1109%2fACCESS.2020.3033537&partnerID=40&md5=c4a310f6659a91dbabb9682ccb025700,"The usage of transportation systems is inevitable; any assistance module which can catalyze the flow involved in transportation systems, parallelly improving the reliability of processes involved is a boon for day-to-day human lives. This paper introduces a novel, cost-effective, and highly responsive Post-active Driving Assistance System, which is ""Adaptive-Mask-Modelling Driving Assistance System"" with intuitive wide field-of-view modeling architecture. The proposed system is a vision-based approach, which processes a panoramic-front view (stitched from temporal synchronous left, right stereo camera feed) & simple monocular-rear view to generate robust & reliable proximity triggers along with co-relative navigation suggestions. The proposed system generates robust objects, adaptive field-of-view masks using FRCNN+Resnet-101_FPN, DSED neural-networks, and are later processed and mutually analyzed at respective stages to trigger proximity alerts and frame reliable navigation suggestions. The proposed DSED network is an Encoder-Decoder-Convolutional-Neural-Network to estimate lane-offset parameters which are responsible for adaptive modeling of field-of-view range (1570-2100) during live inference. Proposed stages, deep-neural-networks, and implemented algorithms, modules are state-of-the-art and achieved outstanding performance with minimal loss(L{p, t}, Lδ, LTotal) values during benchmarking analysis on our custom-built, KITTI, MS-COCO, Pascal-VOC, Make-3D datasets. The proposed assistance-system is tested on our custom-built, multiple public datasets to generalize its reliability and robustness under multiple wild conditions, input traffic scenarios & locations. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Adaptive field of view modeling; Automotive applications; Driving assistance systems; Lane detection and analysis; Object detection and tracking; Spatial auto-correlation; Air navigation; Benchmarking; Computer architecture; Convolutional neural networks; Cost effectiveness; Deep neural networks; Intelligent vehicle highway systems; Man machine systems; Stereo image processing; Stereo vision; Driving assistance systems; Model architecture; Processing architectures; Relative navigation; Reliability and robustness; Transportation system; Vision-based approaches; Wide field of view; Network architecture
Scopus,"Zhu, Y.; Wang, H.; Qu, Z.",Detection method for slow and small target image based on multi-frame accumulation filter,,2017,,,,10.3969/j.issn.1001-506X.2017.08.05,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027706205&doi=10.3969%2fj.issn.1001-506X.2017.08.05&partnerID=40&md5=902623828b52685fbcdc7a1b8bc121f2,"Aiming at the problem that slow and small target (SST) could not be detected continuously in the radar adjacent frames, resulting in radar loss of target, a detection method for SST based on image multi-frame accumulation filter is proposed. First, the interferences between the fixed clutter and the SST are removed by using the inter-frame difference technique. Then, the parameters of the minimum circumscribed rectangle, consisting of F3, F2 and F1, are introduced as the characteristic factors of the filter, and these parameters are extracted in the accumulation region by the image marking technique. Finally, the slow moving clutter is removed in the target by controlling the characteristic factor, which improves the detection performance of the radar to the SST. Simulation results verify the effectiveness of the proposed algorithm. © 2017, Editorial Office of Systems Engineering and Electronics. All right reserved.",Characte-ristic factor; Inter-frame difference technique; Multi-frame accumulation; Slow and small target (SST); Bandpass filters; Clutter (information theory); Radar; Radar imaging; Adjacent frames; Characte-ristic factor; Characteristic factors; Detection methods; Detection performance; Inter-frame differences; Multi-frame; Small targets; Tracking radar
Scopus,"Mannan, S.","Lees' Loss Prevention in the Process Industries: Hazard Identification, Assessment And Control: Fourth Edition",,2012,,,,10.1016/C2009-0-24104-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041156174&doi=10.1016%2fC2009-0-24104-3&partnerID=40&md5=01960fdac03a4ca40047a41013ab9203,"Safety in the process industries is critical for those who work with chemicals and hazardous substances or processes. The field of loss prevention is, and continues to be, of supreme importance to countless companies, municipalities and governments around the world, and Lees' is a detailed reference to defending against hazards. Recognized as the standard work for chemical and process engineering safety professionals, it provides the most complete collection of information on the theory, practice, design elements, equipment, regulations and laws covering the field of process safety. An entire library of alternative books (and cross-referencing systems) would be needed to replace or improve upon it, but everything of importance to safety professionals, engineers and managers can be found in this all-encompassing three volume reference instead. THE process safety encyclopedia, trusted worldwide for over 30 years. Now available in print and online, to aid searchability and portability. Over 3,600 print pages cover the full scope of process safety and loss prevention, compiling theory, practice, standards, legislation, case studies and lessons learned in one resource as opposed to multiple sources. © 2012 Elsevier Inc. All rights reserved.",Chemical equipment; Chemical hazards; Chemical substance; Engineering safety; Hazard Assessment; Hazard control; Hazard identification; Hazardous process; Hazardous substances; Process industries; Process safety; Safety professionals; Loss prevention
Scopus,"Nissan, E.","Computer applications for handling legal evidence, police investigation and case argumentation",,2012,,,,10.1007/978-90-481-8990-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862219121&doi=10.1007%2f978-90-481-8990-8&partnerID=40&md5=585596d82e0d9ff045f310ef26818e65,"This book provides an overview of computer techniques and tools - especially from artificial intelligence (AI) - for handling legal evidence, police intelligence, crime analysis or detection, and forensic testing, with a sustained discussion of methods for the modelling of reasoning and forming an opinion about the evidence, methods for the modelling of argumentation, and computational approaches to dealing with legal, or any, narratives. By the 2000s, the modelling of reasoning on legal evidence has emerged as a significant area within the well-established field of AI & Law. An overview such as this one has never been attempted before. It offers a panoramic view of topics, techniques and tools. It is more than a survey, as topic after topic, the reader can get a closer view of approaches and techniques. One aim is to introduce practitioners of AI to the modelling legal evidence. Another aim is to introduce legal professionals, as well as the more technically oriented among law enforcement professionals, or researchers in police science, to information technology resources from which their own respective field stands to benefit. Computer scientists must not blunder into design choices resulting in tools objectionable for legal professionals, so it is important to be aware of ongoing controversies. A survey is provided of argumentation tools or methods for reasoning about the evidence. Another class of tools considered here is intended to assist in organisational aspects of managing of the evidence. Moreover, tools appropriate for crime detection, intelligence, and investigation include tools based on link analysis and data mining. Concepts and techniques are introduced, along with case studies. So are areas in the forensic sciences. Special chapters are devoted to VIRTOPSY (a procedure for legal medicine) and FLINTS (a tool for the police). This is both an introductory book (possibly a textbook), and a reference for specialists from various quarters. © Springer Science+Business Media Dordrecht 2012.",Crime; Data mining; Surveys; Computational approach; Computer scientists; Computer techniques; Organisational aspects; Panoramic views; Police intelligences; Police investigations; Techniques and tools; Forensic science
Scopus,"Belaroussi, R.; Gruyer, D.",Convergence of a Traffic Signs-Based Fog Density Model,,2015,,,,10.1109/ITSC.2015.132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950272967&doi=10.1109%2fITSC.2015.132&partnerID=40&md5=c4dfb7b083695d75fd4cdea85152c037,"Multiple-vehicle accidents caused by reduced visibility conditions occur during localized fog and often result in fatalities and injuries. The extreme variability in density, predictability, and location of the hazard further complicates the task of improving highway safety conditions. This paper presents a novel approach for estimating visibility condition using an onboard camera and a digital map encoding traffic signs informations. Ability to respond in a timely fashion to sudden local change in visibility conditions is specifically investigated. © 2015 IEEE.",Accidents; Highway engineering; Intelligent systems; Intelligent vehicle highway systems; Transportation; Visibility; Density modeling; Digital map; Highway safety; Onboard camera; Reduced visibility; Vehicle accidents; Visibility conditions; Traffic signs
Scopus,"John, V.; Nithilan, M.K.; Mita, S.; Tehrani, H.; Konishi, M.; Ishimaru, K.; Oishi, T.",Sensor Fusion of Intensity and Depth Cues using the ChiNet for Semantic Segmentation of Road Scenes,,2018,,,,10.1109/IVS.2018.8500476,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056777877&doi=10.1109%2fIVS.2018.8500476&partnerID=40&md5=311c1dc179fe10800dca0d717af4ac36,"Vision-based environment perception is an important research topic for autonomous driving and advanced driver assistance systems. Vision sensors, such as the monocular camera and stereo camera, are widely used for environment perception. The monocular camera provides the appearance information like intensity, and the stereo camera provides the depth information. The appearance and depth information are complementary, and their effective fusion would result in robust environment perception. Consequently, in this paper, we propose a novel deep learning framework, termed as the ChiNet, for the effective sensor fusion of the appearance and depth information for free space and road object estimation. The ChiNet has two input branches and two output branches. The ChiNet input branches contains separate branches for the intensity and depth information. For the output branches, the ChiNet contains separate branches for the free space and road object semantic segmentation. A comparative of the proposed framework with state-of-the-art baseline algorithms is performed using an acquired dataset. Moreover, a detailed parameter analysis is performed to validate the ChiNet architecture as well as the advantages of sensor fusion. The experimental results show that the ChiNet is better than baseline algorithms. We also show that the proposed ChiNet architecture is better than other variations of the ChiNet architecture. © 2018 IEEE.",Automobile drivers; Cameras; Deep learning; Roads and streets; Semantics; Stereo image processing; Stereo vision; Autonomous driving; Depth information; Environment perceptions; Learning frameworks; Monocular cameras; Object estimation; Parameter analysis; Semantic segmentation; Advanced driver assistance systems
ACM,,Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,,2015,,,,,,,
IEEE,L. Li; M. Fang; Y. Yin; J. Lian; Z. Wang,A Traffic Scene Object Detection Method Combining Deep Learning and Stereo Vision Algorithm,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),2021,,,1134-1138,10.1109/RCAR52367.2021.9517460,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517460,"Object detection has been an important topic in the field of intelligent vehicle. The deep learning object detection is based on monocular vision without depth of the scene information and the detection of small objects in complex traffic scenes often has problems such as misdetection and omission, so the detection of small objects in traffic scenes is still a big challenge. In this paper, a method combining depth information with YOLOv5s object detection algorithm is proposed to improve the accuracy of object detection. Firstly, the depth information is obtained by the disparity images which generated by the end-to-end PSMNet network. Secondly, add an Attention Feature Fusion Module (AFFM) to YOLOv5s to improve the accuracy of small object detection. Finally, the depth information is fused with the object detection to reduce the probability of missed detection and false detection, and the distance information is also obtained. The experimental results show that the SUPER_YOLOv5s object detection algorithm combined with stereo vision can reduce the rate of missed detection and improve the detection accuracy.",
IEEE,H. Naimi; T. Akilan; M. A. S. Khalid,Fast Traffic Sign and Light Detection using Deep Learning for Automotive Applications,2021 IEEE Western New York Image and Signal Processing Workshop (WNYISPW),2021,,,1-5,10.1109/WNYISPW53194.2021.9661284,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661284,"Traffic sign and light detections are core components of Advanced Driver Assistance Systems (ADAS) and self-driving vehicles. To this end, the automotive industry is widely exploiting computer vision (CV) and deep learning (DL) techniques. This paper presents a lightweight traffic sign and light detector by harnessing a single-stage, single-shot multi-object detector (SSD). For accelerating the inference speed of the detector, its original backbone, VGG16 is replaced by MobileNet V2 that expertly manages detection speed and network size. In autonomous driving, quicker detection performance with respect to the distance of an object is of particular interest, for a comfortable braking. However, farther distance makes the objects to be detected appear smaller. Unfortunately, the original SSD struggles to detect small objects. Thus, this work further optimizes the number of feature map layers of the SSD for the detection of small objects along with a better trade-off between detection precision and inference time. Experimental analysis confirms the effectiveness of the proposed model, which achieves 2 times (or more) faster detection time than the baseline SSD models and a competitive precision of 76.7%.",Computer vision;object detection;ADAS;deep learning
IEEE,M. Sukkar; R. Jadeja; M. Shukla; R. Mahadeva,A Survey of Deep Learning Approaches for Pedestrian Detection in Autonomous Systems,IEEE Access,2025,13.0,,3994-4007,10.1109/ACCESS.2024.3524501,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818658,"This paper surveys real-time object detection literature critically and analytically, focusing particularly on pedestrian detection for safe autonomous vehicles. It addresses the challenges in the domain, some of the sources of which are variations in age, gender, clothing, lighting, backgrounds, and occlusion. The paper reviews object detection algorithms after providing an overview of deep learning basics and main architectures of neural networks, followed by discussion on existing algorithms along with their strengths, weaknesses, and future research directions. There is a need for pedestrian detection datasets with further complex annotations and multi-source integration, which captures interactions between pedestrians and their surroundings. Incorporating advanced sensors, including LiDAR, infrared, and depth sensors, as the foremost means to enhance the detection capabilities in more adverse conditions, such as low-light situations and occlusion. However, architectures such as YOLO, SSD, and Faster R-CNN, which have led to current improvements in performance, still allow room for improving pedestrian detection accuracy. By filling in these insights and proposed solutions, the paper focus on the development of pedestrian detection technology, how it can be brought into a safer, reliable, real-world applicability towards the system of autonomous driving. All of these results point to continued innovation towards deep learning, multi-sensor integration, and developing datasets to achieve optimal performance levels in real world conditions for autonomous driving systems.",Artificial intelligence;autonomous vehicle;computer vision;deep learning;pedestrian detection
IEEE,X. Qu; Y. Zheng; Y. Zhou; Z. Su,YOLO v8_CAT: Enhancing Small Object Detection in Traffic Light Recognition with Combined Attention Mechanism,2024 10th International Conference on Computer and Communications (ICCC),2024,,,706-710,10.1109/ICCC62609.2024.10941967,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10941967,"This paper introduces YOLO v8_CAT, an advanced object detection model designed to improve the accuracy of small and challenging object detection in traffic light recognition tasks. Traditional object detection models, including earlier YOLO versions, often struggle with accurately detecting small objects and differentiating traffic light states (GREEN, RED, YELLOW, and OFF) due to limited feature refinement capabilities. YOLO v8_CAT enhances the baseline YOLOvS model by incorporating a Combined Attention Mechanism (CAT), which integrates channel and spatial attention paths to prioritize critical features. The channel attention path focuses on essential feature channels, while the spatial attention path emphasizes important regions, allowing YOLO v8_CAT to handle small and complex objects more effectively. Evaluated on the Bosch Small Traffic Lights Dataset, YOLO v8_CAT demonstrates significant performance improvements, achieving 84.5% accuracy for GREEN, 63.8% for RED, 21.5% for YELLOW, and 25.8% for OFF, outperforming YOLOv8 by notable margins. In particular, YOLO v8_CAT shows a 12.4% increase in accuracy for GREEN and an 8% improvement for RED detection. Additionally, YOLO v8_CAT maintains the high real-time processing speed of YOLOv8 at 1010 FPS. These results establish YOLO v8_CAT as a superior model for traffic light detection in autonomous driving, enhancing both detection precision and robustness without compromising computational efficiency.",deep learning;object detection;YOLO v8;Traffic Lights detection
IEEE,Z. Tang; L. Zhu; W. Wang; Y. Gong,PE-SSD: Improved SSD for Small Object Detection,2024 China Automation Congress (CAC),2024,,,383-389,10.1109/CAC63892.2024.10865724,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10865724,"With the flourishing development of deep learning, object detection technology has achieved encouraging results. However, small objects remain a major challenge in the field of object detection due to their lack of texture and contextual information caused by their small size. Despite such fact, we observed that their edges typically exhibit high contrast in relation to the background. Specifically, we innovatively employ a Laplacian image pyramid incorporated with feature enhancement modules rather than the traditional image pyramid, with the aim of more effectively capturing image details and edge information, given that the Laplacian pyramid has the ability to retain such information within its lower levels. Furthermore, we introduce attention mechanisms to optimize the feature fusion process, thus improving the utilization efficiency of multi-scale features. Taking SSD as the baseline, we propose a novel method called PE-SSD, which integrates the advantages of featureized Laplacian pyramid. Experimental results show that our method achieves higher accuracy in detecting small objects compared to traditional SSD. For inputs of size 512 × 512, the extensive experiments on the VisDrone2019 dataset demonstrates that our method achieves an average precision of 15.4%, surpassing traditional SSD networks by 1.2%. Specifically, The mAP_s score reaches 6.6%, which is 0.4% higher than the baseline network, indicating the effectiveness of our method in the field of small object detection.",Small Object Detection;Laplacian Pyramid;Feature Pyramid;Feature Fusion;Attention Module
IEEE,E. Kelmendi; K. Jashari; D. Samanta,Deep Neural Networks: Improved Object Detection for Autonomous Vehicles,2025 International Conference on Intelligent and Cloud Computing (ICoICC),2025,,,1-6,10.1109/ICoICC64033.2025.11052045,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052045,"Deep neural networks may have the potential to revolutionize the way an autonomous vehicle perceives the world through methodologies such as object detection. This will require the interaction of neural networks of different kinds to equip the vehicle with capabilities to identify, for example, signage, images, and other entities associated with vehicles, thereby improving transport efficiency. According to many researchers, deep neural networks do not provide varied performance and low accuracy in identification of images. The study shows challenges that number mock image recognition neural networks thus trying to fill up the gap of integration of different neural networks with an autonomous vehicle system. General methods of the study consist of careful observation of the already existing literature, comparative analysis of deep learning networks, and interpretive figures and tables. As much of the study goes on, so many findings about how autonomous vehicles are added to neural networks for deception detection and optimization through the merging of different deep networks have emerged. Finally, the outputs of this paper constitute one of the very critical aspects of the automotive industry, the technology area of sensors, and artificial intelligence.",Neural Networks;Autonomous Vehicles;Object detection;Artificial Intelligence;Image-to-Image Networks
IEEE,X. Shen; V. V. Lukyanov,An Improved Lightweight Network for Real-Time Detection of Potential Risks for Autonomous Vehicles,2024 International Russian Automation Conference (RusAutoCon),2024,,,583-588,10.1109/RusAutoCon61949.2024.10694343,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10694343,"In this study, we explore advancements in 2D object detection, focusing on the well-regarded KITTI dataset used extensively in autonomous driving research. We present YOLOv8, the latest iteration of cutting-edge single-stage detectors, and detail our network enhancements designed to boost its performance. Our method integrates the BiFPN (Bifurcated Feature Pyramid Network), optimizing the fusion of semantic and localization information crucial for precise object detection. We also add a specialized tiny object detection layer to effectively handle the challenges of small-scale object detection. To further improve our model, we employ extensive dataset augmentation techniques during training, which greatly enhance the network's robustness and generalization abilities. Additionally, we introduce the MCA (Multidimensional Collaborative Attention) module, a sophisticated attention mechanism that significantly improves feature extraction. Through thorough ablation studies, we validate the effectiveness of our enhancements, especially in detecting small targets within the KITTI dataset. This comprehensive research highlights the significance of state-of-the-art technologies like YOLOv8 and demonstrates the potential of our proposed modifications to advance the area of 2D visual object recognition. And detection",autonomous driving;object detection;KITTI dataset;YOLO networks;attention mechanism
IEEE,S. Kim; S. -Y. Park,Expandable Spherical Projection and Feature Fusion Methods for Object Detection from Fisheye Images,2021 17th International Conference on Machine Vision and Applications (MVA),2021,,,1-5,10.23919/MVA51890.2021.9511379,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511379,"One of the key requirements for enhanced autonomous driving systems is accurate detection of the objects from a wide range of view. Large-angle images from a fisheye lens camera can be an effective solution for automotive applications. However, it comes with the cost of strong radial distortions. In particular, the fisheye camera has a photographic effect of exaggerating the size of objects in central regions of the image, while making objects near the marginal area appear smaller. Therefore, we propose the Expandable Spherical Projection that expands center or margin regions to produce straight edges of de-warped objects with less unwanted background in the bounding boxes. In addition to this, we analyze the influence of multi-scale feature fusion in a real-time object detector, which learns to extract more meaningful information for small objects. We present three different types of concatenated YOLOv3-SPP architectures. Moreover, we demonstrate the effectiveness of our proposed projection and feature-fusion using multiple fisheye lens datasets, which shows up to 4.7% AP improvement compared to fisheye images and baseline model.",
IEEE,L. Ding,Applying the Improved YOLOv7 Algorithm to Improve the Real-time Object Detection and Obstacle Avoidance Capabilities of Smart Cars,2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA),2025,,,1559-1563,10.1109/AIITA65135.2025.11047665,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11047665,"To solve the problem of missed detection of small targets and real-time obstacle avoidance of smart cars, this study introduces an improved YOLOv7 (You Only Look Once Version 7) algorithm. First, the channel pruning and quantization compression model is used, and the E-ELAN (Enhanced Efficient Layer Aggregation Network) module is optimized to enhance the fusion of small target features; DSNT (Differentiable Spatial to Numerical Transform) distance prediction, dynamic NMS (Non-Maximum Suppression) optimization and trajectory prediction are integrated to achieve efficient obstacle avoidance warning. The results show that the model has a small target mAP@0.5 (mean Average Precision) of 0.82 in the KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) dataset, an inference speed of 29.5 FPS (Frames Per Second) for traffic signs, and a 25.3% reduction in video memory usage, verifying its practical value in complex scenarios.",You Only Look Once Version 7;Intelligent Vehicle;Small Object Detection;Real-Time Obstacle Avoidance
IEEE,T. Ye; X. Zhang; Y. Zhang; J. Liu,Railway Traffic Object Detection Using Differential Feature Fusion Convolution Neural Network,IEEE Transactions on Intelligent Transportation Systems,2021,22.0,3.0,1375-1387,10.1109/TITS.2020.2969993,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978612,"Railway shunting accidents, in which trains collide with obstacles, often occur because of human error or fatigue. It is therefore necessary to detect traffic objects in front of the trains and inform the driver to take timely action. To detect these objects in railways, we proposed an object-detection method using a differential feature fusion convolutional neural network (DFF-Net). DFF-Net includes two modules: the prior object-detection module and the object-detection module. The prior module produces initial anchor boxes for the subsequent detection module. Taking the initial anchor boxes as input, the object-detection module applies a differential feature fusion sub-module to enrich the sematic information for object detection, enhancing the detection performance, particularly for small objects. In experiments conducted on a railway traffic dataset, compared with the current state-of-the-art detectors, the proposed method exhibited significant higher performance and was more effective and more efficient than the other methods for object detection in railway tracks. Additionally, evaluation results based on PASCAL VOC2007 and VOC2012 indicated that the proposed method was significantly better than the state-of-the-art methods.",Railway traffic object detection;differential feature fusion convolutional neural network;prior module
IEEE,J. Kaszubiak; M. Tornow; R. W. Kuhn; B. Michaelis; C. Knoeppel,Real-time vehicle and lane detection with embedded hardware,"IEEE Proceedings. Intelligent Vehicles Symposium, 2005.",2005,,,619-624,10.1109/IVS.2005.1505172,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1505172,"For autonomously acting robots and driver assistance systems powerful optical stereo sensor systems are required. Object positions and environmental conditions have to be acquired in real-time. In this paper an algorithm based on a hardware-software co-design is applied. A depth-map is generated with a hierarchical detection method. A depth-histogram is generated by using the density distribution of the disparity in the depth-map. It is used for object detection. The object clustering can be accomplished without calculation of 3D-points, due to the almost identical mapping of the objects over the whole distance, within the histogram. A lane detection is applied by using a Hough transform. The suitability at night and the detection of small objects like bikers is proven.",
IEEE,X. Liu; L. Liu; T. Liu,YOLO-CCA: An Encoder-Decoder Framework Vehicle Detector Based on Channel Attention,"2023 IEEE 5th International Conference on Power, Intelligent Computing and Systems (ICPICS)",2023,,,742-747,10.1109/ICPICS58376.2023.10235599,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235599,"Vehicle detection (VD), aiming at detecting the position of vehicles, can largely promote the development of connected and autonomous vehicles and intelligent transportation systems. Most VD algorithms cannot meet the demand of real-time road monitoring, due to some unsolved problems: (i) low accuracy for small vehicle targets that is demanding for real-time vehicle monitoring; (ii) high computational cost that is hard to deploy in edge devices. To solve these problems, this paper introduces a novel VD algorithm called YOLO-CCA inspired by channel attention technique. Efficient Channel Attention (ECA) and Encoder-Decoder modules are adopted into YOLO-CCA algorithm for VD. With the introduction of the Complete-IoU (CIoU) loss function, the convergence rate is also accelerated. YOLO-CCA algorithm is capable of capturing cross-channel information and maximizing it, which leads to high accuracy for vehicle detection, especially for small vehicle targets detection. Extensive experiments based on the UA-DETRAC public dataset demonstrate that YOLO-CCA outperforms the other four baseline algorithms, and improve 5.6% to the best baseline algorithm in small vehicle targets detection.",Vehicle detection;Connected and autonomous vehicles;computer vision;Intelligent transportation systems
IEEE,T. Chen; Y. Yuan; B. Yin; Y. Liao,SP-Pillars: An Efficient LiDAR 3D Objects Detection Framework With Multi-Scale Feature Perception and Optimization,IEEE Access,2025,13.0,,74092-74106,10.1109/ACCESS.2025.3564665,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10978003,"In autonomous driving, achieving rapid detection of target categories and locations is a key technology. However, the data volume of radar point clouds is enormous, and processing efficiency becomes a limiting factor, so the balance between speed and accuracy is crucial. To address this challenge, this paper proposes a 3D object detection algorithm SP-Pillars that can effectively learn point cloud features. Firstly, a Pillar Feature Weighted Network (PFWNet) is proposed for processing point cloud information, which divides the point cloud into pillar structures and uses SPCV feature attention network to focus on its multi-level feature information. After feature extraction and dimensionality reduction, pseudo images are generated. Subsequently, before extracting pseudo image features, a multi-core perception network (PKINet) is introduced to further mine local contextual information and reduce computational complexity, enabling the backbone network to effectively learn features. The experimental evaluation results on the KITTI dataset indicate that the proposed algorithm is reliable and effective. Compared with other related algorithms, this algorithm exhibits excellent detection performance, slightly improving detection speed while maintaining high accuracy, meeting the requirements of real-time processing, and has important application value in optimizing autonomous driving technology.",3D object detection;autonomous driving;feature weighting;multi-scale;point cloud;residual structure
IEEE,Z. Cai; R. Chen; Z. Wu; W. Xue,YOLOv8n-FAWL: Object Detection for Autonomous Driving Using YOLOv8 Network on Edge Devices,IEEE Access,2024,12.0,,158376-158387,10.1109/ACCESS.2024.3480976,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10718290,"In the field of autonomous driving, common challenges include difficulties in detecting small vehicles and pedestrians on the road, high computational demands of algorithms, and low accuracy of detection algorithms. This paper proposes a YOLOv8n-FAWL object detection algorithm tailored for edge computing, incorporating the following three improvements: (1) The Faster-C2f-EMA module is created, designed through the synergy of the FasterNet architecture and the concept of EMA modules, effectively addressing the challenge of suboptimal feature extraction for small objects. (2) The WIOU loss function is adopted to resolve the issue of imbalanced training samples. (3) The LAMP pruning technique is applied to reduce the model parameters and complexity, thereby enhancing the overall model accuracy. The experimental results show that compared to the baseline model, the proposed algorithm achieves improvements of 6.2% and 4.5% in the mAP@0.5, and 3.8% and 2.7% in the mAP@0.5:0.95, on the Udacity and BDD100K-tiny datasets,respectively. In addition, the model parameters we’re reduced by 49.2% and 46%. The model achieved real-time performance at 54 FPS, thereby advancing the development of autonomous driving technology.",YOLOv8n;autonomous driving;LAMP;FasterNet;EMA;WIOU
IEEE,Y. Niu; J. Zhang,YOLOP-MVF: A Multi-Task Autonomous Driving Perception Detection Method Based on Multi Scale Feature Weighted Fusion,IEEE Access,2025,13.0,,91374-91383,10.1109/ACCESS.2025.3572331,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11008744,"To address challenges such as large-scale variations, background interference, and occlusions in multi-task autonomous driving perception, this paper proposes YOLOP-MVF, a multi-task detection framework based on multi-scale feature weighting fusion. The model integrates a sub-pixel 3D fusion module and a triple feature encoding module to enhance the representation of multi-scale features. A multi-scale convolutional attention-weighting mechanism is further introduced to adaptively emphasize critical spatial information. To improve feature extraction flexibility, deformable convolutions are incorporated, enabling dynamic sampling based on input characteristics. Additionally, the Powerful-IoU loss is employed to guide anchor box regression with adaptive penalty and gradient regulation, accelerating convergence. Experimental results on the BDD100K dataset demonstrate that YOLOP-MVF outperforms baseline models, achieving improvements of 1.2% in mIoU, 8.8% in accuracy, and 4.7% in mAP50, validating its effectiveness for robust multi-task perception in complex driving scenarios.",Autonomous driving;multitask learning;drivable area segmentation;lane detection;vehicle detection
IEEE,J. Ding; W. Li; L. Pei; M. Yang; A. Tian; B. Yuan,Novel Pipeline Integrating Cross-Modality and Motion Model for Nearshore Multi-Object Tracking in Optical Video Surveillance,IEEE Transactions on Intelligent Transportation Systems,2024,25.0,9.0,12464-12476,10.1109/TITS.2024.3373370,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10474332,"Nearshore multi-object tracking (NMOT) aims to locat and identify nearshore objects. Most approaches accomplish this task using radar and remote-sensing technologies. In contrast, video data can describe the visual appearance of nearshore objects without prior information, such as identity, location, or movement. In this study, we introduce a cross-modality pipeline to address the four major challenges of NMOT. First, we propose introducing a cross-modality bi-attention transformer (CBT) manage the information interaction between RGB and thermal infrared videos effectively. This decoupling and guidance mechanism laid the foundation for our subsequent processes. Next, we integrate the outputs of the backbone with historical frames to extract crucial temporal features. Subsequently, we refine small object detection performance by employing multi-scale feature alignment (MFA). Observations are generated by the transformer decoder. To tackle challenges arising from extensive occlusion and interactions induced by waves in NMOT, we propose guiding modulation (GM), supplemented by low-confidence boxes and multi-point corner momentum (MCM) to facilitate association. Our approach is simple, online, and real-time, showcasing outstanding performance in benchmark evaluations. The open-source implementation of our work is available at https://github.com/Ding-JianGang/Cross-Modality-MOT-in-Nearshore-Environments.",Intelligent nearshore transportation;multi-object tracking;cross-modality;temporal context;multi-scale feature alignment;guiding modulation
IEEE,A. A. U. Rakhmonov; B. Subramanian; T. Kim; J. Kim,Airy YOLOv5 for Disabled Sign Detection,2023 Fourteenth International Conference on Ubiquitous and Future Networks (ICUFN),2023,,,869-874,10.1109/ICUFN57995.2023.10200853,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200853,"Designated parking spaces for individuals with disabilities are only meant to be used by vehicles with proper handicapped signage. Real-time monitoring is necessary to ensure that only authorized vehicles are parked in these spaces and to prevent unauthorized vehicles from using them. First, this research proposes to replace the backbone of a baseline YOLOv5 model which has 9 blocks with 6 EfficientNet blocks with less parameters but still have a higher accuracy in detecting disabled signs among other signages on the windshield of cars. Second, to compensate for the loss of blocks we have included an attention mechanism before detection part in our architecture which allows us to focus on the important regions needed for the task. Additionally, we propose to use a better optimizer AdamW to prevent overfitting. Based on these improvements, we have created a new object detector named Airy YOLOv5. To evaluate the effectiveness of our proposed method, a dataset containing images of cars with disabled signage on their windshields will be gathered and labeled. Experiments using this dataset show that our model achieves a better F1 score of 0.67 with 5 percent less parameters compared to the baseline model.",depthwise separable convolution;disabled signage;small object detection;supervised learning
IEEE,S. Sivanandham; D. Gunaseelan,Enhanced YOLOv5s Model for Improved Multi-Sized Object Detection in Road Scenes,IEEE Access,2025,13.0,,110110-110127,10.1109/ACCESS.2025.3582136,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11048496,"Detecting objects in complex driving environments is crucial for autonomous vehicles to navigate safely. However, this task becomes challenging when addressing scale variations, occlusions and diverse backgrounds. This paper proposes an enhanced YOLOv5s model for handling varying object sizes from small pedestrians and traffic signs to larger vehicles in road scenes. The proposed enhancement begins by refining the default anchor boxes using the percentile-based quantile method on the distribution of the bounding boxes and the adjustments to the convolution layers for enhanced feature extraction. Smaller kernel sizes and fewer channels are employed in the initial layers to capture fine-grained details, while in deeper layers, the number of channels is progressively increased to capture broader information that better represents larger objects. Furthermore, an efficient channel attention (ECA) mechanism is integrated into the backbone to prioritize key feature channels, thereby enhancing the model’s ability to detect overlapping and small objects. To improve the feature fusion process, a Multi-scale BiFPN block is integrated into the neck of the model. This combines fine-grained spatial details from the shallow layers with more abstract semantic information from deeper layers, enabling the detection of objects across varying scales. Experimental evaluations carried out on the IDD dataset reveal that the enhanced YOLOv5s model achieves a significant gain in prediction accuracy when compared with the original YOLOv5s. To mitigate the effect of class imbalance and improve generalization across varying object sizes, CutMix data augmentation is employed during training. It shows a 48% increase in mean average precision (mAP@0.5) and a 44% and 49% rise in precision and recall, respectively, with an inference time of 14.6ms compared to the baseline model. These improvements underscore the effectiveness of the proposed enhancements in addressing the challenges of detecting multi-sized objects in complex road environments.",Anchor box refinement;attention mechanism;CutMix data augmentation;feature fusion;multi-sized object detection;road scenes;YOLOv5
