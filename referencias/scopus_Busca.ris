TY  - CONF
AU  - Mo, L.
AU  - Leng, S.
AU  - Luo, Z.
AU  - Ou, D.
AU  - Lin, X.
TI  - A study of unmanned deep learning target detection
PY  - 2025
T2  - Proceedings of 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management, ICBAR 2024
DO  - 10.1145/3718751.3718803
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007599085&doi=10.1145%2f3718751.3718803&partnerID=40&md5=ec79a41ddb51788f4841fbc458ec2591
AB  - Aiming at the technical problems of dynamic adjustment and path planning in the process of unmanned target detection, deep learning, target dataset, and target dataset-based detection model research were carried out from three aspects, namely, detection principle, model training, and model experimentation, and technical methods such as deep learning, neural network, and YOLOv5 were applied to achieve environment sensing and target detection. The YOLOv5 target detection model was applied to conduct unmanned deep learning target detection experiments, and the results show that the YOLOv5 target detection model based on the ArgoVerse training set is feasible as a technical solution for unmanned deep learning target detection system. © 2024 Copyright held by the owner/author(s).
KW  - Deep learning
KW  - Self-driving
KW  - Target detection
KW  - YOLO
KW  - Deep learning
KW  - Detection models
KW  - Dynamic adjustment
KW  - Dynamic paths
KW  - Model experimentations
KW  - Model training
KW  - Principle modeling
KW  - Self drivings
KW  - Targets detection
KW  - YOLO
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, B.
AU  - Luo, P.
AU  - Yang, Y.
AU  - Zhao, Z.
AU  - Dong, R.
AU  - Guan, Y.
TI  - A Review and Prospect of Cybersecurity Research on Air Traffic Management Systems
ST  - 空中交通管理系统网络安全研究综述与展望
PY  - 2025
T2  - Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology
DO  - 10.11999/JEIT240966
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007507863&doi=10.11999%2fJEIT240966&partnerID=40&md5=5fcc7db825f56a27cfc3ebf41acc473b
AB  - Significance The air traffic management system is a critical national infrastructure that impacts both aerospace security and the safety of lives and property. With the widespread adoption of information, networking, and intelligent technologies, the modern air traffic management system has evolved into a space-air-ground-sea integrated network, incorporating heterogeneous systems and multiple stakeholders. The network security of the system can no longer be effectively ensured by device redundancy, physical isolation, security by obscurity, or human-in-the-loop strategies. Due to the stringent requirements for aviation airworthiness certification, the implementation of new cybersecurity technologies is often delayed. New types of cyberattacks, such as advanced persistent threats and supply chain attacks, are increasingly prevalent. Vulnerabilities in both hardware and software, particularly in embedded systems and industrial control systems, are continually being exposed, widening the attack surface and increasing the number of potential attack vectors. Cyberattack incidents are frequent, and the network security situation remains critical. Progress The United States’ Next Generation Air Transportation System (NextGen), the European Commission’s Single European Sky Air Traffic Management Research (SESAR), and the Civil Aviation Administration of China have prioritized cybersecurity in their development plans for next-generation air transportation systems. Several countries and organizations, including the United States, Japan, China, the European Union, and Germany, have established frameworks for the information security of air traffic management systems. Although network and information security for air traffic management systems is gaining attention, many countries prioritize operational safety over cybersecurity concerns. Existing security specifications and industry standards are limited in addressing network and information security. Most of them focus on top-level design and strategic directions, with insufficient attention to fundamental theories, core technologies, and key methodologies. Current review literature lacks a comprehensive assessment of assets within air traffic management systems, often focusing only on specific components such as aircraft or airports. Furthermore, research on aviation information security mainly addresses traditional concerns, without fully considering the intelligent and dynamic security challenges facing next-generation air transportation systems. Conclusions This paper comprehensively examines the complexity of the cybersecurity ecosystem in air traffic management systems, considering various entities such as electronic-enabled aircraft, communication, navigation, Surveillance/Air Traffic Management (CNS/ATM), smart airports, and intelligent computing. It focuses on asset categorization, information flow, threat analysis, attack modeling, and defense mechanisms, integrating dynamic flight phases to systematically review the current state of cybersecurity in air traffic management systems. Several scientific issues are identified that must be addressed in constructing a secure ecological framework for air traffic management. Based on the Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) model, this paper analyzes typical attack examples related to the four ecological entities (Figs. 7, 9, 12, and 14) and constructs an ATT&CK matrix for air traffic management systems (Fig. 15). Additionally, with the intelligent development goal of next-generation air transportation systems as a guide, ten typical applications of intelligent air traffic management are outlined (Fig. 13, Table 11), with a systematic analysis of the attack patterns and defense mechanisms of their intelligent algorithms (Tables 12, 13). These findings provide theoretical references for the development of smart civil aviation and the assurance of cybersecurity in China. Prospects Currently, the cybersecurity ecosystem of air traffic management systems is highly complex, with unclear mechanisms, indistinct boundaries for cybersecurity assets, and incomplete security assurance requirements. Moreover, there is a lack of comprehensive, systematic, and holistic cybersecurity design and defense mechanisms, which limits the ability to counter various subjective, human-driven, and emerging types of malicious cyberattacks. This paper highlights key research challenges in areas such as dynamic cybersecurity analysis, attack impact propagation modeling, human-in-the-loop cybersecurity analysis, and distributed intrusion detection systems. Cybersecurity analysis of air traffic management systems should be conducted within the dynamic operational environment of a space-air-ground-sea integrated network, accounting for the cybersecurity ecosystem and analyzing it across different spatial and temporal dimensions. As aircraft are cyber-physical systems, cybersecurity threat analysis should focus on the interrelated propagation mechanisms between security and safety, as well as their cascading failure models. Furthermore, humans serve as the last line of defense in cybersecurity. When performing threat modeling and risk assessment for avionics systems, it is crucial to fully incorporate “human-in-the-loop” characteristics to derive comprehensive and objective conclusions. Finally, the design, testing, certification, and updating of civil aviation avionics systems are constrained by strict airworthiness requirements, preventing the rapid implementation of advanced cybersecurity technologies. Distributed anomaly detection systems, however, currently represent an effective technical approach for combating cyberattacks in air traffic management systems. © 2025 Science Press. All rights reserved.
KW  - Air traffic management system
KW  - Cyber security
KW  - Research challenges
KW  - Research review
KW  - Air navigation
KW  - Airport vehicular traffic
KW  - Authentication
KW  - Helicopter services
KW  - Information management
KW  - Medium access control
KW  - Production control
KW  - Research and development management
KW  - Risk management
KW  - Sensitive data
KW  - Strategic planning
KW  - Supply chain management
KW  - Air Traffic Management
KW  - Air Traffic Management Systems
KW  - Air-traffic management system
KW  - Cyber security
KW  - Cyber-attacks
KW  - Defence mechanisms
KW  - Human-in-the-loop
KW  - Next-generation air transportation systems
KW  - Research challenges
KW  - Research review
KW  - Cyber attacks
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, H.
AU  - Zhang, Q.
AU  - Gong, Y.
AU  - Yao, F.
AU  - Xiao, P.
TI  - MDCFVit-YOLO: A model for nighttime infrared small target vehicle and pedestrian detection
PY  - 2025
T2  - PLOS ONE
DO  - 10.1371/journal.pone.0324700
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008297656&doi=10.1371%2fjournal.pone.0324700&partnerID=40&md5=3a0e8442e113260f4b7361390542ed72
AB  - An MDCFVit-YOLO model based on the YOLOv8 algorithm is proposed to address issues in nighttime infrared object detection such as low visibility, high interference, and low precision in detecting small objects. The backbone network uses the lightweight Repvit model, improving detection performance and reducing model weight through transfer learning. The proposed MPA module integrates multi-scale contextual information, capturing complex dependencies between spatial and channel dimensions, thereby enhancing the representation capability of the neural network. The CSM module dynamically adjusts the weights of feature maps, enhancing the model of sensitivity to small targets. The dynamic automated detection head DAIH improves the accuracy of infrared target detection by dynamically adjusting regression feature maps. Additionally, three innovative loss functions—focalerDIoU, focalerGIOU and focalerShapeIoU are proposed to reduce losses during the training process. Experimental results show that the detection accuracy of 78% for small infrared nighttime targets, with a recall rate of 58.6%, an mAP value of 67%. and a parameter count of 20.9M for the MDCFVit-YOLO model. Compared to the baseline model YOLOv8, the mAP increased by 6.4%, with accuracy and recall rates improved by 4.5% and 5.7%, respectively. This research provides new ideas and methods for infrared target detection, enhancing the detection accuracy and real-time performance. © 2025 Zhang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
KW  - Algorithms
KW  - Humans
KW  - Infrared Rays
KW  - Models, Theoretical
KW  - Neural Networks, Computer
KW  - Pedestrians
KW  - algorithm
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - diagnostic test accuracy study
KW  - female
KW  - human
KW  - infrared radiation
KW  - nerve cell network
KW  - pedestrian
KW  - transfer of learning
KW  - visibility
KW  - artificial neural network
KW  - theoretical model
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Şahin, D.
AU  - Torkul, O.
AU  - Şişci, M.
AU  - Diren, D.D.
AU  - Yılmaz, R.
AU  - Kibar, A.
TI  - Real-Time Classification of Chicken Parts in the Packaging Process Using Object Detection Models Based on Deep Learning
PY  - 2025
T2  - Processes
DO  - 10.3390/pr13041005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003719534&doi=10.3390%2fpr13041005&partnerID=40&md5=17e224354041d7279300df289e2c726e
AB  - Chicken meat plays an important role in the healthy diets of many people and has a large global trade volume. In the chicken meat sector, in some production processes, traditional methods are used. Traditional chicken part sorting methods are often manual and time-consuming, especially during the packaging process. This study aimed to identify and classify the chicken parts for their input during the packaging process with the highest possible accuracy and speed. For this purpose, deep-learning-based object detection models were used. An image dataset was developed for the classification models by collecting the image data of different chicken parts, such as legs, breasts, shanks, wings, and drumsticks. The models were trained by the You Only Look Once version 8 (YOLOv8) algorithm variants and the Real-Time Detection Transformer (RT-DETR) algorithm variants. Then, they were evaluated and compared based on precision, recall, F1-Score, mean average precision (mAP), and Mean Inference Time per frame (MITF) metrics. Based on the obtained results, the YOLOv8s model outperformed the other models developed with other YOLOv8 versions and the RT-DETR algorithm versions by obtaining values of 0.9969, 0.9950, and 0.9807 for the F1-score, mAP@0.5, and mAP@0.5:0.95, respectively. It has been proven suitable for real-time applications with an MITF value of 10.3 ms/image. © 2025 by the authors.
KW  - chicken parts
KW  - deep learning
KW  - image processing
KW  - object detection
KW  - reducing waste and costs
KW  - RT-DETR
KW  - YOLOv8
KW  - Image coding
KW  - Motion analysis
KW  - Optical flows
KW  - Poultry
KW  - Chicken part
KW  - Deep learning
KW  - Images processing
KW  - Objects detection
KW  - Packaging process
KW  - Real-time detection
KW  - Real-time detection transformer
KW  - Reducing costs
KW  - Reducing waste
KW  - You only look once version 8
KW  - Photointerpretation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, S.
AU  - Zhou, H.
AU  - Qian, L.
TI  - A multi-scale small object detection algorithm SMA-YOLO for UAV remote sensing images
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-92344-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000297089&doi=10.1038%2fs41598-025-92344-7&partnerID=40&md5=c3f1c2a079398ca95f659fa98c6a2e3c
AB  - Detecting small objects in complex remote sensing environments presents significant challenges, including insufficient extraction of local spatial information, rigid feature fusion, and limited global feature representation. In addition, improving model performance requires a delicate balance between improving accuracy and managing computational complexity. To address these challenges, we propose the SMA-YOLO algorithm. First, we introduce the Non-Semantic Sparse Attention (NSSA) mechanism in the backbone network, which efficiently extracts non-semantic features related to the task, thus improving the model’s sensitivity to small objects. In the model’s throat, we design a Bidirectional Multi-Branch Auxiliary Feature Pyramid Network (BIMA-FPN), which integrates high-level semantic information with low-level spatial details, improving small object detection while expanding multi-scale receptive fields. Finally, we incorporate a Channel-Space Feature Fusion Adaptive Head (CSFA-Head), which fully handles multi-scale features and adaptively handles consistency problems of different scales, further improving the robustness of the model in complex scenarios. Experimental results on the VisDrone2019 dataset show that SMA-YOLO achieves a 13% improvement in mAP compared to the baseline model, demonstrating exceptional adaptability in small object detection tasks for remote sensing imagery. These results provide valuable insights and new approaches to further advance research in this area. © The Author(s) 2025.
KW  - Feature fusion
KW  - Multi-branch auxiliary
KW  - Object detection
KW  - Remote sensing images
KW  - algorithm
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - diagnosis
KW  - human
KW  - imagery
KW  - receptive field
KW  - remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, Z.
AU  - Wu, J.
AU  - Cai, Y.
AU  - Wang, H.
AU  - Chen, L.
AU  - Liu, Q.
TI  - Dual-stage feature specialization network for robust visual object detection in autonomous vehicles
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-99363-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004205826&doi=10.1038%2fs41598-025-99363-4&partnerID=40&md5=3669b8b1b4d6a6a9029d9e699567395d
AB  - Efficient feature representation is critical for accurate visual perception in autonomous vehicles. Existing two-stage object detection methods often suffer from feature interference between candidate region generation and classification regression tasks, leading to suboptimal performance in complex scenes. To address this, we propose a Dual-Stage Feature Specialization Network (DSFSN) that decouples feature extraction: MobileNetV3 is employed for lightweight candidate region generation, while ResNet-FPN enhances multi-scale feature fusion for precise classification. Extensive experiments on PASCAL VOC and MS COCO datasets demonstrate state-of-the-art performance, achieving 81.6% mAP (9.3% higher than Faster R-CNN) and 29.3% AP on MS COCO, with a 14.9% improvement in small object detection. Real-world tests under diverse conditions (e.g., rain, night) validate the robustness of our method for autonomous driving applications. This work provides a novel framework for balancing accuracy and efficiency in visual perception systems. © The Author(s) 2025.
KW  - Autonomous vehicle
KW  - Complex environment perception
KW  - Complex problem application
KW  - Two-stage feature extraction
KW  - Visual object detection
KW  - article
KW  - autonomous vehicle
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - female
KW  - human
KW  - human experiment
KW  - night
KW  - normal human
KW  - rain
KW  - residual neural network
KW  - specialization
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, L.
AU  - Gong, K.
AU  - Yin, X.
AU  - Fu, T.
AU  - Shangguan, Q.
TI  - Development of a car-following model incorporating the oppression effects of large trucks
PY  - 2025
T2  - Physica A: Statistical Mechanics and its Applications
DO  - 10.1016/j.physa.2025.130793
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009508694&doi=10.1016%2fj.physa.2025.130793&partnerID=40&md5=01a43ee5da089d1aca3d8e475e844d40
AB  - The presence of large trucks on highways significantly alters the driving behavior of surrounding vehicles, especially by disrupting the car-following patterns of smaller vehicles due to their size, speed differences, and visibility constraints. This study focuses on investigating the mechanism of how large trucks on highways impact the car-following behavior of surrounding drivers. The research begins by utilizing unmanned aerial vehicles (UAV) to collect vehicle trajectory data. A novel concept, termed the “oppression effects” of large trucks, is introduced, and its influence is characterized using potential field theory. Subsequently, a car-following model is developed that incorporates the oppression effects of large trucks. To illustrate the distribution of these effects, intensity contour maps are employed based on various motion states of the large truck. Finally, the proposed model is then calibrated using real-world trajectory data, and its predictive accuracy is assessed against benchmark car-following models. The proposed model improves trajectory prediction accuracy by over 40.9 % in RMSE and 22.4 % in MAE compared to classical models. The results demonstrate that the car-following model, which accounts for the oppression effects of large trucks, yields more accurate predictions of the driving behavior of vehicles following large trucks on highways. This research contributes to the theoretical foundation for behavior modeling and risk control in mixed traffic environments involving trucks and cars, ultimately enhancing safety for drivers in proximity to large trucks. © 2025 Elsevier B.V.
KW  - Car-following model
KW  - Highway
KW  - Large trucks
KW  - Oppression effects
KW  - Potential field theory
KW  - Automobiles
KW  - Behavioral research
KW  - Risk assessment
KW  - Trajectories
KW  - Trucks
KW  - Unmanned aerial vehicles (UAV)
KW  - Car following
KW  - Car-following modeling
KW  - Driving behaviour
KW  - Field theory
KW  - Highway
KW  - Large-trucks
KW  - Oppression effect
KW  - Potential field
KW  - Potential field theory
KW  - Small vehicle
KW  - Antennas
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Alesaily, Z.
AU  - Albialy, A.
TI  - Future cities: A bibliometric review, 1875 to 2024
PY  - 2025
T2  - Sustainable Futures
DO  - 10.1016/j.sftr.2025.100801
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007872906&doi=10.1016%2fj.sftr.2025.100801&partnerID=40&md5=ed2bb979bca171d89ce65f6d461a5c82
AB  - The term "future cities" represents a paradigm shift in urban development. It aims to create sustainable urban environments that integrate technology, strategic management, and citizen participation in order to address challenges such as rapid urbanization and climate change. It attains its objectives through the integration of interdisciplinary research, systems thinking, and continuous practice. Despite considerable efforts in the field of future cities' research, there is currently no single comprehensive review that can adequately address the full range of aspects related to the topic. Reviews, on the other hand, tend to be fragmented, concentrating on specific topics like smart cities, sustainable smart cities, livable cities, digitization, the impact of the COVID-19 pandemic, the environment, natural resources, and new innovations and technologies. Therefore, we conducted a bibliometric review of future cities, which included all documents listed in Scopus, Web of Science, and Dimensions from 1875 to 2024. This review identified publication trends, annual growth, and the most prolific authors, institutions, and countries in the field. We also identified networks of co-authorship, as well as the most influential citations by authors, journals, and popular documents. Furthermore, the analysis revealed the key works that established the intellectual and conceptual framework for the field of future cities. The conceptual structure of future cities studies revealed four main clusters: first, smart cities and decision-making; second, urban planning and future development; third, sustainable development; and fourth, city and human studies. This process identified research gaps and potential future directions. In terms of future research directions, the findings suggest a lack of studies that explicitly link the theme of "future cities" with concepts such as urban identity and artificial intelligence, despite AI's growing importance in recent years. This observation highlights the need for increased research efforts in these areas. Furthermore, addressing concerns about intelligence requires research of a contradictory nature. The study concludes with a valuable reference for researchers, urban planners, architects, social scientists, and specialists in related fields of research, including technology and the environment. It emphasizes that the major challenge facing urban planners is to integrate these disciplines into a unified and coherent system that promotes human well-being in a sustainable future. © 2025 The Author(s)
KW  - Cities
KW  - City
KW  - Future
KW  - Future cities
KW  - Smart cities
KW  - Smart sustainable cities
KW  - Sustainable development
KW  - Sustainable futures
KW  - Urban planning
KW  - Urbanization
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ye, X.
AU  - Li, S.
AU  - Gong, W.
AU  - Li, X.
AU  - Li, X.
AU  - Dadashova, B.
AU  - Li, W.
AU  - Du, J.
AU  - Wu, D.
TI  - Street View Imagery in Traffic Crash and Road Safety Analysis: A Review
PY  - 2025
T2  - Applied Spatial Analysis and Policy
DO  - 10.1007/s12061-025-09653-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003804687&doi=10.1007%2fs12061-025-09653-7&partnerID=40&md5=cbb2e192a12ec49c3542e5bcebfe8609
AB  - Preventing traffic crashes presents a formidable challenge due to the intricate interplay between drivers and other participants within a complex urban infrastructure. In recent years, increasing studies on road safety involved computer vision and machine learning to detect visual features from street view imagery (SVI) and explore their impacts on crashes, though the recent progress is poorly understood. This paper conducted a comprehensive review of existing literature to investigate how SVI has been used in traffic crashes and road safety studies, utilizing a broad database collection including Scopus, Web of Science, and Transport Research International Documentation. We categorized SVI-generated features into two types of factors, explored their relationship with traffic crashes, and examined the prevalent detection models. Our review demonstrated that SVI plays an important role in capturing road design and driving environment factors, which significantly influence the frequency and risk of traffic crashes. These findings underscore the significant impact of these street visual factors on road safety. Through a systematic review of recent progress, we also identified challenges and future research opportunities for SVI applications in traffic crash study, such as the potential use of large language models. © The Author(s), under exclusive licence to Springer Nature B.V. 2025.
KW  - Machine learning
KW  - Road safety
KW  - Street view
KW  - Streetscape features
KW  - Traffic crashes
KW  - computer vision
KW  - environmental factor
KW  - machine learning
KW  - road traffic
KW  - safety
KW  - satellite imagery
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Li, X.
AU  - Luan, C.
AU  - Hou, W.
AU  - Liu, H.
AU  - Zhu, Z.
AU  - Xue, L.
AU  - Zhang, J.
AU  - Liu, D.
AU  - Wu, X.
AU  - Wei, L.
AU  - Jian, C.
AU  - Li, J.
TI  - Cross-level interaction fusion network-based RGB-T semantic segmentation for distant targets
PY  - 2025
T2  - Pattern Recognition
DO  - 10.1016/j.patcog.2024.111218
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211574922&doi=10.1016%2fj.patcog.2024.111218&partnerID=40&md5=30c286bd6de2e0cb279b581c791628c5
AB  - RGB-T segmentation represents an innovative approach driven by advancements in multispectral detection and is poised to replace traditional RGB segmentation methods. An effective cross-modality feature fusion module is essential for this technology. The precise segmentation of distant objects is another significant challenge. Focused on these two areas, we propose an end-to-end distant object feature fusion network (DOFFNet) for RGB-T segmentation. Initially, we introduce a cross-level interaction fusion strategy (CLIF) and an inter-correlation fusion method (IFFM) in the encoder to enhance multi-scale feature expression and improve fusion accuracy. Subsequently, we propose a residual dense pixel convolution (R-DPC) in the decoder with a trainable upsampling unit that dynamically reconstructs information lost during encoding, particularly for distant objects whose features may vanish after pooling. Experimental results show that our DOFFNet achieves a top mean pixel accuracy of 75.8% and dramatically improves accuracy for four classes, including objects occupying as little as 0.2%–2% of total pixels. This improvement ensures more reliable and effective performance in practical applications, particularly in scenarios where small object detection is critical. Moreover, it demonstrates potential applicability in other fields like medical imaging and remote sensing. © 2024 The Authors
KW  - Cross modality
KW  - Distant object
KW  - Feature fusion
KW  - Multi-scale information
KW  - Semantic segmentation
KW  - Image coding
KW  - Medical imaging
KW  - Object detection
KW  - Proximity sensors
KW  - Signal encoding
KW  - Cross levels
KW  - Cross modality
KW  - Distant object
KW  - Features fusions
KW  - Innovative approaches
KW  - Multiscale information
KW  - Multispectral detection
KW  - Network-based
KW  - Segmentation methods
KW  - Semantic segmentation
KW  - Semantic Segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Azizi, A.
AU  - Charalambous, P.
AU  - Chrysanthou, Y.
TI  - DeepSafe:Two-level deep learning approach for disaster victims detection
PY  - 2025
T2  - Virtual Reality and Intelligent Hardware
DO  - 10.1016/j.vrih.2024.08.005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002685579&doi=10.1016%2fj.vrih.2024.08.005&partnerID=40&md5=98cd7776fc4740b4eb6791106b682f14
AB  - Background: Efficient disaster victim detection (DVD) in urban areas after natural disasters is crucial for minimizing losses. However, conventional search and rescue (SAR) methods often experience delays, which can hinder the timely detection of victims. SAR teams face various challenges, including limited access to debris and collapsed structures, safety risks due to unstable conditions, and disrupted communication networks. Methods: In this paper, we present DeepSafe, a novel two-level deep learning approach for multilevel classification and object detection using a simulated disaster victim dataset. DeepSafe first employs YOLOv8 to classify images into victim and non-victim categories. Subsequently, Detectron2 is used to precisely locate and outline the victims. Results: Experimental results demonstrate the promising performance of DeepSafe in both victim classification and detection. The model effectively identified and located victims under the challenging conditions presented in the dataset. Conclusion: DeepSafe offers a practical tool for real-time disaster management and SAR operations, significantly improving conventional methods by reducing delays and enhancing victim detection accuracy in disaster-stricken urban areas. © 2024 Beijing Zhongke Journal Publishing Co. Ltd
KW  - Deep learning
KW  - Disaster management
KW  - Victims detection
KW  - Victims identification
KW  - YOLO
KW  - Classification (of information)
KW  - Deep learning
KW  - Disaster prevention
KW  - Collapsed structures
KW  - Deep learning
KW  - Disaster management
KW  - Learning approach
KW  - Natural disasters
KW  - Search and rescue
KW  - Urban areas
KW  - Victim detections
KW  - Victim identifications
KW  - YOLO
KW  - Disasters
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pagire, V.
AU  - Chavali, M.
AU  - Kale, A.
TI  - A comprehensive review of object detection with traditional and deep learning methods
PY  - 2025
T2  - Signal Processing
DO  - 10.1016/j.sigpro.2025.110075
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004257960&doi=10.1016%2fj.sigpro.2025.110075&partnerID=40&md5=53c64d962a3804252a619801ad9a7d13
AB  - Object detection is one of the most important and challenging tasks of computer vision. It has numerous applications in the fields of agriculture, defence, retail markets and manufacturing units, transportation, social media platforms, medical, wildlife monitoring and conservation. This survey aims to give researchers a comprehensive understanding of the current state of object detection algorithms. In this review, object detection and its different aspects have been covered in detail. This review paper starts with a quick overview of object detection followed by traditional and deep learning models for object detection. The section on deep learning models provides a comprehensive overview of one-stage and two-stage object detectors. A detailed discussion is given of the transformer-based detectors and lightweight networks category. Additionally, the evaluation metrics used for object detection methods are discussed systematically. The best object detection algorithms for different applications are discussed at the end of the survey. This survey is useful for beginners who want to study different object detection algorithms and their use in different applications. © 2025 Elsevier B.V.
KW  - Classification
KW  - Deep learning
KW  - Feature extraction
KW  - Lightweight networks
KW  - Object detection
KW  - Traditional methods
KW  - Object recognition
KW  - Deep learning
KW  - Features extraction
KW  - Learning methods
KW  - Learning models
KW  - Lightweight network
KW  - Market units
KW  - Object detection algorithms
KW  - Objects detection
KW  - Retail market
KW  - Traditional method
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, G.
AU  - Jiang, W.
AU  - Sun, C.
AU  - Ning, N.
AU  - Wang, R.
AU  - Buhari, A.
TI  - Object detection algorithm for autonomous driving: Design and real-time performance analysis of AttenRetina model
PY  - 2025
T2  - Alexandria Engineering Journal
DO  - 10.1016/j.aej.2025.02.063
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001072362&doi=10.1016%2fj.aej.2025.02.063&partnerID=40&md5=f62890f3197244dfbbb6517537b662d1
AB  - With the continuous advancement of autonomous driving technology, how to efficiently and accurately detect objects (such as pedestrians, cyclists, traffic signs, etc.) has become a core challenge to improve the safety and reliability of the system. Existing object detection models still face the problem of insufficient accuracy and robustness when dealing with complex backgrounds and occlusions. To this end, this paper proposes the AttenRetina object detection model for autonomous driving, which combines the multi-scale feature fusion module (FPN) and the attention mechanism to significantly improve the detection ability of the model in various scenarios. Experimental results show that AttenRetina performs well on the KITTI and MS COCO datasets, and significantly outperforms other mainstream models in key indicators such as Precision, Recall and mAP. The mAP on the KITTI dataset reaches 0.86, which is more than 12% higher than the basic model, showing its great potential in autonomous driving object detection. The research in this paper provides an effective solution to the object detection problem in autonomous driving systems, and provides an important reference for future algorithm optimization and application. © 2025 The Authors
KW  - Autonomous driving
KW  - Deep learning
KW  - Multi-scale features
KW  - Object localization
KW  - Small object detection
KW  - Autonomous driving
KW  - Deep learning
KW  - Design time
KW  - Detection models
KW  - Multi-scale features
KW  - Object detection algorithms
KW  - Object localization
KW  - Objects detection
KW  - Real time performance
KW  - Small object detection
KW  - Traffic signs
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Dai, W.
AU  - lv, J.
AU  - Xiang, R.
AU  - Jin, S.
TI  - Study on end-to-end detection method for surface defects of automotive sheet metal parts
PY  - 2025
T2  - Journal of Real-Time Image Processing
DO  - 10.1007/s11554-025-01656-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000056862&doi=10.1007%2fs11554-025-01656-4&partnerID=40&md5=80939a1d181f48ffa7807cb556fd01b9
AB  - Sheet metal parts account for more than 60% of the total automotive parts, and their defects can seriously affect the safety of automobile operations. Therefore, it is very important to detect defects in sheet metal parts during the production process. Due to the small size of defects in sheet metal parts, and high detection precision required, the traditional detection method cannot meet the requirements. And the factory production speed is fast, if the detection speed is low, it will cause defects to escape. Therefore, we propose an end-to-end detection method for automotive sheet metal parts surface defects. To effectively improve the detection speed, the dual regression classification strategy is proposed, which removes the NMS post-processing. Gradient information branch is added to provide rich gradient information for the model and mitigate the information loss during long convolution. Use the SPD-Conv module, optimized for small-size defects detection, to retain complete space information. Finally, the model is evaluated on the automotive sheet metal parts defect dataset. The experimental results show that the proposed method is superior to the benchmark methods in precision and speed, with mAP of 92.32% and FPS of 39.06, which achieves end-to-end detection. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.
KW  - Automotive sheet metal part
KW  - convolutional neural network
KW  - Deep learning
KW  - Defect detection
KW  - End-to-end
KW  - Deep neural networks
KW  - Automotive sheet metal part
KW  - Automotive sheet metals
KW  - Convolutional neural network
KW  - Deep learning
KW  - Defect detection
KW  - Detection methods
KW  - Detection speed
KW  - End to end
KW  - Gradient informations
KW  - Sheet metal parts
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, Z.
AU  - Zhang, Z.
TI  - The Research on an Improved YOLOX-Based Algorithm for Small-Object Road Vehicle Detection
PY  - 2025
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics14112179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007694460&doi=10.3390%2felectronics14112179&partnerID=40&md5=35a83152fc5b3f354956d4e01b3fb3d6
AB  - To address the challenges of missed detections and false positives caused by dense vehicle distribution, occlusions, and small object sizes in complex traffic scenarios, this paper proposes an improved YOLOX-based vehicle detection algorithm with three key innovations. First, we design a novel Wavelet-Enhanced Convolution (WEC) module that expands the receptive field to enhance the model’s global perception capability. Building upon this foundation, we integrate the SimAM attention mechanism, which improves feature saturation by adaptively fusing semantic features across different channels and spatial locations, thereby strengthening the network’s multi-scale generalization ability. Furthermore, we develop a Varifocal Intersection over Union (VIoU) bounding-box regression loss function that optimizes convergence in multi-scale feature learning while enhancing global feature extraction capabilities. The experimental results on the VisDrone dataset demonstrate that our improved model achieves performance gains of 0.9% mAP and 1.8% mAP75 compared to the baseline version, effectively improving vehicle detection accuracy. © 2025 by the authors.
KW  - object detection
KW  - SimAM attention
KW  - VIoU
KW  - YOLOX
KW  - Multi-task learning
KW  - Object recognition
KW  - False positive
KW  - Missed detections
KW  - Object size
KW  - Objects detection
KW  - Road vehicles
KW  - SimAM attention
KW  - Small objects
KW  - Varifocal intersection over union
KW  - Vehicles detection
KW  - YOLOX
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Arrasmith, W.W.
TI  - Handbook of systems engineering and analysis of electro-optical and infrared systems: Concepts, principles, and methods: Second edition
PY  - 2025
T2  - Handbook of Systems Engineering and Analysis of Electro-Optical and Infrared Systems: Concepts, Principles, and Methods: Second Edition
DO  - 10.1201/9781003624097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009318236&doi=10.1201%2f9781003624097&partnerID=40&md5=28e4855a0b6d00e906fe3bf85c842e58
AB  - There has been a lot of innovation in systems engineering and some fundamental advances in the fields of optics, imaging, lasers, and photonics that warrant attention. This volume focuses on concepts, principles, and methods of systems engineering-related topics from government, industrial, and academic settings such as development and operations (DevOps), agile methods, and the concept of the "digital twin." Handbook of Systems Engineering and Analysis of Electro-Optical and Infrared Systems: Concepts, Principles, and Methods offers more information on decision and risk analysis and statistical methods in systems engineering such as design of experiments (DOX) methods, hypothesis testing, analysis of variance, blocking, 2k factorial analysis, and regression analysis. It includes new material on systems architecture to properly guide the evolving system design and bridge the gap between the requirements generation and design efforts. The integration of recent high-speed atmospheric turbulence research results in the optical technical examples and case studies to illustrate the new developments is also included. A presentation of new optical technical materials on adaptive optics (AO), atmospheric turbulence compensation (ATC), and laser systems along with more are also key updates that are emphasized in the second edition 2-volume set. Because this volume blends modern-day systems engineering methods with detailed optical systems analysis and applies these methodologies to EO/IR systems, this new edition is an excellent text for professionals in STEM disciplines who work with optical or infrared systems. It's also a great practical reference text for practicing engineers and a solid educational text for graduate-level systems engineering, engineering, science, and technology students. This book is also available as a set Handbook of Systems Engineering and Analysis of Electro-Optical and Infrared Systems (978-1-032-22242-4). © 2025 William Wolfgang Arrasmith. All rights reserved.
KW  - Adaptive optics
KW  - Atmospheric turbulence
KW  - Engineering education
KW  - Engineering research
KW  - Infrared devices
KW  - Optical systems
KW  - Statistical methods
KW  - STEM (science, technology, engineering and mathematics)
KW  - Students
KW  - Systems analysis
KW  - Systems thinking
KW  - Agile methods
KW  - Development and operations
KW  - Electro-optical systems
KW  - Hypothesis testing
KW  - Infrared systems
KW  - Optical-
KW  - Risk analyze
KW  - System concepts
KW  - System methods
KW  - Systems principles
KW  - Design of experiments
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, M.
AU  - Han, S.
TI  - ETS-YOLO: An Efficient YOLO-based Model for Real-Time Traffic Sign Recognition
PY  - 2025
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-025-04276-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006762553&doi=10.1007%2fs11760-025-04276-4&partnerID=40&md5=4dc35886c0e5557757463a7e932141d3
AB  - In addressing the challenges inherent in traffic sign recognition, such as susceptibility to interference, difficulty in detecting small targets, and the trade-off between real-time performance and accuracy, we propose an Efficient Traffic Sign YOLO (ETS-YOLO) model to fulfill the demands of real-time sign recognition. Built on the baseline YOLOv5 model, we adopt Partial Convolution (PConv) to redesign the Cross Stage Partial bottleneck including 3 convolutional layers (C3) module of the network, resulting in the proposed C3Efficient module, which reduces redundant calculations and memory accesses, making the model more efficient and lightweight. Accordingly, we utilize the Dynamic Upsampler (DySample) up-sampling to enhance the up-sampling effect during the Feature Pyramid Network (FPN) stage and design a new Normalized Wasserstein Distance (NWD) loss to redefine the anchor positional loss function mechanism, leading to better detection accuracy for tiny objects. Also, applying the Soft-NMS algorithm facilitates anchor filtering optimization, notably improving the detection accuracy of adjacent occluded targets. As a result of these improvements, the ETS-YOLO model achieves a mean Average Precision (mAP_0.5) of 0.805 when trained and tested on 45 types of traffic signs within the TT100K dataset, demonstrating a noteworthy improvement of 0.052 compared to the baseline model. In terms of model complexity, the ETS-YOLO model has 6.0 million parameters and a computational load of 14.3 GFLOPs, which represent reductions of 16.0% in model size and 12.3% in computational load, respectively, compared to the baseline model. Meanwhile, the model achieves an inference latency of 26.3 ms, showing a decrease of 10% in inference time over the baseline model. Ultimately, our model achieves a favorable balance between lightweight and accuracy compared to other state-of-the-art models. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - Anchor filtering optimization
KW  - Model light-weighting
KW  - Tiny object detection
KW  - Traffic sign recognition
KW  - You only look once (YOLO)
KW  - Anchor filtering
KW  - Anchor filtering optimization
KW  - Lightweighting
KW  - Model light
KW  - Model light-weighting
KW  - Objects detection
KW  - Optimisations
KW  - Tiny object detection
KW  - Traffic sign recognition
KW  - You only look once
KW  - NP-hard
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kang, S.
AU  - Sun, Y.
AU  - Li, S.
AU  - Xu, Y.
AU  - Li, Y.
AU  - Chen, G.
AU  - Xue, F.
TI  - A lightweight neural network search algorithm based on in-place distillation and performance prediction for hardware-aware optimization
PY  - 2025
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2025.110775
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001817504&doi=10.1016%2fj.engappai.2025.110775&partnerID=40&md5=4f5fb52a8eaeacfe8bdfe7e598ca2272
AB  - Due to the limited computing resources of edge devices, traditional object detection algorithms struggle to meet the efficiency and accuracy requirements of autonomous driving. Consequently, designing a neural network model that balances hardware resource requirements, operating speed, and accuracy is crucial. To address this, by integrating algorithm with hardware characteristics, we propose a lightweight neural network architecture search algorithm based on in-place distillation and performance predictor (LNIP). Initially, we focus on optimizing the operators of the you only look once version 8 nano (YOLOv8n) and dynamically adjust its network structure. Then, we trained a super-network using a progressive shrinking strategy, the sandwich rule, and in-place distillation. Subsequently, we employed a Gaussian process to model the relationship between network architecture and accuracy, utilizing encoding methods and custom kernel function to develop high-performance predictor. Finally, during the search process, we introduce a reward function based on Pareto optimality to balance the performance of the model with hardware constraints. Building upon this foundation, we design an efficient search algorithm based on the performance predictor to progressively explore the optimal network structure tailored to hardware characteristics. We compared our lightweight network with state-of-the-art methods on the BDD100K, COCO, and PASCAL VOC datasets and deployed it on the Black Sesame A1000 and NVIDIA Xavier for comprehensive evaluation. On the NVIDIA Xavier, the lightweight network achieves a latency of 11.81 ms and an edge precision of 46.1 %. These experimental results demonstrate that our method outperforms existing methods in balancing hardware constraints and model performance. © 2025 Elsevier Ltd
KW  - In-place distillation
KW  - Neural network architecture search
KW  - Object detection
KW  - Pareto optimality
KW  - Performance predictor
KW  - Hardware characteristics
KW  - Hardware constraints
KW  - In-place distillation
KW  - Neural network architecture
KW  - Neural network architecture search
KW  - Neural-networks
KW  - Objects detection
KW  - Pareto-optimality
KW  - Performance predictor
KW  - Search Algorithms
KW  - Pareto principle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shi, P.
AU  - Wu, W.
AU  - Yang, A.
TI  - MPVF: Multi-Modal 3D Object Detection Algorithm with Pointwise and Voxelwise Fusion
PY  - 2025
T2  - Algorithms
DO  - 10.3390/a18030172
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001304732&doi=10.3390%2fa18030172&partnerID=40&md5=ff35a35273858c7415e72a7769b13646
AB  - 3D object detection plays a pivotal role in achieving accurate environmental perception, particularly in complex traffic scenarios where single-modal detection methods often fail to meet precision requirements. This highlights the necessity of multi-modal fusion approaches to enhance detection performance. However, existing camera-LiDAR intermediate fusion methods suffer from insufficient interaction between local and global features and limited fine-grained feature extraction capabilities, which results in inadequate small object detection and unstable performance in complex scenes. To address these issues, the multi-modal 3D object detection algorithm with pointwise and voxelwise fusion (MPVF) is proposed, which enhances multi-modal feature interaction and optimizes feature extraction strategies to improve detection precision and robustness. First, the pointwise and voxelwise fusion (PVWF) module is proposed to combine local features from the pointwise fusion (PWF) module with global features from the voxelwise fusion (VWF) module, enhancing the interaction between features across modalities, improving small object detection capabilities, and boosting model performance in complex scenes. Second, an expressive feature extraction module, improved ResNet-101 and feature pyramid (IRFP), is developed, comprising the improved ResNet-101 (IR) and feature pyramid (FP) modules. The IR module uses a group convolution strategy to inject high-level semantic features into the PWF and VWF modules, improving extraction efficiency. The FP module, placed at an intermediate stage, captures fine-grained features at various resolutions, enhancing the model’s precision and robustness. Finally, evaluation on the KITTI dataset demonstrates a mean Average Precision (mAP) of 69.24%, a 2.75% improvement over GraphAlign++. Detection accuracy for cars, pedestrians, and cyclists reaches 85.12%, 48.61%, and 70.12%, respectively, with the proposed method excelling in pedestrian and cyclist detection. © 2025 by the authors.
KW  - 3D object detection
KW  - autonomous driving
KW  - image
KW  - multi-modal fusion
KW  - point cloud
KW  - Adaptive boosting
KW  - Image coding
KW  - Image fusion
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous driving
KW  - Fusion modules
KW  - Image
KW  - Multi-modal
KW  - Multi-modal fusion
KW  - Objects detection
KW  - Point wise
KW  - Point-clouds
KW  - Bicycles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, Y.
AU  - Chen, Z.
AU  - Yang, H.
AU  - Liao, J.
TI  - GS-LinYOLOv10: A drone-based model for real-time construction site safety monitoring
PY  - 2025
T2  - Alexandria Engineering Journal
DO  - 10.1016/j.aej.2025.01.021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280077&doi=10.1016%2fj.aej.2025.01.021&partnerID=40&md5=a7748bfc0241b41b8c8137c9281c3d63
AB  - Real-time safety monitoring on construction sites is essential for ensuring worker safety, but traditional detection methods face challenges in dynamic environments with moving objects, occlusions, and complex conditions. To address these limitations, we propose GS-LinYOLOv10, an improved model based on YOLOv10, specifically designed for drone-based safety monitoring. The GSConv module introduces a lightweight feature extraction mechanism, reducing computational complexity without compromising detection accuracy. The Linformer-based attention mechanism efficiently captures global context, addressing challenges in dynamic and complex environments. The model integrates IoT sensor data for real-time feedback, incorporates the GSConv module for lightweight feature extraction, and utilizes a Linformer-based attention mechanism to efficiently capture global context. These innovations reduce computational complexity while significantly improving detection accuracy. Experimental results show that GS-LinYOLOv10 achieves a precision of 91.2% and a mean average precision (mAP) of 89.4%, outperforming existing models. The integration of IoT sensors allows the drone system to dynamically adjust its monitoring focus, improving adaptability to changing environments and enhancing hazard detection. This research provides an advanced, drone-based IoT-enhanced solution for real-time construction site safety monitoring, offering a more effective and efficient approach to safety management. © 2025
KW  - Construction site safety
KW  - Drone-based monitoring
KW  - GSConv
KW  - IoT integration
KW  - Linformer
KW  - Real-time safety detection
KW  - Aircraft
KW  - Safety engineering
KW  - Construction site safety
KW  - Drone-based monitoring
KW  - Dynamic environments
KW  - Gsconv
KW  - IoT integration
KW  - Linformer
KW  - Real- time
KW  - Real-time construction
KW  - Real-time safety detection
KW  - Safety monitoring
KW  - Aircraft detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, J.
AU  - Chen, X.
AU  - Gao, J.
AU  - Tang, Y.
TI  - Lightweight detection model for infrared targets of traffic participants in complex environments
ST  - 复杂交通环境下的交通参与者红外目标轻量化检测模型
PY  - 2025
T2  - Journal of Safety and Environment
DO  - 10.13637/j.issn.1009-6094.2024.1652
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009100962&doi=10.13637%2fj.issn.1009-6094.2024.1652&partnerID=40&md5=da8ddb18111104ffe1696894991020d8
AB  - To tackle the challenges of low recognition accuracy, subpar real-time performance, and difficulties in lightweight deployment for multi-class infrared target detection of traffic participants In Intelligent Connected Vehicles (ICVs) and traffic monitoring systems, we propose a lightweight detection model called CNDS YOLO, which is based on an enhanced YOLOv9m architecture. First, we designed a lightweight CE MobileNetv3 structure that incorporates the Generalized Efficient Layer Aggregation Network (C ELAN) to replace the entire backbone network. This approach significantly reduces the parameter count while enhancing detection accuracy. Second, Depthwise Separable Convolution (DSConv) was integrated to replace conventional convolution in the neck feature fusion layer, auxiliary reversible CBL module, and DS SPPF module. This modification further improves real-time performance and enables lightweight deployment capabilities. Additionally, a normalized N Swin Transformer module was incorporated into the neck feature fusion layer, significantly enhancing the model’s ability to capture and fuse features related to infrared targets, which in turn improves detection accuracy and stability. Furthermore, a new 160 × 160 small target detection head was developed at the detection end to increase sensitivity to small and occluded targets. Finally, experiments conducted on the FLIR2 dataset demonstrated that the CNDS YOLO model outperformed the baseline YOLOv9m model, reducing parameters and FLOPs by 31. 0% and 35. 5%, respectively, while enhancing detection speed (FS) and mean Average Precision (AP) by 24. 2% and 7. 0%, respectively. Compared to mainstream models, the CNDS YOLO model achieved the highest values in both Average Precision (AP) and detection speed (FS) . In the ablation experiments, each of the four improved modules within CNDS YOLO was quantitatively assessed. For example, the DSConv module resulted in a reduction of parameters and FLOPs by 24. 5% and 37. 6%, respectively, while the N Swin Transformer module improved Recall (R) and AP by 4. 1% and 4. 3%, respectively. The visual detection experiments effectively validated the model’s sensitivity to real-world infrared targets, demonstrating its robustness and suitability for deployment. The CNDS YOLO model significantly improves the speed and accuracy of embedded devices in detecting traffic participants within complex environments, thereby offering theoretical support for the safe operation of intelligent transportation systems. © 2025 Science China Press. All rights reserved.
KW  - CNDS YOLO model
KW  - infrared target
KW  - lightweight
KW  - safety engineering
KW  - small target detection
KW  - traffic participants
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, H.
AU  - Wang, X.
TI  - Perception of distance and speed of front vehicle based on vehicle image features
ST  - 基 于 车 辆 图 像 特 征 的 前 车 距 离 与 速 度 感 知
PY  - 2025
T2  - Zhejiang Daxue Xuebao (Gongxue Ban)/Journal of Zhejiang University (Engineering Science)
DO  - 10.3785/j.issn.1008-973X.2025.06.013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006945786&doi=10.3785%2fj.issn.1008-973X.2025.06.013&partnerID=40&md5=d038ef0579e9824c311da68be5f6fb4a
AB  - A multimodal perception method for distance and speed of front vehicle integrating vehicle image features was proposed for front vehicle detection and operational state perception in driving scenarios. The position features of vehicles in images were detected by an improved SW-YOLOv8n model, and the relative lateral and longitudinal distances to the front vehicle were calculated using geometric algorithms. A feature extraction network was designed to extract vehicle features, where image feature vectors were fused through serial concatenation, and a neural network for vehicle distance measurement was established. The multi-feature fusion module was integrated with the distance measurement neural network to construct an end-to-end front vehicle distance perception model and a vehicle tracking-based speed estimation model, which synchronously output precise distance estimations and stable speed tracking results. Experimental results demonstrated that on the test dataset, the SW-YOLOv8n model achieved improvements of 1.6 percentage points in mAP50 and 2.3 percentage points in mAP50−95 compared to the baseline YOLOv8n, while maintaining a detection speed of 260.11 frames per second. Within a lateral range of 9.5 m and a longitudinal range of 50 m, under unobstructed conditions, the preceding vehicle distance perception model exhibited an average relative error of 1.87% between predicted and actual distances, while under occluded conditions, the average relative error was 2.02%. The speed measurement results of the tracking-based model exhibited significant stability, confirming the method’s effectiveness for front vehicle distance and speed perception tasks. © 2025 Zhejiang University. All rights reserved.
KW  - deep learning
KW  - object detection
KW  - state perception
KW  - vehicle distance measurement
KW  - vehicle speed measurement
KW  - Aircraft detection
KW  - Deep learning
KW  - Distance perception
KW  - Image features
KW  - Neural-networks
KW  - Objects detection
KW  - Perception model
KW  - State perception
KW  - Vehicle distance measurement
KW  - Vehicle images
KW  - Vehicle speed measurement
KW  - Distance measurement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nan, Z.
AU  - Liu, W.
AU  - Zhu, G.
AU  - Zhao, H.
AU  - Xia, W.
AU  - Lin, X.
AU  - Yang, Y.
TI  - LiDAR-Camera joint obstacle detection algorithm for railway track area
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2025.127089
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219499447&doi=10.1016%2fj.eswa.2025.127089&partnerID=40&md5=980d23a81478dfb062e42b8eaf810c62
AB  - The application of multi-sensor systems in railway security holds significant research potential. We propose a joint decision-making strategy for obstacle detection (OD) in track areas that integrates LiDAR and camera sensors. LiDAR can accurately detect the geometric information of obstacles in the region without being affected by lighting conditions. To accurately assess the danger level of obstacles, we introduce a camera sensor to classify obstacles based on known locations, thereby enhancing the accuracy of detecting potentially hazardous obstacles. We first use LiDAR to obtain the point cloud data for the detection area. A point cloud algorithm designed explicitly for static obstacle recognition is applied to extract obstacle point cloud information. Using an external transformation relationship, we capture the corresponding obstacle images, fusing them with the point cloud data to serve as input images for a neural network. Subsequently, we introduce MS-YOLO-DLKA, an image OD network that combines a multi-scale feature extraction module (MS-Block) and a large convolution kernel module (D-LKA) based on YOLOv5. On our railway track obstacle dataset, the network achieved an accuracy of 85 %, a recall rate of 95.8 %, and a mAP value of 0.91, outperforming several SOTA (state-of-the-art) networks regarding comprehensive application performance. In test scenarios, our equipment has achieved OD within a range of 50 m for obstacles as small as 20 cm × 20 cm × 20 cm, providing a new railway security and monitoring solution. © 2025 Elsevier Ltd
KW  - Camera
KW  - Data fusion
KW  - LiDAR
KW  - MS-YOLO-DLKA
KW  - ROI
KW  - Metadata
KW  - Network security
KW  - Railroad tracks
KW  - Railroad transportation
KW  - Railroads
KW  - Sensor data fusion
KW  - Camera sensor
KW  - Detection algorithm
KW  - LiDAR
KW  - MS-YOLO-DLKA
KW  - Obstacles detection
KW  - Point cloud data
KW  - Point-clouds
KW  - Railway securities
KW  - Railway track
KW  - ROI
KW  - Obstacle detectors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Aromoye, I.A.
AU  - Hiung, L.H.
AU  - Sebastian, P.
TI  - P-DETR: A transformer-based algorithm for pipeline structure detection
PY  - 2025
T2  - Results in Engineering
DO  - 10.1016/j.rineng.2025.104652
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000323559&doi=10.1016%2fj.rineng.2025.104652&partnerID=40&md5=e0eda7cad7df22a428a7b3699d2eb320
AB  - Pipelines are essential transportation infrastructure for oil and gas, but they are vulnerable to defects such as cracks, joint failure, and corrosion due to extreme weather conditions. These defects can result in oil and gas leakage, which prompts environmental and economic damages. Hence, regular inspection of pipelines is necessary. The industry has increasingly relied on using drones for pipeline inspections, though the inspection is still done manually by the drone operator or offline via recorded video footage from the drone. This paper proposes using the Pipe Detection Transformer (P-DETR), a novel transformer-based model designed for pipeline detection and potential integration with aerial robots or drones to enable autonomous pipeline inspection. P-DETR introduces significant improvements to the original Detection Transformer (DETR) framework to enhance its detection performance, particularly for small-sized pipes - a key limitation of the baseline DETR. The major contribution is a Feature Normalization and Transformation (FNT) module, which fuses multiple layers of the convolutional backbone to provide a focused representation of small-sized features before processing by the transformer module. Experimental results validate the superiority of P-DETR, achieving an overall mAP of 55 %, a 3 AP improvement over DETR, and significantly increasing precision for small-sized pipe detection by 8.6 AP (from 1.9 to 10.5). Additionally, precision improvements for medium- and large-sized pipes were 10.8 AP (from 10.8 to 21.6) and 2.2AP (from 64.4 to 66.6), respectively, with an overall recall of 73.9 %, a 4 AP improved performance over DETR. The results from extensive experiments highlight the superior performance of the proposed P-DETR model over the original DETR, UP-DETR, R-DETR, Skip-DETR, and other standard object detection models, including YOLOv3 and SSD. © 2025 The Author(s)
KW  - DETR
KW  - Machine learning
KW  - Oil and gas
KW  - Pipe detection
KW  - Pipelines, Deep learning
KW  - Transformers
KW  - Electric towers
KW  - Electric transformer testing
KW  - Gas piping systems
KW  - Petroleum transportation
KW  - Pipelines
KW  - Detection transformer
KW  - Machine-learning
KW  - Oil and gas
KW  - Performance
KW  - Pipe detection
KW  - Pipeline inspection
KW  - Pipeline structure
KW  - Pipeline, deep learning
KW  - Structure detection
KW  - Transformer
KW  - Pipeline corrosion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jin, Z.
AU  - He, T.
AU  - Qiao, L.
AU  - Duan, J.
AU  - Shi, X.
AU  - Yan, B.
AU  - Guo, C.
TI  - MES-YOLO: An efficient lightweight maritime search and rescue object detection algorithm with improved feature fusion pyramid network
PY  - 2025
T2  - Journal of Visual Communication and Image Representation
DO  - 10.1016/j.jvcir.2025.104453
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002037003&doi=10.1016%2fj.jvcir.2025.104453&partnerID=40&md5=7d962112ed89e485504ede67c26fabb1
AB  - Maritime Search and Rescue (SAR) object detection is challenged by environmental complexity, variability in object scales, and real-time computation constraints of Unmanned Aerial Vehicles (UAVs). Our MES-YOLO algorithm, designed for maritime UAV imagery, employs an innovative Multi Asymptotic Feature Pyramid Network (MAFPN) to enhance detection accuracy across scales. It integrates an Efficient Module (EMO) and Inverted Residual Mobile Blocks (iRMB) to maintain a lightweight model while enhancing key information perception.The SIoU loss function is used to optimize the detection performance of the model. Tests on the SeaDronesSee dataset show that MES-YOLO increased average precision (mAP50) from 81.5% to 87.1%, reduced parameter count by 43.3%, and improved the F1 score by 6.8%, with a model size only 58.3% that of YOLOv8, surpassing YOLO series and other mainstream algorithms in robustness to background illumination and imaging angles. © 2025 Elsevier Inc.
KW  - Adaptive spatial fusion
KW  - Feature enhancement
KW  - Maritime search and rescue
KW  - Object detection
KW  - Transformer
KW  - Yolov8
KW  - Aircraft detection
KW  - Image enhancement
KW  - Adaptive spatial fusion
KW  - Aerial vehicle
KW  - Feature enhancement
KW  - Maritime search and rescue
KW  - Object detection algorithms
KW  - Objects detection
KW  - Pyramid network
KW  - Search and rescue
KW  - Transformer
KW  - Yolov8
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jafari, A.A.
AU  - Agarwal, A.
AU  - Ozcinar, C.
AU  - Anbarjafari, G.
TI  - An Integral-Differential Probabilistic Fusion Framework of YOLO v8 and GPT-4o for High-Fidelity Tiny Object Recognition and Collision Threat Confidence in Autonomous Driving
PY  - 2025
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-025-04243-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006689176&doi=10.1007%2fs11760-025-04243-z&partnerID=40&md5=9e1eba8c3b45ecfb0e4a099a825cbed4
AB  - We present a novel Integral-Differential Probabilistic Fusion Framework that synergistically integrates a state-of-the-art deep object detection network (YOLO v8) with a multimodal large language model (GPT-4o) to tackle the longstanding challenge of tiny object recognition and collision threat assessment in autonomous driving. Our central hypothesis is that the fusion of precise spatial detection with rich semantic and relational reasoning-mathematically formalized using probability theory, differential equations, and integral calculus-can yield a continuous Threat Confidence Level that robustly quantifies collision risk in real time. To validate our approach, we conduct extensive experiments on the 100K Vehicle Dashcam Image Dataset, demonstrating that our framework achieves an overall classification accuracy exceeding 90% across diverse scenarios, including varying illumination and occlusion conditions. The results underscore the potential of dynamic thresholding in reducing false positives while preserving sensitivity to true threats, paving the way for more reliable and safe autonomous driving systems. This work not only advances the state-of-the-art in tiny object detection but also lays a rigorous theoretical foundation for future research in multimodal sensor fusion and real-time risk assessment. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - Autonomous driving
KW  - Deep learning
KW  - Multimodal large language model
KW  - Probabilistic fusion
KW  - Tiny object detection
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Differentiation (calculus)
KW  - Integral equations
KW  - Autonomous driving
KW  - Deep learning
KW  - Language model
KW  - Multi-modal
KW  - Multimodal large language model
KW  - Objects detection
KW  - Objects recognition
KW  - Probabilistic fusion
KW  - Probabilistics
KW  - Tiny object detection
KW  - Risk assessment
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bae, C.
AU  - Choi, E.
AU  - Lee, S.
TI  - DT-CAS: Collision Avoidance System for Unmanned Aerial Vehicles in Digital Twin Environments
ST  - DT-CAS: 디지털 트윈 환경에서 무인항공기 간 충돌 회피 시스템
PY  - 2025
T2  - Journal of the Korean Society for Aeronautical and Space Sciences
DO  - 10.5139/JKSAS.2025.53.4.445
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001268704&doi=10.5139%2fJKSAS.2025.53.4.445&partnerID=40&md5=888405e97d7f8f1012d85042f255dc84
AB  - A collision avoidance system is essential for the safe operation of multiple Unmanned Aerial Vehicles(UAVs). Previous studies utilizing sensors for UAV collision avoidance may exhibit performance variations depending on the sensor used, while reinforcement learning-based approaches face challenges related to hardware limitations and power consumption due to computational demands. This paper proposes a Digital Twin-based Collision Avoidance System(DT-CAS) that predicts potential collision points based on pre-determined flight paths in a digital twin environment and controls UAVs to avoid collisions. DT-CAS extracts flight path intersections of UAVs within the digital twin environment and controls the entry order to avoid collisions. DT-CAS can monitor UAVs for collision detection and avoidance with an average error of about 6.75cm. Also, DT-CAS was able to avoid collisions in scenarios with varying flight distances, speeds, and wind conditions, with only an about 5% increase in flight time and battery usage. © 2025 The Korean Society for Aeronautical and Space Sciences.
KW  - Collision Avoidance System
KW  - Control
KW  - Digital Twin
KW  - Monitoring
KW  - Unmanned Aerial Vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Aibibu, T.
AU  - Lan, J.
AU  - Zeng, Y.
AU  - Hu, J.
AU  - Yong, Z.
TI  - Multiview angle UAV infrared image simulation with segmented model and object detection for traffic surveillance
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-89585-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218816176&doi=10.1038%2fs41598-025-89585-x&partnerID=40&md5=dfbb79bfe88d28460dd8cb5959a4192b
AB  - With the rapid development of infrared (IR) imaging UAV technology, infrared aerial image processing technology has been applied in different fields. But it is not very convenient to obtain real aerial images in some cases because of flight limitations, acquisition costs and other factors. So, it is necessary to simulate UAV infrared images by computer. This paper proposed an improved infrared aerial image simulation method based on open source AirSim. By improving the original AirSim infrared image simulation method, the simulation quality of the infrared image is improved via 3-dimensional segmented model processing. The infrared aerial images of the traffic scene with different viewing angles are simulated via the proposed method in this paper and we constructed infrared traffic scene simulation dataset (IR-TSS) containing seven types of objects. We propose the efficient EfficientNCSP-Net net for the IR-TSS dataset and use popular methods for comparative experiments. The experimental results show that the proposed EfficientNCSP-Net has an mAP50 greater than 96% for object detection on IR-TSS dataset, which is better than those of the existing methods. This paper not only contributes to research on infrared image simulations of traffic scenes, but also has referential significance in other aerial image simulation fields. © The Author(s) 2025.
KW  - article
KW  - diagnosis
KW  - human experiment
KW  - image processing
KW  - infrared radiation
KW  - male
KW  - simulation
KW  - thermography
KW  - traffic
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, D.
AU  - Huang, H.
AU  - Huang, D.
AU  - Liu, Z.
TI  - Review of Application of BEV Perceptual Learning in Autonomous Driving
ST  - BEV 感知学习在自动驾驶中的应用综述
PY  - 2025
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2407-0501
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007330614&doi=10.3778%2fj.issn.1002-8331.2407-0501&partnerID=40&md5=c8bc15f3a058c389c3750e7baa074e20
AB  - As the types of sensors used as acquisition inputs in the autonomous driving perception module continue to develop, it becomes more and more difficult to represent the multi-modal data uniformly. BEV perception learning in the automatic driving perception task module can make multi-modal data unified integration into a feature space, which has better development potential compared with other perception learning models. The reasons for the good development potential of BEV perception model are summarized from five aspects: research significance, spatial deployment, preparation work, algorithm development, and evaluation index. The BEV perception model can be summarized into four series from a framework perspective: Lift-Splat-Lss series, IPM reverse perspective conversion, MLP view conversion and Transformer view conversion. The input data can be summarized into two categories: the first type of pure image feature input includes monocular camera input and multi-camera input; the second type of fusion data input is not only the simple data fusion of point cloud data and image features, but also the knowledge distillation fusion guided or supervised by point cloud data and the fusion of height segmentation by guided slice. It provides an overview of the application of four kinds of automatic driving tasks in BEV perception model, such as multi-target tracking, map segmentation, lane detection and 3D target detection, and summarizes the shortcomings of the four series of current BEV perception learning frameworks. © 2025 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
KW  - 3D target detection
KW  - BEV perception learning
KW  - lane detection
KW  - map segmentation
KW  - multi-modal data fusion
KW  - multi-target tracking
KW  - view conversion
KW  - Automatic target recognition
KW  - Data integration
KW  - Depth perception
KW  - Image fusion
KW  - Image segmentation
KW  - Information fusion
KW  - Metadata
KW  - Modal analysis
KW  - Network security
KW  - Photointerpretation
KW  - Sensor data fusion
KW  - 3d target detection
KW  - BEV perception learning
KW  - Lane detection
KW  - MAP segmentation
KW  - Multi-modal data
KW  - Multi-modal data fusion
KW  - Multi-target-tracking
KW  - Perception model
KW  - Targets detection
KW  - View conversion
KW  - Target tracking
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tian, Y.
AU  - Lin, F.
AU  - Li, Y.
AU  - Zhang, T.
AU  - Zhang, Q.
AU  - Fu, X.
AU  - Huang, J.
AU  - Dai, X.
AU  - Wang, Y.
AU  - Tian, C.
AU  - Li, B.
AU  - Lv, Y.
AU  - Kovács, L.
AU  - Wang, F.-Y.
TI  - UAVs meet LLMs: Overviews and perspectives towards agentic low-altitude mobility
PY  - 2025
T2  - Information Fusion
DO  - 10.1016/j.inffus.2025.103158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002285252&doi=10.1016%2fj.inffus.2025.103158&partnerID=40&md5=5bcb4e0cccd08b4c389aa7cd6128f3f3
AB  - Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced transformative advancements across various domains, like transportation, logistics, and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs extend traditional systems’ perception and action capabilities, garnering widespread attention from academia and industry. However, current UAV operations primarily depend on human control, with only limited autonomy in simple scenarios, and lack the intelligence and adaptability needed for more complex environments and tasks. The emergence of large language models (LLMs) demonstrates remarkable problem-solving and generalization capabilities, offering a promising pathway for advancing UAV intelligence. This paper explores the integration of LLMs and UAVs, beginning with an overview of UAV systems’ fundamental components and functionalities, followed by an overview of the state-of-the-art LLM technology. Subsequently, it systematically highlights the multimodal data resources available for UAVs, which provide critical support for training and evaluation. Furthermore, key tasks and application scenarios where UAVs and LLMs converge are categorized and analyzed. Finally, a reference roadmap towards agentic UAVs is proposed to enable UAVs to achieve agentic intelligence through autonomous perception, memory, reasoning, and tool utilization. Related resources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs. © 2025
KW  - Foundation intelligence
KW  - Large language models
KW  - Low altitude mobility systems
KW  - Unmanned aerial vehicles
KW  - Maneuverability
KW  - Street traffic control
KW  - Aerial vehicle
KW  - Foundation intelligence
KW  - Language model
KW  - Large language model
KW  - Low altitude mobility system
KW  - Low altitudes
KW  - Manoeuvrability
KW  - Mobility systems
KW  - Transportation-logistics
KW  - Unmanned aerial vehicle
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mahmud, B.U.
AU  - Hong, G.
AU  - Lalwani, V.R.
AU  - Brown, N.
AU  - Asher, Z.D.
TI  - Real-Time Identification of Look-Alike Medical Vials Using Mixed Reality-Enabled Deep Learning
PY  - 2025
T2  - Future Internet
DO  - 10.3390/fi17050223
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006463451&doi=10.3390%2ffi17050223&partnerID=40&md5=7aafb804915b4b81dd523fdcc0e45169
AB  - The accurate identification of look-alike medical vials is essential for patient safety, particularly when similar vials contain different substances, volumes, or concentrations. Traditional methods, such as manual selection or barcode-based identification, are prone to human error or face reliability issues under varying lighting conditions. This study addresses these challenges by introducing a real-time deep learning-based vial identification system, leveraging a Lightweight YOLOv4 model optimized for edge devices. The system is integrated into a Mixed Reality (MR) environment, enabling the real-time detection and annotation of vials with immediate operator feedback. Compared to standard barcode-based methods and the baseline YOLOv4-Tiny model, the proposed approach improves identification accuracy while maintaining low computational overhead. The experimental evaluations demonstrate a mean average precision (mAP) of 98.76 percent, with an inference speed of 68 milliseconds per frame on HoloLens 2, achieving real-time performance. The results highlight the model’s robustness in diverse lighting conditions and its ability to mitigate misclassifications of visually similar vials. By combining deep learning with MR, this system offers a more reliable and efficient alternative for pharmaceutical and medical applications, paving the way for AI-driven MR-assisted workflows in critical healthcare environments. © 2025 by the authors.
KW  - AI
KW  - AI in healthcare
KW  - deep learning
KW  - HoloLens
KW  - look-alike vial
KW  - mixed reality
KW  - object detection
KW  - real time detection
KW  - Patient treatment
KW  - AI in healthcare
KW  - Deep learning
KW  - Hololens
KW  - Lighting conditions
KW  - Look-alike vial
KW  - Mixed reality
KW  - Objects detection
KW  - Patient safety
KW  - Real-time detection
KW  - Real-time identification
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chang, B.R.
AU  - Tsai, H.-F.
AU  - Syu, J.-S.
TI  - Implementing High-Speed Object Detection and Steering Angle Prediction for Self-Driving Control
PY  - 2025
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics14091874
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004848133&doi=10.3390%2felectronics14091874&partnerID=40&md5=01e6acf5e6cb833eda3fdf8de6bdbdd8
AB  - In the previous work, we proposed LWGSE-YOLOv4-tiny and LWDSG-ResNet18, leveraging depthwise separable and Ghost Convolutions for fast self-driving control while achieving a detection speed of 24.9 FPS. However, the system fell short of Level 4 autonomous driving safety requirements. That is, the control response speed of object detection integrated with steering angle prediction must exceed 39.2 FPS. This study enhances YOLOv11n with dual convolution and RepGhost bottleneck, forming DuCRG-YOLOv11n, significantly improving the object detection speed while maintaining accuracy. Similarly, DuC-ResNet18 improves steering angle prediction speed and accuracy. Our approach achieves 50.7 FPS, meeting Level 4 safety standards. Compared to previous work, DuCRG-YOLOv11n boosts feature extraction speed by 912.97%, while DuC-ResNet18 enhances prediction speed by 45.37% and accuracy by 12.26%. © 2025 by the authors.
KW  - Dual Conv
KW  - DuC-ResNet18
KW  - DuCRG-YOLOv11n
KW  - object detection
KW  - reparameterization
KW  - RepGhost bottleneck
KW  - steering angle prediction
KW  - Object recognition
KW  - Driving control
KW  - Dual conv
KW  - DuC-resnet18
KW  - DuCRG-yolov11n
KW  - Objects detection
KW  - Reparameterization
KW  - Repghost bottleneck
KW  - Self drivings
KW  - Steering angle prediction
KW  - Steering angles
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Yang, D.
AU  - Song, T.
AU  - Ye, Y.
AU  - Song, Y.
AU  - Zhou, J.
AU  - Chen, J.
TI  - A lightweight object detector based on changeable-size lightweight convolution and context augmentation module for images captured by UAVs
PY  - 2025
T2  - Visual Computer
DO  - 10.1007/s00371-024-03749-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213706543&doi=10.1007%2fs00371-024-03749-w&partnerID=40&md5=fc0105327af0839595648716b5313517
AB  - Object detection based on images captured by UAVs has become a hot research topic. However, images have a rich variety of small targets and complex backgrounds. This poses significant challenges for existing object detectors. Furthermore, due to the limitations of UAV platforms, it is difficult to deploy complicated models. Therefore, a novel lightweight detection network LDN-UAV is introduced in this work. Firstly, the YOLOv5s network is redesigned to obtain the YOLOv5ss basic network, which reduces the complexity of the model while improving the detection performance. Moreover, a lightweight feature enhancement module is devised, which enhances the spatial utilization of features. Next, we propose a novel lightweight convolutional operation that maps receptive-field features to a specific size via a shared MLP, which simplifies the model and improves the performance. Moreover, a context augmentation module is created, which aggregates contextual information to increase the benefits of features through MLP branching and Softmax. Finally, a lightweight Decoupled-Head is designed to ensure efficient performance of the detection. To validate the advantages of proposed lightweight convolutions, we conduct extensive experiments on COCO2017 and VOC 7+12. Additionally, for images captured by UAVs, relevant experiments are performed based on VisDrone-DET2021. The results of all experiments demonstrate that the proposed method achieves better detection performance compared to state-of-the-art lightweight detectors. Compared to the original baseline model, LDN-UAV uses only 2.46M parameters and increased the metrics mAP50 and mAP by 8.6 % and 5.9 %, respectively. Code is available at https://github.com/CV-ZhangXin/LDN-UAV. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Changeable-size convolution
KW  - Context augmentation module
KW  - Images captured by UAVs
KW  - Object detection
KW  - Aircraft detection
KW  - Change detection
KW  - Image enhancement
KW  - Object detection
KW  - Object recognition
KW  - Object tracking
KW  - Changeable-size convolution
KW  - Context augmentation module
KW  - Detection performance
KW  - Hot research topics
KW  - Image captured by UAV
KW  - Object detectors
KW  - Objects detection
KW  - Small targets
KW  - Target background
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Peng, R.
AU  - Liao, C.
AU  - Pan, W.
AU  - Gou, X.
AU  - Zhang, J.
AU  - Lin, Y.
TI  - Improved YOLOv7 for small object detection in airports: Task-oriented feature learning with Gaussian Wasserstein loss and attention mechanisms
PY  - 2025
T2  - Neurocomputing
DO  - 10.1016/j.neucom.2025.129844
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000608898&doi=10.1016%2fj.neucom.2025.129844&partnerID=40&md5=f5a4c5ba731d3c28cb6f383721b1e8ce
AB  - Small objects in the airport scene, such as Persons and Vehicles, can lead to low accuracy and robustness of the object detection task. To address the above problems, an improved YOLOv7 model is proposed to detect certain objects on the airport scene. Considering the perspective distortion of the monitoring camera on the airport surface, the deformable convolutional feature extractor (DCFE) is first designed to adaptively extract features from input images for irregular occlusion objects. To learn task-oriented features from different channels, the attention mechanism is incorporated into the backbone to focus on informative concepts in a data-driven manner, formulating an attention feature extractor (AttFE). During the model training, the Normalized Gaussian Wasserstein distance (NWD) is considered as the loss function to measure the prediction errors after converting the bounding boxes into Gaussian distribution, thereby enhancing the ability to fit the small objects. A real-world airport surface dataset (ASD) is constructed to validate the proposed model. Extensive experimental results demonstrate that the proposed model outperforms selective baselines, achieving a 1.2% absolute improvement in mAP over the original YOLOv7 network. Experiments conducted on multiple common datasets and the results demonstrate that the proposed model exhibits superior performance in terms of mAP. All proposed technical modules contribute to expected performance improvement. Most importantly, the proposed model achieves higher performance for small objects and has the desired robustness over occluded objects. © 2025 Elsevier B.V.
KW  - Airport object detection
KW  - Deformable convolution
KW  - Normalized Gaussian Wasserstein distance
KW  - Small objects
KW  - YOLOv7
KW  - Airport runways
KW  - Object detection
KW  - Object recognition
KW  - Object tracking
KW  - Airport object
KW  - Airport object detection
KW  - Deformable convolution
KW  - Gaussians
KW  - Normalized gaussian wasserstein distance
KW  - Objects detection
KW  - Performance
KW  - Small objects
KW  - Wasserstein distance
KW  - YOLOv7
KW  - ablation therapy
KW  - aircraft
KW  - airport
KW  - Article
KW  - attention network
KW  - back propagation
KW  - convolution algorithm
KW  - convolutional neural network
KW  - feature extraction
KW  - human
KW  - kernel method
KW  - learning
KW  - motor vehicle
KW  - normal distribution
KW  - normalized gaussian wasserstein distance
KW  - novel object recognition test
KW  - outcomes research
KW  - prediction error
KW  - quantitative analysis
KW  - response generalization
KW  - sensitivity analysis
KW  - yolov7 model
KW  - Gaussian distribution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, J.
AU  - Wu, Y.
AU  - Deng, R.
AU  - Xu, S.
AU  - Gao, J.
AU  - Burke, A.
TI  - A Survey of Autonomous Driving from a Deep Learning Perspective
PY  - 2025
T2  - ACM Computing Surveys
DO  - 10.1145/3729420
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009652283&doi=10.1145%2f3729420&partnerID=40&md5=ef50f8e07db173ebeb72c6fc74d0bf4c
AB  - Autonomous driving represents a significant advancement in the transportation industry, enhancing vehicle intelligence, optimizing traffic management, and improving user experiences. Central to these innovations is deep learning, which enables systems to handle complex data and make informed decisions. Our survey explores critical applications of deep learning in autonomous driving, such as perception and detection, localization and mapping, and decision-making and control. We investigate specialized deep learning techniques, including convolutional neural networks, recurrent neural networks, self-attention transformers, and their variants, among others. These methods are applied within various learning paradigms - supervised, unsupervised, and reinforcement learning - to suit the specific needs of autonomous driving. Our analysis evaluates the effectiveness, benefits, and limitations of these technologies, focusing on their integration with other intelligent algorithms to enhance system performance. Furthermore, we examine the architectures of autonomous systems, analyzing how knowledge and information are organized from modular, pipeline-based frameworks to comprehensive end-to-end models. By presenting an exhaustive overview of the progressing domain of autonomous driving and bridging various research areas, our survey aims to synthesize diverse research threads into a unified narrative. This effort not only aims to enhance our understanding but also pushes the boundaries of what is achievable in this interdisciplinary field.  © 2025 Copyright held by the owner/author(s).
KW  - Autonomous driving
KW  - decision-making
KW  - deep learning
KW  - end-to-end
KW  - perception
KW  - real-world
KW  - reinforcement learning
KW  - sensor fusion
KW  - simulation
KW  - trajectory planning
KW  - Automobile drivers
KW  - Autonomous vehicles
KW  - Behavioral research
KW  - Convolutional neural networks
KW  - Decision making
KW  - Deep neural networks
KW  - Deep reinforcement learning
KW  - Human computer interaction
KW  - Human engineering
KW  - Intelligent vehicle highway systems
KW  - Learning systems
KW  - Traffic control
KW  - User experience
KW  - Autonomous driving
KW  - Decisions makings
KW  - Deep learning
KW  - End to end
KW  - Real-world
KW  - Reinforcement learnings
KW  - Sensor fusion
KW  - Simulation
KW  - Trajectory Planning
KW  - Transportation industry
KW  - Reinforcement learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ullah, R.
AU  - Zhang, S.
AU  - Asif, M.
AU  - Wahab, F.
TI  - Multimodal learning-based speech enhancement and separation, recent innovations, new horizons, challenges and real-world applications
PY  - 2025
T2  - Computers in Biology and Medicine
DO  - 10.1016/j.compbiomed.2025.110082
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001492793&doi=10.1016%2fj.compbiomed.2025.110082&partnerID=40&md5=fed4e3bdc349dccef8592fb3b276071d
AB  - With the increasing global prevalence of disabling hearing loss, speech enhancement technologies have become crucial for overcoming communication barriers and improving the quality of life for those affected. Multimodal learning has emerged as a powerful approach for speech enhancement and separation, integrating information from various sensory modalities such as audio signals, visual cues, and textual data. Despite substantial progress, challenges remain in synchronizing modalities, ensuring model robustness, and achieving scalability for real-time applications. This paper provides a comprehensive review of the latest advances in the most promising strategy, multimodal learning for speech enhancement and separation. We underscore the limitations of various methods in noisy and dynamic real-world environments and demonstrate how multimodal systems leverage complementary information from lip movements, text transcripts, and even brain signals to enhance performance. Critical deep learning architectures are covered, such as Transformers, Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and generative models like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models. Various fusion strategies, including early and late fusion and attention mechanisms, are explored to address challenges in aligning and integrating multimodal inputs effectively. Furthermore, the paper explores important real-world applications in areas like automatic driver monitoring in autonomous vehicles, emotion recognition for mental health monitoring, augmented reality in interactive retail, smart surveillance for public safety, remote healthcare and telemedicine, and hearing assistive devices. Additionally, critical advanced procedures, comparisons, future challenges, and prospects are discussed to guide future research in multimodal learning for speech enhancement and separation, offering a roadmap for new horizons in this transformative field. © 2025 Elsevier Ltd
KW  - Audio-visual signal processing
KW  - Deep learning
KW  - Fusion techniques
KW  - Multimodal learning
KW  - Speech denoising
KW  - Speech separation
KW  - Deep Learning
KW  - Humans
KW  - Neural Networks, Computer
KW  - Speech
KW  - Convolutional neural networks
KW  - Emotion Recognition
KW  - Graph neural networks
KW  - Hearing aids
KW  - Speech enhancement
KW  - Audio-visual
KW  - Audio-visual signal processing
KW  - Deep learning
KW  - Fusion techniques
KW  - Hearing loss
KW  - Multi-modal learning
KW  - Real-world
KW  - Speech denoising
KW  - Speech separation
KW  - Visual signal processing
KW  - augmented reality
KW  - autoencoder
KW  - autonomous vehicle
KW  - communication barrier
KW  - convolutional neural network
KW  - deep learning
KW  - diffusion
KW  - generative adversarial network
KW  - generative model
KW  - graph neural network
KW  - health survey
KW  - hearing
KW  - hearing impairment
KW  - human
KW  - learning
KW  - mental health
KW  - prevalence
KW  - quality of life
KW  - review
KW  - self help device
KW  - signal processing
KW  - speech
KW  - telemedicine
KW  - artificial neural network
KW  - Generative adversarial networks
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, T.
AU  - Liu, B.
AU  - Chen, P.
TI  - Dynamic Cascade Detector for Storage Tanks and Ships in Optical Remote Sensing Images
PY  - 2025
T2  - Remote Sensing
DO  - 10.3390/rs17111882
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007731419&doi=10.3390%2frs17111882&partnerID=40&md5=3c0d00cdc28ec140dc6e7f4326fb0f94
AB  - Regional Convolutional Neural Network (RCNN)−based detectors have played a crucial role in object detection in remote sensing images due to their exceptional detection capabilities. Some studies have shown that different stages should have different Intersections of Union (IoU) thresholds to distinguish positive and negative samples because each stage has different IoU distributions. However, these studies have overlooked the fact that the IoU distribution at each stage changes continuously during the training process. Therefore, the IoU threshold at each stage should also be adjusted continuously to adapt to the changes in the IoU distribution. We realized that the IoU distribution at each stage is very similar to a Gaussian skewed distribution. In this paper, we introduce a novel dynamic IoU threshold method based on the Cascade RCNN architecture, called the Dynamic Cascade detector, with reference to the Gaussian skewed distribution. We tested the effectiveness of this method by detecting horizontal storage tanks and rotated ships in optical remote sensing images. Our experiments demonstrated that this technique can significantly improve detection results, as evaluated based on the COCO metric. In addition, the threshold range of the last stage impacts other stages, so the threshold range of one stage may change significantly when the number of stages changes. Furthermore, the threshold may not always increase during the training process and may decrease when the IoU distribution resembles a negatively skewed distribution. © 2025 by the authors.
KW  - Dynamic RCNN
KW  - Gaussian skewed distribution
KW  - object detection
KW  - optical remote sensing image
KW  - Convolutional neural networks
KW  - Gaussian distribution
KW  - Vehicle detection
KW  - Convolutional neural network
KW  - Dynamic regional convolutional neural network
KW  - Gaussian skewed distribution
KW  - Gaussians
KW  - Objects detection
KW  - Optical remote sensing
KW  - Optical remote sensing image
KW  - Remote sensing images
KW  - Skewed distribution
KW  - Storage tank
KW  - Optical remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Asamoah, J.K.
AU  - Agyei Kyem, B.
AU  - Obeng-Amoako, N.D.
AU  - Aboah, A.
TI  - SAAM-ReflectNet: Sign-aware attention-based multitasking framework for integrated traffic sign detection and retroreflectivity estimation
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2025.128003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005071568&doi=10.1016%2fj.eswa.2025.128003&partnerID=40&md5=4a14c7126837522de293ced9635f4386
AB  - Traffic sign retroreflectivity is essential for roadway safety, particularly in low-light and adverse weather conditions. Traditional methods, such as handheld retroreflectometers and nighttime inspections, are labor-intensive, costly, and unsuitable for large-scale implementation. To address these limitations, we developed SAAM-ReflectNet, a deep learning framework that unifies traffic sign detection, classification, and retroreflectivity estimation into a single automated pipeline. Our RetroNet backbone, developed as part of this study, extracts robust spatial and semantic features to enhance feature representation. The Sign-Aware Attention Module we designed prioritizes critical traffic sign regions, improving detection and classification accuracy by focusing on the most relevant areas. Additionally, our multimodal fusion layers seamlessly integrate RGB imagery with LiDAR intensity data, enabling reliable retroreflectivity estimation. ReflectNet achieved a mean Average Precision (mAP) of 0.635 at IoU=0.5 and 0.522 across IoU thresholds from 0.5 to 0.95, alongside Root Mean Squared Errors (RMSE) of 0.169 for foreground and 0.147 for background reflectivity. Across 15 evaluation runs, performance improvements were statistically significant compared to all baselines (p < 0.05), underscoring the consistency and reliability of ReflectNet.These findings underscore the reliability, scalability, and transferability of our approach, establishing ReflectNet as a transformative tool for intelligent transportation systems and proactive traffic sign maintenance. © 2025 Elsevier Ltd
KW  - Computer vision
KW  - LiDAR
KW  - Multi-task learning
KW  - Object detection
KW  - Retroreflectivity
KW  - Traffic signs
KW  - Transportation systems
KW  - Advanced driver assistance systems
KW  - Air traffic control
KW  - Deep learning
KW  - Highway traffic control
KW  - Multi-task learning
KW  - Multimodal transportation
KW  - Street traffic control
KW  - Variable message signs
KW  - Adverse weather
KW  - Condition
KW  - LiDAR
KW  - Low light
KW  - Multitask learning
KW  - Objects detection
KW  - Retroreflectivity
KW  - Roadway safety
KW  - Traffic sign detection
KW  - Transportation system
KW  - Road and street markings
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ren, J.
AU  - Wen, C.
AU  - Zhang, L.
AU  - Su, H.
AU  - Yang, C.
AU  - Lv, Y.
AU  - Yang, N.
AU  - Qin, X.
TI  - High performance point-Voxel feature set abstraction with mamba for 3D object detection
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2025.128127
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005225379&doi=10.1016%2fj.eswa.2025.128127&partnerID=40&md5=7a75a007a506bfe9b339fde987426d0f
AB  - In the field of autonomous driving, a two-stage three-dimensional object detection approach has seen significant advancements. However, challenges persist in terms of detection accuracy, which can have a profound impact on the safety of autonomous vehicles. This study examined four critical issues that impair the accuracy and efficiency of the model: limited acceptance fields, slow acquisition of global features from voxels, challenges in capturing keypoint features, and uncertainties associated with network post-processing. To address these challenges, we propose four novel techniques: (1) a non-empty voxel feature extraction method that utilises linear angular attention to broaden the receptive field; (2) an efficient voxel feature extraction and downsampling approach based on Mamba2, designed to accelerate the acquisition of global voxel features; (3) a node extraction strategy that employs the Kolmogorov-Arnold Network (KAN) to extract key point features via segmented farthest point sampling (S-FPS); (4) a fuzzy non-maximum suppression (Fuzzy-NMS) method that refines suppression thresholds during the post-processing phase. By integrating these techniques, we introduce a High-Performance Point-Voxel Region Convolutional Neural Network (HP-PV-RCNN) algorithm specifically tailored for precise 3D object detection. We validated the effectiveness of the HP-PV-RCNN algorithm through comprehensive experiments using the Kitti, NuScenes, and Waymo open datasets. Specifically, our proposed network attained average precisions of 83.73 % for vehicles, 76.32 % for bicycles, and 53.52 % for pedestrians in the medium-difficulty category of the Kitti dataset for detecting these entities. The code and model are available at https://github.com/jlauwcj/HP-PV-RCNN. © 2025 Elsevier Ltd
KW  - 3D Object detection
KW  - Automatic driving
KW  - Hybrid point-voxel approach
KW  - Mamba
KW  - Point cloud
KW  - Fuzzy neural networks
KW  - Image segmentation
KW  - Pedestrian safety
KW  - 3D object
KW  - 3d object detection
KW  - Automatic driving
KW  - Hybrid point-voxel approach
KW  - Keypoints
KW  - Mamba
KW  - Objects detection
KW  - Performance points
KW  - Point-clouds
KW  - Post-processing
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, B.
AU  - Tao, T.
AU  - Wu, W.
AU  - Zhang, Y.
AU  - Meng, X.
AU  - Yang, J.
TI  - MultiDistiller: Efficient Multimodal 3D Detection via Knowledge Distillation for Drones and Autonomous Vehicles
PY  - 2025
T2  - Drones
DO  - 10.3390/drones9050322
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006653864&doi=10.3390%2fdrones9050322&partnerID=40&md5=4caf598885092898a9e674c3323a1da0
AB  - Real-time 3D object detection is a cornerstone for the safe operation of drones and autonomous vehicles (AVs)—drones must avoid millimeter-scale power lines in cluttered airspace, while AVs require instantaneous recognition of pedestrians and vehicles in dynamic urban environments. Although significant progress has been made in detection methods based on point clouds, cameras, and multimodal fusion, the computational complexity of existing high-precision models struggles to meet the real-time requirements of vehicular edge devices. Additionally, during the model lightweighting process, issues such as multimodal feature coupling failure and the imbalance between classification and localization performance often arise. To address these challenges, this paper proposes a knowledge distillation framework for multimodal 3D object detection, incorporating attention guidance, rank-aware learning, and interactive feature supervision to achieve efficient model compression and performance optimization. Specifically: To enhance the student model’s ability to focus on key channel and spatial features, we introduce attention-guided feature distillation, leveraging a bird’s-eye view foreground mask and a dual-attention mechanism. To mitigate the degradation of classification performance when transitioning from two-stage to single-stage detectors, we propose ranking-aware category distillation by modeling anchor-level distribution. To address the insufficient cross-modal feature extraction capability, we enhance the student network’s image features using the teacher network’s point cloud spatial priors, thereby constructing a LiDAR-image cross-modal feature alignment mechanism. Experimental results demonstrate the effectiveness of the proposed approach in multimodal 3D object detection. On the KITTI dataset, our method improves network performance by 4.89% even after reducing the number of channels by half. © 2025 by the authors.
KW  - 3D object detection
KW  - autonomous vehicles
KW  - drones
KW  - intelligent perception
KW  - knowledge distillation
KW  - LiDAR
KW  - multimodal fusion
KW  - Distillation equipment
KW  - Image analysis
KW  - Image enhancement
KW  - Learning to rank
KW  - Students
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous Vehicles
KW  - Intelligent perception
KW  - Knowledge distillation
KW  - LiDAR
KW  - Multi-modal
KW  - Multi-modal fusion
KW  - Objects detection
KW  - Point-clouds
KW  - Vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yao, X.
AU  - Liu, P.
AU  - Zhou, J.
AU  - Wang, Z.
AU  - Fan, S.
AU  - Wang, Y.
TI  - MAT-PointPillars: Enhanced PointPillars algorithm based on multi-scale attention mechanisms and transformer
PY  - 2025
T2  - PLOS ONE
DO  - 10.1371/journal.pone.0325373
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009074468&doi=10.1371%2fjournal.pone.0325373&partnerID=40&md5=2c0feb52fb841da5da2dea545dfc455a
AB  - Aiming at the problem that small and irregular detection targets such as cyclists have low detection accuracy and inaccurate recognition by existing 3D target detection algorithms, MAT-PointPillars (Multi-scale Attention and Transformer PointPillars), a 3D object detection algorithm, extends PointPillars with multi-scale vision Transformers and attention mechanisms. First, the algorithm employs pillar coding for semantic point cloud encoding and introduces an attention mechanism to refine the backbone’s upsampling process. Furthermore, the Transformer Encoder is introduced to improve the upsampling structure of the third stage of the backbone. On the KITTI dataset, our algorithm achieved 3D average detection accuracy (AP3D) of 81.15%, 62.02%, and 58.68% across three difficulty levels. Compared with the baseline model, the proposed algorithm improves AP3D by 2.44%, 1.19%, and 1.23% respectively. The real-time 3D object detection system is built based on ROS, and average running frames per second of the system is 22.63, which is higher than the sampling frequency of conventional LiDAR. By ensuring sufficient detection speed, the MAT-PointPillars algorithm can increase detection accuracy of cyclists in real-world scenarios. © 2025 Yao et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
KW  - algorithm
KW  - article
KW  - cyclist
KW  - detection algorithm
KW  - diagnosis
KW  - human
KW  - human experiment
KW  - Mien (people)
KW  - velocity
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Giri, K.J.
TI  - SO-YOLOv8: A novel deep learning-based approach for small object detection with YOLO beyond COCO
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2025.127447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002047156&doi=10.1016%2fj.eswa.2025.127447&partnerID=40&md5=7e4ad25415622f5d7f111d04f3ce6ad9
AB  - Small object detection in images is a significant challenge in computer vision due to issues like low resolution, occlusion, and scale variation, often resulting in existing models missing important details or requiring complex, large-scale setups. This paper introduces SO-YOLOv8, an enhanced version of the YOLO model that focuses on small object detection. The proposed model uses advanced hyperparameter optimization, sophisticated data augmentation, and multi-scale training to improve detection accuracy. SO-YOLOv8 also includes a Squeeze-and-Excitation (SE) block, which helps the model better recognize features of small objects. Experimental results on the PASCAL VOC 2012 dataset, a benchmark known for diverse and challenging object scales, demonstrate substantial improvements, achieving a precision of 1.0, showing an increase of 6% and an enhanced mean Average Precision (mAP) score of 0.79, reflecting a 1% increase in mAP compared to YOLOv8. While the mAP gain may seem marginal, even a slight improvement in small object detection significantly impacts real-world applications such as autonomous vehicles (detecting distant pedestrians or small road hazards), surveillance and security (identifying concealed objects in crowded environments), medical imaging (spotting small anomalies like tumors), and remote sensing (detecting small objects in satellite or drone imagery). Also, the 6% increase in precision indicates a significant reduction in false positives, making the detection system more reliable and reducing misclassifications that could otherwise lead to critical errors. These findings confirm that targeted customization of YOLO's architecture can effectively address the challenges associated with small object detection. This research contributes to the ongoing development of object detection methodologies and establishes a robust foundation for future work in small object detection. © 2025 Elsevier Ltd
KW  - Computer vision
KW  - Deep learning
KW  - Optimization
KW  - Precision
KW  - Small object detection
KW  - YOLOv8
KW  - Deep learning
KW  - Large-scales
KW  - Learning-based approach
KW  - Lower resolution
KW  - Model use
KW  - Optimisations
KW  - Precision
KW  - Small object detection
KW  - Small objects
KW  - YOLOv8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xie, B.J.
AU  - Li, H.
AU  - Luan, Z.
AU  - Li, X.X.
AU  - Lei, Z.
TI  - A lightweight coal mine pedestrian detector for video surveillance systems with multi-level feature fusion and channel pruning
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-87157-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218947395&doi=10.1038%2fs41598-025-87157-7&partnerID=40&md5=afb3894a7cc74242857e7e14245cc8c2
AB  - Pedestrian detection in coal mines is crucial for video surveillance systems. Limited computational resources pose challenges to deploying large models, affecting detection efficiency. To address this, we propose a lightweight pedestrian in coal mine detector with multi-level feature fusion. Our approach integrates the backbone network with coordinate attention, introducing a bidirectional feature pyramid network and a thin neck technique to enhance multi-scale detection capability while reducing computational load. We also employ regression loss with a dynamic focus mechanism for bounding box regression to minimize model errors. The Linkage Channel Pruning method enforces channel-level sparsity on the designed detector to achieve network slimming and secondary lightweight development. Results on a proprietary dataset demonstrate our method’s parameters (0.61 M), computational load (2.0 GFLOPs), model size (1.48 MB), detection accuracy (0.966), and inference time (2.1 ms). Compared to the baseline, our method achieves a 4.96 × reduction in parameters, a 4.05 × reduction in computational load, a 4.02 × reduction in model size, a 59.62% reduction in inference time, and a 1.2% accuracy improvement. Experimental validation on proprietary and public datasets confirms that our method exhibits state-of-the-art lightweight performance, accuracy, and real-time capability, demonstrating significant potential in practical engineering applications. The insights gained provide technical references and real-time accident prevention for coal mine video surveillance systems. © The Author(s) 2025.
KW  - Accident prevention
KW  - Channel pruning
KW  - Coal mine pedestrian detection
KW  - Lightweight architecture
KW  - Video surveillance
KW  - accident prevention
KW  - article
KW  - coal mining
KW  - human
KW  - pedestrian
KW  - video surveillance
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Seidaliyeva, U.
AU  - Ilipbayeva, L.
AU  - Utebayeva, D.
AU  - Smailov, N.
AU  - Matson, E.T.
AU  - Tashtay, Y.
AU  - Turumbetov, M.
AU  - Sabibolda, A.
TI  - LiDAR Technology for UAV Detection: From Fundamentals and Operational Principles to Advanced Detection and Classification Techniques
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25092757
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004919528&doi=10.3390%2fs25092757&partnerID=40&md5=b3324ff0bb470f63f9c65933eaede5dd
AB  - As unmanned aerial vehicles (UAVs) are increasingly employed across various industries, the demand for robust and accurate detection has become crucial. Light detection and ranging (LiDAR) has developed as a vital sensor technology due to its ability to provide rich 3D spatial information, particularly in applications such as security and airspace monitoring. This review systematically explores recent innovations in LiDAR-based drone detection, deeply focusing on the principles and components of LiDAR sensors, their classifications based on different parameters and scanning mechanisms, and the approaches for processing LiDAR data. The review briefly compares recent research works in LiDAR-based only and its fusion with other sensor modalities, the real-world applications of LiDAR with deep learning, as well as the major challenges in sensor fusion-based UAV detection. © 2025 by the authors.
KW  - 3D object detection
KW  - deep learning
KW  - deep learning for point cloud processing
KW  - drone detection
KW  - LiDAR
KW  - LiDAR classifications
KW  - object detection
KW  - point clouds
KW  - scanning mechanism
KW  - UAV detection
KW  - unmanned aerial vehicles (UAVs)
KW  - Air navigation
KW  - Micro air vehicle (MAV)
KW  - Target drones
KW  - 3D object
KW  - 3d object detection
KW  - Aerial vehicle
KW  - Cloud processing
KW  - Deep learning
KW  - Deep learning for point cloud processing
KW  - Drone detection
KW  - Light detection and ranging
KW  - Light detection and ranging classification
KW  - Objects detection
KW  - Point-clouds
KW  - Scanning mechanisms
KW  - Unmanned aerial vehicle
KW  - Unmanned aerial vehicle detection
KW  - Vehicles detection
KW  - classification
KW  - cloud computing
KW  - deep learning
KW  - diagnosis
KW  - drone
KW  - human
KW  - review
KW  - sensor
KW  - unmanned aerial vehicle
KW  - Drones
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Trinh, L.
AU  - Mercelis, S.
AU  - Anwar, A.
TI  - A comprehensive review of datasets and deep learning techniques for vision in unmanned surface vehicles
PY  - 2025
T2  - Ocean Engineering
DO  - 10.1016/j.oceaneng.2025.121501
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005586392&doi=10.1016%2fj.oceaneng.2025.121501&partnerID=40&md5=061e8d05c620f68b5bf8249becd089b9
AB  - Unmanned Surface Vehicles (USVs) have emerged as a major platform in maritime operations, capable of supporting a wide range of applications. USVs allow for difficult unmanned tasks in harsh maritime environments. With the rapid development of USVs, many vision tasks such as detection and segmentation become increasingly important. Datasets play an important role in encouraging and improving the research and development of reliable vision algorithms for USVs. In this regard, a large number of recent studies have focused on the release of vision datasets for USVs. Along with the development of datasets, a variety of deep learning techniques have also been studied, with a focus on USVs. However, there is a lack of a systematic review of recent studies in both datasets and vision techniques to provide a comprehensive picture of the current development of vision on USVs, including limitations and trends. In this study, we provide a comprehensive review of both USV datasets and deep learning techniques for vision tasks. Our review was conducted using a large number of vision datasets from USVs. We elaborate several challenges and potential opportunities for research and development in USV vision based on a thorough analysis of current datasets and deep learning techniques. © 2025 Elsevier Ltd
KW  - Computer vision
KW  - Datasets
KW  - Deep learning
KW  - Unmanned surface vessels
KW  - 'current
KW  - Dataset
KW  - Deep learning
KW  - Learning techniques
KW  - Maritime environment
KW  - Maritime operation
KW  - Research and development
KW  - Surface vehicles
KW  - Unmanned surface vessels
KW  - Vision algorithms
KW  - algorithm
KW  - computer vision
KW  - data set
KW  - machine learning
KW  - unmanned vehicle
KW  - vessel
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Qiu, J.
AU  - Zhang, W.
AU  - Xu, S.
AU  - Zhou, H.
TI  - DP-YOLO: A lightweight traffic sign detection model for small object detection
PY  - 2025
T2  - Digital Signal Processing: A Review Journal
DO  - 10.1016/j.dsp.2025.105311
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005082446&doi=10.1016%2fj.dsp.2025.105311&partnerID=40&md5=9d09d118f701fc06916e6002e84eab5e
AB  - Autonomous driving is a critical area in artificial intelligence, with vast potential for development. While current object detection algorithms have shown strong performance in traffic sign detection, they still face difficulties with small object recognition, often resulting in missed or false detections. To address this, we propose DP-YOLO, a traffic sign detection algorithm based on YOLOv8s. To enhance detection accuracy for small objects and reduce the model's parameter count, we first removed the large object detection layer from the baseline model and added a small object detection layer. In the feature extraction stage, we design the DBBNCSPELAN4 module to boost the network's feature extraction capability. Additionally, we propose the PTCSP module, incorporating Transformer technology into the model's feature processing network and reducing both parameters and computational cost. Finally, we introduce the W3F_MPDIoU loss to mitigate the impact of low-quality samples on the model and enhance its robustness. Experiments demonstrate that, compared to YOLOv8s, DP-YOLO reduces the model's parameter count by 77.0%, while achieving improvements in mAP0.5 by 5.8% on the TT100K dataset, 2.7% on the GTSDB dataset, and 1.3% on the CCTSDB dataset. Experimental results demonstrate that the proposed method effectively enhances the detection capability for small-sized traffic signs and exhibits high potential for edge deployment. © 2025 The Author(s)
KW  - Autonomous driving
KW  - Lightweight
KW  - Small object detection
KW  - Traffic sign detection
KW  - TT100K
KW  - Vehicle detection
KW  - 'current
KW  - Autonomous driving
KW  - Detection models
KW  - Features extraction
KW  - Lightweight
KW  - Modeling parameters
KW  - Small object detection
KW  - Small objects
KW  - Traffic sign detection
KW  - Tt100k
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ye, Z.
AU  - You, J.
AU  - Gu, J.
AU  - Kou, H.
AU  - Li, G.
TI  - Modeling and Simulation of Urban Laser Countermeasures Against Low-Slow-Small UAVs
PY  - 2025
T2  - Drones
DO  - 10.3390/drones9060419
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009043503&doi=10.3390%2fdrones9060419&partnerID=40&md5=965933aa268971c4e31fdf3ee0d2e006
AB  - This study addresses the modeling and simulation challenges of urban laser countermeasure systems against Low-Slow-Small (LSS) UAVs by proposing a physics simulation framework integrating Geographic Information System (GIS)-based dynamic 3D real-world scenes and constructing a hybrid Anti-UAV dataset combining real and simulated data. A three-stage target tracking system is developed, encompassing target acquisition, coarse tracking, and precise tracking. Furthermore, the UAV-D-Fine detection algorithm is introduced, significantly improving small-target detection accuracy and efficiency. The simulation platform achieves dynamic fusion between target models and GIS real-scene models, enabling a full physical simulation of UAV takeoff, tracking, aiming, and laser engagement, with additional validation of laser antenna tracking performance. Experimental results demonstrate the superior performance of the proposed algorithm in both simulated and real-world environments, ensuring accurate UAV detection and sustained tracking, thereby providing robust support for low-altitude UAV laser countermeasure missions. © 2025 by the authors.
KW  - drone countermeasures
KW  - drone detection
KW  - maneuvering target
KW  - real-time tracking
KW  - Antennas
KW  - Clutter (information theory)
KW  - Drones
KW  - Geographic information systems
KW  - Information use
KW  - Simulation platform
KW  - Target drones
KW  - Drone countermeasure
KW  - Drone detection
KW  - Geographic information
KW  - Manoeuvring target
KW  - Model and simulation
KW  - Physics simulation
KW  - Real time tracking
KW  - Real-world
KW  - Simulation framework
KW  - Small UAV
KW  - Aircraft detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rabecka, D.V.
AU  - Pari, B.J.
TI  - A HYBRID FRAMEWORK FOR OBJECT DETECTION AND SEGMENTATION IN AUTONOMOUS VEHICLES USING YOLO NAS AND MASK R-CNN
PY  - 2025
T2  - Journal of Theoretical and Applied Information Technology
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001986858&partnerID=40&md5=7a8243e1c6fa45983908c97f50d6c3f9
AB  - A leading opinion prescription that can precisely establish and segregate objects in complex environments is necessary for the hurried development of self-driving autos. The new hybrid framework shown in this work improves object detection and segmentation performance by combining Mask R-CNN with You Only Look Once Neural Architecture Search (YOLO NAS). With the neck and head of YOLO-NAS retained, this study tries to boost the performance of YOLO-NAS by substituting a combination of Res Net and Feature Pyramid Network (FPN) for the default Rep Ne X t backbone. Additionally, to increase segmentation capabilities, the study integrates Mask R-CNN. Our methodology leverages the efficiency of YOLO NAS for fast object detection and the precision of Mask R-CNN for complex segmentation tasks using the KITTI dataset, a leading benchmark in autonomous driving research. This approach resolves issues such as disparate object sizes, obstructions, and complex backgrounds that are frequently encountered when driving in urban areas. Our cloud-based approach outperforms previous approaches in terms of precision, recall, and F1 scores. The results of our experiments show that this combination approach could greatly contribute to the development of more reliable and safer autonomous driving systems, paving the way for advancements in real-time perception technology. © Little Lion Scientific.
KW  - Autonomous Vehicles
KW  - Convolution Neural Network
KW  - Object Detection
KW  - You Only Look Once
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Singh, N.
AU  - Maurya, C.P.
AU  - Mahaur, B.
AU  - Singh, S.K.
TI  - Improved YOLOv11 with weights pruning for road object detection in rainy environment
PY  - 2025
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-025-04070-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002983615&doi=10.1007%2fs11760-025-04070-2&partnerID=40&md5=c82889b6199f1fe69d6977ff4bd3477d
AB  - Object detection on roadways is crucial for autonomous driving and advanced driver assistance systems. However, adverse weather conditions, notably rain, significantly degrade the performance of these systems. This paper presents a novel approach to improve the detection of road objects in rainy weather scenarios by applying YOLOv11 model. This includes specialized data augmentation techniques to simulate rainy conditions, adjustments in network architecture to improve resilience to rain-induced noise, and optimized training strategies to enhance model performance. The study leverages BDD100K, Cityscapes, and DAWN-Rainy datasets of various road scenarios under different rain intensities. We systematically augment these datasets to ensure the model learns to identify objects obscured by rain streaks and reflections. Such enhancements enable better handling of occlusions and reduced visibility in the feature extraction layers. Also, to ensure the model’s efficiency and suitability for real-time applications, we apply a network pruning technique, which reduces the model size and computational requirements without sacrificing performance. Extensive experiments demonstrate that our model has a comparable mean Average Precision with the baseline YOLOv11 but at a 2x compression ratio under rainy conditions. This research contributes to the field of autonomous driving by providing a more reliable object detection system for adverse weather conditions, improving overall road safety. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - Deep learning
KW  - Rain generation and augmentation
KW  - Road object detection
KW  - Weight pruning
KW  - Data compression ratio
KW  - Deep learning
KW  - Driver licensing
KW  - Motor transportation
KW  - Network security
KW  - Rain
KW  - Weather forecasting
KW  - Adverse weather
KW  - Autonomous driving
KW  - Condition
KW  - Deep learning
KW  - Objects detection
KW  - Performance
KW  - Rain generation and augmentation
KW  - Rainy conditions
KW  - Road object detection
KW  - Weight pruning
KW  - Advanced driver assistance systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, P.
AU  - Li, H.
AU  - Luo, X.
AU  - Hu, L.
AU  - Yang, R.
AU  - Zeng, N.
TI  - From data analysis to intelligent maintenance: a survey on visual defect detection in aero-engines
PY  - 2025
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/add6c8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005735634&doi=10.1088%2f1361-6501%2fadd6c8&partnerID=40&md5=c2db984d4f1cfb158423a49a7fbd5538
AB  - In this paper, a systematic review of aero-engine defect detection methods is presented, encompassing the general procedure, traditional and intelligent detection algorithms, performance optimization, and future trends. The complete process and innovative theories of aero-engine visual defect detection are analyzed in this overview. Specifically, a five-level taxonomy is designed, with each level further subdivided to provide deeper insights, from data acquisition and task-oriented detection with nondestructive testing (NDT), to practical applications. By leveraging multiscale feature fusion-based detection, these methods achieve enhanced precision in identifying defects across varying scales and complexities. Moreover, in-depth discussions and outlooks on performance optimization and efficient deployment strategies are provided to promote advanced intelligent maintenance solutions for high-end equipment, which may encourage more multidisciplinary collaborations. Compared to other existing surveys, this work comprehensively outlines how computer vision (CV)-based methods can assist in aero-engine defect detection for intelligent decision-making, and a connection between NDT technology and CV-based inspection has been established, thereby drawing greater attention to the application of artificial intelligence to further enhance the development of industrial predictive maintenance. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.
KW  - aero-engine
KW  - computer vision
KW  - defect detection
KW  - industrial artificial intelligence
KW  - multiscale feature fusion
KW  - Leak detection
KW  - Predictive maintenance
KW  - Aero-engine
KW  - Defect detection
KW  - Engine defects
KW  - Features fusions
KW  - Industrial artificial intelligence
KW  - Intelligent maintenance
KW  - Multi-scale features
KW  - Multiscale feature fusion
KW  - Performance optimizations
KW  - Visual defects
KW  - Taxonomies
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - He, S.
AU  - Chen, H.
AU  - He, L.
AU  - Xu, E.
AU  - Tang, T.
TI  - Active collision avoidance system based on TimesNet behavioral game model and ABAPF risk quantification map
PY  - 2025
T2  - Measurement: Journal of the International Measurement Confederation
DO  - 10.1016/j.measurement.2025.116670
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214681386&doi=10.1016%2fj.measurement.2025.116670&partnerID=40&md5=c22b09a6653b384f205dbf643f5fb55a
AB  - Active collision avoidance is necessary to avoid vehicle collisions. However, existing methods often have difficulties in detecting the potential risks of nearby vehicles and do not fully consider the impact of collision avoidance behavior on nearby vehicles. In order to address the two major challenges of collision avoidance behaviors interacting with each other and short accident response time, a vehicle active collision avoidance system based on TimesNet to predict vehicle intentions with time dependence and Adaptive Boundary Artificial Potential Field (ABAPF) is proposed. Firstly, the vehicles communicate through intelligent connected vehicles, and the surrounding road environment and vehicle motion are used as the risk sources to predict the travelling intentions of the surrounding vehicles for vehicle intentions using two TimesNet networks based on vehicle chassis information and driver habits. Then, a risk map containing potential risks is generated by ABAPF, and a safety factor assessment method is proposed to generate collision avoidance paths. The simulation and real vehicle test results show that the accuracy of the system is 94.2%, and the collision avoidance path has excellent performance in the evaluation. Avoids collisions well in the event of an accident. © 2025 Elsevier Ltd
KW  - Collision avoidance
KW  - Driver habit
KW  - Intent interaction
KW  - Safety factor assessment
KW  - TimesNet
KW  - Risk assessment
KW  - Vehicle safety
KW  - Artificial potential fields
KW  - Avoidance behaviour
KW  - Collision avoidance systems
KW  - Collisions avoidance
KW  - Driver habit
KW  - Game models
KW  - Intent interaction
KW  - Potential risks
KW  - Safety factor assessment
KW  - Timesnet
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ren, Z.
AU  - Yao, K.
AU  - Sheng, S.
AU  - Wang, B.
AU  - Lang, X.
AU  - Wan, D.
AU  - Fu, W.
TI  - YOLO-SDH: improved YOLOv5 using scaled decoupled head for object detection
PY  - 2025
T2  - International Journal of Machine Learning and Cybernetics
DO  - 10.1007/s13042-024-02357-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205894324&doi=10.1007%2fs13042-024-02357-3&partnerID=40&md5=43af934027e3b4aa343ab0d4c0610960
AB  - As a crucial component of computer vision tasks, object detection serves a significant role in a variety of fields, including autonomous driving, defect detection, and remote sensing image recognition. However, the majority of current object detection networks fail to achieve a decent balance between detection accuracy and detection efficiency, and there is room for improvement in terms of detection accuracy. In response, to improve detection accuracy, a more efficient network framework, YOLO-SDH, was proposed in this paper based on You Only Look Once v5 (YOLOv5). In addition, a decoupled head that automatically adjusts the number of channels according to the model size was proposed, which can enhance the network’s detection effect by separating the classification and regression tasks.On the premise of requiring less computation, a lightweight deformable convolution module is proposed so that the convolution can extract ROI over a wider range, thereby enhancing the accuracy of the object detection network. Experiments were run on the datasets of PASCAL VOC2012, NEU-DET, AW, and RSOD. In comparison to the original YOLOv5, the mAP 0.5 of YOLO-SDH improved by 1.29–3.03%, the F1-score improved by 1.2–3.2%, the Precision improved by 0.7–4.2%, demonstrating the algorithm’s efficacy and superiority. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Convolutional neural network
KW  - Decoupled head
KW  - Deep learning algorithm
KW  - Deformable CovNets
KW  - Object detection
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Image recognition
KW  - Object detection
KW  - Object recognition
KW  - Regression analysis
KW  - Autonomous driving
KW  - Convolutional neural network
KW  - Decoupled head
KW  - Deep learning algorithm
KW  - Defect detection
KW  - Deformable covnet
KW  - Detection accuracy
KW  - Detection networks
KW  - Driving defects
KW  - Objects detection
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, W.
AU  - Zhang, S.
AU  - Liu, H.
AU  - Zou, J.
TI  - A multimodal automatic generation and annotation framework for prohibited and restricted goods in online transactions
PY  - 2025
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2025.111000
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004022351&doi=10.1016%2fj.engappai.2025.111000&partnerID=40&md5=a4d5ebc9eaa91de3089ca7ef0499f3cb
AB  - Transactions of prohibited and restricted goods on e-commerce platforms threaten consumer safety and hinder Internet economy growth. However, the lack of such datasets hinders intelligent identification and effective regulation. To address this issue, a large-scale multimodal dataset of prohibited and restricted goods is constructed, comprising 18,446 images and 36,892 texts. Nevertheless, this is insufficient due to the diverse forms and deep concealment of prohibited and restricted products in online transactions. Therefore, we propose a multimodal automatic generation and annotation framework for prohibited and restricted goods in online transactions. This framework consists of an image generation module, a text description module, and an image annotation module. The image generation module is utilized to generate more diverse images for prohibited and restricted goods. The text description module can generate text descriptions for the generated images. The image annotation module is employed to annotate the generated images for various visual tasks. This framework achieved a 24.71 user rating for image generation, a 92.72 % mean average precision for image annotation, and an 81.67 % semantic score for image description. Experimental results demonstrate the effectiveness and accuracy of the proposed automatic annotation framework, which can enhance the effective supervision of prohibited and restricted goods. © 2025 Elsevier Ltd
KW  - Automatic annotation
KW  - Image generation
KW  - Large language model
KW  - Multimodal dataset
KW  - Online transaction
KW  - Prohibited and restricted goods
KW  - Marketplaces
KW  - Automatic annotation
KW  - Automatic Generation
KW  - Image annotation
KW  - Image generations
KW  - Language model
KW  - Large language model
KW  - Multi-modal
KW  - Multi-modal dataset
KW  - Online transaction
KW  - Prohibited and restricted good
KW  - Electronic money
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Baumann, N.
AU  - Ghignone, E.
AU  - Kühne, J.
AU  - Bastuck, N.
AU  - Becker, J.
AU  - Imholz, N.
AU  - Kränzlin, T.
AU  - Lim, T.Y.
AU  - Lötscher, M.
AU  - Schwarzenbach, L.
AU  - Tognoni, L.
AU  - Vogt, C.
AU  - Carron, A.
AU  - Magno, M.
TI  - ForzaETH Race Stack—Scaled Autonomous Head-to-Head Racing on Fully Commercial Off-the-Shelf Hardware
PY  - 2025
T2  - Journal of Field Robotics
DO  - 10.1002/rob.22429
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204468559&doi=10.1002%2frob.22429&partnerID=40&md5=12a274c729ec6dcaf6427338e19cb546
AB  - Autonomous racing in robotics combines high-speed dynamics with the necessity for reliability and real-time decision-making. While such racing pushes software and hardware to their limits, many existing full-system solutions necessitate complex, custom hardware and software, and usually focus on Time-TrIals rather than full unrestricted Head-to-head racing, due to financial and safety constraints. This limits their reproducibility, making advancements and replication feasible mostly for well-resourced laboratories with comprehensive expertise in mechanical, electrical, and robotics fields. Researchers interested in the autonomy domain but with only partial experience in one of these fields, need to spend significant time with familiarization and integration. The ForzaETH Race Stack addresses this gap by providing an autonomous racing software platform designed for F1TENTH, a 1:10 scaled Head-to-Head autonomous racing competition, which simplifies replication by using commercial off-the-shelf hardware. This approach enhances the competitive aspect of autonomous racing and provides an accessible platform for research and development in the field. The ForzaETH Race Stack is designed with modularity and operational ease of use in mind, allowing customization and adaptability to various environmental conditions, such as track friction and layout, which is exemplified by the various modularly implemented state estimation and control systems. Capable of handling both Time-Trials and Head-to-Head racing, the stack has demonstrated its effectiveness, robustness, and adaptability in the field by winning the official F1TENTH international competition multiple times. Furthermore, the stack demonstrated its reliability and performance at unprecedented scales, up to over (Formula presented.) on tracks up to 150 m in length. © 2024 Wiley Periodicals LLC.
KW  - autonomous driving
KW  - autonomous racing
KW  - motion control
KW  - open source software
KW  - path planning
KW  - robotic perception
KW  - state estimation
KW  - Commercial off-the-shelf
KW  - Motion planning
KW  - Robot programming
KW  - Robustness (control systems)
KW  - State estimation
KW  - Autonomous driving
KW  - Autonomous racing
KW  - Commercial off-the-shelf hardwares
KW  - High Speed
KW  - Open-source softwares
KW  - Real time decision-making
KW  - Real-time decision making
KW  - Robotic perception
KW  - Software and hardwares
KW  - Speed dynamics
KW  - Open source software
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - P, M.P.
AU  - K, U.
TI  - Video prediction based on temporal aggregation and recurrent propagation for surveillance videos
PY  - 2025
T2  - MethodsX
DO  - 10.1016/j.mex.2025.103402
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007893407&doi=10.1016%2fj.mex.2025.103402&partnerID=40&md5=c039dbbce93eaf39f0c78caed5da3d5c
AB  - Video prediction is essential for recreating absent frames in video sequences while maintaining temporal and spatial coherence. This procedure, known as video inpainting, seeks to reconstruct missing segments by utilizing data from available frames. Frame interpolation, a fundamental component of this methodology, detects and produces intermediary frames between input sequences. The suggested methodology presents a Bidirectional Video Prediction Network (BVPN) for precisely forecasting absent frames that occur before, after, or between specified input frames. The BVPN framework incorporates temporal aggregation and recurrent propagation to improve forecast accuracy. Temporal aggregation employs a series of reference frames to generate absent content by harnessing existing spatial and temporal data, hence assuring seamless coherence. Recurrent propagation enhances temporal consistency by integrating pertinent information from prior time steps to progressively improve predictions. The timing of frames is constantly controlled through intermediate activations in the BVPN, allowing for accurate synchronization and improved temporal alignment. A fusion module integrates intermediate interpretations to generate cohesive final outputs. Experimental assessments indicate that the suggested method surpasses current state-of-the-art techniques in video inpainting and prediction, attaining enhanced smoothness and precision. Surveillance video datasets demonstrate substantial enhancements in predictive accuracy, highlighting the strength and efficacy of the suggested strategy in practical application. • The proposed method integrates bidirectional video prediction, temporal aggregation, and recurrent propagation to effectively reconstruct missing intermediate video frames with enhanced accuracy. • Comparative analysis using the UCF-Crime dataset demonstrates higher PSNR and SSIM values for the proposed method, indicating improved frame quality and temporal consistency over existing techniques. • This research provides a robust framework for future advancements in video frame prediction, contributing to applications in anomaly detection, surveillance, and video restoration. © 2025 The Author(s)
KW  - Frames
KW  - Inpainting
KW  - Interpolation
KW  - Prediction
KW  - Recurrent propagation
KW  - Temporal aggregation
KW  - Time steps
KW  - article
KW  - crime
KW  - female
KW  - forecasting
KW  - human
KW  - middle aged
KW  - outlier detection
KW  - prediction
KW  - videorecording
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, S.
AU  - Chen, L.
AU  - Yang, K.
TI  - DGSS-YOLOv8s: A Real-Time Model for Small and Complex Object Detection in Autonomous Vehicles
PY  - 2025
T2  - Algorithms
DO  - 10.3390/a18060358
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009135853&doi=10.3390%2fa18060358&partnerID=40&md5=151710d8105d67c424f1a5a670f1f281
AB  - Object detection in complex road scenes is vital for autonomous driving, facing challenges such as object occlusion, small target sizes, and irregularly shaped targets. To address these issues, this paper introduces DGSS-YOLOv8s, a model designed to enhance detection accuracy and high-FPS performance within the You Only Look Once version 8 small (YOLOv8s) framework. The key innovation lies in the synergistic integration of several architectural enhancements: the DCNv3_LKA_C2f module, leveraging Deformable Convolution v3 (DCNv3) and Large Kernel Attention (LKA) for better the capture of complex object shapes; an Optimized Feature Pyramid Network structure (Optimized-GFPN) for improved multi-scale feature fusion; the Detect_SA module, incorporating spatial Self-Attention (SA) at the detection head for broader context awareness; and an Inner-Shape Intersection over Union (IoU) loss function to improve bounding box regression accuracy. These components collectively target the aforementioned challenges in road environments. Evaluations on the Berkeley DeepDrive 100K (BDD100K) and Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) datasets demonstrate the model’s effectiveness. Compared to baseline YOLOv8s, DGSS-YOLOv8s achieves mean Average Precision (mAP)@50 improvements of 2.4% (BDD100K) and 4.6% (KITTI). Significant gains were observed for challenging categories, notably 87.3% mAP@50 for cyclists on KITTI, and small object detection (AP-small) improved by up to 9.7% on KITTI. Crucially, DGSS-YOLOv8s achieved high processing speeds suitable for autonomous driving, operating at 103.1 FPS (BDD100K) and 102.5 FPS (KITTI) on an NVIDIA GeForce RTX 4090 GPU. These results highlight that DGSS-YOLOv8s effectively balances enhanced detection accuracy for complex scenarios with high processing speed, demonstrating its potential for demanding autonomous driving applications. © 2025 by the authors.
KW  - autonomous driving
KW  - high FPS detection
KW  - multi-scale feature fusion
KW  - small object detection
KW  - YOLOv8 optimization
KW  - Automobile drivers
KW  - Autonomous vehicles
KW  - Complex networks
KW  - Feature extraction
KW  - Object recognition
KW  - Roads and streets
KW  - Traffic control
KW  - Vehicle detection
KW  - Autonomous driving
KW  - Complex objects
KW  - Features fusions
KW  - High FPS detection
KW  - Institutes of technologies
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Optimisations
KW  - Small object detection
KW  - You only look once version 8 small optimization
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, C.
AU  - Jiang, S.
AU  - Cao, X.
TI  - Small-Target Detection Algorithm Based on STDA-YOLOv8
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25092861
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004827736&doi=10.3390%2fs25092861&partnerID=40&md5=73a3d6cddd5ab9a7193385b2eb449ac6
AB  - Due to the inherent limitations of detection networks and the imbalance in training data, small-target detection has always been a challenging issue in the field of target detection. To address the issues of false positives and missed detections in small-target detection scenarios, a new algorithm based on STDA-YOLOv8 is proposed for small-target detection. A novel network architecture for small-target detection is designed, incorporating a Contextual Augmentation Module (CAM) and a Feature Refinement Module (FRM) to enhance the detection performance for small targets. The CAM introduces multi-scale dilated convolutions, where convolutional kernels with different dilation rates capture contextual information from various receptive fields, enabling more accurate extraction of small-target features. The FRM performs adaptive feature fusion in both channel and spatial dimensions, significantly improving the detection precision for small targets. Addressing the issue of a significant disparity in the number of annotations between small and larger objects in existing classic public datasets, a new data augmentation method called Copy–Reduce–Paste is introduced. Ablation and comparative experiments conducted on the proposed STDA-YOLOv8 model demonstrate that on the VisDrone dataset, its accuracy improved by 5.3% compared to YOLOv8, reaching 93.5%; on the PASCAL VOC dataset, its accuracy increased by 5.7% compared to YOLOv8, achieving 94.2%, outperforming current mainstream target detection models and small-target detection algorithms like QueryDet, effectively enhancing small-target detection capabilities. © 2025 by the authors.
KW  - contextual augmentation
KW  - feature refinement
KW  - small-target detection
KW  - YOLOv8
KW  - Feature extraction
KW  - Contextual augmentation
KW  - Detection networks
KW  - Feature refinement
KW  - Inherent limitations
KW  - Small target detection
KW  - Small targets
KW  - Target detection algorithm
KW  - Targets detection
KW  - Training data
KW  - YOLOv8
KW  - algorithm
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - diagnosis
KW  - diagnostic test accuracy study
KW  - false positive result
KW  - human
KW  - Radar target recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, K.
AU  - Zhang, H.
AU  - Lyu, W.
AU  - Guo, Q.
AU  - Deng, Z.
AU  - Xu, W.
TI  - Efficient progressive aggregation enhancement network for defect detection
PY  - 2025
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/adbf86
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000529369&doi=10.1088%2f1361-6501%2fadbf86&partnerID=40&md5=93588d14de9dc806fc085c64bc8fb24e
AB  - Computer vision-based deep learning models are of great significance in industrial defect quality detection. Unlike natural objects, defects in industrial products are typically quite small and exhibit highly uneven scales, resulting in the suboptimal performance of conventional object detectors when encountered with complex defect detection. Hence, this paper introduces an efficient progressive aggregation enhanced network (EPAE-Net) with the goal of strengthening defect detection performance in complex scenarios. Firstly, a global context feature enhancement module is designed to model the global context of images, enhancing the model’s ability to perceive key information. Secondly, a downsampling module is designed using self-calibrated convolution to improve the detection performance of small targets. Subsequently, multiplex aggregation FPN is constructed to alleviate the interference caused by information conflicts during feature fusion, further enhance the interaction between cross-layer features, and enhance the detection ability of the model for defects with extreme aspect ratio. Finally, the efficient complete intersection over union loss function is introduced to refine the network and further enhance the performance of network defect detection. The mAP of the proposed EPAE-Net on the Tianchi fabric dataset, printed circuit board dataset, and NEU-DET dataset reaches 77.1%, 98.7%, and 81.5%, respectively. Compared with other state-of-the-art methods, EPAE-Net shows strong competitiveness. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.
KW  - attention mechanism
KW  - deep learning models
KW  - feature augmentation
KW  - global context feature
KW  - industrial defect detection
KW  - Contrastive Learning
KW  - Deep learning
KW  - Attention mechanisms
KW  - Context features
KW  - Deep learning model
KW  - Defect detection
KW  - Detection performance
KW  - Feature augmentation
KW  - Global context
KW  - Global context feature
KW  - Industrial defect detection
KW  - Learning models
KW  - Aspect ratio
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - CAO, X.
AU  - NING, X.
AU  - LIU, S.
AU  - LIAN, X.
AU  - WANG, H.
AU  - ZHANG, G.
AU  - CHEN, F.
AU  - ZHANG, J.
AU  - LIU, B.
AU  - CHEN, Z.
TI  - Spacecraft intelligent orbital game technology: A review
PY  - 2025
T2  - Chinese Journal of Aeronautics
DO  - 10.1016/j.cja.2025.103480
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005082994&doi=10.1016%2fj.cja.2025.103480&partnerID=40&md5=fa0db1e06584bc73730374e9984d5134
AB  - In recent years, the availability of space orbital resources has been declining, and the increasing frequency of spacecraft close approach events has heightened the urgency for enhanced space security measures. This paper establishes a comprehensive framework for intelligent orbital game technology in space, encompassing four core technologies: threat perception of non-cooperative targets, intent recognition, situation assessment, and intelligent orbital game countermeasures. The concepts of multi-turn, multi-round and multi-match in space orbital games are defined, clarifying the core technological requirements for intelligent space orbital games and establishing a cohesive technological framework. Subsequently, the current status of research on these four core technologies is investigated. The challenges faced in the existing research are analyzed, and potential solutions for future studies are proposed. This paper aims to provide readers with a thorough understanding of the latest advancements in space intelligent orbital game technology, along with insights into the future directions and challenges in this field. © 2025 The Author(s)
KW  - Game confrontation
KW  - Intelligent orbital game
KW  - Intent recognition
KW  - Situation assessment
KW  - Threat perception
KW  - Core technology
KW  - Four-core
KW  - Game confrontation
KW  - Game technologies
KW  - Intelligent orbital game
KW  - Intent recognition
KW  - Orbitals
KW  - Situation assessment
KW  - Space security
KW  - Threat perception
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, C.
AU  - Song, C.
AU  - Xu, T.
AU  - Jiang, R.
TI  - Precision Weeding in Agriculture: A Comprehensive Review of Intelligent Laser Robots Leveraging Deep Learning Techniques
PY  - 2025
T2  - Agriculture (Switzerland)
DO  - 10.3390/agriculture15111213
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007793342&doi=10.3390%2fagriculture15111213&partnerID=40&md5=5c66772df048d6c99acebbe80b03ed60
AB  - With the advancement of modern agriculture, intelligent laser robots driven by deep learning have emerged as an effective solution to address the limitations of traditional weeding methods. These robots offer precise and efficient weed control, crucial for boosting agricultural productivity. This paper provides a comprehensive review of recent research on laser weeding applications using intelligent robots. Firstly, we introduce the content analysis method employed to organize the reviewed literature. Subsequently, we present the workflow of weeding systems, emphasizing key technologies such as the perception, decision-making, and execution layers. A detailed discussion follows on the application of deep learning algorithms, including Convolutional Neural Networks (CNNs), YOLO, and Faster R-CNN, in weed control. Here, we show that these algorithms can achieve high accuracy in weed detection, with YOLO demonstrating particularly fast and accurate performance. Furthermore, we analyze the challenges and open problems associated with deep learning detection systems and explore future trends in this research field. By summarizing the role of intelligent laser robots powered by deep learning, we aim to provide insights for researchers and practitioners in agriculture, fostering further innovation and development in this promising area. © 2025 by the authors.
KW  - artificial intelligence in agriculture
KW  - autonomous weeding
KW  - deep learning
KW  - intelligent laser robots
KW  - precision agriculture
KW  - weeding in agriculture
KW  - agricultural development
KW  - artificial intelligence
KW  - artificial neural network
KW  - laser method
KW  - machine learning
KW  - precision agriculture
KW  - robotics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ma, Q.-L.
AU  - Li, Z.-H.
AU  - Yan, H.
AU  - Zou, Z.
TI  - Object Detection During Vehicle Operation Based on Self-attention and Multi-point Cloud Feature Fusion
ST  - 基于自注意力和多点云特征融合的行车目标检测
PY  - 2025
T2  - Zhongguo Gonglu Xuebao/China Journal of Highway and Transport
DO  - 10.19721/j.cnki.1001-7372.2025.05.021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008444767&doi=10.19721%2fj.cnki.1001-7372.2025.05.021&partnerID=40&md5=3c00c74eccc6eb2679dd3b8d42026b1e
AB  - This study proposes a multi-feature fusion object detection model called MF-Point, based on the optimization of the PointPillars and PointRCNN detection methods. The purpose is to enhance the perception capabilities of autonomous vehicles in terms of the size, position, and orientation of objects in motion. The model integrates Full Self-Attention (FSA) mechanism to enhance network context awareness. Additionally, Dynamic Self-Attention (DSA) mechanism is employed during the upsampling process to effectively propagate structural information throughout the node network, thereby improving the spatial semantic representation of point-cloud features. Moreover, the baseline algorithm's network incorporates a new pillar feature extraction branch parallel to its original point feature extraction branch. Both branches adopt self-attention variants to generate comprehensive point features that are embedded into the baseline framework to improve the capability of the model to process and interpret point-cloud data. Experiments were conducted using the KITTI 3D object detection dataset, focusing on three target classes: vehicles, cyclists, and pedestrians. The results demonstrate that the improved MF-Point algorithm achieved detection accuracy improvements of 2.19%, 2.68%, and 2.12% for vehicles, pedestrians, and cyclists, respectively, in the 3D mode. In the Bird's Eye View (BEV) mode, the detection accuracies for these three target classes improved by 0.59%, 0.88%, and 4.67%, respectively. To further evaluate the generalizability and practical performance of the MF-point model, it was deployed in the ROS system. The model was validated using the 64-line KITTI dataset and 32-line real-world LiDAR data, achieving detection speeds of 8.4 frame·s-1 and 10.7 frame·s-1, respectively. Experiments demonstrate that the MF-Point model improves the detection performance for occluded moving objects, reducing the miss rate by 2.1%. The model exhibits high real-time performance and reliable detection accuracy on both public datasets and real-world tests. © 2025 Chang'an University. All rights reserved.
KW  - 3D target detection
KW  - automotive engineering
KW  - autonomous driving
KW  - LiDAR point cloud
KW  - self-attention mechanism
KW  - Automobile drivers
KW  - Autonomous vehicles
KW  - Extraction
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Optical radar
KW  - Pillar extraction
KW  - Target tracking
KW  - Three dimensional computer graphics
KW  - Vehicle detection
KW  - 3d target detection
KW  - Attention mechanisms
KW  - Automotives
KW  - Autonomous driving
KW  - Detection accuracy
KW  - LiDAR point cloud
KW  - Objects detection
KW  - Point-clouds
KW  - Self-attention mechanism
KW  - Targets detection
KW  - Automotive engineering
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Li, X.
AU  - Lin, M.
TI  - FE-YOLO: Fourier enhancement YOLO for end-to-end object detection in low-light conditions
PY  - 2025
T2  - Digital Signal Processing: A Review Journal
DO  - 10.1016/j.dsp.2025.105355
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006988873&doi=10.1016%2fj.dsp.2025.105355&partnerID=40&md5=027023ee1de00b76c5fda73d7d011727
AB  - Current low-light image object detection algorithms using deep learning often have low accuracy, high computational load, and poor model generalization. This paper presents an efficient end-to-end low-light image object detection network called FE-YOLO, which combines the amplitude and phase information from the Fourier transform with the object detection ability of YOLO. Firstly, a Fourier Enhanced Network (FENet) is proposed, which incorporates a Frequency Domain Processing Block (FPB). The FPB extracts frequency domain information and enhances image brightness and contrast by expanding the amplitude, improving image quality in low-light conditions. Secondly, a new approach to optimize both image enhancement and object detection simultaneously is presented. It combines the enhancement loss and the detection loss into a joint loss function, using two specific loss functions: amplitude difference loss and phase similarity loss. The functions accurately constrain the amplitude and phase of the image, balancing image enhancement and structure information preservation, thus improving the performance of object detection. Thirdly, a Joint comprehensive training strategy is used on FE-YOLO to improve its generalization through an end-to-end joint training approach. Experiments are conducted using low-light image datasets such as ExDark and DarkFace. The experimental results demonstrate that FENet outperforms other low-light image enhancement models and shows better noise immunity. FE-YOLO also performs excellently in low-light object detection. Furthermore, experiments using YOLOv8 confirm that it outperforms current state-of-the-art low-light image object detection algorithms. The detailed experimental results and program code have been made public at: https://github.com/tgliyang1985/FE-YOLO. © 2025
KW  - Amplitude difference loss
KW  - Fourier enhancement network
KW  - Low-light image
KW  - Object detection
KW  - Phase similarity loss
KW  - Fourier transforms
KW  - Frequency domain analysis
KW  - Image coding
KW  - Object detection
KW  - Photointerpretation
KW  - Amplitude difference
KW  - Amplitude difference loss
KW  - End to end
KW  - Fourier
KW  - Fourier enhancement network
KW  - Image object detection
KW  - Low light conditions
KW  - Low-light images
KW  - Objects detection
KW  - Phase similarity loss
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Norouzi, M.
AU  - Hosseini, S.H.
AU  - Khoshnevisan, M.
AU  - Moshiri, B.
TI  - Applications of pre-trained CNN models and data fusion techniques in Unity3D for connected vehicles
PY  - 2025
T2  - Applied Intelligence
DO  - 10.1007/s10489-024-06213-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218211183&doi=10.1007%2fs10489-024-06213-3&partnerID=40&md5=af9feb64217344de50b6c1ce6e6e9e72
AB  - Intelligent Transportation Systems (ITS) aim to enhance road safety and Internet of Things (IoT)-related solutions are crucial in achieving this objective. By leveraging Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) technologies, drivers can access valuable information about their surroundings. This research utilized the Unity 3D game engine to simulate various traffic scenarios, exploring a stochastic environment with two data sources: camera and road sign labels. We developed a full-duplex communication system to enable the communication between Python and Unity. This allows the vehicle to capture images in Unity and classify them using Convolutional Neural Network (CNN) models coded in Python. To improve road sign detection accuracy, we applied multi-sensor Data Fusion (DF) techniques to fuse the information received from the sources. We applied DF methods such as the Kalman filter, Dempster-Shafer theory, and Fuzzy Integral Operators to combine the two sources of information. Furthermore, our proposed CNN model incorporates an Ordered Weighted Averaging (OWA) layer to fuse information from three pre-trained CNN models. Our results show that the proposed model integrating the OWA layer achieved an accuracy of 98.81%, outperforming six state-of-the-art models. We compared the Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). In our work, EKF exhibited a lower execution time (0.02 seconds), yielding less accurate results. UKF, however, provided a more accurate estimate while being more computationally complex. Furthermore, the Dempster-Shafer model showed approximately 30% better accuracy compared to the Fuzzy Integral Operator. Using this methodology on autonomous vehicles in our virtual environment led to making more accurate decisions, even in a variety of weather conditions and accident scenarios. The findings of this research contribute to the development of more efficient and safer vehicles. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
KW  - Connected vehicles
KW  - Convolutional Neural Networks (CNN)
KW  - Data fusion
KW  - Dempster-shafer theory
KW  - Kalman filter
KW  - OWA operator
KW  - Unity game engine
KW  - Autonomous vehicles
KW  - Biocommunications
KW  - Convolutional neural networks
KW  - Highway accidents
KW  - Image segmentation
KW  - Motor transportation
KW  - Multilayer neural networks
KW  - Problem oriented languages
KW  - Procedure oriented languages
KW  - Road and street markings
KW  - Sensor data fusion
KW  - Steganography
KW  - Connected vehicle
KW  - Convolutional neural network
KW  - Data fusion technique
KW  - Dempster-Shafer theory
KW  - Fuzzy integral
KW  - Game Engine
KW  - Neural network model
KW  - Ordered weighted averaging operator
KW  - Unity game engine
KW  - Extended Kalman filters
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Adam, M.A.A.
AU  - Tapamo, J.R.
TI  - Survey on Image-Based Vehicle Detection Methods
PY  - 2025
T2  - World Electric Vehicle Journal
DO  - 10.3390/wevj16060303
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008902787&doi=10.3390%2fwevj16060303&partnerID=40&md5=8963876b2bb11d9c07865fb65a3191c6
AB  - Vehicle detection is essential for real-world applications such as road surveillance, intelligent transportation systems, and autonomous driving, where high accuracy and real-time performance are critical. However, achieving robust detection remains challenging due to scene complexity, occlusion, scale variation, and varying lighting conditions. Over the past two decades, numerous studies have been proposed to address these issues. This study presents a comprehensive and structured survey of image-based vehicle detection methods, systematically comparing classical machine learning techniques based on handcrafted features with modern deep learning approaches. Deep learning methods are categorized into one-stage detectors (e.g., YOLO, SSD, FCOS, CenterNet), two-stage detectors (e.g., Faster R-CNN, Mask R-CNN), transformer-based detectors (e.g., DETR, Swin Transformer), and GAN-based methods, highlighting architectural trade-offs concerning speed, accuracy, and practical deployment. We analyze widely adopted performance metrics from recent studies, evaluate characteristics and limitations of popular vehicle detection datasets, and explicitly discuss technical challenges, including domain generalization, environmental variability, computational constraints, and annotation quality. The survey concludes by clearly identifying open research challenges and promising future directions, such as efficient edge deployment strategies, multimodal data fusion, transformer-based enhancements, and integration with Vehicle-to-Everything (V2X) communication systems. © 2025 by the authors.
KW  - classical and deep learning
KW  - object detection
KW  - real-time detection
KW  - vehicle detection
KW  - Automobile drivers
KW  - Data communication systems
KW  - Deep learning
KW  - Human computer interaction
KW  - Intelligent vehicle highway systems
KW  - Learning systems
KW  - Motor transportation
KW  - Object detection
KW  - Object recognition
KW  - Security systems
KW  - Signal detection
KW  - Traffic control
KW  - Vehicle detection
KW  - Vehicle performance
KW  - Vehicle to vehicle communications
KW  - Autonomous driving
KW  - Classical and deep learning
KW  - Detection methods
KW  - High-accuracy
KW  - Image-based
KW  - Intelligent transportation systems
KW  - Objects detection
KW  - Real-time detection
KW  - Real-world
KW  - Vehicles detection
KW  - Economic and social effects
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mudavath, T.
AU  - Mamidi, A.
TI  - Object detection challenges: Navigating through varied weather conditions—Acomprehensive survey
PY  - 2025
T2  - Journal of Ambient Intelligence and Humanized Computing
DO  - 10.1007/s12652-025-04956-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001483013&doi=10.1007%2fs12652-025-04956-6&partnerID=40&md5=392bd506047ba6313295540d5c830d2e
AB  - Detecting objects in computer vision is a challenging task, especially under varied weather conditions such as rain, fog, snow, etc. which degrades the visibility, and illumination changes in the image. These conditions create specific challenges in object detection in recognizing the distinct objects in the image. The influence of changing weather conditions remains the cause for concern although deep learning has revolutionized object detection through multi-stage detectors that offer improved accuracy over one-stage detectors, which allow rapid inference. This survey offers a comprehensive exploration of the various object detection methods, and datasets, and emphasizes the challenges posed under the challenging weather conditions. By analyzing current methodologies in object detection and identifying gaps in existing research, this paper provides the limitations under the weather conditions and highlights the research opportunities under weather conditions for the development of better object detection methods that can be suitable for surveillance applications. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.
KW  - And YOLO
KW  - Classification
KW  - Deep learning
KW  - Localization
KW  - Machine learning
KW  - Object detection
KW  - Weather conditions
KW  - Computer vision
KW  - Object detection
KW  - Object recognition
KW  - Security systems
KW  - And YOLO
KW  - Condition
KW  - Deep learning
KW  - Detecting objects
KW  - Illumination changes
KW  - Localisation
KW  - Machine-learning
KW  - Object detection method
KW  - Objects detection
KW  - Weather condition
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wutke, M.
AU  - Debiasi, D.
AU  - Tomar, S.
AU  - Probst, J.
AU  - Kemper, N.
AU  - Gevers, K.
AU  - Lieboldt, M.-A.
AU  - Traulsen, I.
TI  - Multistage pig identification using a sequential ear tag detection pipeline
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-05283-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008710490&doi=10.1038%2fs41598-025-05283-8&partnerID=40&md5=d6e2ae34dbd984f5811a881bb390b203
AB  - Reliable animal identification in livestock husbandry is essential for various applications, including behavioral monitoring, welfare assessment, and the analysis of social structures. Although recent advancements in deep learning models have improved animal identification using biometric markers, their applicability remains limited for species without distinctive traits like pigs. Consequently, synthetic features such as ear tags have become widely adopted. However, challenges such as poor lighting conditions and the complexity of ear tag coding continue to restrict the effectiveness of Computer Vision and Deep Learning techniques. In this study, we introduce a robust, lighting-invariant method for individual pig identification that leverages commercially available ear tags within a sequential detection pipeline. Our approach employs four object detection models in succession to detect pigs, localize ear tags, perform rotation correction via pin detection, and recognize digits, ultimately generating a reliable ID proposal. In a first evaluation stage, we assessed the performance of each model independently, achieving a mAP0.95 value of 0.970, 0.979, 0.974 and 0.979 for the pig detection, ear tag detection, pin detection and ID classification model, respectively. In addition, our method was further evaluated in two different camera environments to assess its performance in both familiar and unfamiliar conditions. The results demonstrate that the proposed approach achieves a very high precision of 0.996 in a familiar top-down camera scenario and maintained a strong generalization performance in an unfamiliar, close-up setup with a precision of 0.913 and a recall of 0.903. Furthermore, by publicly proposing three custom datasets for ear tag, pin, and digit detection, we aim to support reproducibility and further research in automated animal identification for precision livestock farming. The findings of this study demonstrate the effectiveness of ID-based animal identification and the proposed method could be integrated within advanced multi-object tracking systems to enable continuous animal observation and for monitoring specific target areas, thereby significantly enhancing overall livestock management systems. © The Author(s) 2025.
KW  - Animal identification
KW  - Computer vision
KW  - YOLOv10
KW  - Animal Husbandry
KW  - Animal Identification Systems
KW  - Animals
KW  - Deep Learning
KW  - Ear
KW  - Swine
KW  - animal
KW  - animal husbandry
KW  - animal identification
KW  - deep learning
KW  - ear
KW  - pig
KW  - procedures
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mohammed, A.
AU  - Ibrahim, H.M.
AU  - Omar, N.M.
TI  - Optimizing RetinaNet anchors using differential evolution for improved object detection
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-02888-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008583584&doi=10.1038%2fs41598-025-02888-x&partnerID=40&md5=f0eb35854be8d576186b6d9dadee7fe3
AB  - Object detection is a fundamental task in computer vision. It has two primary types: one-stage detectors known for their high speed and efficiency, and two-stage detectors, which offer higher accuracy but are often slower due to their complex architecture. Balancing these two aspects has been a significant challenge in the field. RetinaNet, a premier single-stage object detector, is renowned for its remarkable balance between speed and accuracy. Its success is largely due to the groundbreaking focal loss function, which adeptly addresses the issue of class imbalance prevalent in object detection tasks. This innovative approach significantly enhances detection accuracy while maintaining high speed, making RetinaNet an ideal choice for a wide range of real-world applications. However, its performance decreases when applied to datasets containing objects with unique characteristics, such as objects with elongated or squat shapes. In such cases, the default anchor parameters may not fully meet the requirements of these specialized objects. To overcome this limitation, we present an enhancement to the RetinaNet model to improve its ability to handle variations in objects across different domains. Specifically, we propose an optimization algorithm based on Differential Evolution (DE) that adjusts anchor scales and ratios while determining the most appropriate number of these parameters for each dataset based on the annotated data. Through extensive experiments on datasets spanning diverse domains such as the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI), the Unconstrained Face Detection Dataset (UFDD), the TomatoPlantFactoryDataset, and the widely used Common Objects in Context (COCO) 2017 benchmark, we demonstrate that our proposed method significantly outperforms both the original RetinaNet and anchor-free methods by a considerable margin. © The Author(s) 2025.
KW  - Anchor optimization
KW  - Computer vision
KW  - Deep learning
KW  - Differential evolution
KW  - Object detection
KW  - RetinaNet
KW  - algorithm
KW  - article
KW  - benchmarking
KW  - computer vision
KW  - deep learning
KW  - diagnosis
KW  - evolution
KW  - velocity
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, Y.
AU  - Wu, X.
AU  - Li, Y.
AU  - Sun, H.
AU  - Fan, D.
TI  - Algorithm for surface flow velocity measurement in trunk canal based on improved YOLOv8 and DeepSORT
PY  - 2025
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2025.110344
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219056857&doi=10.1016%2fj.engappai.2025.110344&partnerID=40&md5=d845ff914925a27cb83fe49e27019bd0
AB  - The velocity measurement of trunk canal and river plays an important role in agriculture and forestry irrigation scheduling, water resources management and flood prediction. Particle flow measurement technology can realize non-contact and high-precision flow measurement, but in practical application, the particle size is small, the shape is different and the dynamic change brings great challenges to the application of this method. To solve these problems, this paper proposed the surface velocity measurement method of trunk canal based on improved YOLOv8(You Only Look Once Version 8) and DeepSORT(Deep Simple Online and Realtime Tracking), and introduced tiny detection layer and channel attention mechanism to improve YOLOv8's detection capability of small targets. In DeepSORT, IBN-Net(Intent-Based Networking-Network) network structure and GIoU(Generalized Intersection over Union) matching are introduced to solve the problem of discontinuity or loss of target tracking in complex cases, which improves the accuracy and robustness of target tracking. The experimental results show that the improved YOLOv8 improves AP(Average Precision) and mAP(mean Average Precision) by nearly 5% and 0.2% respectively. The performance of the improved DeepSORT has been improved across the board, especially IDP and MOTA, which have improved by 25.2% and 5.6% respectively. The algorithm also has good accuracy in actual velocity measurement. © 2025 The Authors
KW  - Deep simple Online and Realtime Tracking(DeepSORT) target tracking
KW  - Generalized intersection over Union(GIoU)
KW  - Intent-based Networking-network(IBN-Net)
KW  - Surface velocity measurement of trunk canal
KW  - You only look once Version 8(YOLOV8) target detection
KW  - Acceleration measurement
KW  - Flood control
KW  - Flow measurement
KW  - Flow velocity
KW  - Flowmeters
KW  - Irrigation canals
KW  - Water management
KW  - Deep simple online and realtime tracking target tracking
KW  - Generalized intersection
KW  - Generalized intersection over union
KW  - Intent-based networking-network
KW  - Measurements of
KW  - On-line tracking
KW  - Real time tracking
KW  - Simple++
KW  - Surface velocity
KW  - Surface velocity measurement of trunk canal
KW  - Targets detection
KW  - Targets tracking
KW  - You only look once version 8 target detection
KW  - Velocity measurement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gu, Q.
AU  - Han, Z.
AU  - Kong, S.
AU  - Huang, H.
AU  - Li, Y.
AU  - Fan, Q.
AU  - Wu, R.
TI  - DCYOLO: Dual negative weighting label assignment and cross-layer decouple head for YOLO in remote sensing images
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2025.127595
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002487689&doi=10.1016%2fj.eswa.2025.127595&partnerID=40&md5=63ed929cdcd53d3323a0f4c379461ece
AB  - Existing remote sensing detection methods have achieved excellent detection performance, but the current detectors still present some problems: 1. In the process of label assignment and loss calculation, only one negative weight is assigned to calculate the loss of negative samples. 2. The detector only fuses multi-scale features at the Feature Pyramids Networks (FPN), while the output is predicted at the detector head, which makes the detector head lack some of the multi-scale features. 3. A single down sampling method can result in the loss of small and tiny object features. To solve the above problems, in this work we propose an anchor-free detector based on YOLOv8. Our proposed method contains three improvements: An integrated down sampling module (IDS), a Cross-layer Decouple Head (CDH), and a new label assignment method called Dual Negative Weight Task Alignment Learning (DNW-TAL). The IDS integrates three down sampling methods to alleviate the problem of missing features on small and tiny objects. CDH fuses and enhances cross-layer features by encoding and decoding the features. The DNW-TAL first uses CGS to select the actual samples for training. In terms of sample assignment, DNW-TAL divides the negative samples into those inside and outside the ground truth (GT), and obtains the negative sample weights for its loss calculation based on the distribution and performance of the samples. Ablation experiments on the VisDrone and AI-TOD datasets show that our proposed methods improve the APmaxDets=100IOU=0.50:0.95 metrics by 2.2% and 1.9%, which is a performance improvement of 13.75% and 13.97% relative to the baseline. On the VisDrone, AI-TOD, and DOTA v1.5 datasets, the mAP of our proposed method achieved 24.3%, 28.6% and 67.4%, respectively, which are the best results compared to the comparison methods, proving that DCYOLO is comparable to other SOTAs. © 2025 Elsevier Ltd
KW  - Anchor-free
KW  - Deep learning
KW  - Label assignment
KW  - Object detection
KW  - Remote sensing image
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Remote sensing
KW  - Anchor-free
KW  - Cross layer
KW  - Deep learning
KW  - Down sampling
KW  - Label assignment
KW  - Loss calculation
KW  - Multi-scale features
KW  - Negative samples
KW  - Objects detection
KW  - Remote sensing images
KW  - Proximity sensors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Z.
AU  - Zhu, Y.
AU  - Zhang, Y.
AU  - Liu, S.
TI  - An effective deep learning approach enabling miners’ protective equipment detection and tracking using improved YOLOv7 architecture
PY  - 2025
T2  - Computers and Electrical Engineering
DO  - 10.1016/j.compeleceng.2025.110173
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217957979&doi=10.1016%2fj.compeleceng.2025.110173&partnerID=40&md5=ce03f3adcea0e1c14269c00da3e00a3c
AB  - In the complex underground mining environment, ensuring the correct wearing of personal protective equipment (PPE) is crucial for coal mine safety production. To overcome the limitations of existing PPE detection and tracking technologies, which often suffer from low precision, slow performance, and complex feature extraction processes, this paper introduces an enhanced, lightweight, and high-precision object detection network model based on YOLOv7. The proposed model incorporates a streamlined backbone feature extraction architecture that combines the Mobile Inverted Bottleneck Convolution module with the GhostBottleneck Lightweight module. This integration significantly improves the detection accuracy of miners’ PPE while simultaneously reducing the number of network parameters. Furthermore, the model adopts adaptive spatial feature fusion to enhance its capability in effectively integrating cross-scale features, thereby further boosting its detection performance. To enable continuous and stable tracking of miners’ PPE usage, this paper integrates the DeepSort tracking algorithm, which is based on OSNet, with the improved YOLOv7 detection model. This combination constructs an efficient video-based multi-object tracking algorithm, providing essential support for enhancing the tracking performance of coal miners’ PPE. Experimental results demonstrate that, compared to other state-of-the-art methods, the proposed model achieves a 2.25% increase in mean Average Precision (mAP), a 2.91% improvement in F1 score, a 0.41% enhancement in precision, and a 5.34% increase in recall for PPE detection. Additionally, it exhibits significant improvements in multi-object tracking metrics, with a 5.9% increase in Multi-Object Tracking Accuracy (MOTA), a 3.5% increase in Multi-Object Tracking Precision (MOTP), and a 6.2% increase in IDF1 score. These results fully validate the model's efficient detection and tracking capabilities for miners’ PPE in complex underground mining environments. © 2025 Elsevier Ltd
KW  - DeepSort
KW  - Object detection and tracking
KW  - PPE
KW  - YOLOv7
KW  - Miners
KW  - Mining equipment
KW  - Protective clothing
KW  - Underground equipment
KW  - Deepsort
KW  - Detection and tracking
KW  - Features extraction
KW  - Learning approach
KW  - Mining environments
KW  - Multi-object tracking
KW  - Object detection and tracking
KW  - Personal protective equipment
KW  - Underground mining
KW  - YOLOv7
KW  - Coal mines
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, R.
AU  - Ji, X.
AU  - Loughney, S.
AU  - Wang, J.
AU  - Yang, Z.
TI  - Visual perception for long-distance and small target detection in autonomous maritime navigation
PY  - 2025
T2  - Ocean Engineering
DO  - 10.1016/j.oceaneng.2025.121447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004548784&doi=10.1016%2fj.oceaneng.2025.121447&partnerID=40&md5=3e138e138b8aeb373d6a4527ff0e23d6
AB  - In the pursuit of advancing autonomous maritime navigation, this study aimed to develop a novel architecture designed to enhance the detection accuracy of distant and small targets under the constraints of real-time performance and robustness. Through the innovative integration of the Convolutional Block Attention Module (CBAM) into the detection model's backbone, the study achieved superior feature extraction capabilities tailored for the complexities of maritime environments. Further optimization of the Spatial Pyramid Pooling (SPP) module ensured model compactness and computational efficiency, vital for deployment on edge devices. A key methodological novelty lay in the incorporation of the S-IoU loss function, which offers superior bounding box regression accuracy over the traditional Generalized Intersection over Union, directly contributing to more precise navigation and effective obstacle avoidance. The proposed enhancements collectively yielded a 5.1 % increase in mAP@50 %, accompanied by an 11.2 % reduction in model parameters and a 12.6 % decrease in computational complexity (GFLOPs). These findings underscore the potential of the presented architecture to significantly contribute to maritime safety, presenting an optimized solution for collision avoidance and navigation assistance in congested sea routes and adverse weather conditions. © 2025
KW  - Autonomous ships
KW  - Computer version
KW  - Object detection
KW  - Small target
KW  - Visual navigation aid
KW  - Marine safety
KW  - NP-hard
KW  - Object detection
KW  - Waterway transportation
KW  - Autonomous ship
KW  - Computer versions
KW  - Maritime navigation
KW  - Navigation aids
KW  - Objects detection
KW  - Small target detection
KW  - Small targets
KW  - Visual Navigation
KW  - Visual navigation aid
KW  - Visual perception
KW  - detection method
KW  - maritime transportation
KW  - navigation aid
KW  - pattern recognition
KW  - regression analysis
KW  - vessel
KW  - Marine navigation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xue, B.
AU  - Zhang, B.
AU  - Cheng, Q.
TI  - Experiment study on UAV target detection algorithm based on YOLOv8n-ACW
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-91394-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001738010&doi=10.1038%2fs41598-025-91394-1&partnerID=40&md5=9cef6fb1c8dd0ff5db6bb7324cc4eb94
AB  - To address the challenges associated with dense and occluded targets in small target detection utilizing unmanned aerial vehicle (UAV), we propose an enhanced detection algorithm referred as the YOLOv8n-ACW. Building upon the YOLOv8n baseline network model, we have integrated Adown into the Backbone and developed a CCDHead to further improve the drone’s capability to recognize small targets. Additionally, WIoU-V3 has been introduced as the loss function. Experiment results derived from the Visdrone2019 dataset indicate that, the YOLOv8n- ACW has achieved a 4.2% increase in mAP50(%) compared to the baseline model, while simultaneously reducing the parameter count by 36.7%, exhibiting superior capabilities in detecting small targets. Furthermore, utilizing a self-constructed dataset of G5-Pro drones for target detection experiments, the results indicate that the enhanced model has robust generalization capabilities in real-world environments. The UAV target detection experiment combines experimental simulation with real-world testing, while combining scientific exploration with educational objectives. This experiment has high fidelity, excellent functional scalability, and strong practicality, aiming to cultivate students’ comprehensive practical and innovative abilities. © The Author(s) 2025.
KW  - Deep learning
KW  - Experiment study
KW  - Target detection
KW  - Unmanned aerial vehicle
KW  - article
KW  - controlled study
KW  - deep learning
KW  - detection algorithm
KW  - drone
KW  - nonhuman
KW  - pharmacology
KW  - simulation
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, K.
AU  - He, G.
AU  - Li, X.
TI  - A Lightweight Multi-Scale Context Detail Network for Efficient Target Detection in Resource-Constrained Environments
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25123800
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009030092&doi=10.3390%2fs25123800&partnerID=40&md5=1bdc14d788001d89fa584bb9febda855
AB  - Target detection in resource-constrained environments faces multiple challenges such as the use of camouflage, diverse target sizes, and harsh environmental conditions. Moreover, the need for solutions suitable for edge computing environments, which have limited computational resources, adds complexity to the task. To meet these challenges, we propose MSCDNet (Multi-Scale Context Detail Network), an innovative and lightweight architecture designed specifically for efficient target detection in such environments. MSCDNet integrates three key components: the Multi-Scale Fusion Module, which improves the representation of features at various target scales; the Context Merge Module, which enables adaptive feature integration across scales to handle a wide range of target conditions; and the Detail Enhance Module, which emphasizes preserving crucial edge and texture details for detecting camouflaged targets. Extensive evaluations highlight the effectiveness of MSCDNet, which achieves 40.1% mAP50-95, 86.1% precision, and 68.1% recall while maintaining a low computational load with only 2.22 M parameters and 6.0 G FLOPs. When compared to other models, MSCDNet outperforms YOLO-family variants by 1.9% in mAP50-95 and uses 14% fewer parameters. Additional generalization tests on VisDrone2019 and BDD100K further validate its robustness, with improvements of 1.1% in mAP50 on VisDrone and 1.2% in mAP50-95 on BDD100K over baseline models. These results affirm that MSCDNet is well suited for tactical deployment in scenarios with limited computational resources, where reliable target detection is paramount. © 2025 by the authors.
KW  - context-aware modulation
KW  - edge computing
KW  - lightweight neural network
KW  - multi-scale feature fusion
KW  - target detection
KW  - Camouflage
KW  - Edge detection
KW  - Feature extraction
KW  - Radar target recognition
KW  - Target tracking
KW  - Context-Aware
KW  - Context-aware modulation
KW  - Edge computing
KW  - Features fusions
KW  - Lightweight neural network
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Multi-scales
KW  - Neural-networks
KW  - Targets detection
KW  - Edge computing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Wang, Y.
AU  - Zhou, C.
AU  - Wang, H.
TI  - SOR-YOLO: a strip-shaped object detection network for traffic columns and traffic cones
PY  - 2025
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-025-04017-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003109115&doi=10.1007%2fs11760-025-04017-7&partnerID=40&md5=9945b5d8a0f3b2f5719f8479662c08f8
AB  - Traffic columns and traffic cones, as crucial traffic guidance tools, are often used together to maintain the orderliness of road traffic. However, due to their small size and frequent use in large quantities and high density, it is difficult for autonomous vehicles to accurately capture and locate them. Therefore, we choose the YOLOv8n network as the baseline and propose a novel strip-shaped object recognition model, namely SOR-YOLO. Firstly, we utilize the SGE attention mechanism and GhostConv module to construct a lightweight grouped enhanced C2f module, namely LGEC2f module, which reduces the number of parameters and computations of the network. Secondly, we integrate the MSCA module into the SPPF module to form the MSCSPPF module, thus enhancing the network’s ability to recognize strip-shaped objects. Finally, we replace the loss function of the YOLOv8n network with the WIoU loss function to accelerate the convergence of the model. Experiments show that SOR-YOLO improves the mAP50 and mAP75 by 2.7% and 3.0% respectively, and the mAP50-95 by approximately 1.1% compared to YOLOv8n in the recognition of traffic columns and traffic cones, demonstrating higher recognition accuracy. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - Dense environment
KW  - Small object detection
KW  - Strip-shaped object
KW  - YOLOv8
KW  - Air traffic control
KW  - Cones
KW  - Electronic guidance systems
KW  - Highway traffic control
KW  - Image segmentation
KW  - Magnetic levitation vehicles
KW  - Motor transportation
KW  - Object detection
KW  - Robotics
KW  - Street traffic control
KW  - Autonomous Vehicles
KW  - Dense environment
KW  - Detection networks
KW  - Loss functions
KW  - Objects detection
KW  - Road traffic
KW  - Small object detection
KW  - Strip-shaped object
KW  - Traffic cones
KW  - YOLOv8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Chiroma, H.
TI  - Emerging trends in artificial intelligence: Integrating theories and practice
PY  - 2025
T2  - Emerging Trends in Artificial Intelligence: Integrating theories and practice
DO  - 10.1088/978-0-7503-6320-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009189858&doi=10.1088%2f978-0-7503-6320-4&partnerID=40&md5=df45ebe6f0dd4cdd2b3108086a17b00f
AB  - Emerging Artificial Intelligence: Integrating theories and practice is specifically tailored for researchers in AI who are eager to delve into the latest developments and explore the integration of theoretical concepts with practical applications. This book is an essential guide for researchers in AI seeking to bridge the gap between theory and practice, embark on a transformative journey through the latest advancements in AI and gain a comprehensive understanding of their real-world applications. After reading the book, the reader will be able to clearly understand the industrial applications of each emerging AI domain and appreciate the operations of AI in industry as well as society in general. Part of IOP Series in Next Generation Computing. Key features • Case studies within the text and summaries at the end of each chapter. • Emerging field of artificial intelligence at a glance. • Industrial applications of the emerging artificial intelligence field. • Shows the connection between theories and practical applications of emerging artificial intelligence. • Presents a concise summary of the different aspect of artificial intelligence from origin to current times in one spot. © IOP Publishing Ltd 2025. All rights reserved.
KW  - Artificial intelligence
KW  - 'current
KW  - Case-studies
KW  - Emerging trends
KW  - Key feature
KW  - Latest development
KW  - Real-world
KW  - Theory and practice
KW  - Computation theory
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, Z.
AU  - Ma, Y.
AU  - Gong, Z.
AU  - Cao, M.
AU  - Yang, Y.
AU  - Wang, Z.
AU  - Wang, T.
AU  - Li, J.
AU  - Liu, Y.
TI  - R-AFPN: a residual asymptotic feature pyramid network for UAV aerial photography of small targets
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-00008-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004686757&doi=10.1038%2fs41598-025-00008-3&partnerID=40&md5=ca5c89a11bda1b0f26a81e4a2d5ccf95
AB  - This study proposes an improved Residual Asymptotic Feature Pyramid Network (R-AFPN) to address challenges in small target detection from the Unmanned Aerial Vehicle (UAV) perspectives, such as scale imbalance, feature extraction difficulty, occlusion, and computational constraints. The R-AFPN integrates three key modules: Residual Asymptotic Feature Fusion (RAFF) for adaptive spatial fusion and cross-scale linking, Shallow Information Extraction (SIE) for capturing detailed shallow features, and Hierarchical Feature Fusion (HFF) for bottom-up incremental fusion to enhance deep feature details. Experimental results demonstrate that R-AFPN-L achieves 50.7% AP50 on the TinyPerson dataset and 48.9% mAP50 on the VisDrone2019 dataset, outperforming the baseline by 3% and 1.2%, respectively, while reducing parameters by 15.1%. This approach offers a lightweight, efficient solution for small target detection in UAV applications. © The Author(s) 2025.
KW  - Asymptotic feature fusion
KW  - Context learning
KW  - Residual connectivity
KW  - Small target detection
KW  - UAV
KW  - adult
KW  - aged
KW  - article
KW  - context learning
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - human
KW  - male
KW  - photography
KW  - therapy
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, H.
AU  - Fu, G.
AU  - Shao, H.
AU  - Wang, Y.
AU  - Shao, Y.
AU  - Chu, H.
AU  - Deng, H.
TI  - Small Object Detection in Aerial Imagery Using Multi-Scale Hiearchical Feature Fusion Based Approach
ST  - 融合多尺度层级特征的航拍小目标检测
PY  - 2025
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2408-0105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007328921&doi=10.3778%2fj.issn.1002-8331.2408-0105&partnerID=40&md5=a304f4dc1f754d28f9921f02fe4c6f2e
AB  - Aiming at the problem of low accuracy in detecting small objects due to large field of view, small object size, and dense distribution in aerial images, a multi-scale feature fusion aerial detection algorithm based on improved YOLOv8 is proposed. Firstly, a lightweight L-MobileViT module is constructed to capture effective relationships between features, mitigate information loss, and improve the detection performance of the model. Secondly, a hierarchical multi-scale fusion module HF (hierarchical fusion) is proposed to integrate deep spatial information and shallow semantic information, enhancing the detection capability of small objects in dense scenes. Finally, a tiny detection head is added, and a large detection head is removed based on YOLOv8 to focus on the detection ability of small objects and reduce the missed detection rate of small objects. Experimental results show that the improved model achieves superior performance on the VisDrone2019 and UAV-TrafficTinyDataset datasets, with mAP@50 increased by 17.6% and 15.7%, respectively, compared to the baseline model. The detection effect of small objects is significantly improved, and the overall performance is better than mainstream aerial detection algorithms. This demonstrates that the improved algorithm has better generalization and robustness, making it suitable for detection tasks in aerial scenarios. © 2025 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
KW  - L-MobileViT
KW  - multi-level feature fusion
KW  - small target detection
KW  - UAV images
KW  - YOLOv8
KW  - Aerial detection
KW  - Detection algorithm
KW  - Features fusions
KW  - L-mobilevit
KW  - Multi-level feature fusion
KW  - Multilevels
KW  - Small objects
KW  - Small target detection
KW  - UAV image
KW  - YOLOv8
KW  - Target drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, D.
AU  - Zhang, G.
AU  - Li, Z.
AU  - Liu, K.
AU  - Luo, W.
TI  - Light-YOLO: A lightweight and high-performance network for detecting small obstacles on roads at night
PY  - 2025
T2  - Computer Vision and Image Understanding
DO  - 10.1016/j.cviu.2025.104428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008508782&doi=10.1016%2fj.cviu.2025.104428&partnerID=40&md5=169a0703af23d6c804b0b8efa5b5c9b8
AB  - To address the challenges of detecting small obstacles and model portability, this study proposes a lightweight, high-precision, and high-speed small obstacle detection network at nighttime road environments referred to as Light-YOLO. First, the SPDConvMobileNetV3 feature extraction network is introduced, which significantly reduces the total number of parameters while enhancing the ability to capture small obstacle details. Next, to make the network more focused on small obstacles at nighttime conditions, a loss function called Wise-IoU is incorporated, which is more suitable to low-quality images. Finally, to improve overall model performance without increasing the total number of parameters, a parameter-free attention mechanism (SimAM) is integrated. By comparing the publicly available data with the self-built dataset, the experimental results show that Light-YOLO achieves a mean average precision (mAP50) of 97.1% while maintaining a high image processing speed. Additionally, compared to other advanced models in the same series, Light-YOLO has fewer parameters, a smaller computational load (GFLOPs), and reduced model weight (Best.pt). Overall, Light-YOLO strikes a balance between lightweight design, accuracy, and speed, making it more suitable for hardware-constrained devices. © 2025 Elsevier Inc.
KW  - Lightweight network
KW  - Night road
KW  - Parameter-free attention mechanism
KW  - Small obstacle detection
KW  - SPDConvMobileNetV3
KW  - Wise-IoU
KW  - Computer software portability
KW  - Image processing
KW  - Obstacle detectors
KW  - Attention mechanisms
KW  - High performance networks
KW  - High-precision
KW  - Lightweight network
KW  - Night road
KW  - Obstacles detection
KW  - Parameter-free attention mechanism
KW  - Small obstacle detection
KW  - Spdconvmobilenetv3
KW  - Wise-IoU
KW  - Roads and streets
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Han, Y.
AU  - Wang, C.
AU  - Luo, H.
AU  - Wang, H.
AU  - Chen, Z.
AU  - Xia, Y.
AU  - Yun, L.
TI  - LRDS-YOLO enhances small object detection in UAV aerial images with a lightweight and efficient design
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-07021-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009535307&doi=10.1038%2fs41598-025-07021-6&partnerID=40&md5=9ce696b8b65e42b35b7b462dc451bec2
AB  - Small object detection in UAV aerial images is challenging due to low contrast, complex backgrounds, and limited computational resources. Traditional methods struggle with high miss detection rates and poor localization accuracy caused by information loss, weak cross-layer feature interaction, and rigid detection heads. To address these issues, we propose LRDS-YOLO, a lightweight and efficient model tailored for UAV applications. The model incorporates a Light Adaptive-weight Downsampling (LAD) module to retain fine-grained small object features and reduce information loss. A Re-Calibration Feature Pyramid Network (Re-Calibration FPN) enhances multi-scale feature fusion using bidirectional interactions and resolution-aware hybrid attention. The SegNext Attention mechanism improves target focus while suppressing background noise, and the dynamic detection head (DyHead) optimizes multi-dimensional feature weighting for robust detection. Experiments show that LRDS-YOLO achieves 43.6% mAP50 on VisDrone2019, 11.4% higher than the baseline, with only 4.17M parameters and 24.1 GFLOPs, striking a balance between accuracy and efficiency. On the HIT-UAV infrared dataset, it reaches 84.5% mAP50, demonstrating strong generalization. With its lightweight design and high precision, LRDS-YOLO offers an effective real-time solution for UAV-based small object detection. © The Author(s) 2025.
KW  - Attention mechanisms
KW  - Feature pyramid network
KW  - Real time
KW  - Small object detection
KW  - UAV (unmanned aerial vehicle)
KW  - adult
KW  - article
KW  - background noise
KW  - calibration
KW  - controlled study
KW  - diagnosis
KW  - female
KW  - human
KW  - male
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xing, J.
AU  - Zhan, C.
AU  - Ma, J.
AU  - Chao, Z.
AU  - Liu, Y.
TI  - Lightweight detection model for safe wear at worksites using GPD-YOLOv8 algorithm
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-024-83391-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214426180&doi=10.1038%2fs41598-024-83391-7&partnerID=40&md5=6566889a78aeb234d0f2976265bd832d
AB  - To address the significantly elevated safety risks associated with construction workers’ improper use of helmets and reflective clothing, we propose an enhanced YOLOv8 model tailored for safety wear detection. Firstly, this study introduces the P2 detection layer within the YOLOv8 architecture, which substantially enriches semantic feature representation. Additionally, a lightweight Ghost module is integrated to replace the original backbone of YOLOv8, thereby reducing the parameter count and computational burden. Moreover, we incorporate a Dynamic Head (Dyhead) that employs an attention mechanism to effectively extract features and spatial location information critical for site safety wear detection. This adaptation significantly enhances the model’s representational power without adding computational overhead. Furthermore, we adopt an Exponential Moving Average (EMA) SlideLoss function, which not only boosts accuracy but also ensures the stability of our safety wear detection model’s performance. Comparative evaluation of the experimental results indicates that our proposed model achieves a 6.2% improvement in mean Average Precision (mAP) compared to the baseline YOLOv8 model, while also increasing the detection speed by 55.88% in terms of frames per second (FPS). © The Author(s) 2024.
KW  - Deep learning
KW  - Ghost module
KW  - Site safety wearable detection
KW  - YOLOv8
KW  - adaptation
KW  - algorithm
KW  - article
KW  - clothing
KW  - construction worker
KW  - controlled study
KW  - deep learning
KW  - female
KW  - helmet
KW  - human
KW  - safety
KW  - velocity
KW  - wearable device
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhuo, S.
AU  - Shi, J.
AU  - Bai, H.
AU  - Zhou, X.
AU  - Kan, J.
AU  - Cai, J.
TI  - Improved Threshold-free Automatic Dependent Surveillance-Broadcast preamble detection algorithm based on deep learning framework
PY  - 2025
T2  - Digital Signal Processing: A Review Journal
DO  - 10.1016/j.dsp.2025.105307
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004923925&doi=10.1016%2fj.dsp.2025.105307&partnerID=40&md5=f3f88bb461ded53bbe311624cba80c1f
AB  - In the study of Automatic Dependent Surveillance-Broadcast (ADS-B) signal decoding in S-mode, accurate detection of the signal preamble is a critical prerequisite for successful decoding. To address the challenges of low detection accuracy and slow processing speed in low Signal-to-Noise Ratio (SNR) environments, we propose an intelligent ADS-B signal preamble detection algorithm. First, an improved You Only Look Once version 8 (YOLOv8) object detection model is utilized to precisely capture the ADS-B signal Preamble in the frequency domain. Next, a coordinate transformation method is employed to obtain the temporal position of the preamble pulses within the time domain signal. Finally, an enhanced threshold-free cross-correlation preamble detection algorithm is applied to achieve precise preamble detection in the time domain. Experimental results demonstrate that, in both simulated datasets and real-world measurement environments, the proposed algorithm effectively mitigates the issue of preamble detection accuracy degradation caused by threshold fluctuations under low-SNR conditions. Specifically, the proposed algorithm achieves detection accuracies of 58.7% and 99.8% at SNR = -3 dB and 15 dB, respectively, surpassing traditional detection algorithms in accuracy. © 2025 Elsevier Inc.
KW  - ADS-B signal
KW  - Coordinate transformation
KW  - Cross-correlation detection
KW  - S mode
KW  - YOLOv8
KW  - Automatic dependent surveillance broadcasts
KW  - Automatic dependent surveillance-broadcast signal
KW  - Broadcast signals
KW  - Coordinate transformations
KW  - Correlation detection
KW  - Cross-correlation detection
KW  - Cross-correlations
KW  - Preamble detection
KW  - S mode
KW  - You only look once version 8
KW  - Cutoff frequency
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bongomin, O.
AU  - Mwape, M.C.
AU  - Mpofu, N.S.
AU  - Bahunde, B.K.
AU  - Kidega, R.
AU  - Mpungu, I.L.
AU  - Tumusiime, G.
AU  - Owino, C.A.
AU  - Goussongtogue, Y.M.
AU  - Yemane, A.
AU  - Kyokunzire, P.
AU  - Malanda, C.
AU  - Komakech, J.
AU  - Tigalana, D.
AU  - Gumisiriza, O.
AU  - Ngulube, G.
TI  - Digital twin technology advancing industry 4.0 and industry 5.0 across sectors
PY  - 2025
T2  - Results in Engineering
DO  - 10.1016/j.rineng.2025.105583
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007788941&doi=10.1016%2fj.rineng.2025.105583&partnerID=40&md5=3f4d6604e4d3623daa4481e133e76beb
AB  - Digital Twin (DT) technology is transforming industrial systems by integrating physical assets with digital models, enabling real-time monitoring, predictive analytics, and process optimization, particularly within the framework of Industry 4.0 (I4.0). As the global industrial landscape shifts toward Industry 5.0 (I5.0), DTs are increasingly being redefined to support human-centric innovation, sustainability, and system resilience. This review examines the evolving role of DTs in bridging the efficiency-driven goals of I4.0 with the inclusive, sustainable objectives of I5.0. It explores ten enabling technologies such as artificial intelligence (AI), internet of things, blockchain, cloud and edge computing, and extended reality, while discussing both the opportunities and challenges posed by I5.0. The study emphasizes key principles of DTs, including real-time synchronization, feedback mechanisms, and lifecycle integration. A detailed sectorial analysis across manufacturing, infrastructure, energy, transportation, mining, agriculture, and healthcare illustrates how DTs are being applied in diverse contexts to enhance operational efficiency, product quality, and decision-making. The mapping of applications by country, sector, and industrial focus reveals growing trends toward I5.0 in areas such as logistics and infrastructure. Common application domains include monitoring, optimization, prediction, and decision support. Despite their potential, DT adoption faces challenges including high implementation costs, data integration issues, cybersecurity concerns, and lack of standardization. The review discusses these barriers alongside the importance of validation and security for trusted deployment. It concludes by identifying future directions, including cognitive twins, industrial metaverse integration, and ethical AI. DTs are positioned as foundational technologies for advancing sustainable, resilient, and human-centered industrial systems. © 2025 The Authors
KW  - Artificial intelligence
KW  - Digital twins
KW  - Human-centric innovation
KW  - Industry 4.0
KW  - Industry 5.0
KW  - Predictive analytics
KW  - Sustainability
KW  - Computer control
KW  - Decision making
KW  - Model predictive control
KW  - Network security
KW  - Trusted computing
KW  - Analytic optimization
KW  - Digital modeling
KW  - Human-centric
KW  - Human-centric innovation
KW  - Industrial systems
KW  - Industry 5.0
KW  - Physical assets
KW  - Process optimisation
KW  - Real time monitoring
KW  - System resiliences
KW  - Prediction models
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Silva, R.M.
AU  - Azevedo, G.F.
AU  - Berto, M.V.V.
AU  - Rocha, J.R.
AU  - Fidelis, E.C.
AU  - Nogueira, M.V.
AU  - Lisboa, P.H.
AU  - Almeida, T.A.
TI  - Vulnerable road user detection and safety enhancement: A comprehensive survey
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2025.128529
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008325759&doi=10.1016%2fj.eswa.2025.128529&partnerID=40&md5=ccabfc1bdcf5ff6ab17ff366aed13424
AB  - Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study investigates the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is mandatory for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development. © 2025 Elsevier Ltd
KW  - Collision avoidance
KW  - Intention prediction
KW  - Machine learning
KW  - Object detection
KW  - Sensor data processing
KW  - Sensor datasets
KW  - Simulation environments
KW  - Traffic communication ecosystem
KW  - Traffic sensors
KW  - Vulnerable road user
KW  - Accident prevention
KW  - Behavioral research
KW  - Cutting
KW  - Data communication systems
KW  - Data handling
KW  - Ecosystems
KW  - Highway accidents
KW  - Learning algorithms
KW  - Learning systems
KW  - Machine learning
KW  - Motor transportation
KW  - Object recognition
KW  - Roads and streets
KW  - Sensor data fusion
KW  - Surveying
KW  - Telecommunication traffic
KW  - Traffic control
KW  - Traffic signals
KW  - Collisions avoidance
KW  - Intention predictions
KW  - Machine-learning
KW  - Objects detection
KW  - Road users
KW  - Sensor data processing
KW  - Sensor dataset
KW  - Simulation environment
KW  - Traffic communication
KW  - Traffic communication ecosystem
KW  - Traffic sensors
KW  - Vulnerable road user
KW  - Collision avoidance
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Essien, E.
AU  - Frimpong, S.
TI  - Enhancing Autonomous Truck Navigation in Underground Mines: A Review of 3D Object Detection Systems, Challenges, and Future Trends
PY  - 2025
T2  - Drones
DO  - 10.3390/drones9060433
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008956975&doi=10.3390%2fdrones9060433&partnerID=40&md5=7d3f3efb5e648d0c4b889b7ef4e9489a
AB  - Integrating autonomous haulage systems into underground mining has revolutionized safety and operational efficiency. However, deploying 3D detection systems for autonomous truck navigation in such an environment faces persistent challenges due to dust, occlusion, complex terrains, and low visibility. This affects their reliability and real-time processing. While existing reviews have discussed object detection techniques and sensor-based systems, providing valuable insights into their applications, only a few have addressed the unique underground challenges that affect 3D detection models. This review synthesizes the current advancements in 3D object detection models for underground autonomous truck navigation. It assesses deep learning algorithms, fusion techniques, multi-modal sensor suites, and limited datasets in an underground detection system. This study uses systematic database searches with selection criteria for relevance to underground perception. The findings of this work show that the mid-level fusion method for combining different sensor suites enhances robust detection. Though YOLO (You Only Look Once)-based detection models provide superior real-time performance, challenges persist in small object detection, computational trade-offs, and data scarcity. This paper concludes by identifying research gaps and proposing future directions for a more scalable and resilient underground perception system. The main novelty is its review of underground 3D detection systems in autonomous trucks. © 2025 by the authors.
KW  - 3D object detection
KW  - autonomous trucks
KW  - deep learning
KW  - sensor fusion
KW  - underground mines
KW  - YOLO algorithms
KW  - Automobiles
KW  - Autonomous vehicles
KW  - Data mining
KW  - Learning systems
KW  - Mine trucks
KW  - Navigation
KW  - Object detection
KW  - Object recognition
KW  - Real time systems
KW  - Underground mine transportation
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous truck
KW  - Deep learning
KW  - Detection models
KW  - Detection system
KW  - Objects detection
KW  - Sensor fusion
KW  - Underground mine
KW  - You only look once algorithm
KW  - Deep learning
KW  - Economic and social effects
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, H.
AU  - Liu, Y.
TI  - YOLO-LiRa: lightweight detection algorithm for small aerial targets
PY  - 2025
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/addbf9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007510172&doi=10.1088%2f1361-6501%2faddbf9&partnerID=40&md5=b930694dc37052e8f7b2fb519a0097cf
AB  - With the rapid development of drone technology, aerial small target detection has become an important research direction in the field of computer vision. This paper proposes a lightweight object detection algorithm based on YOLOv11, YOLO-LiRa (Lightweight Intelligent Remote-sensing Algorithm), to address the issues of small size, complex background, and dense targets faced by small object detection in aerial images. Incorporating Monte Carlo attention mechanism and partial convolution into the C3K2 module enhances the extraction of small target features and reduces computational complexity. Referring to the lightweight design concept of MobileNetV3 to optimize the backbone network, combined with GSConv and VoVGSCSP modules, the multi-scale feature fusion capability of the neck network is enhanced. Moreover, the feature map resolution and detection performance were optimized using the DySample upsampling operator. Experiments on the publicly available AI-TOD datasets have shown that YOLO-LiRa achieves 0.27 on the mAP50-95 evaluation metric, reducing the parameter count by 24.4% and computational complexity by 12.5% compared to the Baseline model, while achieving a good balance between detection accuracy and speed. Compared with other mainstream object detection algorithms, YOLO-LiRa exhibits more competitive performance in small object detection tasks. The method proposed in this article is suitable for applications such as unmanned aerial vehicle monitoring, intelligent transportation, and agricultural monitoring that require high lightweight and real-time performance. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.
KW  - aerial images
KW  - lightweight
KW  - small target detection
KW  - YOLOv11
KW  - Aerial photography
KW  - Drones
KW  - Aerial images
KW  - Aerial targets
KW  - Complex background
KW  - Detection algorithm
KW  - Lightweight
KW  - Object detection algorithms
KW  - Remote sensing algorithms
KW  - Small object detection
KW  - Small target detection
KW  - YOLOv11
KW  - Target drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hussain, K.
AU  - Moreira, C.
AU  - Pereira, J.
AU  - Jardim, S.
AU  - Jorge, J.
TI  - A Comprehensive Literature Review on Modular Approaches to Autonomous Driving: Deep Learning for Road and Racing Scenarios
PY  - 2025
T2  - Smart Cities
DO  - 10.3390/smartcities8030079
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009305631&doi=10.3390%2fsmartcities8030079&partnerID=40&md5=988ae5f31d95a5e04ae003f620960aae
AB  - Highlights: What are the main accomplishments of this study? A comprehensive analysis of deep learning techniques in both on-road and autonomousracing cars, highlighting distinct challenges and requirements for each context. The identification of critical challenges for future research, to ensure safety and performancein autonomous systems. What are the implications of the main findings? The detailed evaluation of planning methods and performance metrics points to opportunitiesto refine existing methodologies and identify emerging research areasthat can guide the development of more efficient, robust, and scalable autonomousdriving technologies. The challenges identified in sensor fusion, environmental robustness, and computationalefficiency imply that addressing these issues is critical to progress inautonomous systems. Autonomous driving technology is advancing rapidly, driven by integrating advanced intelligent systems. Autonomous vehicles typically follow a modular structure, organized into perception, planning, and control components. Unlike previous surveys, which often focus on specific modular system components or single driving environments, our review uniquely compares both settings, highlighting how deep learning and reinforcement learning methods address the challenges specific to each. We present an in-depth analysis of local and global planning methods, including the integration of benchmarks, simulations, and real-time platforms. Additionally, we compare various evaluation metrics and performance outcomes for current methodologies. Finally, we offer insights into emerging research directions based on the latest advancements, providing a roadmap for future innovation in autonomous driving. © 2025 by the authors.
KW  - autonomous driving
KW  - autonomous racing
KW  - control
KW  - deep learning
KW  - modular system
KW  - perception
KW  - planning
KW  - safety
KW  - simulations
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yan, B.
AU  - Li, X.
AU  - Yan, R.
TI  - An AI-Based Horticultural Plant Fruit Visual Detection Algorithm for Apple Fruits
PY  - 2025
T2  - Horticulturae
DO  - 10.3390/horticulturae11050541
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006582024&doi=10.3390%2fhorticulturae11050541&partnerID=40&md5=4d9f0009d4cb7ea5e914ab545d923a4f
AB  - In order to improve the perception accuracy of the apple tree fruit recognition model and to reduce the model size, a lightweight apple target recognition method based on an improved YOLOv5s artificial intelligence algorithm was proposed, and relevant experiments were designed. The Depthwise Separable Convolution (DWConv) module has many advantages: (1) It has high computational efficiency, reducing the number of parameters and calculations in the model; (2) It makes the model lightweight and easy to deploy in hardware; (3) DWConv can be combined with other modules to enhance the multi-scale feature extraction capability of the detection network and improve the ability to capture multi-scale information; (4) It balances the detection accuracy and speed of the model; (5) DWConv can flexibly adapt to different network structures. Because of its efficient computing modes, lightweight design, and flexible structural adaptation, the DWConv module has significant advantages in multi-scale feature extraction, real-time performance improvement, and small-object detection. Therefore, this method improves the original YOLOv5s network architecture by replacing the embedded Depthwise Separable Convolution in its Backbone network, which reduces the size and parameter count of the model while ensuring detection accuracy. The experimental results show that for the test-set images, the proposed improved model has an average recognition accuracy of 92.3% for apple targets, a recognition time of 0.033 s for a single image, and a model volume of 11.1 MB. Compared with the original YOLOv5s model, the average recognition accuracy was increased by 0.8%, the recognition speed was increased by 23.3%, and the model volume was compressed by 20.7%, effectively achieving lightweight improvement of the apple detection model and improving the accuracy and speed of detection. The detection algorithm proposed in the study can be extended to the intelligent measurement of apple biological and physical characteristics, including for size measurement, shape analysis, and color analysis. The proposed method can improve the intelligence level of orchard management and horticultural technology, reduce labor costs, assist precision agriculture technology, and promote the transformation of the horticultural industry toward sustainable development. © 2025 by the authors.
KW  - apple fruits
KW  - artificial intelligence
KW  - deep learning
KW  - horticulture
KW  - plants
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ye, J.
AU  - Li, P.
AU  - Zhang, Y.
AU  - Guo, Z.
AU  - Zeng, S.
AU  - Zhan, Y.
TI  - MLHI-Net: multi-level hybrid lightweight water body segmentation network for urban shoreline detection
PY  - 2025
T2  - Scientific Reports
DO  - 10.1038/s41598-025-87209-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218238669&doi=10.1038%2fs41598-025-87209-y&partnerID=40&md5=58214d180f5b30364f159dca780d110b
AB  - The capacity to detect shorelines is critical for the autonomous navigation of Unmanned Surface Vehicles (USVs). The majority of extant methods are unable to adapt to the discrimination of high similarity features between the shore and reflections in complex and diverse environments. Moreover, they are also incapable of accurately extracting fuzzy boundaries caused by different scenes and climatic conditions. To address these challenges, this paper proposes a multi-level hybrid lightweight water segmentation network, MLHI-Net. First, we design a convolutional module (ORRD) compatible with over-parameterized and redundancy removal techniques based on lightweight design. The over-parameterized convolutional layers enhance the interactive ability of feature representation and context information. The removal of spatial and channel redundancy, in conjunction with interactive reconstruction, serves to simulate attention and enhance the learning ability of waterscape. Then, we design a multi-branch two-layer attention fusion module (MDA), which achieves diverse attention and global optimisation of edge details by connecting spatial attention, channel attention and pixel attention in parallel. thereby guiding feature fusion and solving the problem of receptive field mismatch. This module guides feature fusion and solves the problem of receptive field mismatch. To validate the proposed methodology, a dataset, CityWater, was constructed, with multiple fields and climatic conditions, and a substantial number of experiments were conducted on this and other public datasets. Experimental results show that MLHI-Net outperforms other advanced segmentation networks in Mean Intersection over Union (MIoU) and Pixel Accuracy (PA) on the CityWater and USVInland datasets, with MIoU of 97.86% and PA of 98.92% on the CityWater dataset, and MIoU of 98.12% and PA of 99.10% on the USVInland dataset. Additionally, the network’s computational GLOPS is 13.45 G, and the number of parameters is 46.92 M, which can meet the requirements for real-time detection. The MLHI-Net has been shown to perform well in a range of environments. In addition, it has good generalisation capabilities, providing reliable support to the autonomous system. © The Author(s) 2025.
KW  - MDA
KW  - ORRD
KW  - Shoreline detection
KW  - Unmanned surface vehicles
KW  - Water segmentation network
KW  - article
KW  - controlled study
KW  - human
KW  - hybrid
KW  - receptive field
KW  - simulation
KW  - spatial attention
KW  - water
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Tan, L.
AU  - Xu, X.
AU  - Zhang, Z.
TI  - Room-level localization method in industrial workshops using LiDAR-based point cloud registration and object recognition
PY  - 2025
T2  - Applied Intelligence
DO  - 10.1007/s10489-025-06244-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217748150&doi=10.1007%2fs10489-025-06244-4&partnerID=40&md5=1fd16b487d52f204115022f1ed545aed
AB  - In this work, we aim to achieve room-level localization for mobile robots in industrial workshops. It is difficult to obtain precise localization information via common methods because of the complexity of the industrial environment. Our findings show that precise room-level localization can be achieved via LiDAR-based point cloud registration and object recognition. For this purpose, we formulate room-level localization as a classification problem. Registration and object recognition are used to extract features from point clouds. After the data enhancement algorithm, called Stacked Auto Encoder is employed to overcome the issue of limited feature data, the neural network algorithm is leveraged to address the classification problem. To this end, we collected point cloud data from industrial workshops and performed experimental validation. We evaluated the recognition performance of the algorithm in a metallurgical workshop and achieved good accuracy. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
KW  - Data enhancement
KW  - Mobile robot
KW  - Neural network algorithm
KW  - Object recognition
KW  - Registration
KW  - Room-level localization
KW  - Industrial robots
KW  - Interpolation
KW  - Metadata
KW  - Object detection
KW  - Data enhancement
KW  - Localisation
KW  - Localization for mobile robot
KW  - Localization information
KW  - Localization method
KW  - Neural networks algorithms
KW  - Objects recognition
KW  - Point cloud registration
KW  - Registration
KW  - Room-level localization
KW  - Mobile robots
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, B.
AU  - Ma, Z.
AU  - Li, X.
TI  - Boosting Detection Accuracy: An Enhanced YOLOv8 for Small Target Detection in Remote Sensing
PY  - 2025
T2  - International Journal of Computational Intelligence Systems
DO  - 10.1007/s44196-025-00895-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008519377&doi=10.1007%2fs44196-025-00895-4&partnerID=40&md5=646059da8ebd9236b9b63c5bf441ad7c
AB  - Object detection in remote sensing images has broad applications in military reconnaissance, urban planning, and disaster management. However, detecting small targets and extracting multi-scale features in complex scenes remain challenging. This paper presents YOLOv8s-Improved, enhancing small-target detection via PPHGNetV2, Progressive Feature Pyramid Network (AFPN-P2), and Diverse Branch Block (DBB) modules. Experiments on the DIOR and VisDrone2019 datasets show that YOLOv8s-Improved achieves mAP scores of 0.824 and 45.3%, respectively, representing improvements of 1.5 and 6.4 percentage points over the baseline YOLOv8s model (0.809 and 38.9%). The improved model demonstrates strong performance in multi-category object detection, particularly in complex scenes. The results suggest that the proposed method addresses the challenges of small target detection in remote sensing and exhibits generalization capabilities across different datasets. © The Author(s) 2025.
KW  - DIOR dataset
KW  - Diverse branch block (DBB)
KW  - PPHGNetV2
KW  - Remote sensing object detection
KW  - Small target detection
KW  - VisDrone2019 dataset
KW  - Complex networks
KW  - Disaster prevention
KW  - Disasters
KW  - Military applications
KW  - Object detection
KW  - Object recognition
KW  - Radar target recognition
KW  - Target tracking
KW  - Urban planning
KW  - Complex scenes
KW  - Detection accuracy
KW  - DIOR dataset
KW  - Diverse branch block
KW  - Objects detection
KW  - PPHGNetV2
KW  - Remote sensing object detection
KW  - Remote-sensing
KW  - Small target detection
KW  - Visdrone2019 dataset
KW  - Remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ma, Z.
AU  - Luo, P.
AU  - Shen, X.
TI  - LMSOE-Net: lightweight multi-scale small object enhancement network for UAV aerial images
PY  - 2025
T2  - Complex and Intelligent Systems
DO  - 10.1007/s40747-025-01971-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007889727&doi=10.1007%2fs40747-025-01971-0&partnerID=40&md5=10292128f9178df893a0a09e9dd90354
AB  - Detecting objects of varying scales, especially small ones, in Unmanned Aerial Vehicle (UAV) aerial images across diverse scenarios and viewpoints using onboard edge devices is a major challenge in computer vision. To tackle this issue, we propose a lightweight multi-scale small object enhancement network (LMSOE-Net) based on the YOLOv8 architecture. To improve both detection performance and model efficiency, we introduce the Efficient Multi Scale Pyramid (EMSP) neck network. This versatile feature fusion network enhances multi-scale feature extraction and integration by using convolutional modules with varying kernel sizes. This design enables more effective feature extraction and facilitates the fusion of local details with channel features. Additionally, we replace the Spatial Pyramid Pooling Fast (SPPF) module in YOLOv8 with the Feature Pyramid Shared Convolution (FPSC) module. This upgrade strengthens the network’s ability to capture fine details and complex patterns, improving multi-scale feature extraction without a significant increase in parameters. We further optimize the model by incorporating shared convolution with detail-enhancement capabilities in the detection head, which improves the detection of small objects across different scales. We evaluate LMSOE-Net through ablation and comparison experiments on the VisDrone2019 and DOTAv1.5 datasets. Compared to three variants of YOLOv8, LMSOE-Net reduces parameters and computational complexity by approximately 30%, while improving mAP@0.5 and mAP@0.5:0.95 by 1–2%. The results demonstrate that our approach significantly boosts detection accuracy and optimizes model efficiency through the integration of our proposed enhancement modules. The source codes are at: https://github.com/ljdl1/LMSOE-Net. © The Author(s) 2025.
KW  - Lightweight
KW  - Multi-scale
KW  - Small object detection
KW  - UAV
KW  - YOLOv8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Du, Y.
AU  - Chen, L.
AU  - Hao, X.
TI  - RL-Net: a rapid and lightweight network for detecting tiny vehicle targets in remote sensing images
PY  - 2025
T2  - Complex and Intelligent Systems
DO  - 10.1007/s40747-025-01956-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007522033&doi=10.1007%2fs40747-025-01956-z&partnerID=40&md5=578edcb1b423632018bd32d4db7c149e
AB  - Traffic accidents remain a critical issue that significantly impacts public safety and poses major challenges to intelligent transportation systems. The integration of Unmanned Aerial Vehicles (UAVs) with object detection technology offers a promising solution to this problem. However, existing detection networks often exhibit limitations such as missed detections and inadequate real-time performance, particularly for small vehicle targets in remote sensing images, and fail to meet the efficiency requirements of edge computing devices. To address these challenges, this study proposes RL-Net, a rapid and lightweight network model based on enhancements to the YOLOv9s architecture. First, MobileNetV4 is introduced to optimize initial feature extraction, significantly improving the network’s efficiency. Second, the Lightweight Spatial Pyramid Pooling Fast (LSPPF) structure is designed to enhance multiscale feature extraction while accelerating computational speed. Additionally, the Lightweight Representation Cross Stage Partial with ELAN (LRepCSPELAN) module is proposed to further reduce the model’s memory and computational resource demands. Finally, an enhanced feature fusion network is designed to improve detection performance for tiny vehicle targets. Comprehensive evaluations on the VisDrone2019 and UA-DETRAC datasets demonstrate that, compared to YOLOv9s model, RL-Net achieves a 34.5% reduction in parameters, a 4.4% reduction in computations, a 30.8% increase in Frames Per Second (FPS), and improvements of 2.5% in mean Average Precision (mAP) and 2% in recall, effectively balancing detection efficiency and performance. © The Author(s) 2025.
KW  - Edge computing
KW  - Lightweight
KW  - Object detection
KW  - Tiny target
KW  - UAVs
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, X.
AU  - Fang, Y.
AU  - Feng, J.
AU  - Yin, H.
TI  - EL-YOLOv8: a lightweight algorithm for efficient detection of pipeline welding defects in X-ray images
PY  - 2025
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-025-03877-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218426758&doi=10.1007%2fs11760-025-03877-3&partnerID=40&md5=9044584c12ee63c01bbf725afdefa2e9
AB  - To address the problems of low detection efficiency and false detection of small target defects in the intelligent detection of X-ray image weld defects of industrial pressure pipelines, this paper proposed a more efficient and light EL-YOLOv8 weld defect detection algorithm. Firstly, data augmentation was performed to solve the problem of unclear defects and insufficient data sets. In order to improve the original YOLOv8 model, the FasterNetBlock was combined with the Efficient Multi-Scale Attention (EMA) module to design a lightweight multi-scale feature Faster-EMA module, which was fused with the CSPDarknet53 to Two-stage FPN (C2f) module in the backbone network. The C2f-Faster-EMA module is proposed to realize multi-scale feature object detection and enhance the feature extraction ability. The experimental results show that compared with five mainstream defect detection algorithms such as YOLOv8-Ghost, the proposed model achieves 91.5% mAP@0.5 defect accuracy on the self-developed X-ray welding defect image dataset, which is 2.8% higher than that of the baseline model. Compared with four mainstream lightweight models such as MobileNet V2, the parameter number of the proposed model is 2.5, the FPS reaches 205, and the processing speed is the fastest and the model is the lightest while maintaining a high accuracy, achieving lightweight. At the same time, on the public datasets COCO128 and ImageNet100, the AP@0.50:0.95 of our model is 2.5% higher than that of YOLOv8, which proves that this model also has good generalizability. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - Defect detection
KW  - Lightweight
KW  - Weld
KW  - YOLOv8
KW  - X ray detectors
KW  - Defect detection
KW  - Defect detection algorithm
KW  - Efficient detection
KW  - Lightweight
KW  - Multi-scale features
KW  - Multi-scales
KW  - Pipe line welding
KW  - Welding defects
KW  - X-ray image
KW  - YOLOv8
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Sun, Y.
AU  - Qin, Z.
AU  - Wang, Z.
AU  - Geng, Y.
TI  - CSEANet: Cross-Stage Enhanced Aggregation Network for Detecting Surface Bolt Defects in Railway Steel Truss Bridges
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25113500
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007818984&doi=10.3390%2fs25113500&partnerID=40&md5=0a33acaddd479c07fabe3430ab12fdf7
AB  - The accurate detection of surface bolt defects in railway steel truss bridges plays a vital role in maintaining structural integrity. Conventional manual inspection techniques require extensive labor and introduce subjective assessments, frequently yielding variable results across inspections. While UAV-based approaches have recently been developed, they still encounter significant technical obstacles, including small target recognition, background complexity, and computational limitations. To overcome these challenges, CSEANet is introduced—an improved YOLOv8-based framework tailored for bolt defect detection. Our approach introduces three innovations: (1) a sliding-window SAF preprocessing method that improves small target representation and reduces background noise, achieving a 0.404 mAP improvement compared with not using it; (2) a refined network architecture with BSBlock and MBConvBlock for efficient feature extraction with reduced redundancy; and (3) a novel BoltFusionFPN module to enhance multi-scale feature fusion. Experiments show that CSEANet achieves an mAP@50:95 of 0.952, confirming its suitability for UAV-based inspections in resource-constrained environments. This framework enables reliable, real-time bolt defect detection, supporting safer railway operations and infrastructure maintenance. © 2025 by the authors.
KW  - bolt defect detection
KW  - multi-scale feature fusion
KW  - railway safety
KW  - small object detection
KW  - UAV-based inspection
KW  - Railroad transportation
KW  - Bolt defect detection
KW  - Defect detection
KW  - Features fusions
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Railway safety
KW  - Small object detection
KW  - Small targets
KW  - Steel truss bridge
KW  - UAV-based inspection
KW  - Subways
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Peng, S.
AU  - Zhang, X.
AU  - Zhou, L.
AU  - Wang, P.
TI  - YOLO-CBD: Classroom Behavior Detection Method Based on Behavior Feature Extraction and Aggregation
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25103073
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006657579&doi=10.3390%2fs25103073&partnerID=40&md5=ed80e149c43883dd74cdbcf83423850f
AB  - Classroom behavior can effectively reflect learning states, and thus classroom behavior detection is crucial for improving teaching methods and enhancing teaching quality. To address issues such as severe occlusions and large scale variations in student behavior detection, this paper proposes a classroom behavior detection model, named YOLO-CBD (YOLOv10s Classroom Behavior Detection). Firstly, BiFormer attention is introduced to redesign the Efficientv2 network, leading to a novel backbone network for efficient feature extraction of student classroom behaviors. The proposed attention module enables accurate localization in densely populated student settings. Secondly, a novel feature aggregation module is designed for replacing the basic C2f module in the YOLOv10s neck network and enhancing the capability to detect occluded targets effectively. Additionally, a feature pyramid network with efficient feature fusion is constructed to address inconsistencies among features of different scales. Finally, the Wise-IoU loss function is incorporated to handle sample imbalance issues. Experimental results show that, compared to the baseline model, YOLO-CBD improves precision by 5.7%, recall by 3.7%, and mAP50 by 3.5%, achieving effective classroom behavior detection. © 2025 by the authors.
KW  - classroom behavior detection
KW  - feature aggregation
KW  - feature extraction
KW  - multi-scale feature fusion
KW  - YOLOv10
KW  - Engineering education
KW  - Students
KW  - Behavior detection
KW  - Classroom behavior detection
KW  - Detection methods
KW  - Feature aggregation
KW  - Features extraction
KW  - Features fusions
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Teaching methods
KW  - YOLOv10
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - human
KW  - learning
KW  - male
KW  - neck
KW  - nonhuman
KW  - Teaching
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fonod, R.
AU  - Cho, H.
AU  - Yeo, H.
AU  - Geroliminis, N.
TI  - Advanced computer vision for extracting georeferenced vehicle trajectories from drone imagery
PY  - 2025
T2  - Transportation Research Part C: Emerging Technologies
DO  - 10.1016/j.trc.2025.105205
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009507433&doi=10.1016%2fj.trc.2025.105205&partnerID=40&md5=761b73429e7592816084f547b14ea42e
AB  - This paper presents a comprehensive framework for extracting georeferenced vehicle trajectories from high-altitude drone imagery, addressing key challenges in urban traffic monitoring and the limitations of traditional ground-based systems. Our approach integrates several novel contributions, including a tailored object detector optimized for high-altitude bird's-eye view perspectives, a unique track stabilization method that uses detected vehicle bounding boxes as exclusion masks during image registration, and an orthophoto and master frame-based georeferencing strategy that enhances consistent alignment across multiple drone viewpoints. Additionally, our framework features robust vehicle dimension estimation and detailed road segmentation, enabling comprehensive traffic dynamics analysis. Conducted in the Songdo International Business District, South Korea, the study utilized a multi-drone experiment covering 20 intersections, capturing approximately 12TB of ultra-high-definition video data over four days. The framework produced two high-quality datasets: the Songdo Traffic dataset, comprising approximately 700,000 unique vehicle trajectories, and the Songdo Vision dataset, containing over 5000 human-annotated images with about 300,000 vehicle instances categorized into four classes. Comparisons with high-precision sensor data from an instrumented probe vehicle highlight the accuracy and consistency of our extraction pipeline in dense urban environments. The public release of the Songdo Traffic and Songdo Vision datasets, along with the complete source code for the extraction pipeline, establishes new benchmarks in data quality, reproducibility, and scalability in traffic research. The results demonstrate the potential of integrating drone technology with advanced computer vision methods for precise and cost-effective urban traffic monitoring, providing valuable resources for developing intelligent transportation systems and enhancing traffic management strategies. © 2025 The Authors
KW  - Georeferenced vehicle trajectories
KW  - Machine vision
KW  - Multi-drone data collection
KW  - Traffic monitoring
KW  - Urban traffic
KW  - Vehicle tracking
KW  - Video image processing
KW  - Incheon [South Korea]
KW  - Songdo
KW  - South Korea
KW  - Advanced traffic management systems
KW  - Advanced traveler information systems
KW  - Aircraft detection
KW  - Benchmarking
KW  - Computer vision
KW  - Data handling
KW  - Drones
KW  - Highway traffic control
KW  - Image enhancement
KW  - Image registration
KW  - Image segmentation
KW  - Information management
KW  - Motor transportation
KW  - Street traffic control
KW  - Urban planning
KW  - Urban transportation
KW  - Vehicle detection
KW  - Video recording
KW  - Video signal processing
KW  - Data collection
KW  - Georeferenced vehicle trajectory
KW  - Ground-based systems
KW  - Machine-vision
KW  - Multi-drone data collection
KW  - Object detectors
KW  - Traffic monitoring
KW  - Urban traffic
KW  - Vehicle trajectories
KW  - Video-image processing
KW  - computer vision
KW  - imagery
KW  - monitoring system
KW  - tracking
KW  - traffic management
KW  - trajectory
KW  - transport vehicle
KW  - unmanned vehicle
KW  - urban transport
KW  - Cost effectiveness
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, M.
AU  - Lan, J.
AU  - Wang, L.
TI  - Road object localization and detection of thermal infrared images using a recursive gated convolution cross-aggregation network
PY  - 2025
T2  - Measurement: Journal of the International Measurement Confederation
DO  - 10.1016/j.measurement.2025.116971
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217262407&doi=10.1016%2fj.measurement.2025.116971&partnerID=40&md5=1bf25860b21ca4d2d87fdb34d3ec4a4f
AB  - Achieving accuracy perception is critical for ensuring vehicle safety in autonomous driving. Among sensing technologies, thermal infrared imaging provides the advantage of all-light capability. However, detection and positioning performance declines due to the low contrast of infrared images and the complex traffic backgrounds. To address this challenge, this paper proposes the recursive gated convolution cross-aggregation network (RGC-CAN). Firstly, the recursive gated convolutional network (RGC-Net) is proposed as the feature extraction module. This network employs gated convolution within a recursive-positional attention architecture to capture high-order spatial interactions, enhancing the contrast between the object and the background. Secondly, the cross-aggregation networks (CA-Net) is proposed for feature fusion. By leveraging both top-down and lateral connections, CA-Net fully exploits the semantic abstract features from deeper layers and the pixel features from shallower layers, improving overall feature representation. Finally, the cross-data interaction loss function (CILF) is proposed, which enhances the model's capabilities in sample classification and position regression. Experimental evaluations on the FLIR-ADAS dataset and KAIST dataset show that the proposed network achieves mean average precision of 90.4% and 96.9%, respectively, surpassing current state-of-the-art (SOTA) methods. Additionally, a series of ablation studies and extended experiments validate the effectiveness and broad applicability of our network. © 2025 Elsevier Ltd
KW  - Autonomous driving
KW  - Gated convolution
KW  - Object detection
KW  - Thermal infrared image
KW  - Object detection
KW  - Object recognition
KW  - Aggregation network
KW  - Autonomous driving
KW  - Detection performance
KW  - Gated convolution
KW  - Object localization
KW  - Objects detection
KW  - Sensing technology
KW  - Thermal infrared images
KW  - Thermal infrared imaging
KW  - Vehicle safety
KW  - Infrared imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, H.
AU  - Fu, W.
AU  - Wang, X.
AU  - Li, D.
AU  - Zhu, D.
AU  - Su, X.
TI  - An improved and lightweight small-scale foreign object debris detection model
PY  - 2025
T2  - Cluster Computing
DO  - 10.1007/s10586-024-05002-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003671787&doi=10.1007%2fs10586-024-05002-4&partnerID=40&md5=e1a35d5a61a055a1f6b6e721a6b042ca
AB  - Foreign object debris (FOD) on runways may cause irreparable damage and detecting FOD in intellectual technology has attracted more and more attention. Due to the limitation of the equipment, the main goal of the FOD detection methods is to obtain as high accuracy as possible by employing the model with as few parameters as possible. In this article, we propose a Lightweight FOD detection model named LF-YOLO, which can improve the detection accuracy of small FOD items and simultaneously decrease the parameters of the implemented model. The proposed LF-YOLO follows the overall framework of YOLOv8 and can be viewed as one of its improved variants. Firstly, to compensate for the loss of information regarding small objects during feature extraction, high-resolution feature maps were incorporated into the detection layer to fuse multiscale features, and the large object detection layer was removed from the model. Secondly, a lightweight backbone with strong feature extraction ability was developed by introducing Lightweight Downsampling Convolution (LDConv) modules. Subsequently, the lightweight Slim-Head was proposed by introducing slim-neck and Group-RepConv with Efficient Channel Attention Mechanism Head (GREHead) modules. Ultimately, to validate the effectiveness of the proposed model, comparative experiments were conducted using the small target FOD dataset. The results demonstrate that the proposed LF-YOLO can achieve better accuracy, over the other state-of-the-art (SOTA) methods with small parameters. (Our code is available at https://github.com/zflyy/LF-YOLO.git). © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
KW  - Deep learning
KW  - Foreign object debris (FOD)
KW  - Lightweight design
KW  - Object detection
KW  - YOLOv8 model
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Deep learning
KW  - Detection methods
KW  - Detection models
KW  - Features extraction
KW  - Foreign object debris
KW  - Lightweight design
KW  - Objects detection
KW  - Small scale
KW  - YOLOv8 model
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yao, J.
AU  - Lei, J.
AU  - Zhou, J.
AU  - Liu, C.
TI  - FG-YOLO: an improved YOLOv8 algorithm for real-time fire and smoke detection
PY  - 2025
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-025-03894-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219641237&doi=10.1007%2fs11760-025-03894-2&partnerID=40&md5=a159a4ba2b5ffa37e2019634fd361d21
AB  - To overcome the limitations of conventional fire detection methods in accurately recognizing small-scale, multi-target flames and irregularly shaped, low-quantity smoke due to insufficient feature extraction, this study proposes an enhanced YOLOv8-based algorithm called FG-YOLO. By integrating the FSCNet backbone and the GScELAN efficient layer aggregation module, FG-YOLO effectively mitigates feature detail loss that commonly arises from convolutional iterations. This advancement significantly boosts the representation ability for multi-scale, occluded, and small-object features, thereby improving detection accuracy while reducing computational overhead. The proposed approach was evaluated on the FASD dataset, which comprises 15,778 manually annotated images of flames and smoke labeled with the LabelImg tool. Experimental results show that FG-YOLO achieves a 5.4% increase in accuracy, a 6.8% increase in recall, a 4.5% increase in mAP50, and a 9.9% increase in mAP50-95 compared to the YOLOv8 baseline, alongside an inference speed of 76.51 FPS. With only 2.8 million parameters, FG-YOLO delivers high performance with optimized computational efficiency, offering a robust solution for real-time fire and smoke detection in safety-critical applications. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2025.
KW  - FG-YOLO
KW  - Fire and smoke detection
KW  - Multi-scale feature fusion
KW  - Small object detection
KW  - Fires
KW  - Inference engines
KW  - Premixed flames
KW  - Features fusions
KW  - FG-YOLO
KW  - Fire detection
KW  - Fire-detection method
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Real- time
KW  - Small object detection
KW  - Small scale
KW  - Smoke detection
KW  - Smoke detectors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Trinh, M.L.
AU  - Nguyen, D.T.
AU  - Dinh, L.Q.
AU  - Nguyen, M.D.
AU  - Setiadi, D.R.I.M.
AU  - Nguyen, M.T.
TI  - Unmanned Aerial Vehicles (UAV) Networking Algorithms: Communication, Control, and AI-Based Approaches
PY  - 2025
T2  - Algorithms
DO  - 10.3390/a18050244
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006623990&doi=10.3390%2fa18050244&partnerID=40&md5=d8676793527bbab7e84158af1d9c9086
AB  - This paper focuses on algorithms and technologies for unmanned aerial vehicles (UAVs) networking across different fields of applications. Given the limitations of UAVs in both computations and communications, UAVs usually need algorithms for either low latency or energy efficiency. In addition, coverage problems should be considered to improve UAV deployment in many monitoring or sensing applications. Hence, this work firstly addresses common applications of UAV groups or swarms. Communication routing protocols are then reviewed, as they can make UAVs capable of supporting these applications. Furthermore, control algorithms are examined to ensure UAVs operate in optimal positions for specific purposes. AI-based approaches are considered to enhance UAV performance. We provide either the latest work or evaluations of existing results that can suggest suitable solutions for specific practical applications. This work can be considered as a comprehensive survey for both general and specific problems associated with UAVs in monitoring and sensing fields. © 2025 by the authors.
KW  - artificial intelligence (AI)
KW  - communications
KW  - formation control
KW  - routing protocols
KW  - UAV networking
KW  - UAVs
KW  - Carrier communication
KW  - Drones
KW  - Micro air vehicle (MAV)
KW  - Speech transmission
KW  - Target drones
KW  - Telegraph
KW  - Aerial vehicle
KW  - Artificial intelligence
KW  - Communication control
KW  - Formation control
KW  - Lower energies
KW  - Routing-protocol
KW  - Unmanned aerial vehicle
KW  - Unmanned aerial vehicle networking
KW  - Vehicle networkings
KW  - Signaling
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Patel, R.
AU  - Chandalia, D.
AU  - Nayak, A.
AU  - Jeyabose, A.
AU  - Jijo, D.
TI  - CGI-Based Synthetic Data Generation and Detection Pipeline for Small Objects in Aerial Imagery
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3553530
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003033354&doi=10.1109%2fACCESS.2025.3553530&partnerID=40&md5=cb7a3ba6e9cbbd3619bc5f07587cfbe9
AB  - Small object detection in drone-captured imagery presents critical challenges across environmental monitoring, urban planning, and disaster management. Current approaches struggle with accurate landmark identification due to varying altitudes, diverse landscapes, and object scale variability. This research proposes an innovative two-stage object detection pipeline specifically designed for aerial imagery analysis. Our approach integrates advanced data generation techniques with a novel detection methodology. Experimental results demonstrate significant performance improvements: precision increased by 29.3% (from 0.616 to 0.796), recall improved by 24% (0.699 to 0.867), and mean Average Precision (mAP@0.5) enhanced by 23.2% (0.677 to 0.834) compared to traditional YOLO models. The pipeline combines Computer-Generated Imagery (CGI) for synthetic data creation with a two-stage detection approach. The first stage employs YOLOv9 for efficient region of interest localization, while the second stage utilizes a ResNet-based model for precise classification. By addressing challenges associated with small object detection, our methodology provides a scalable solution for automated landmark recognition in diverse aerial environments. © 2013 IEEE.
KW  - AAV object detection
KW  - Aerial imagery
KW  - drone-based image recognition
KW  - small object detection
KW  - synthetic data generation
KW  - two-stage object detection
KW  - Aerial photography
KW  - Aircraft detection
KW  - Direct air capture
KW  - Network security
KW  - Urban planning
KW  - Aerial imagery
KW  - Computer generated imagery
KW  - Data-detection
KW  - Drone-based image recognition
KW  - Objects detection
KW  - Small object detection
KW  - Synthetic data generations
KW  - Two-stage object detection
KW  - UAV object detection
KW  - Drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lin, F.
AU  - Wang, B.
AU  - Chen, Z.
AU  - Zhang, X.
AU  - Song, C.
AU  - Yang, L.
AU  - Cheng, J.C.P.
TI  - Efficient visual inspection of fire safety equipment in buildings
PY  - 2025
T2  - Automation in Construction
DO  - 10.1016/j.autcon.2025.105970
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215955960&doi=10.1016%2fj.autcon.2025.105970&partnerID=40&md5=ee4d05c93d9177dabb1185dad317975d
AB  - Fire safety equipment (FSE) in buildings is critical in ensuring occupant safety and mitigating losses during emergencies. However, its effectiveness is frequently compromised by inadequate maintenance. As buildings increase size and complexity, traditional manual inspection methods become impractical due to scalability and data management challenges. To address these issues, this paper proposes an advanced FSE detection framework with improvement strategies. The process commences with the developed YOLO-FSE algorithm, which is capable of identifying objects of varying sizes. This is complemented by precise localization of these objects through an enhanced tracking algorithm and visual simultaneous localization and mapping (vSLAM). The experiments demonstrate that this approach can effectively detect various fire safety equipment with the potential to replace labor-intensive manual methods. Notably, the YOLO-FSE network achieves a 7.9 % improvement in mean Average Precision (mAP) at a threshold of 0.5 (mAP@0.5), and a 9.4 % increase in mAP@0.95, indicating significant enhancements in detection accuracy. © 2025 Elsevier B.V.
KW  - Equipment detection
KW  - FSE
KW  - Object tracking
KW  - Visual positioning
KW  - Fire extinguishers
KW  - Equipment detection
KW  - Fire safety
KW  - Fire safety equipment
KW  - In-buildings
KW  - Inadequate maintenance
KW  - Object Tracking
KW  - Occupant safety
KW  - Safety equipments
KW  - Visual inspection
KW  - Visual positioning
KW  - Inspection equipment
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, H.
AU  - Ma, Y.
AU  - Hong, T.
AU  - Gong, T.
TI  - Enhanced and lightweight design of small object detector based on YOLOv5s model
PY  - 2025
T2  - International Journal of Machine Learning and Cybernetics
DO  - 10.1007/s13042-024-02383-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204434062&doi=10.1007%2fs13042-024-02383-1&partnerID=40&md5=34d64ad120bf2a7908ba5d39baeaf1c2
AB  - Owing to the challenges of dense target distribution and complex backgrounds in small detection targets, existing small object detection algorithms suffer from poor performance and high model complexity, which is extremely difficult to deploy on embedded platforms. To address above issues, we optimized the YOLOv5s model structure to enhance detection accuracy. To avoid incurring extra computational expenses, we introduced a local pruning strategy to reduce redundancy, which enables the detection model more suitable for embedded systems. Considering pruning may cause accuracy degradation, we employ knowledge distillation techniques combining feature distillation and output distillation. Specifically, we transfer the knowledge from a high-precision teacher model to a student model, enabling exceptional real-time performance. The experimental results on the VisDrone2019 dataset show that compared to the original algorithm, our model has reduced the parameter count by 50.38%, computation by 51.81%, and model size by 52.94%, totaling just 8 M. The average precision (mAP@0.5) improved to 42.2%. Our proposed model outperforms the current state-of-the-art methods for small object detection in terms of both accuracy and computational efficiency. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Local pruning
KW  - Model optimization
KW  - Small object detection
KW  - Two-stage distillation
KW  - Complex background
KW  - Detection targets
KW  - Lightweight design
KW  - Local pruning
KW  - Model optimization
KW  - Object detection algorithms
KW  - Object detectors
KW  - Small object detection
KW  - Small objects
KW  - Two-stage distillation
KW  - Teaching
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, Z.
AU  - Bo, K.
AU  - Hsu, C.-Y.
AU  - Liao, L.
TI  - Lightweight unmanned aerial vehicle object detection algorithm based on improved YOLOv8
PY  - 2025
T2  - Intelligent Data Analysis
DO  - 10.3233/IDA-230929
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005407150&doi=10.3233%2fIDA-230929&partnerID=40&md5=6e32a27524995bc75afcb2f98d73b27a
AB  - With the rapid development of unmanned aerial vehicle (UAV) technology and computer vision, real-time object detection in UAV aerial images has become a current research hotspot. However, the detection tasks in UAV aerial images face challenges such as disparate object scales, numerous small objects, and mutual occlusion. To address these issues, this paper proposes the ASM-YOLO model, which enhances the original model by replacing the Neck part of YOLOv8 with an efficient bidirectional cross-scale connections and adaptive feature fusion (ABiFPN). Additionally, a Structural Feature Enhancement Module (SFE) is introduced to inject features extracted by the backbone network into the Neck part, enhancing inter-network information exchange. Furthermore, the MPDIoU bounding box loss function is employed to replace the original CIoU bounding box loss function. A series of experiments was conducted on the VisDrone-DET dataset, and comparisons were made with the baseline network YOLOv8s. The experimental results demonstrate that the proposed model in this study achieved reductions of 26.1% and 24.7% in terms of parameter count and model size, respectively. Additionally, during testing on the evaluation set, the proposed model exhibited improvements of 7.4% and 4.6% in the AP50 and mAP metrics, respectively, compared to the YOLOv8s baseline model, thereby validating the practicality and effectiveness of the proposed model. Subsequently, the generalizability of the algorithm was validated on the DOTA and DIOR datasets, which share similarities with aerial images captured by drones. The experimental results indicate significant enhancements on both datasets. © 2024 – IOS Press. All rights reserved.
KW  - computer vision
KW  - drone aerial images
KW  - feature fusion
KW  - multi-scale object detection
KW  - real-time object detection
KW  - Aerial photography
KW  - Feature extraction
KW  - Micro air vehicle (MAV)
KW  - Object detection
KW  - Object recognition
KW  - Target drones
KW  - Aerial images
KW  - Aerial vehicle
KW  - Drone aerial image
KW  - Features fusions
KW  - Multi-scale object detection
KW  - Multi-scales
KW  - Objects detection
KW  - Real- time
KW  - Real-time object detection
KW  - Drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, R.
AU  - Huang, M.
AU  - Wang, L.
AU  - Hu, Y.
AU  - Tao, Y.
TI  - Roadside Object Detection Algorithm Based on Multiscale Sequence Fusion
ST  - 基 于 多 尺 度 序 列 融 合 的 路 侧 目 标 检 测 算 法
PY  - 2025
T2  - Laser and Optoelectronics Progress
DO  - 10.3788/LOP241187
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218980485&doi=10.3788%2fLOP241187&partnerID=40&md5=760781b6cf7a96841f8e89c061d63f77
AB  - This study develops a lightweight roadside object detection algorithm called MQ-YOLO. The algorithm is based on multiscale sequence fusion. It addresses the challenges of low detection accuracy for small and occluded targets and the large number of model parameters in urban traffic roadside object detection tasks. We design a D-C2f module based on multi-branch feature extraction to enhance feature representation while maintaining speed. To strengthen the integration of information from multiscale sequences and enhance feature extraction for small targets, the plural-scale sequence fusion (PSF) module is designed to reconstruct the feature fusion layer. Multiple attention mechanisms are incorporated into the detection head for greater focus on the salient semantic information of occluded targets. To enhance the detection performance of the model, a loss function based on the normalized Wasserstein distance is introduced. Experimental results on the DAIR-V2X-I dataset demonstrate that MQ-YOLO achieves improved mAP@50 and mAP@ (50‒95) by 3. 9 percentage point and 6. 0 percentage point compared to the valuses obtained with baseline YOLOv8n with 3. 96 Mbit parameters. Experiments on the DAIR-V2X-SPD-I dataset show that the model has good generalizability. During roadside deployment, the model reaches detection speeds of 62. 5 frame/s, meeting current roadside object detection requirement for edge deployment in urban traffic. © 2025 Universitat zu Koln. All rights reserved.
KW  - feature fusion
KW  - lightweight
KW  - plural-scale sequence
KW  - roadside target detection
KW  - YOLOv8n
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Chhetri, T.R.
TI  - Improving decision making using semantic web technologies
PY  - 2025
T2  - Improving Decision Making Using Semantic Web Technologies
DO  - 10.1007/978-3-658-45877-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005031077&doi=10.1007%2f978-3-658-45877-5&partnerID=40&md5=1eaa39d133e102e6fe123b32c38d3a82
AB  - As technology becomes integral to our lives, its influence on decision making in smart cities, healthcare, and manufacturing is undeniable. However, challenges such as limited contextual awareness, domain knowledge, explainability of machine learning (ML), and issues of interoperability, data quality, and GDPR (General Data Protection Regulation) compliance in data sharing hinder effective decision making. This book addresses these critical challenges by exploring how the synergy of semantic technologies (SW), like ontologies and knowledge graphs, with or without ML, can overcome these challenges to improve decision making. Through real-world case studies in data sharing, manufacturing, and agriculture, it offers theoretical and practical insights and guidelines of how SW can enhance prediction accuracy, integrate domain knowledge, support ML explainability, and tackle interoperability, data quality, and GDPR challenges. © Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2025. All rights reserved.
KW  - Artificial intelligence (AI)
KW  - Data privacy
KW  - Data sharing
KW  - Decision making
KW  - Edge intelligence
KW  - Explainable AI (XAI)
KW  - General data protection regulation (GDPR) compliance
KW  - Internet of things (IoT)
KW  - Knowledge graphs
KW  - Domain Knowledge
KW  - Knowledge graph
KW  - Artificial intelligence
KW  - Data Sharing
KW  - Decisions makings
KW  - Edge intelligence
KW  - Explainable artificial intelligence (XAI)
KW  - General data protection regulation  compliance
KW  - General data protection regulations
KW  - Internet of thing
KW  - Knowledge graphs
KW  - Regulation compliance
KW  - Decision making
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ismael, M.
AU  - Cornil, M.
TI  - Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment
PY  - 2025
T2  - Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
DO  - 10.5220/0013132600003912
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001839253&doi=10.5220%2f0013132600003912&partnerID=40&md5=6fefa73aa5dd847534c32133f3d536a6
AB  - This paper presents an outdoor tracking system using Real-Time Kinematic (RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s)) in urban areas where the accurate tracking of objects is critical and where displaying occluded information is important for safety reasons. The approach presented here replaces 2D screens/tablets and offers distinct advantages, particularly in scenarios demanding hands-free operation. The integration of RTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD represents a promising solution for outdoor applications. This paper provides valuable insights into leveraging the combined potential of RTK and OST-HMD for outdoor tracking tasks from the perspectives of systems integration, performance optimization, and usability. The main contributions of this paper are: 1) a system for seamlessly merging RTK systems with OST-HMD to enable relatively precise and intuitive outdoor tracking, 2) an approach to determine a global location to achieve the position relative to the world, 3) an approach referred to as ’semi-dynamic’ for system assessment. © 2025 by SCITEPRESS - Science and Technology Publications, Lda.
KW  - Augmented Reality
KW  - OST-HMD
KW  - RTK Systems
KW  - Tracking and Visual Navigation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, P.
AU  - Wang, P.
AU  - Zhu, D.
AU  - Zhou, B.
AU  - Lv, J.
AU  - Ye, Z.
TI  - SFANet: Efficient Detection of Vehicle Targets in SAR Images Based on SAR-Specialized Feature Aggregation
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3573019
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005980872&doi=10.1109%2fTIM.2025.3573019&partnerID=40&md5=499935ec131e9681ab7d2aa67f42987c
AB  - With the swift progress of deep learning technologies, the detection of ground vehicles in synthetic aperture radar (SAR) images has emerged as a prominent research focus. In this article, research is conducted on the detection of ground vehicles using MiniSAR mounted on unmanned aerial vehicles. However, the substantial background interference and high similarity between ground vehicles make it challenging for object detection. To address these issues, an efficient object detection network based on SAR-specialized feature attention (SFA) and lightweight global detector (LGD) is proposed in this article. First, the SFA is proposed, which introduces proportional attention to different detection layers. By analyzing and focusing on spatial and channel information, it alleviates the classification challenges when dealing with highly similar ground vehicles and limited information. Second, a novel detector LGD is proposed, which combines Swin Transformer and lightweight convolutions. LGD captures deeper semantic information through global attention, enabling the network to detect vehicles in complex scenes. Finally, transfer learning is utilized to load pre-classification weights of the backbone. To verify the effectiveness of the proposed algorithm, comprehensive experiments were performed across multiple datasets. The results indicate that the proposed method offers substantial performance enhancements. © 2025 IEEE. All rights reserved.
KW  - Feature attention
KW  - global attention
KW  - lightweight
KW  - MiniSAR
KW  - object detection
KW  - Automobiles
KW  - Bicycles
KW  - Buses
KW  - Cabs (truck)
KW  - Drones
KW  - Magnetic levitation vehicles
KW  - Motorcycles
KW  - Efficient detection
KW  - Feature aggregation
KW  - Feature attention
KW  - Global attention
KW  - Image-based
KW  - Lightweight
KW  - MiniSAR
KW  - Objects detection
KW  - Synthetic aperture radar images
KW  - Vehicle targets
KW  - Vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, S.
AU  - Zhang, W.
AU  - Tao, S.
AU  - Chai, Y.
AU  - Bahri, P.A.
AU  - Wang, H.
TI  - A Dynamic Pig Face Detection Method Based on Spatial Channel Weight Optimization Attention Mechanism and Adaptive Anchor Point Selection
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3573344
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006625080&doi=10.1109%2fTIM.2025.3573344&partnerID=40&md5=5c060b0883b2c0655442e1a9d633ff08
AB  - Pig face detection plays a crucial role in the field of agricultural farming, especially in precise feeding and disease monitoring. This study proposes a method called adaptive region-based convolutional neural network (A-RCNN) for multiangled dirty pig face detection in outdoor environments. First, in order to address the interference caused by dirty cleaning surfaces, a feature enhancement module (FEM) is designed to improve the network’s classification ability. Second, in order to ensure that the anchor points are more in line with the shape of the pig surface, an anchor point selection module (APSM) is introduced to generate high-quality area recommendations. Finally, in response to the interference problem of complex outdoor backgrounds, this article adopts a dynamic training strategy (DTS) to optimize the final detection results using these high-quality region suggestions. This study conducted an in-depth exploration of the publicly available JD dataset, and the experimental results showed that compared with existing methods, this method demonstrated excellent performance, achieving 56.3% mean average precision (mAP) and an improvement of 6.02% compared to the baseline Faster RCNN. In addition, to verify the practical application effect of the model, this article also deployed it on the edge device Raspberry Pi 4B, further confirming the effectiveness and practicality of the model. © 1963-2012 IEEE.
KW  - Anchor point selection
KW  - attention mechanism
KW  - dynamic training
KW  - pig face detection
KW  - Face recognition
KW  - Surface cleaning
KW  - Anchor point
KW  - Anchor point selection
KW  - Attention mechanisms
KW  - Dynamic training
KW  - Face detection methods
KW  - Faces detection
KW  - High quality
KW  - Pig face detection
KW  - Point selection
KW  - Spatial channels
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Alkandary, K.
AU  - Yildiz, A.S.
AU  - Meng, H.
TI  - A Comparative Study of YOLO Series (v3–v10) with DeepSORT and StrongSORT: A Real-Time Tracking Performance Study
PY  - 2025
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics14050876
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000520215&doi=10.3390%2felectronics14050876&partnerID=40&md5=26515be2a3df3740cf57dde144426ad5
AB  - Many previous studies have explored the integration of a specific You Only Look Once (YOLO) model with real-time trackers like Deep Simple Online and Realtime Tracker (DeepSORT) and Strong Simple Online and Realtime Tracker (StrongSORT). However, few have conducted a comprehensive and in-depth analysis of integrating the family of YOLO models with these real-time trackers to study the performance of the resulting pipeline and draw critical conclusions. This work aims to fill this gap, with the primary objective of investigating the effectiveness of integrating the YOLO series, in light-sized versions, with the real-time DeepSORT and StrongSORT tracking algorithms for real-time object tracking in a computationally limited environment. This work will systematically compare various lightweight YOLO versions, from YOLO version 3 (YOLOv3) to YOLO version 10 (YOLOv10), combined with both tracking algorithms. It will evaluate their performance using detailed metrics across diverse and challenging real-world datasets: the Multiple Object Tracking 2017 (MOT17) and Multiple Object Tracking 2020 (MOT20) datasets. The goal of this work is to assess the robustness and accuracy of these light models in multiple complex real-world environments in scenarios with limited computational resources. Our findings reveal that YOLO version 5 (YOLOv5), when combined with either tracker (DeepSORT or StrongSORT), offers not only a solid baseline in terms of the model’s size (enabling real-time performance on edge devices) but also competitive overall performance (in terms of Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP)). The results suggest a strong correlation between the choice regarding the YOLO version and the tracker’s overall performance. © 2025 by the authors.
KW  - autonomous driving
KW  - DeepSORT
KW  - detection
KW  - StrongSORT
KW  - tracking
KW  - YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tian, S.
AU  - Zhao, K.
AU  - Song, L.
TI  - Research on Small Target Detection Algorithm for Autonomous Vehicle Scenarios
PY  - 2025
T2  - Journal of Advanced Transportation
DO  - 10.1155/atr/8452511
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002470346&doi=10.1155%2fatr%2f8452511&partnerID=40&md5=30a332076810faa9e817297c161ef75a
AB  - In recent years, road traffic object detection has gained prominence in areas such as traffic monitoring, autonomous driving, and road safety. Nonetheless, existing algorithms offer room for improvement, particularly when detecting distant or inherently small targets, such as vehicles and pedestrians, from camera perspectives. By addressing the detection accuracy issues associated with small targets, this study introduces the YOLOv5s-LGC detection algorithm. This model incorporates a multiscale feature fusion network and leverages the lightweight GhostNet module to reduce model parameters. Furthermore, the GC attention module is employed to mitigate background interference, thereby enhancing the average detection accuracy across all categories. Through data analysis, target detection at different scales and sampling rates is determined. Experiments indicate that the YOLOv5s-LGC model surpasses the baseline YOLOv5s in detection accuracy on the Partial_BDD100K and KITTI datasets by 3.3% and 1.6%, respectively. This improvement in locating and classifying small targets presents a novel approach for applying object detection algorithms in road traffic scenarios. Copyright © 2025 Sheng Tian et al. Journal of Advanced Transportation published by John Wiley & Sons Ltd.
KW  - autonomous driving
KW  - global context attention module
KW  - road traffic
KW  - target detection
KW  - Motor transportation
KW  - Object detection
KW  - Vehicle detection
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Detection accuracy
KW  - Global context
KW  - Global context attention module
KW  - Road traffic
KW  - Small target detection
KW  - Small targets
KW  - Target detection algorithm
KW  - Targets detection
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Porto, J.V.d.A.
AU  - Szemes, P.T.
AU  - Pistori, H.
AU  - Menyhárt, J.
TI  - Trending Machine Learning Methods for Vehicle, Pedestrian, and Traffic for Detection and Tracking Task in the Post-Covid Era: A Literature Review
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3565901
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003977666&doi=10.1109%2fACCESS.2025.3565901&partnerID=40&md5=376a57c7006030a8b6a401693cc49c5a
AB  - This study, aimed at professionals in research and development in the fields of computer vision, artificial intelligence, and intelligent transportation, presents a systematic literature review on recent machine learning methodologies applied to the detection and tracking of vehicles, pedestrians, and traffic flow. The analysis of articles published between 2022 and 2025 (early access) in the post-COVID era explored the integration of machine learning and deep learning to address traffic challenges, allowing for the comparison of different approaches and the formulation of hypotheses based on the 46 articles that comprised the review corpus. Furthermore, the evaluation of the reported metrics revealed inconsistencies in the methodologies employed, attributed to the lack of standardization across the studies. In light of this, this work proposes alternatives for future experiments, emphasizing the emerging potential of the field through the adoption of new standardization systems and the exploration of experimental combinations. © 2013 IEEE.
KW  - Deep learning
KW  - detection
KW  - machine learning
KW  - tracking
KW  - urban mobility
KW  - Calibration
KW  - Contrastive Learning
KW  - Navigation
KW  - Supervised learning
KW  - Traffic control
KW  - Urban transportation
KW  - Vehicles
KW  - Deep learning
KW  - Detection
KW  - Detection and tracking
KW  - Literature reviews
KW  - Machine learning methods
KW  - Machine-learning
KW  - Research and development
KW  - Tracking
KW  - Urban mobility
KW  - Vehicle traffic
KW  - Deep learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ma, Y.
AU  - Zhang, P.
AU  - Tan, F.
TI  - Improved YOLOv8s Model for Smoke and Flame Detection in Complex Backgrounds
ST  - 面向复杂背景下烟雾火焰检测的改进 YOLOv8s 算法
PY  - 2025
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2406-0348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007359684&doi=10.3778%2fj.issn.1002-8331.2406-0348&partnerID=40&md5=2bf317561cf1751b3fc55f1763e8f7c6
AB  - Aiming to address issues such as confusion between smoke flame targets and background within complex backgrounds, which often result in low accuracy of smoke flame detection, an enhanced model based on YOLOv8s for detecting smoke flames within complex backgrounds is proposed. Firstly, the feature channels are highly similar to each other, and in order to effectively utilize the redundancy across different channels and improve the ability of model to differentiate between smoke and flame targets and backgrounds, the C2fFR (C2f with partial rep conv) lightweight feature extraction module is introduced. Secondly, the MCFM (multi-scale context fusion module) is designed to capture and utilize contextual information for enhancing feature representation. Lastly, the Inner-SIoU loss function is employed to address bounding box mismatches and the regression ability of the model is improved for high IoU samples. Experimental results demonstrate that compared to the baseline YOLOv8s model, the enhanced YOLOv8s smoke flame detection model achieves improvements of 4.6 percentage points in mAP@50 and 2.3 percentage points in mAP@50:95. Moreover, it reduces the number of model parameters by 18.9% and computation by 8.1%. while maintaining an FPS (frame per second) of 93. Additionally, it exhibits superior detection performance when compared to other mainstream detection algorithms. © 2025 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
KW  - C2fFR
KW  - Inner-SIoU
KW  - multi-scale context fusion module
KW  - YOLOv8s
KW  - Flame research
KW  - Premixed flames
KW  - Regression analysis
KW  - Smoke detectors
KW  - C2fFR
KW  - Complex background
KW  - Flame detection
KW  - Fusion modules
KW  - Inner-SIoU
KW  - Multi-scale context fusion module
KW  - Multi-scales
KW  - Percentage points
KW  - Target and background
KW  - YOLOv8
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Khorsand, H.
AU  - Arezoomandan, S.
AU  - Han, D.K.
TI  - Enhanced Long-Range UAV Detection: Leveraging Slicing Aided Hyper Inference with YOLOv8
PY  - 2025
T2  - Digest of Technical Papers - IEEE International Conference on Consumer Electronics
DO  - 10.1109/ICCE63647.2025.10930186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006535293&doi=10.1109%2fICCE63647.2025.10930186&partnerID=40&md5=2ac52e3535d3bd666db3bd8d3d3880ec
AB  - The increasing use of Unmanned Aerial Vehicles (UAVs) in commercial applications has highlighted the urgent need for advanced detection systems that can reliably identify small drones from long distances. Detecting small drones at extended ranges remains challenging due to their minimal size within the image frame. While higher-resolution cameras can capture more details, traditional object detection methods struggle with resolution constraints, leading to significant degradation in detection accuracy after downscaling. To address these limitations, this research explores the application of the Slicing Aided Hyper Inference (SAHI) method, which enhances object detection by dividing high-resolution images into smaller, overlapping patches that align with the input resolution requirements of detectors like YOLO. By preserving critical pixel information through this approach, SAHI significantly improves the detection accuracy of small, distant drones. To evaluate this, we conducted a series of experiments using various drone datasets, including Long Range Drone Detection (LRDD), Drone vs. Birds, DetFly, and GAN-translated synthetic images. Results show a significant improvement in detecting small drones with SAHI compared to the baseline YOLOv8 model. © 2025 IEEE.
KW  - Slicing aided hyper inference
KW  - Small object detection
KW  - UAV detection
KW  - YOLO
KW  - Micro air vehicle (MAV)
KW  - Photointerpretation
KW  - Target drones
KW  - Advanced detections
KW  - Aerial vehicle
KW  - Commercial applications
KW  - Detection accuracy
KW  - Detection system
KW  - Slicing aided hyper inference
KW  - Small object detection
KW  - Unmanned aerial vehicle detection
KW  - Vehicles detection
KW  - YOLO
KW  - Drones
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Guo, D.
AU  - Xu, P.
AU  - Cai, M.
AU  - Liu, E.
AU  - Wang, M.
AU  - Shan, Z.
AU  - Jiang, F.
TI  - DBG-YOLO: Efficient detection of hidden dangers of manhole covers based on deep learning YOLO network
PY  - 2025
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-025-20982-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008458870&doi=10.1007%2fs11042-025-20982-0&partnerID=40&md5=9a5fc551819fbd33588bf33fd4fcc374
AB  - Manhole covers are a critical component of urban infrastructure, and their damage poses significant threats to road safety and structural integrity. Real-time detection and tagging of manhole covers enable timely maintenance, reducing traffic risks and improving infrastructure reliability. To address this issue, we propose an improved manhole cover hazard detection algorithm, DBG-YOLO, based on the YOLOv8n framework. The proposed DBG-YOLO model integrates a Dilated Reparam Block (DRB) into the C2f module of the backbone network, enhancing both the receptive field and feature representation capabilities. This optimization significantly improves detection accuracy for small objects and complex scenarios. When integrated with comprehensive data augmentation strategies, the framework demonstrates exceptional adaptability to low-light conditions, achieving robust detection even under insufficient illumination. For feature fusion, the neck network incorporates a Bidirectional Feature Pyramid Network (BiFPN) and a Global Attention Mechanism (GAM), forming an advanced multi-scale architecture that effectively addresses partial occlusion challenges. By replacing the conventional loss function with SlideLoss, the model further refines bounding box regression accuracy, ensuring precise localization in demanding environments. Collectively, these innovations, synergized with adaptive data augmentation, provide a holistic solution to detection limitations in low-light and occluded scenarios. Experimental results demonstrate that DBG-YOLO achieves superior performance with 93.6% mAP@50 at 167.5 FPS, outperforming both Faster R-CNN (81.4%/13.4 FPS) and YOLOv5s (91.3%/142.7 FPS) in terms of accuracy and inference speed. This lightweight architecture establishes a new state-of-the-art balance between detection accuracy, computational efficiency, and model complexity. This innovative approach provides robust technical support for real-time detection tasks, enhancing urban infrastructure monitoring, improving road safety, and contributing to the development of smart cities. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
KW  - Bidirectional feature pyramid network
KW  - Dilated reparam block
KW  - Global attention mechanism
KW  - Manhole cover hazard detection
KW  - Accident prevention
KW  - Complex networks
KW  - Critical infrastructures
KW  - Deep learning
KW  - Hazards
KW  - Motor transportation
KW  - Network architecture
KW  - Object detection
KW  - Roads and streets
KW  - Smart city
KW  - Attention mechanisms
KW  - Bidirectional feature pyramid network
KW  - Dilated reparam block
KW  - Feature pyramid
KW  - Global attention mechanism
KW  - Hazard detection
KW  - Manhole cover
KW  - Manhole cover hazard detection
KW  - Pyramid network
KW  - Urban infrastructure
KW  - Computational efficiency
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, J.
AU  - Zhao, F.
AU  - Jin, Z.
TI  - LEN-YOLO: a lightweight remote sensing small aircraft object detection model for satellite on-orbit detection
PY  - 2025
T2  - Journal of Real-Time Image Processing
DO  - 10.1007/s11554-024-01601-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212458255&doi=10.1007%2fs11554-024-01601-x&partnerID=40&md5=cfaec125d577ad801049e399c453af58
AB  - The performance of conventional detection algorithms in small aircraft target detection is often unsatisfactory due to the intricate backgrounds of remote sensing images and the diminutive size of aircraft targets. Furthermore, prevalent deep learning algorithms typically prove overly complex for integration into resource-constrained satellite platforms. In response to these challenges, an enhanced algorithm named LEN-YOLO (Lite backbone - Enhanced Neck - YOLO) has been devised to enhance detection accuracy while preserving model simplicity for the detection of small aircraft in satellite on-orbit scenarios. First, the EIoU Loss is adopted for target localization, enabling the network to effectively focus on small aircraft targets. Second, a Lite backbone is designed by discarding high semantic information, using low-semantic feature maps to detect small targets. Finally, a Bidirectional Weighted FPN based on SimAM and GSConv (BSG-FPN) is proposed to fuse feature maps of different scales to increase detailed information. Experimental results on RSOD and DIOR datasets demonstrate compared to the baseline YOLOv5, LEN-YOLO achieves an increase of 5.1% and 4.2% in APs respectively. Notably, parameters are reduced by 78.3% and floating-point operations by 33.2%. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Lightweight
KW  - Object detection
KW  - Remote sensing
KW  - Small aircraft detect
KW  - YOLO
KW  - Aircraft detection
KW  - Deep learning
KW  - Object tracking
KW  - Remote sensing
KW  - Small satellites
KW  - Aircraft targets
KW  - Detection models
KW  - Feature map
KW  - Lightweight
KW  - Objects detection
KW  - On orbit
KW  - Remote-sensing
KW  - Small aircraft
KW  - Small aircraft detect
KW  - YOLO
KW  - Proximity sensors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gerdan Koc, D.
AU  - Vatandas, M.
TI  - Development and Performance Analysis of an Autonomous Agricultural Vehicle for Fruit Transportation
PY  - 2025
T2  - Journal of Field Robotics
DO  - 10.1002/rob.22573
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004214031&doi=10.1002%2frob.22573&partnerID=40&md5=26f1962c9c3afa17f40a591f84d253a3
AB  - Reducing product damage, preserving quality, and enhancing efficiency from harvest to consumption are crucial for sustainable agriculture. The integration of advanced information and communication technologies into agricultural practices plays a vital role in meeting these goals. This study introduces an autonomous transport vehicle designed for the efficient logistics of fruit transportation in agricultural settings. The vehicle's software framework is constructed on the Robot Operating System (ROS) and incorporates an enhanced hybrid navigation system that merges the Extended Kalman Filter (EKF) with Simultaneous Localization and Mapping (SLAM) for precise localization. The A* algorithm facilitates global path planning, whereas the Dynamic Window Approach (DWA) guarantees real-time obstacle avoidance. Essential hardware components comprise high-resolution LIDAR for environmental mapping, an Inertial Measurement Unit (IMU) for motion estimation, and wheel encoders for odometry. The performance evaluation was executed across five distinct terrain types: concrete, fine-tilled soil, coarse-tilled soil, asphalt, and grass. The vehicle attained optimal path-following precision on concrete, exhibiting a deviation of 5.39 cm at a speed of 0.3 m/s with a 200 kg payload, whereas tracking errors escalated on uneven terrains like grass and coarse-tilled soil. Maneuverability assessments verified a turning radius of 60.0 cm for 90° turns and 125.0 cm for 180° turns, ensuring suitability in restricted agricultural environments. Finite element analysis (FEA) evaluated structural durability under diverse loads (2000–4000 N), indicating a minimum safety factor of 1.23, thereby affirming structural stability under static conditions. This study demonstrates the potential of autonomous transport vehicles to revolutionize agricultural logistics by reducing labor dependency, improving operational efficiency, and supporting sustainable farming. © 2025 The Author(s). Journal of Field Robotics published by Wiley Periodicals LLC.
KW  - Autonomous agricultural vehicle
KW  - Combined positioning
KW  - Motion planning
KW  - Path planning
KW  - Agricultural robots
KW  - Fruits
KW  - Maneuverability
KW  - Navigation
KW  - Tracked vehicles
KW  - Tractors (agricultural)
KW  - Vehicle performance
KW  - Advanced informations
KW  - Autonomous agricultural vehicles
KW  - Autonomous transport vehicles
KW  - Combined positioning
KW  - Information and Communication Technologies
KW  - Motion-planning
KW  - Performances analysis
KW  - Product damage
KW  - Sustainable agriculture
KW  - Tilled soils
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Eom, T.-H.
AU  - Kim, H.-J.
TI  - A Novel Approach to Object Detection Under Diverse Illumination Conditions Using Row-Wise Exposure Images
PY  - 2025
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2025.3554806
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002021042&doi=10.1109%2fJSEN.2025.3554806&partnerID=40&md5=aa7a6a5960e43a94f862a22c00d7d5e7
AB  - In this study, we present a novel approach to object detection utilizing row-wise exposure (RWE) images to substantially improve object detection performance in low- and high-illumination conditions. Unlike previous RWE imaging techniques that require row-wise merging for high dynamic range (HDR) synthesis, our system directly utilizes raw RWE images without postprocessing. It enables the proposed object detection algorithm to effectively overcome the limitations of conventional systems in dynamic and variable lighting scenarios. Additionally, we introduce a tailored data augmentation strategy optimized for the unique characteristics of RWE images, enhancing model training and robustness without relying on dedicated RWE datasets. We developed a prototype CMOS image sensor (CIS) with RWE functionality to demonstrate the practical viability of our approach. This prototype was instrumental in validating the system’s effectiveness in real-world conditions. The proposed data augmentation method, designed specifically for raw RWE images, enriches the training dataset, enabling models to adapt to various lighting situations and improve their generalization abilities. Our experiments employed widely adopted object detection models, such as YOLOv7 and YOLOv9, along with standard datasets, such as MS COCO and HDR4RTT, to evaluate model performance under varying illumination coefficients and motion blur intensities. The results showed significant performance improvements over existing object detection methods, especially in challenging illumination conditions and dynamic environments. © 2001-2012 IEEE.
KW  - Challenging illumination conditions
KW  - CMOS imager sensor (CIS)
KW  - high dynamic range (HDR)
KW  - machine vision
KW  - object detection
KW  - row-wise exposure (RWE) images
KW  - Image coding
KW  - Image enhancement
KW  - Laser beams
KW  - Machine vision
KW  - Remote sensing
KW  - Challenging illumination condition
KW  - CMOS imager sensor
KW  - CMOS imagers
KW  - Data augmentation
KW  - High dynamic range
KW  - Illumination conditions
KW  - Machine-vision
KW  - Objects detection
KW  - Row-wise exposure image
KW  - CMOS integrated circuits
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Yang, K.
AU  - Qiu, C.
AU  - Wang, L.
AU  - Cai, Y.
AU  - Wei, H.
AU  - Yu, Q.
AU  - Huang, P.
TI  - HYFF-CB: Hybrid Feature Fusion Visual Model for Cargo Boxes
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25061865
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001001187&doi=10.3390%2fs25061865&partnerID=40&md5=380e4ba094408a5dd599297173ae1ef7
AB  - In automatic loading and unloading systems, it is crucial to accurately detect the locations of boxes inside trucks in real time. However, the existing methods for box detection have multiple shortcomings, and can hardly meet the strict requirements of actual production. When the truck environment is complex, the currently common models based on convolutional neural networks show certain limitations in the practical application of box detection. For example, these models fail to effectively handle the size inconsistency and occlusion of boxes, resulting in a decrease in detection accuracy. These problems seriously restrict the performance and reliability of automatic loading and unloading systems, making it impossible to achieve ideal detection accuracy, speed, and adaptability. Therefore, there is an urgent need for a new and more effective box detection method. To this end, this paper proposes a new model, HYFF-CB, which incorporates key technologies such as a location attention mechanism, a fusion-enhanced pyramid structure, and a synergistic weighted loss system. After real-time images of a truck were obtained by an industrial camera, the HYFF-CB model was used to detect the boxes in the truck, having the capability to accurately detect the stacking locations and quantity of the boxes. After rigorous testing, the HYFF-CB model was compared with other existing models. The results show that the HYFF-CB model has apparent advantages in detection rate. With its detection performance and effect fully meeting the actual application requirements of automatic loading and unloading systems, the HYFF-CB model can excellently adapt to various complex and changing scenarios for the application of automatic loading and unloading. © 2025 by the authors.
KW  - automatic loading and unloading
KW  - box detection
KW  - machine learning
KW  - synergistic weighted loss
KW  - Automobile testing
KW  - Automobiles
KW  - Convolutional neural networks
KW  - Image enhancement
KW  - Automatic loading
KW  - Automatic loading and unloading
KW  - Box detection
KW  - Detection accuracy
KW  - Hybrid features
KW  - Loading and unloading
KW  - Loading system
KW  - Machine-learning
KW  - Synergistic weighted loss
KW  - Unloading systems
KW  - article
KW  - camera
KW  - controlled study
KW  - convolutional neural network
KW  - diagnosis
KW  - hybrid
KW  - machine learning
KW  - male
KW  - reliability
KW  - velocity
KW  - Trucks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, E.
AU  - Leather, O.
AU  - Zhuang, A.
AU  - Budhwani, A.
AU  - Dempster, R.
AU  - Li, Q.
AU  - Al-Sharman, M.
AU  - Rayside, D.
AU  - Melek, W.
TI  - RALACs: Action Recognition in Autonomous Vehicles Using Interaction Encoding and Optical Flow
PY  - 2025
T2  - IEEE Transactions on Cybernetics
DO  - 10.1109/TCYB.2024.3515104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213558463&doi=10.1109%2fTCYB.2024.3515104&partnerID=40&md5=6ea71a1011f2b4ad2816309a1d566064
AB  - When applied to autonomous vehicle (AV) settings, action recognition can enhance an environment model's situational awareness. This is especially prevalent in scenarios where traditional geometric descriptions and heuristics in AVs are insufficient. However, action recognition has traditionally been studied for humans, and its limited adaptability to noisy, un-clipped, un-pampered, raw RGB data has limited its application in other fields. To push for the advancement and adoption of action recognition into AVs, this work proposes a novel two-stage action recognition system, termed RALACs. RALACs formulates the problem of action recognition for road scenes, and bridges the gap between it and the established field of human action recognition. This work shows how attention layers can be useful for encoding the relations across agents, and stresses how such a scheme can be class-agnostic. Furthermore, to address the dynamic nature of agents on the road, RALACs constructs a novel approach to adapting Region of Interest (ROI) alignment to agent tracks for downstream action classification. Finally, our scheme also considers the problem of active agent detection, and utilizes a novel application of fusing optical flow maps to discern relevant agents in a road scene. We show that our proposed scheme can outperform the baseline on the ICCV2021 Road Challenge dataset (Singh et al., 2023) algorithm and by deploying it on a real vehicle platform, we provide preliminary insight to the usefulness of action recognition in decision making.  © 2013 IEEE.
KW  - Action recognition
KW  - autonomous vehicles
KW  - ICCV2021 Road Challenge
KW  - interaction encoding
KW  - motion prediction
KW  - optical flow
KW  - Autonomous vehicles
KW  - Image coding
KW  - Magnetic levitation vehicles
KW  - Motor transportation
KW  - Optical character recognition
KW  - Action recognition
KW  - Autonomous Vehicles
KW  - Encodings
KW  - Environment models
KW  - Iccv2021 road challenge
KW  - Interaction encoding
KW  - Motion prediction
KW  - Optical-
KW  - Setting action
KW  - Situational awareness
KW  - Optical flows
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Z.
AU  - Dang, C.
AU  - Wang, L.
TI  - Personnel Search and Rescue Detector Based on Reconfigurable FPGA Accelerator: A Lightweight Multi-Scale Parallel Drone-Mounted Detector
PY  - 2025
T2  - IEEE Geoscience and Remote Sensing Letters
DO  - 10.1109/LGRS.2025.3550349
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000049139&doi=10.1109%2fLGRS.2025.3550349&partnerID=40&md5=a91c6b58d39809e8e80ee01b2677d11e
AB  - Natural disasters cause significant human and economic losses every year, making timely search and rescue crucial. In remote, hard-to-reach areas and during network outages after disasters, rescue missions become even more challenging. Traditional edge device solutions rely on extensive hardware resources and high power consumption to support precision and speed, which cannot meet the numerous constraints of post-disaster scenarios. Moreover, existing state-of-the-art detectors (e.g., YOLO series, RetinaNet) have chain-structured networks with narrow receptive fields and single-scale features, and cannot achieve high-speed inference through parallel computing. This letter proposes a custom convolutional neural network (CNN) solution for search and rescue, equipped with field-programmable gate array (FPGA) accelerators on drones. This method is autonomous, low-cost, highly accurate, and efficient for drone rescue missions. Tested on the Lacmus Drone Dataset (LADD), it achieved an accuracy of 87.5%, inference speed of 24.7 ms, and an efficiency ratio of 19.1. Additionally, the search and rescue NET (SRNET) incorporates a finite element method (FEM) structure, which expands the receptive field through a multi-scale parallel architecture, well-suited for the parallel processing capabilities of FPGAs. A reconfigurable FPGA accelerator was also developed, reusing different modules based on the instruction set. It innovatively employs kernel partitioning, significantly reducing redundant access to input feature maps and kernels, balancing parallel computation capabilities while consuming less system power. In large-scale disaster search and rescue, this approach outperforms other systems. © 2004-2012 IEEE.
KW  - Accelerator
KW  - convolutional neural network (CNN)
KW  - finite element method (FEM) field-programmable gate array (FPGA)
KW  - optical remote sensing image
KW  - reconfigurable
KW  - ship detection
KW  - Aircraft accidents
KW  - Analog storage
KW  - Balancing
KW  - Convolutional neural networks
KW  - Digital storage
KW  - Network security
KW  - Parallel architectures
KW  - Reconfigurable architectures
KW  - Reconfigurable hardware
KW  - Risk management
KW  - Accelerato r
KW  - Convolutional neural network
KW  - FEM FPGA
KW  - Multi-scales
KW  - Optical remote sen ing image
KW  - Optical-
KW  - Reconfigurable
KW  - Reconfigurable FPGA
KW  - Search and rescue
KW  - Ship detection
KW  - array
KW  - artificial neural network
KW  - detection method
KW  - finite element method
KW  - hardware
KW  - instrumentation
KW  - satellite imagery
KW  - search and rescue
KW  - Drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Richter, Y.
AU  - Zach, S.
AU  - Blum, M.Y.
AU  - Pinhasi, G.A.
AU  - Pinhasi, Y.
TI  - Tracking of Low Radar Cross-Section Super-Sonic Objects Using Millimeter Wavelength Doppler Radar and Adaptive Digital Signal Processing
PY  - 2025
T2  - Remote Sensing
DO  - 10.3390/rs17040650
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219178732&doi=10.3390%2frs17040650&partnerID=40&md5=0551a4c31a800c9388d617b94550c523
AB  - Small targets with low radar cross-section (RCS) and high velocities are very hard to track by radar as long as the frequent variations in speed and location demand shorten the integration temporal window. In this paper, we propose a technique for tracking evasive targets using a continuous wave (CW) radar array of multiple transmitters operating in the millimeter wavelength (MMW). The scheme is demonstrated to detect supersonic moving objects, such as rifle projectiles, with extremely short integration times while utilizing an adaptive processing algorithm of the received signal. Operation at extremely high frequencies qualifies spatial discrimination, leading to resolution improvement over radars operating in commonly used lower frequencies. CW transmissions result in efficient average power utilization and consumption of narrow bandwidths. It is shown that although CW radars are not naturally designed to estimate distances, the array arrangement can track the instantaneous location and velocity of even supersonic targets. Since a CW radar measures the target velocity via the Doppler frequency shift, it is resistant to the detection of undesired immovable objects in multi-scattering scenarios; thus, the tracking ability is not impaired in a stationary, cluttered environment. Using the presented radar scheme is shown to enable the processing of extremely weak signals that are reflected from objects with a low RCS. In the presented approach, the significant improvement in resolution is beneficial for the reduction in the required detection time. In addition, in relation to reducing the target recording time for processing, the presented scheme stimulates the detection and tracking of objects that make frequent changes in their velocity and position. © 2025 by the authors.
KW  - adaptive detection
KW  - adaptive DSP radar
KW  - bi-static radar
KW  - doppler radar
KW  - millimeter wave radar
KW  - MIMO radar
KW  - stealth object detection
KW  - target tracking
KW  - Image analysis
KW  - Image coding
KW  - Image segmentation
KW  - Image thinning
KW  - MIMO radar
KW  - Radar clutter
KW  - Radar cross section
KW  - Radar signal processing
KW  - Radar target recognition
KW  - Radar transmitters
KW  - Textile classing
KW  - Adaptive detection
KW  - Adaptive DSP radar
KW  - Bistatic radars
KW  - Doppler
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Objects detection
KW  - Radar cross-sections
KW  - Stealth object detection
KW  - Targets tracking
KW  - Doppler radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Samarin, A.
AU  - Savelev, A.
AU  - Toropov, A.
AU  - Dzestelova, A.
AU  - Motyko, A.
AU  - Kotenko, E.
AU  - Mikhailova, E.
AU  - Malykh, V.
TI  - Modified Attention Block for Detecting Cars at a Great Distance
PY  - 2025
T2  - Studies in Computational Intelligence
DO  - 10.1007/978-3-031-80463-2_13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000283930&doi=10.1007%2f978-3-031-80463-2_13&partnerID=40&md5=fa60f7f814df2ea4c8bb06cf267be89c
AB  - This study aims to enhance the detection of small objects within intricate visual environments by harnessing the latest deep-learning advancements. We propose a novel method that features a dual-stream self-attention mechanism integrated within a multi-head framework, along with an innovative output reweighting technique to further refine detection accuracy. The core of our approach is specifically designed to address the difficulties posed by small objects that often overlap multiple tokens in feature maps, a common limitation of traditional detection models. By dynamically adjusting the attention scale across various heads, our method facilitates detailed feature capture at multiple levels of granularity, significantly improving the model’s capability to detect and describe small objects. Additionally, we introduce a softmax-based reweighting function that selectively emphasizes crucial features for object recognition, reducing noise and irrelevant information. Our model, named SSD-MSDSSA-ORT, not only exceeds the accuracy of existing state-of-the-art solutions but also showcases superior processing efficiency and scalability. These contributions advance the theoretical understanding of attention mechanisms in deep neural networks and offer practical enhancements for real-world applications requiring precise small object detection. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.
KW  - Attention block
KW  - Car Detection
KW  - Small Object Recognition
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Hou, X.
TI  - Global induced local network for infrared: dim small target detection
PY  - 2025
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/ad86da
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215119552&doi=10.1088%2f1361-6501%2fad86da&partnerID=40&md5=05ce6cc1abb5aa5da63628c461d5c075
AB  - It is challenging to detect infrared dim targets submerged in complicated backgrounds due to their small size and faint intensity. The previous attention-based detection networks frequently require global long-range dependence. Significant calculations are required to determine the target’s sparse but meaningful position. To prevent wasting calculations on the background, this paper offers a detection network guided by global context for local feature learning, named global induced local network (GILNet). It designs a global location module (GLM) and a local feature interaction module (LFIM) to capture the global position and features of targets, respectively. More specifically, using global context interaction, the GLM finds the region that might contain dim small targets, that is, the coarse location. In the coarsely located regions, the LFIM further acquires feature information about targets. Next, we also design an eight-directional attention operation to obtain the contour information of targets in the low feature map. It is fused with the high feature map in the multi-directional feature fusion module, which retains more semantic and spatial information about targets. Finally, quantitative and qualitative analysis show that the GILNet performs better than eight comparison methods on two public datasets. © 2024 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.
KW  - context guidance
KW  - eight-directional attention
KW  - feature interaction
KW  - global to local
KW  - infrared dim small target detection
KW  - Infrared devices
KW  - Text mining
KW  - Context guidance
KW  - Detection networks
KW  - Dim small target detection
KW  - Eight-directional attention
KW  - Feature interactions
KW  - Global to local
KW  - Infrared dim small target detection
KW  - Infrared dim small targets
KW  - Local feature
KW  - Local networks
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ying, X.
AU  - Xiao, C.
AU  - An, W.
AU  - Li, R.
AU  - He, X.
AU  - Li, B.
AU  - Cao, X.
AU  - Li, Z.
AU  - Wang, Y.
AU  - Hu, M.
AU  - Xu, Q.
AU  - Lin, Z.
AU  - Li, M.
AU  - Zhou, S.
AU  - Liu, L.
AU  - Sheng, W.
TI  - Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines
PY  - 2025
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
DO  - 10.1109/TPAMI.2025.3544621
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000462112&doi=10.1109%2fTPAMI.2025.3544621&partnerID=40&md5=9b41a7fa1c2f5c693d9864871f40c668
AB  - Visible-thermal small object detection (RGBT SOD) is a significant yet challenging task with a wide range of applications, including video surveillance, traffic monitoring, search and rescue. However, existing studies mainly focus on either visible or thermal modality, while RGBT SOD is rarely explored. Although some RGBT datasets have been developed, the insufficient quantity, limited diversity, unitary application, misaligned images and large target size cannot provide an impartial benchmark to evaluate RGBT SOD algorithms. In this paper, we build the first large-scale benchmark with high diversity for RGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93 K frames and 1.2 M manual annotations. RGBT-Tiny contains abundant objects (7 categories) and high-diversity scenes (8 types that cover different illumination and density variations). Note that, over 81% of objects are smaller than 16×16, and we provide paired bounding box annotations with tracking ID to offer an extremely challenging benchmark with wide-range applications, such as RGBT image fusion, object detection and tracking. In addition, we propose a scale adaptive fitness (SAFit) measure that exhibits high robustness on both small and large objects. The proposed SAFit can provide reasonable performance evaluation and promote detection performance. Based on the proposed RGBT-Tiny dataset, extensive evaluations have been conducted with IoU and SAFit metrics, including 30 recent state-of-the-art algorithms that cover four different types (i.e., visible generic object detection, visible SOD, thermal SOD and RGBT object detection). © 1979-2012 IEEE.
KW  - benchmark dataset
KW  - tiny object detection
KW  - Visible-thermal
KW  - Image annotation
KW  - Object detection
KW  - Object recognition
KW  - RGB color model
KW  - Adaptive fitness
KW  - Benchmark datasets
KW  - Objects detection
KW  - Search and rescue
KW  - Small object detection
KW  - Thermal
KW  - Tiny object detection
KW  - Traffic monitoring
KW  - Video surveillance
KW  - Visible-thermal
KW  - algorithm
KW  - article
KW  - benchmarking
KW  - controlled study
KW  - diagnosis
KW  - human
KW  - illumination
KW  - male
KW  - video surveillance
KW  - Image fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shi, D.
AU  - Zhao, C.
AU  - Shao, J.
AU  - Feng, M.
AU  - Luo, L.
AU  - Ouyang, B.
AU  - Huang, J.
TI  - Context-Aware Enhanced Feature Refinement for small object detection with Deformable DETR
PY  - 2025
T2  - Frontiers in Neurorobotics
DO  - 10.3389/fnbot.2025.1588565
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008868278&doi=10.3389%2ffnbot.2025.1588565&partnerID=40&md5=1c83f2b9db8174a57729dabf540ce61a
AB  - Small object detection is a critical task in applications like autonomous driving and ship black smoke detection. While Deformable DETR has advanced small object detection, it faces limitations due to its reliance on CNNs for feature extraction, which restricts global context understanding and results in suboptimal feature representation. Additionally, it struggles with detecting small objects that occupy only a few pixels due to significant size disparities. To overcome these challenges, we propose the Context-Aware Enhanced Feature Refinement Deformable DETR, an improved Deformable DETR network. Our approach introduces Mask Attention in the backbone to improve feature extraction while effectively suppressing irrelevant background information. Furthermore, we propose a Context-Aware Enhanced Feature Refinement Encoder to address the issue of small objects with limited pixel representation. Experimental results demonstrate that our method outperforms the baseline, achieving a 2.1% improvement in mAP. Copyright © 2025 Shi, Zhao, Shao, Feng, Luo, Ouyang and Huang.
KW  - autonomouts driving
KW  - Deformable DETR
KW  - feature extraction
KW  - mask attention
KW  - small object detection
KW  - Computer vision
KW  - Extraction
KW  - Image coding
KW  - Object detection
KW  - Object recognition
KW  - Pixels
KW  - Smoke
KW  - Autonomous driving
KW  - Autonomouts driving
KW  - Context-Aware
KW  - Critical tasks
KW  - Deformable DETR
KW  - Feature refinement
KW  - Features extraction
KW  - Mask attention
KW  - Small object detection
KW  - Small objects
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - nonhuman
KW  - ship
KW  - Copyrights
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xu, Y.
AU  - Lu, J.
AU  - Wang, C.
TI  - YOLO-SOD: Improved YOLO Small Object Detection
PY  - 2025
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 
DO  - 10.1007/978-981-96-0125-7_14
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210317772&doi=10.1007%2f978-981-96-0125-7_14&partnerID=40&md5=e981465e19269459be97e102d99236ad
AB  - Small object detection has important application value in the fields of environmental monitoring, resource detection and analysis, etc. However, the current general object detectors are not very ideal for the detection of small objects. To this end, this paper proposes an efficient, low-complexity, anchor-free small object detection framework YOLO-SOD based on YOLOv8. First, a content-aware feature recombination upsampling operator (CARAFE) is integrated in the upsampling operation part of the framework to achieve more accurate and efficient feature reconstruction. Then, the spatial and channel reconstruction convolutional block (SCConv) is widely integrated in BackBone and Neck to reduce the computational cost caused by redundant feature extraction in visual tasks. Finally, the occlusion-aware attention module (SEAM) is introduced at the end of the detection framework to help the model more accurately identify occluded objects. The ablation experiment on the general small object detection dataset VisDrone2021 proves the effectiveness of several modules introduced in this paper for small object detection. On the VisDrone2021 dataset, YOLO-SOD can achieve an accuracy of 31.1% AP50:95 and 52.3% AP50, which are 2.5% and 3.1% higher than the baseline model respectively. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.
KW  - Attention Mechanism
KW  - CNN
KW  - Object Detection
KW  - Small Object Detection
KW  - Upsample
KW  - 'current
KW  - Attention mechanisms
KW  - Detection framework
KW  - Environmental Monitoring
KW  - Object detectors
KW  - Objects detection
KW  - Small object detection
KW  - Small objects
KW  - Upsample
KW  - Upsampling
KW  - Image reconstruction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Singhal, N.
AU  - Prasad, L.
TI  - MIRYO: A Hybrid Model for Detecting Vehicles in Noisy Images
PY  - 2025
T2  - Journal of Engineering Science and Technology Review
DO  - 10.25103/jestr.182.16
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007309535&doi=10.25103%2fjestr.182.16&partnerID=40&md5=90a27611271ab4dbb8803eaac983db96
AB  - Detection of vehicles is a key task in many smart transportation applications, involving traffic management, road infrastructure, autonomous driving, and other challenges that arise due to the daily growth in vehicle numbers. Several deep learning (DL) based techniques have previously been explored and investigated by the researchers for vehicle detection, but vehicle detection in noisy images is still considered a difficult task. Low-light, low-resolution, and other environmental noise have a substantial impact on images, significantly reducing vehicle detection system performance. In this paper, we implemented MIRYO, a hybrid model for detection of vehicles in challenging images based on MirNet-v2 and modified Yolov3. The task of vehicle detection was completed by creating a two-stage pipeline. MIRNet-v2 was used in the first stage of this pipeline to reduce noise and improve image contrast in low-quality challenging images. The second stage of this pipeline, used modified YOLOv3 to detect and localize vehicles in images. The hybrid model MIRYO is evaluated on two baseline datasets: the challenging MIOTCD and the high-resolution Highway dataset, and its performance is compared to that of the Yolov3, Yolov4, and Yolov5 architectures on the same dataset. MIRYO achieved an overall mAP of 76.9% on the MIOTCD dataset, while YOLOv3, YOLOv4, and YOLOv5 achieved 75.1%, 74%, and 75%, respectively, and a mAP of 94.8% on the Highway dataset, while YOLOv3, YOLOv4, and YOLOv5 achieved 94.5%, 94.9%, and 94.8%, respectively. © 2025 School of Science, DUTH. All rights reserved.
KW  - deep learning
KW  - MIRNet
KW  - MIRYO
KW  - Vehicle detection
KW  - YOLO
KW  - Advanced public transportation systems
KW  - Advanced traffic management systems
KW  - Advanced traveler information systems
KW  - Autonomous vehicles
KW  - Highway traffic control
KW  - Railroad traffic control
KW  - Street traffic control
KW  - Vehicle performance
KW  - Autonomous driving
KW  - Deep learning
KW  - Hybrid model
KW  - MIRNet
KW  - MIRYO
KW  - Noisy image
KW  - Road infrastructures
KW  - Traffic management
KW  - Vehicles detection
KW  - YOLO
KW  - Highway administration
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, H.
AU  - Xie, J.
AU  - Wang, Y.
AU  - Fu, L.
AU  - Zhou, Y.
AU  - Zhou, X.
TI  - Optimized Data Distribution Learning for Enhancing Vision Transformer-Based Object Detection in Remote Sensing Images
PY  - 2025
T2  - Photogrammetric Record
DO  - 10.1111/phor.70004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000184533&doi=10.1111%2fphor.70004&partnerID=40&md5=6d4092328f40a2ca9da2da15e68aa021
AB  - Existing Vision Transformer (ViT)-based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the over-reliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster R-CNN architecture, named ODDL-Net. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plug-and-play components—namely, the OA, OMA, and FFR modules—ensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDL-Net, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLH-Bridge datasets demonstrate that both variants of ODDL-Net achieve impressive accuracy, surpassing 23 state-of-the-art methods introduced since 2023. Specifically, ODDL-Net-NViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLH-Bridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster R-CNN-ResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for high-accuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViT-based detectors, offering a practical approach that requires little model modification. © 2025 Remote Sensing and Photogrammetry Society and John Wiley & Sons Ltd.
KW  - deep learning
KW  - object detection
KW  - ODDL-net
KW  - optimized data distribution learning
KW  - remote sensing images
KW  - Distribution transformers
KW  - Image coding
KW  - Image enhancement
KW  - Network security
KW  - Remote sensing
KW  - Augmentation techniques
KW  - Data distribution
KW  - Deep learning
KW  - Detection framework
KW  - Learning strategy
KW  - Objects detection
KW  - Optimized data distribution learning
KW  - Optimized data distribution learning-net
KW  - Remote sensing images
KW  - algorithm
KW  - data set
KW  - image processing
KW  - machine learning
KW  - optimization
KW  - remote sensing
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, X.
AU  - Du, X.
AU  - Ma, C.
AU  - Hu, Z.
AU  - Yang, W.
AU  - Zheng, B.
TI  - Atmospheric neutron single event effects for multiple convolutional neural networks based on 28-nm and 16-nm SoC
PY  - 2025
T2  - Chinese Physics B
DO  - 10.1088/1674-1056/ad8b38
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215254282&doi=10.1088%2f1674-1056%2fad8b38&partnerID=40&md5=77c95d5e0f4959cdeaa9985a5d6542aa
AB  - The single event effects (SEEs) evaluations caused by atmospheric neutrons were conducted on three different convolutional neural network (CNN) models (Yolov3, MNIST, and ResNet50) in the atmospheric neutron irradiation spectrometer (ANIS) at the China Spallation Neutron Source (CSNS). The Yolov3 and MNIST models were implemented on the XILINX 28-nm system-on-chip (SoC). Meanwhile, the Yolov3 and ResNet50 models were deployed on the XILINX 16-nm FinFET UltraScale+MPSoC. The atmospheric neutron SEEs on the tested CNN systems were comprehensively evaluated from six aspects, including chip type, network architecture, deployment methods, inference time, datasets, and the position of the anchor boxes. The various types of SEE soft errors, SEE cross-sections, and their distribution were analyzed to explore the radiation sensitivities and rules of 28-nm and 16-nm SoC. The current research can provide the technology support of radiation-resistant design of CNN system for developing and applying high-reliability, long-lifespan domestic artificial intelligence chips. © 2025 Chinese Physical Society and IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.
KW  - atmospheric neutron
KW  - convolutional neural network
KW  - single event effects
KW  - system on chip
KW  - FinFET
KW  - Forward error correction
KW  - Integrated circuit design
KW  - Network-on-chip
KW  - Neutron irradiation apparatus
KW  - Neutron sources
KW  - Neutron spectrometers
KW  - Neutrons
KW  - Radiation hardening
KW  - Atmospheric neutrons
KW  - Convolutional neural network
KW  - Effect evaluation
KW  - FinFETs
KW  - Network-based
KW  - Neural network model
KW  - Neural network systems
KW  - Single event effects
KW  - Spallation neutron sources
KW  - Systems-on-Chip
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Guan, R.
AU  - Jia, L.
AU  - Yao, S.
AU  - Yang, F.
AU  - Xu, S.
AU  - Purwanto, E.
AU  - Zhu, X.
AU  - Man, K.L.
AU  - Lim, E.G.
AU  - Smith, J.
AU  - Hu, X.
AU  - Yue, Y.
TI  - WaterVG: Waterway Visual Grounding Based on Text-Guided Vision and mmWave Radar
PY  - 2025
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2025.3527011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217017429&doi=10.1109%2fTITS.2025.3527011&partnerID=40&md5=88ccf83b94c2070733a23c4baf8d6296
AB  - Waterway perception is critical for the special operations and autonomous navigation of Unmanned Surface Vessels (USVs), but current perception schemes are sensor-based, neglecting the interaction between humans and USVs for embodied perception in various operations. Therefore, inspired by visual grounding, we present WaterVG, the inaugural visual grounding dataset tailored for USV-based waterway perception guided by human prompts. WaterVG contains a wealth of prompts describing multiple targets, with instance-level annotations, including bounding boxes and masks. Specifically, WaterVG comprises 11,568 samples and 34,987 referred targets, integrating both visual and radar characteristics. The text-guided two-sensor pattern provides a fine granularity of text prompts aligned with the visual and radar features of the referent targets, containing both qualitative and numeric descriptions. To enhance the endurance and maintain the normal operations of USVs in open waterways, we propose Potamoi, a low-power visual grounding model. Potamoi is a multi-task model employing a sophisticated Phased Heterogeneous Modality Fusion (PHMF) mechanism, which includes Adaptive Radar Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). The ARW module utilizes a gating mechanism to adaptively extract essential radar features for fusion with visual inputs, ensuring prompt alignment. MHSCA, characterized by its low parameter count and computational efficiency (FLOPs), effectively integrates contextual information from both sensors with linguistic features, delivering outstanding performance in visual grounding tasks. Comprehensive experiments and evaluations on WaterVG demonstrate that Potamoi achieves state-of-the-art results compared to existing methods. © 2000-2011 IEEE.
KW  - interactive perception
KW  - multi-modal learning
KW  - perception of unmanned surface vessels
KW  - Visual grounding
KW  - Air navigation
KW  - Clutter (information theory)
KW  - Computer vision
KW  - Adaptive radar
KW  - Autonomous navigation
KW  - Current perception
KW  - Interactive perception
KW  - Mm waves
KW  - Multi-modal learning
KW  - Perception of unmanned surface vessel
KW  - Special operations
KW  - Unmanned surface vessels
KW  - Visual grounding
KW  - Radar target recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Zhai, Y.
AU  - Zhu, L.
AU  - Xu, L.
AU  - Zhao, Y.
AU  - Yuan, H.
TI  - Sheep-YOLO: a lightweight daily behavior identification and counting method for housed sheep
PY  - 2025
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/ad9f8d
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215391396&doi=10.1088%2f1361-6501%2fad9f8d&partnerID=40&md5=7e0500475e2d41ae8ed97fbecc097bff
AB  - Daily behavior detection and monitoring of sheep is crucial for assessing their health status. In recent years, computer vision has been widely used in livestock behavior detection, but it usually requires large memory and computational resources. In addition, most studies have focused only on the behavior of sheep during the day, while the behavior of sheep during the night is equally important for a comprehensive understanding of their health status and well-being. Therefore, in this study, we developed a lightweight daily behavior detection and counting method for housed sheep to detect lying, feeding, and standing behaviors, and to count the number of each behavior as well as the total number of sheep. First, we propose a new PCBAM module and incorporate it into the neck part of YOLOv8n to enhance the feature information contained in the feature map, second, we use the slim neck design paradigm incorporating GSConv to lighten and improve the model operation efficiency, and finally, we reconstruct the detection head to eliminate the redundant small target detection head, reduce the model computational burden, and improve the detection performance of medium and large targets. The Sheep-YOLO model is validated using the daily behavioral dataset of housed sheep, and the experimental results show that the improved model is effective in detecting sheep behavior in complex environments, and the mAP@0.5 is improved by 5.4% compared to the baseline model, and in particular, the lying and feeding behaviors of sheep are improved by 7.2% and 8.8%, respectively. Comparative experiments with other mainstream target detection algorithms validate the advantages of our proposed model for sheep behavior detection. This study provides an effective solution for behavioral detection and counting of housed sheep. © 2024 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.
KW  - attentional mechanisms
KW  - daily behavior
KW  - object detection
KW  - sheep behavior
KW  - smart farming
KW  - Attentional mechanism
KW  - Behavior detection
KW  - Behavior identifications
KW  - Behaviour monitoring
KW  - Daily behaviors
KW  - Health status
KW  - Memory resources
KW  - Objects detection
KW  - Sheep behaviour
KW  - Smart farming
KW  - Livestock
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Naga Jyothi Vandavasi, B.
AU  - Shakeera, S.
AU  - Narayanaswamy, V.
AU  - Ramadass Gidugu, A.
AU  - Venkataraman, H.
TI  - EM and Vision-Aided Multisensor Fusion Homing Guidance System (MSF-HGS) for Intelligent AUVs
PY  - 2025
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3494041
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210391749&doi=10.1109%2fJSEN.2024.3494041&partnerID=40&md5=141236eba69fceba82306b3656c77ac1
AB  - Autonomous underwater vehicle (AUV) is an efficient technological tool for ocean exploration in the mid-21st century. Submerged docking stations (DSs) serve as charging stations to enhance vehicle endurance and enable long-range operations. In this regard, precise homing guidance techniques are essential for navigating toward homing and DS in unpredictable ocean environments. This article proposes an unsupervised multisensor fusion homing guidance system (MSF-HGS) based on Kalman filter (KF), extended KF (EKF), and advanced filter-like cubature KF (CKF) techniques. The system uses data from K-nearest neighbors (KNNs) techniques for electromagnetic (EM) field and deep learning (DL)-based underwater images. The novelty of this work lies in the fusion of different sampling frequency data (EM+ vision) in time series. Performance was validated by integrating multirate sensor data through field experiments, with EM field measurements at 2 Hz and optical images at ten frames per second collected onboard an AUV, referenced to the DS. Real-time measurements and computed AUV kinematics are fused to determine the optimal guidance for intelligent homing of AUV toward the DS. The performance of unsupervised sensor fusion of KF and EKF is demonstrated in real-time in different water bodies. The DL system is resistant to stray magnetic fields and turbid water conditions within range of 7 m and terminal homing precision within ±0.2 m from the center of dock. The success probability is 99.65% using the proposed mechanism, while it is 90% with conventional KF methods, 95% with EKF technique, and 96% with CKF technique. © 2001-2012 IEEE.
KW  - Autonomous underwater vehicle (AUV)
KW  - deep learning (DL)
KW  - docking
KW  - electromagnetic (EM)
KW  - homing
KW  - sensor fusion
KW  - vision
KW  - Autonomous underwater vehicles
KW  - Image coding
KW  - Image enhancement
KW  - Image sampling
KW  - Magnetic levitation
KW  - Magnetic levitation vehicles
KW  - Photomapping
KW  - Sensor data fusion
KW  - Underwater imaging
KW  - Underwater photography
KW  - Autonomous underwater vehicles]
KW  - Deep learning
KW  - Docking
KW  - Docking station
KW  - Electromagnetics
KW  - Homing
KW  - Homing guidance
KW  - Kalman filter technique
KW  - Multi-sensor fusion
KW  - Sensor fusion
KW  - Extended Kalman filters
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Trigka, M.
AU  - Dritsas, E.
TI  - A Comprehensive Survey of Machine Learning Techniques and Models for Object Detection
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25010214
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214516774&doi=10.3390%2fs25010214&partnerID=40&md5=9432d943f719252fdeeadcd94ea2123d
AB  - Object detection is a pivotal research domain within computer vision, with applications spanning from autonomous vehicles to medical diagnostics. This comprehensive survey presents an in-depth analysis of the evolution and significant advancements in object detection, emphasizing the critical role of machine learning (ML) and deep learning (DL) techniques. We explore a wide spectrum of methodologies, ranging from traditional approaches to the latest DL models, thoroughly evaluating their performance, strengths, and limitations. Additionally, the survey delves into various metrics for assessing model effectiveness, including precision, recall, and intersection over union (IoU), while addressing ongoing challenges in the field, such as managing occlusions, varying object scales, and improving real-time processing capabilities. Furthermore, we critically examine recent breakthroughs, including advanced architectures like Transformers, and discuss challenges and future research directions aimed at overcoming existing barriers. By synthesizing current advancements, this survey provides valuable insights for enhancing the robustness, accuracy, and efficiency of object detection systems across diverse and challenging applications. © 2025 by the authors.
KW  - deep learning
KW  - machine learning
KW  - models
KW  - object detection
KW  - techniques
KW  - Contrastive Learning
KW  - Deep learning
KW  - Deep reinforcement learning
KW  - Federated learning
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - In-depth analysis
KW  - Machine learning models
KW  - Machine learning techniques
KW  - Machine-learning
KW  - Medical diagnostics
KW  - Objects detection
KW  - Research domains
KW  - Technique
KW  - autonomous vehicle
KW  - benchmarking
KW  - computer vision
KW  - deep learning
KW  - diagnosis
KW  - human
KW  - machine learning
KW  - review
KW  - Adversarial machine learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, C.
AU  - Wu, Z.
AU  - Liang, Y.
TI  - Traffic sign installation angle measurement method based on the You Only Look Once algorithm and binocular stereo vision
PY  - 2025
T2  - Journal of Electronic Imaging
DO  - 10.1117/1.JEI.34.2.023014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005416205&doi=10.1117%2f1.JEI.34.2.023014&partnerID=40&md5=ff1d40ff795fff28a2cc545c8e52032f
AB  - We propose a framework for traffic sign installation angle measurement. The framework integrates an enhanced You Only Look Once-Lite (YOLO-Lite) detection model with binocular stereo vision. The enhanced YOLO-Lite detection model incorporates a lightweight attention convolution (LAC) module and adaptive downsampling mechanism (ADM). Both the LAC module and ADM contribute to improving the accuracy of traffic sign detection. The LAC module enhances traffic sign detection accuracy by integrating spatial and channel attention mechanisms, enabling the extraction of discriminative features from small targets in complex backgrounds. Also, the LAC module reduces the number of training parameters through depthwise separable convolutions, contributing to improved computational efficiency without compromising detection accuracy. Simultaneously, the ADM module mitigates information loss during multi-scale processing through adaptive downsampling strategies that preserve critical features. An Iterative Geometry Encoding Volume with MobileNetV2 enhances performance through MobileNetV2-based lightweight encoding and single-iteration convolutional gated recurrent unit optimization. The model maintains the consistency of contour matching and multi-scale depth estimation while reducing the computational burden. The parameters and inference time of the overall framework are only respectively 7.8 M and 1.37 s, with an angle measurement error of 4.80%. This meets the accuracy and real-time requirements for traffic sign installation angle measurement implemented on embedded devices. The proposed framework provides an efficient, cost-effective solution for the standardized installation and maintenance of road traffic infrastructure.  © 2025 SPIE and IS&T.
KW  - angular measurement
KW  - binocular stereo vision
KW  - embedded devices
KW  - Iterative Geometry Encoding Volume with MobileNetV2
KW  - you only look once-lite
KW  - Binocular-stereo visions
KW  - Detection accuracy
KW  - Detection models
KW  - Down sampling
KW  - Embedded device
KW  - Encodings
KW  - Installation angle
KW  - Iterative geometry encoding volume with mobilenetv2
KW  - Traffic sign detection
KW  - You only look once-lite
KW  - Angle measurement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tian, D.
AU  - Li, J.
AU  - Lei, J.
TI  - Multi-sensor information fusion in Internet of Vehicles based on deep learning: A review
PY  - 2025
T2  - Neurocomputing
DO  - 10.1016/j.neucom.2024.128886
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208768433&doi=10.1016%2fj.neucom.2024.128886&partnerID=40&md5=5db0d7f8ff949124d2de66c90a95c8b1
AB  - Environmental perception is a crucial component of intelligent driving technology, providing the informational foundation for intelligent decision-making and collaborative control. Due to the limitations of single sensors and the continuous advancements in deep learning and sensor technologies, multi-sensor information fusion in the Internet of Vehicles (IoV) has emerged as a major research hotspot. This approach is also a primary solution for achieving full self-driving. However, given the complexity of the technology, there are still many challenges in achieving accurate and reliable real-time multi-source information perception. Current discussions often focus on specific aspects of multi-sensor fusion in intelligent driving, while detailed discussions on sensor fusion in the context of the IoV are relatively scarce. To provide a comprehensive discussion and analysis of multi-sensor information fusion in IoV, this paper first provides a detailed introduction to its developmental background and the commonly involved sensors. Subsequently, a detailed analysis of the strategies, deep learning architectures, and methods for multi-sensor information fusion in the IoV is presented. Finally, the specific applications and key issues related to multi-sensor information fusion in IoV are discussed from multiple perspectives, along with an analysis of future development trends. This paper aims to serve as a valuable reference for advancing multi-sensor information fusion technology in IoV environments and supporting the realization of full self-driving. © 2024 Elsevier B.V.
KW  - Deep learning
KW  - Intelligent driving
KW  - Internet of Vehicles
KW  - Multi-sensor information fusion
KW  - Collaborative control
KW  - Deep learning
KW  - Environmental perceptions
KW  - Intelligent decision-making
KW  - Intelligent driving
KW  - Internet of vehicle
KW  - Learning technology
KW  - Multi-sensor information fusion
KW  - Self drivings
KW  - Single sensor
KW  - deep learning
KW  - human
KW  - internet of things
KW  - sensor
KW  - short survey
KW  - surgical technology
KW  - Information fusion
M3  - Short survey
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Patil, A.N.
AU  - Mohanty, S.N.
AU  - Khan, T.
TI  - SkyViewSentinel: A Deep Learning-Driven Military Object Detection Application for Remote-Sensing Satellite Images
PY  - 2025
T2  - Adolescent Psychiatry
DO  - 10.2174/0126662558333174241009112642
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005081767&doi=10.2174%2f0126662558333174241009112642&partnerID=40&md5=8299e6545d63501de78582987c061909
AB  - Background: In today’s ever-changing world, military forces face significant challenges in maintaining situational awareness and responding swiftly to emerging threats. Traditional aerial surveillance often fails to give timely and thorough intelligence over large areas. Limited coverage, mistakes, and difficulty noticing small changes on the ground hinder military operations. To address these problems, this paper introduces the development of a deep learning-based web application named “SkyViewSentinel”, a solution tailored specifically for military aerial surveillance. Methods: The application framework i.e., SkyViewSentinel has been developed through multiple stages i.e., (i) pre-process the Xview overhead Satellite imagery dataset using Ground Truth refinement and image partitioning method, (ii) employed a SOTA deep model i.e., YOLOv8 as a baseline architecture for the research problem and assessed the performance on experimental dataset, (iii) a series of rigorous experiments have been conducted using deep model and obtained results are reported. (iv) Finally, the trained model has been seamlessly integrated into the web application and develops a comprehensive web-based object detection application. The developed application detects military-based objects from real-time satellite images. Results: The developed application has shown promising results in identifying military objects from satellite images, outperforming other contemporary methods. The designed framework has achieved an overall mAP score of 0.315 for all nine classes of military-based objects. For certain specific classes, detection accuracy exceeds 70%, demonstrating the robustness of the framework. Conclusion: The designed web application enables users to detect military-based objects in the region provided by the user. By harnessing the power of satellite object recognition technology, SkyViewSentinel provides a new way to monitor and understand activities in operational areas. © 2024 Bentham Science Publishers.
KW  - Military objects
KW  - Object detection
KW  - Satellite imagery
KW  - SkyViewSentinel
KW  - XView dataset
KW  - YOLOv8
KW  - army
KW  - article
KW  - awareness
KW  - controlled study
KW  - deep learning
KW  - human
KW  - intelligence
KW  - remote sensing
KW  - satellite imagery
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, M.
AU  - Wang, H.
AU  - Li, Y.
AU  - Chen, L.
AU  - Cai, Y.
AU  - Shao, Z.
TI  - MSAFusion: Object Detection Based on Multisensor Adaptive Fusion Under BEV
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3548792
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001539723&doi=10.1109%2fTIM.2025.3548792&partnerID=40&md5=70a3c3096089884aadb805a46a4baa1d
AB  - Object detection is a critical component of autonomous driving perception. To achieve comprehensive environmental perception, mainstream methods commonly rely on multimodal sensor fusion. However, existing solutions often face challenges such as low sensor utilization and suboptimal fusion strategies. To address these issues, this article proposes MSAFusion, a multisensor adaptive fusion framework based on a bird’s eye view (BEV). In our framework, we extract multiview features using Vision Mamba (Vim), generate BEV queries through positional encoding for preliminary fusion with multimodal features, and employ a deep Q-network (DQN) for adaptive fusion based on feature consistency and continuity. This approach enables efficient utilization of multimodal sensors and optimal fusion across diverse environments. Extensive experiments on the nuScenes and Radiate datasets demonstrate that MSAFusion achieves state-of-the-art performance, delivering superior panoramic environmental perception, improved object detection accuracy, and enhanced flexibility compared to existing multisensor fusion methods. © 1963-2012 IEEE.
KW  - Autonomous driving
KW  - environment perception
KW  - multisensor adaptive fusion
KW  - Vision Mamba (Vim)
KW  - Object recognition
KW  - Signal encoding
KW  - Adaptive fusion
KW  - Autonomous driving
KW  - Environment perceptions
KW  - Environmental perceptions
KW  - Multi sensor
KW  - Multi-sensor adaptive fusion
KW  - Multimodal sensor
KW  - Objects detection
KW  - Sensor fusion
KW  - Vision mamba
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Liang, H.
AU  - Yang, Y.
AU  - Hu, J.
AU  - Yang, J.
AU  - Liu, F.
AU  - Yuan, S.
TI  - Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds
PY  - 2025
T2  - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings
DO  - 10.1109/ICASSP49660.2025.10890359
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009588079&doi=10.1109%2fICASSP49660.2025.10890359&partnerID=40&md5=932f3c099d5e52d9b5c716dd352f8e97
AB  - Compact UAV systems, while advancing delivery and surveillance, pose significant security challenges due to their small size, which hinders detection by traditional methods. This paper presents a cost-effective, unsupervised UAV detection method using spatial-temporal sequence processing to fuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios. Our approach segments point clouds into foreground and background, analyzes spatial-temporal data, and employs a scoring mechanism to enhance detection accuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024 UG2+ Challenge, demonstrating its practical effectiveness. We plan to open-source all designs, code and sample data for the research community @ github.com/lianghanfang/UnLiDAR-UAV-Est. © 2025 IEEE.
KW  - Point Clouds
KW  - Trajectory Estimation
KW  - UAV detection
KW  - Unsupervised
KW  - Computer vision
KW  - Cost effectiveness
KW  - Data accuracy
KW  - Open systems
KW  - Remote sensing
KW  - Robotics
KW  - Unmanned aerial vehicles (UAV)
KW  - 3-D trajectory
KW  - Cost effective
KW  - Detection methods
KW  - Point-clouds
KW  - Security challenges
KW  - Sparse point cloud
KW  - Trajectory estimation
KW  - UAV detection
KW  - UAV systems
KW  - Unsupervised
KW  - Aircraft detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Satama-Bermeo, G.
AU  - Lopez-Guede, J.M.
AU  - Rahebi, J.
AU  - Teso-Fz-Betoño, D.
AU  - Boyano, A.
AU  - Akizu-Gardoki, O.
TI  - PRISMA Review: Drones and AI in Inventory Creation of Signage
PY  - 2025
T2  - Drones
DO  - 10.3390/drones9030221
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001276737&doi=10.3390%2fdrones9030221&partnerID=40&md5=d8465d25a127e4430c6bc574ca30976f
AB  - This systematic review explores the integration of unmanned aerial vehicles (UAVs) and artificial intelligence (AI) in automating road signage inventory creation, employing the preferred reporting items for systematic reviews and meta-analyses (PRISMA) methodology to analyze recent advancements. The study evaluates cutting-edge technologies, including UAVs equipped with deep learning algorithms and advanced sensors like light detection and ranging (LiDAR) and multispectral cameras, highlighting their roles in enhancing traffic sign detection and classification. Key challenges include detecting minor or partially obscured signs and adapting to diverse environmental conditions. The findings reveal significant progress in automation, with notable improvements in accuracy, efficiency, and real-time processing capabilities. However, limitations such as computational demands and environmental variability persist. By providing a comprehensive synthesis of current methodologies and performance metrics, this review establishes a robust foundation for future research to advance automated road infrastructure management to improve safety and operational efficiency in urban and rural settings. © 2025 by the authors.
KW  - AI
KW  - data fusion
KW  - LiDAR
KW  - R-CNN
KW  - UAVs
KW  - Yolo
KW  - Deep learning
KW  - Drones
KW  - Highway administration
KW  - Road and street markings
KW  - Sensor data fusion
KW  - Advanced sensors
KW  - Aerial vehicle
KW  - Cutting edge technology
KW  - Light detection and ranging
KW  - Meta-analysis
KW  - R-CNN
KW  - Road signage
KW  - Systematic Review
KW  - Unmanned aerial vehicle
KW  - Yolo
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Feng, Y.
AU  - Wang, N.
AU  - Lu, G.
AU  - Mei, S.
TI  - Aerial Person Detection for Search and Rescue: Survey and Benchmarks
PY  - 2025
T2  - Journal of Remote Sensing (United States)
DO  - 10.34133/remotesensing.0474
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002996073&doi=10.34133%2fremotesensing.0474&partnerID=40&md5=37dc3264acc3dfedd7d6d1c8d937a9de
AB  - Robust person detection in aerial images under all-weather conditions stands as a fundamental technology pivotal to the efficacy of intelligent search and rescue (SaR) tasks. However, the challenges stem from the varied postures, sparsity, diminutiveness, and faintness of personnel objects when viewed from an air-to-ground perspective, leading to issues with insufficient feature representation and suboptimal detection accuracy. This survey commences by underscoring the extensive potential applications and the prevailing limitations associated with aerial person detection (APD) within the scope of drone-assisted SaR scenarios. To meet the requirement of APD applications, we thoroughly investigate advancements and challenges in 4 related methodologies, including object-aware methods for size and perspective variability, sample-oriented methods with sparse distribution, information-fusion methods for the issue of lighting or visibility, and lightweight methods on constrained devices. Furthermore, to foster advancements in APD, we have conducted a comprehensive APD dataset labeled as “VTSaR”, which stands out from the existing publicly accessible APD datasets by offering a greater diversity of scenes, varying personnel behaviors, flexible capture angles, differing capture heights, and an inclusion of aligned visible and infrared samples along with synthetic samples. Finally, we evaluate the performance of mainstream detection methods on VTSaR benchmarks, advocating for APD’s broader application across various domains. Copyright © 2025, Xiangqing Zhang et al. Exclusive licensee Aerospace Information Research Institute, Chinese Academy of Sciences. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License (CC BY 4.0).
KW  - Aerial photography
KW  - Aircraft detection
KW  - Antenna grounds
KW  - Benchmarking
KW  - Labeled data
KW  - Network security
KW  - Aerial images
KW  - Assisted search
KW  - Condition
KW  - Detection accuracy
KW  - Feature representation
KW  - Intelligent search
KW  - Person detection
KW  - Search and rescue
KW  - Search and rescue tasks
KW  - Suboptimal detection
KW  - Information fusion
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ma, K.
AU  - Guo, B.
AU  - Liu, S.
AU  - Fang, C.
AU  - Luo, S.
AU  - Zheng, Z.
AU  - Yu, Z.
TI  - AdaShift: Anti-Collapse and Real-Time Deep Model Evolution for Mobile Vision Applications
PY  - 2025
T2  - IEEE Transactions on Mobile Computing
DO  - 10.1109/TMC.2025.3572215
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006775330&doi=10.1109%2fTMC.2025.3572215&partnerID=40&md5=644728abd4f6fe2d7b5cce9f80c27c68
AB  - As computational hardware advance, integrating deep learning (DL) models into mobile devices has become ubiquitous for visual tasks. However, "data distribution shift"in live sensory data can lead to a degradation in the accuracy of mobile DL models. Conventional domain adaptation methods, constrained by their dependence on pre-compiled static datasets for offline adaptation, exhibit fundamental limitations in real-time practicality. While modern online adaptation methodologies enable incremental model evolution, they remain plagued by two critical shortcomings: computational latency from excessive resource demands on mobile devices that compromise temporal responsiveness, and accuracy collapse stemming from error accumulation through unreliable pseudo-labeling processes. To address these challenges, we introduce AdaShift, an innovative cloud-assisted framework enabling real-time online model adaptation for vision-based mobile systems operating under non-stationary data distributions. Specifically, to ensure real-time performance, the adaptation trigger and plug-and-play adaptation mechanisms are proposed to minimize redundant adaptation requests and reduce per-request costs. To prevent accuracy collapse, AdaShift introduces a novel anti-collapse parameter restoration mechanism that explicitly recovers knowledge, ensuring stable accuracy improvements during model evolution. Through extensive experiments across various vision tasks and model architectures, AdaShift demonstrates superior accuracy and 100ms-level adaptation latency, achieving an optimal balance between accuracy and real-time performance compared to baselines.  © 2025 IEEE.
KW  - Cloud-assisted online model adaptation
KW  - Real-time adaptation performance
KW  - Resource-constrained mobile devices
KW  - Cloud-assisted online model adaptation
KW  - Learning models
KW  - Model Adaptation
KW  - Model evolution
KW  - On-line modelling
KW  - Performance
KW  - Real- time
KW  - Real-time adaptation
KW  - Real-time adaptation performance
KW  - Resource-constrained mobile device
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Raj, R.
AU  - Kos, A.
TI  - An Extensive Study of Convolutional Neural Networks: Applications in Computer Vision for Improved Robotics Perceptions
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25041033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219211046&doi=10.3390%2fs25041033&partnerID=40&md5=2dffa75404263c2ac1b8e7953c584c7c
AB  - Convolutional neural networks (CNNs), a type of artificial neural network (ANN) in the deep learning (DL) domain, have gained popularity in several computer vision applications and are attracting research in other fields, including robotic perception. CNNs are developed to autonomously and effectively acquire spatial patterns of characteristics using backpropagation, leveraging an array of elements, including convolutional layers, pooling layers, and fully connected layers. Current reviews predominantly emphasize CNNs’ applications in various contexts, neglecting a comprehensive perspective on CNNs and failing to address certain recently presented new ideas, including robotic perception. This review paper presents an overview of the fundamental principles of CNNs and their applications in diverse computer vision tasks for robotic perception while addressing the corresponding challenges and future prospects for the domain of computer vision in improved robotic perception. This paper addresses the history, basic concepts, working principles, applications, and the most important components of CNNs. Understanding the concepts, benefits, and constraints associated with CNNs is crucial for exploiting their possibilities in robotic perception, with the aim of enhancing robotic performance and intelligence. © 2025 by the authors.
KW  - artificial intelligence (AI)
KW  - computer vision
KW  - convolutional neural network (CNN)
KW  - deep learning (DL)
KW  - machine learning (ML)
KW  - mobile robot (MR)
KW  - perception
KW  - Deep neural networks
KW  - Mobile robots
KW  - Artificial intelligence
KW  - Computer vision applications
KW  - Convolutional neural network
KW  - Deep learning
KW  - Machine learning
KW  - Machine-learning
KW  - Mobile robot
KW  - Neural network application
KW  - Neural-networks
KW  - artificial intelligence
KW  - artificial neural network
KW  - back propagation
KW  - computer
KW  - computer vision
KW  - convolutional neural network
KW  - deep learning
KW  - diagnosis
KW  - human
KW  - intelligence
KW  - machine learning
KW  - perception
KW  - review
KW  - robotics
KW  - Convolutional neural networks
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Al Akbar, R.
AU  - Samuel, D.W.
AU  - Adinata, M.F.
TI  - Mobility Aid for the Visually Impaired Using Machine Learning and Spatial Audio
PY  - 2025
T2  - Journal of Robotics and Control (JRC)
DO  - 10.18196/jrc.v6i2.25245
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003201474&doi=10.18196%2fjrc.v6i2.25245&partnerID=40&md5=7b95120e881f6b753a20d90df9f22183
AB  - Assistive technology is crucial in enhancing the quality of life for individuals with disabilities, including the visually impaired. Many mobility aids lack advanced features such as real-time machine learning-based object detection and spatial audio for environmental awareness. This research contributes to developing more intelligent and adaptable assistive technology for visually impaired individuals, promoting improved navigation and environmental awareness. This research presents a head-mounted mobility aid that integrates a time-of-flight camera, a web camera, and a touch sensor with K-Means clustering, Convolutional Neural Networks (CNNs), and concurrent programming on a Raspberry Pi 4B to detect and classify surrounding obstacles and objects. The system converts obstacle data into spatial audio, allowing users to perceive their surroundings through sound direction and intensity. Object recognition is activated via a touch sensor, providing distance and directional information relative to the user using audio description. The concurrent programming implementation improves execution time by 50.22% compared to Infinite Loop Design (ILD), enhancing real-time responsiveness. However, the system has limitations, including object recognition limited to 80 predefined categories, a 4-meter detection range, reduced accuracy under high-intensity sunlight, and potential interference in spatial audio perception due to external noise. Assistive technology to help the mobility of blind people using advanced technology based on machine learning has developed in a form that can be used flexibly for the user's mobility. © 2025 Department of Agribusiness, Universitas Muhammadiyah Yogyakarta. All rights reserved.
KW  - Assistive Technology
KW  - Blind People
KW  - Concurrent Programming
KW  - Image Recognition
KW  - K-Means
KW  - Time-of-Flight Camera
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ding, X.
AU  - Gu, J.
AU  - Geng, J.
AU  - Liu, X.
AU  - Li, H.
AU  - Luo, J.
AU  - Wang, X.
TI  - Feature-enhanced small object detection algorithm for surface defects of cigarette pack
PY  - 2025
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.3069428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005024411&doi=10.1117%2f12.3069428&partnerID=40&md5=a755d148b7e71cbe73236804b10d04fe
AB  - The surface defects of cigarette pack are characterized by varying scales and complex backgrounds. Small target detection algorithms, as a potential solution, have been proven to have the ability to identify such surface defects. However, the existing small target detection algorithms still have many limitations, especially in terms of detection accuracy, model complexity, and computing speed. To address these challenges, we propose a lightweight cigarette pack defect detection algorithm, YOLOv5s-FDE. In the backbone network, we introduce the GSConv structure, which incorporates channel shuffle and depthwise separable convolution to construct the feature enhancement module (FEM) with a cross-stage architecture. It aims to reduce information loss and enhance feature utilization. In the feature fusion stage, we introduce the DySample upsampling module, which effectively preserves target detail features through a dynamic sampling mechanism, and reduces the memory usage. Finally, the E-Triplet cross-dimensional attention module is proposed, which leverages a multi-branch structure to enhance cross-dimensional information interaction and suppress irrelevant background information. Experimental results demonstrate that, compared to the YOLOv5s baseline algorithm, YOLOv5s-FDE achieves an average precision of 95.6%. The number of model parameters and computational cost are reduced by 0.88M and 3.5G, respectively. The proposed method outperforms recent state-of-the-art detection algorithms, whiles providing theoretical support for intelligent defect detection of cigarette pack. © 2025 SPIE.
KW  - Attention mechanism
KW  - Cigarette pack defect detection
KW  - Dynamic upsampling
KW  - Feature enhancement
KW  - YOLOv5s
KW  - Image coding
KW  - Image compression
KW  - Attention mechanisms
KW  - Cigarette pack defect detection
KW  - Cigarette packs
KW  - Defect detection
KW  - Dynamic upsampling
KW  - Feature enhancement
KW  - Small target detection
KW  - Target detection algorithm
KW  - Upsampling
KW  - YOLOv5
KW  - Image segmentation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Partheepan, S.
AU  - Sanati, F.
AU  - Hassan, J.
TI  - Evaluating YOLO Variants With Transfer Learning for Real-Time UAV Obstacle Detection in Simulated Forest Environments
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3577251
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007620958&doi=10.1109%2fACCESS.2025.3577251&partnerID=40&md5=7a3f1c9e92f6827d6a73b7dc8b55161b
AB  - Autonomous navigation in forest environments poses significant challenges for Uncrewed Aerial Vehicles (UAVs), where reliable obstacle detection is essential for safe operation. This study presents a detailed performance evaluation of recent YOLO models (v8–v11) using a realistic simulation environment built on Unreal Engine and AirSim. To the best of our knowledge, this is the first study to comprehensively benchmark YOLOv8–v11 in a UAV-based obstacle detection context, incorporating both onboard deployment factors and the impact of transfer learning (TL) using EfficientNet backbones. We analyze detection accuracy (mAP@50, precision, recall), inference time, FPS, and memory usage across all YOLO configurations, from lightweight (YOLOv8s) to high-performance models (YOLOv11x). TL significantly improved model performance; for instance, YOLOv8s showed a recall increase from 0.6318 to 0.7804 and mAP@50 from 0.7228 to 0.8447. YOLOv8m and YOLOv10b emerged as optimal models, offering the best trade-off between speed and detection accuracy for real-time UAV deployment. Larger models like YOLOv9c and YOLOv10x delivered superior accuracy but required more computational resources. The results offer practical guidance for selecting YOLO architectures in UAV-based obstacle detection tasks, balancing accuracy, speed, and deployment feasibility in complex environments. © 2013 IEEE.
KW  - Obstacle detection
KW  - real-time processing
KW  - transfer learning
KW  - UAV
KW  - YOLO
KW  - Micro air vehicle (MAV)
KW  - Target drones
KW  - Transfer learning
KW  - Aerial vehicle
KW  - Autonomous navigation
KW  - Detection accuracy
KW  - Forest environments
KW  - Obstacles detection
KW  - Real- time
KW  - Realtime processing
KW  - Transfer learning
KW  - Unmanned aerial vehicle
KW  - YOLO
KW  - Drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Mei, L.
AU  - Liu, R.
AU  - Jiang, W.
AU  - Yin, Z.
AU  - Deng, X.
AU  - He, T.
TI  - Multi-Modal Fusion Sensing: A Comprehensive Review of Millimeter-Wave Radar and Its Integration With Other Modalities
PY  - 2025
T2  - IEEE Communications Surveys and Tutorials
DO  - 10.1109/COMST.2024.3398004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192996492&doi=10.1109%2fCOMST.2024.3398004&partnerID=40&md5=47d1249aacf0521e823beafed658867f
AB  - Millimeter-wave (mmWave) radar, with its high resolution, sensitivity to micro-vibrations, and adaptability to various environmental conditions, holds immense potential across multi-modal fusion sensing. Although there exist review papers on mmWave radar, there is a noticeable lack of comprehensive reviews focusing on its multi-modal fusion sensing capabilities. Addressing this gap, our review offers an extensive exploration of mmWave radar multi-modal fusion sensing, emphasizing its integration with other modalities. This review discusses the complex realm of millimeter-wave radar multi-modal fusion sensing, detailing its importance, hardware and software aspects, principles, and current potential and applications. It delves into data characteristics and datasets associated with mmWave radar, focusing on Doppler, point cloud, and multi-modal data formats. The review highlights how these data types enhance multi-modal fusion sensing and discusses methodologies, including signal processing and learning algorithms. Three categories of multi-modal fusion methodologies are proposed to optimally manage and interpret fused data. Various practical applications of mmWave radar multi-modal fusion sensing are illustrated, underlining the unique capabilities it provides when integrated with other sensors. The review concludes by identifying potential future research avenues, underscoring the immense potential of this field for further exploration and advancement. © 1998-2012 IEEE.
KW  - millimeter-wave radar
KW  - Multi-modal fusion sensing
KW  - review
KW  - Application programs
KW  - Millimeter waves
KW  - Modal analysis
KW  - Radar antennas
KW  - Signal processing
KW  - High resolution
KW  - Microvibrations
KW  - Millimeter-wave radar
KW  - Millimeterwave communications
KW  - Millimetre-wave radar
KW  - Multi-modal fusion
KW  - Multi-modal fusion sensing
KW  - Radars antennas
KW  - Antenna arrays
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, L.
AU  - Yuan, H.
AU  - Chen, S.
AU  - Zhou, B.
AU  - Guo, Y.
TI  - A lightweight deep learning model for real-time rectangle NdFeB surface defect detection with high accuracy on a global scale
PY  - 2025
T2  - Journal of Real-Time Image Processing
DO  - 10.1007/s11554-024-01592-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211370700&doi=10.1007%2fs11554-024-01592-9&partnerID=40&md5=e592a8162a87133bc612785e34138bbb
AB  - To solve the problem that it is difficult to detect dynamic tiny square neodymium-iron-boron (NdFeB) surface defects in the case of limited computing resources, this paper proposes a square NdFeB magnet surface defect detection method based on the YOLO (YOLOv8-FCW) lightweight network. Initially, the lightweight global adaptive feature enhancement module (DFNet) network is used as the backbone feature extraction net-work. By customizing the depth of the feature matrix and reducing unnecessary branch structures, the model complexity is reduced while enhancing the network’s ability to extract multi-scale feature information. Subsequently, the deformable convolution module (DCNv3) is utilized to acquire twice downsampling feature maps without information loss, aiming to expand the receptive field for small-sized defects. Finally, to further improve detection accuracy, the Wise-IOU (WIOU) v3 bounding box loss function is introduced to focus on the samples that are difficult to identify and reduce the gradient penalty for low-quality samples. The experimental results show that the YOLOv8-FCW algorithm achieves a mean Average Precision (mAP@0.5) of 78.6% on the rectangle NdFeB magnet dataset, with a model parameter quantity and computational cost reduction of 33.2% and 24.7%, respectively compared with the baseline, and requires less computational resources for higher detection accuracy compared to other mainstream object detection algorithms. Finally, the model was deployed to industrial Automated Optical Inspection (AOI) devices using TensorRT. This deployment reduced the inference time for a single image to 2.7 ms and increased speed by 6.6 times, enabling dynamic micro-detection of surface defects in square NdFeB. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Defect detection
KW  - DFNet
KW  - Lightweight network
KW  - NdFeB magnet
KW  - TensoRT
KW  - YOLOv8
KW  - Accelerator magnets
KW  - Electromagnetic induction
KW  - Forward error correction
KW  - Image enhancement
KW  - Light sensitive materials
KW  - Neodymium compounds
KW  - Particle beams
KW  - Photons
KW  - Superconducting films
KW  - Defect detection
KW  - Detection accuracy
KW  - DFNet
KW  - Learning models
KW  - Lightweight network
KW  - NdFeB magnet
KW  - Real- time
KW  - Surface defect detections
KW  - Tensort
KW  - YOLOv8
KW  - Neodymium alloys
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mumba, E.
AU  - Sulaiman Gezawa, A.
AU  - Liu, C.
TI  - Enhanced autonomous driving within Webots simulation for student experiments
PY  - 2025
T2  - Intelligent Service Robotics
DO  - 10.1007/s11370-025-00608-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007978282&doi=10.1007%2fs11370-025-00608-y&partnerID=40&md5=75c0ada2d6c0a86bd0dfa54beefaee78
AB  - As artificial intelligence (AI) advances in autonomous driving, autonomous navigation systems should be integrated into educational platforms to provide students practical experience in robotics and machine learning. In this paper, we propose the detailed study of autonomous driving within the Webots simulation platform, specifically designed for student experiments. We incorporate advanced techniques such as obstacle detection, precise lane following, and intelligent traffic light recognition based on an improved YOLOv8 architecture for robust detection of road signs and traffic lights. We utilized the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) which are integral to YOLOv8’s multi-scale feature extraction and enhance their efficiency by replacing standard convolutions with depthwise separable convolution in the neck of network. This modification reduces the computational overhead while preserving feature representation quality, leading to faster inference speeds and improved small-object detection accuracy, thus improving autonomous navigation performance in simulated scenarios. To consider the performance of our proposed model, we compare it against widely used and well-established object detection models on KITTI and RF100 datasets. The experimental results demonstrate that the proposed model exhibits a good advantage in the field of autonomous driving and boosting student engagement and preparing them for future careers in AI and robotics. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.
KW  - Autonomous navigation
KW  - Object detection
KW  - Sensor integration
KW  - Student teaching experiments
KW  - YOLOv8
KW  - Autonomous vehicles
KW  - Convolution
KW  - Intelligent robots
KW  - Learning systems
KW  - Navigation
KW  - Navigation systems
KW  - Object recognition
KW  - Simulation platform
KW  - Students
KW  - Traffic signs
KW  - Autonomous driving
KW  - Autonomous navigation
KW  - Autonomous navigation systems
KW  - Educational platforms
KW  - Objects detection
KW  - Practical experience
KW  - Sensor integration
KW  - Student experiments
KW  - Student teaching experiment
KW  - YOLOv8
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, X.
AU  - Zhang, B.
AU  - Wan, W.
TI  - Adaptive cross-modal fusion for robust multi-modal object detection in infrared–visible imaging
PY  - 2025
T2  - Optical Review
DO  - 10.1007/s10043-025-00977-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004839417&doi=10.1007%2fs10043-025-00977-w&partnerID=40&md5=d3949840622048a911868a6794cf45e8
AB  - Given the challenges faced by object detection methods that rely on visible light in complex environments, many researchers have begun to explore the combination of infrared and visible imaging for multi-modal detection. Existing results show that multi-modal fusion has proven effective for improving object detection outcomes. However, most current multi-modal detection methods rely on fixed-parameter feature fusion techniques, failing to account for the imaging differences across diverse environments and the complementary information between different modalities. In this paper, we propose a multi-modal object detection method based on adaptive weight fusion, utilizing the dual-stream framework to extract features from both modalities separately. We design a Cross-Modal Feature Interaction (CMFI) module to integrate global information across modalities and capture long-range dependencies. In addition, we introduce an Adaptive Modal Weight Calculation (AMWC) module, which fully accounts for the characteristics of different modalities in various environments and the complementarity among the modalities. This module dynamically adjusts the fusion weights within the CMFI module based on the input from different modalities. Moreover, a novel loss function is introduced to regulate the internal parameter adjustments of the AMWC module. We conduct extensive experiments on three representative datasets, using mAP@0.5 and mAP@0.5:0.95 as evaluation metrics. Our model achieved 79.1% and 40.6% on the FLIR dataset, 81.9% and 52.1% on M3FD, and 73.5% and 32.5% on KAIST. © The Optical Society of Japan 2025.
KW  - Deep learning
KW  - Infrared images
KW  - Modal weight
KW  - Multi-modal feature fusion
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, H.
AU  - Yu, Y.
AU  - He, J.
AU  - Pang, C.
AU  - Zhang, X.
TI  - Multistage Fusion Object Detection With Marine Radar and Camera in Complex Maritime Context
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3571112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005793143&doi=10.1109%2fTIM.2025.3571112&partnerID=40&md5=1768489ed44c78f117bd7ec20c0c1dce
AB  - The marine object detection is an essential component for maritime autonomous surface ship (MASS) systems, enabling them to perceive obstacle objects and avoid them. Existing detection algorithms primarily rely on visual sensors and face serious limitations in complex maritime environments. In this article, we propose a novel joint object detection method based on the fusion of marine radar and camera data for robust maritime object detection. By employing a multistage hybrid-level fusion architecture, the proposed method efficiently fuses marine radar data with camera images, thereby enhancing the accuracy and robustness of object detection in complex maritime contexts. In addition, inspired by vision transformer detectors, a new transformer-based fusion module (TFM) is proposed to mitigate semantic disparity between the two modalities and enhance radar information guidance through extracted object queries. Extensive experiments on public maritime perception datasets demonstrate the significant advantages of the proposed method in various maritime environments. The framework also exhibits strong generalization capability, making it applicable to diverse maritime settings, including challenging conditions like low visibility and high-background noise. In public maritime perception datasets, the proposed method achieves state-of-the-art accuracy compared with other single-modality baselines and radar-camera fusion baselines, outperforming the excellent visual object detection model YOLOv8-x in terms of mAP0.35 increased by 13.72% in common scenarios and 24.85% in complex scenarios. © 1963-2012 IEEE.
KW  - Marine radar
KW  - maritime autonomous surface ship (MASS) perception
KW  - multisensor fusion
KW  - multistage fusion
KW  - object detection
KW  - Detection algorithm
KW  - Marine objects
KW  - Maritime autonomous surface ship perception
KW  - Maritime environment
KW  - Multi stage fusion
KW  - Multi-sensor fusion
KW  - Objects detection
KW  - Ship systems
KW  - Surface ship
KW  - Visual sensor
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Character, L.
AU  - Moline, M.
AU  - Breece, M.W.
AU  - White, E.
AU  - Davis, D.
AU  - Colbourn, C.
TI  - Deep Learning for Detection of Underwater Aircraft Wrecks from US Conflicts
PY  - 2025
T2  - Journal of Computer Applications in Archaeology
DO  - 10.5334/jcaa.179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005485410&doi=10.5334%2fjcaa.179&partnerID=40&md5=45826253972350e4c579af9fa476d618
AB  - There are more than 72,000 missing in action service members from WWII, many of whom were lost at sea in aircraft. Using high-resolution sidescan sonar and a YOLOv7 model, we present a deep learning approach to expedite the search for these individuals. Our training dataset is the largest aircraft wreck-focused dataset that has been published to date, with 19 unique aircraft wrecks, composed of 290 individual fragments and located across six countries. Our trained model produces an F1 score of 0.74. We tested the model on newly collected data and the model correctly identified three out of four previously unknown aircraft. As the model becomes more accurate and trust in the model is built, we envision a human-in-the-loop approach in which newly collected data are input into the model in the field, the model is run with a confidence threshold of 0.5, and predictions are then checked by the human field team. As trust builds, the confidence threshold can be made higher so that fewer predictions are generated and human review time is minimized. © 2025 The Author(s).
KW  - aircraft wreck
KW  - autonomous underwater vehicle
KW  - Deep learning
KW  - marine archaeology
KW  - sidescan sonar
KW  - World War II
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Angeljulie, J.
AU  - Pradish Pranam, S.
AU  - Yosuva, B.
AU  - Shuaib Khan, S.
TI  - Reliable Roadside Assistance and Fast Solutions for Emergency Breakdown States by EF-YOLOv8m
PY  - 2025
T2  - 4th International Conference on Sentiment Analysis and Deep Learning, ICSADL 2025 - Proceedings
DO  - 10.1109/ICSADL65848.2025.10933303
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002456072&doi=10.1109%2fICSADL65848.2025.10933303&partnerID=40&md5=3a60fc42d2ac9ee71bcf57f7e0bb1768
AB  - In the technological development era and fast-paced world, Artificial Intelligence techniques and Computer Vision algorithms are transfiguring the automotive industrial services, especially in reliable roadside assistance and emergency services for vehicle breakdowns. Significant AI-oriented algorithms including predictive maintenance, route optimization, resource allocation, and reinforcement learning techniques are used to address crucial problems like optimized resource distribution, rapid response, and proactive fault detection. This article incorporates AI-oriented algorithms for enhancing the accuracy, responsiveness, and efficiency of reliable roadside assistance and fast solutions for emergency breakdown states using the EF-YOLOv8m (Efficient - You Only Look Once version 8) algorithm. This proposed algorithm facilitates the visual prediction of breakdown situations and provides a fast solution for real-time prediction, analysis, and response in emergencies. The deployment of the EF - YOLOv8m algorithm leads to accurate prediction of vehicle breakdown situations, estimates road conditions, and detects critical faults to ensure the optimization of resource allocation and fast decision-making. Using its greater speed, accuracy, and lightweight model architecture, EE-YOLOv8 may process real-time image/video inputs and sensor data from vehicle-attached cameras, IoT devices, and surveillance drones in real time. Based on performance metrics such as confidence scores, detection accuracy, and frames per second (FPS), this model is used to analyse the reliability of EF-YOLOv8m in dynamic roadside environments. Additionally, the incorporation of AI -oriented and predictive maintenance algorithms improves the effectiveness of dispatch functions, reducing delay and guaranteeing timely assistance. Experimental results show the potential of YOLOv8m to transfigure the services of roadside assistance and provide an ascendable, high-performance solution that can address the difficulties of emergency breakdown situations with reliability and precision.  © 2025 IEEE.
KW  - Artificial Intelligence
KW  - Computer Vision
KW  - Efficient YOLOv8
KW  - Emergency Breakdown States
KW  - Internet of Things (IoT)
KW  - Reliable Roadside Assistance
KW  - Aircraft detection
KW  - Intelligent systems
KW  - Reinforcement learning
KW  - Risk assessment
KW  - Satellites
KW  - Artificial intelligence techniques
KW  - Automotives
KW  - Computer vision algorithms
KW  - Efficient YOLOv8
KW  - Emergency breakdown state
KW  - Fast solutions
KW  - Internet of thing
KW  - Predictive maintenance
KW  - Reliable roadside assistance
KW  - Technological development
KW  - Resource allocation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sharma, M.
AU  - Kaswan, K.S.
AU  - Yadav, D.K.
TI  - Performance comparison of optical flow and background subtraction and discrete wavelet transform methods for moving objects
PY  - 2025
T2  - IAES International Journal of Robotics and Automation
DO  - 10.11591/ijra.v14i1.pp93-102
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219016941&doi=10.11591%2fijra.v14i1.pp93-102&partnerID=40&md5=f9c3d1c39428eb6cf71084f2c9b326d8
AB  - Self-driving cars and other autonomous vehicles rely on systems that can recognize and follow objects. The ways help people make safe decisions and navigate by showing things like people, cars, obstacles, and traffic lights. Computer vision algorithms encompass both object detection and tracking. Different methods are specifically developed for picture or video analysis not only to identify items within the visual content but also to accurately determine their precise locations. This can operate independently as an algorithm or as a constituent of an item-tracking system. Object tracking algorithms can be used to follow objects over video frames, providing a contrasting approach. The research article focuses on the mathematical model simulation of optical flow, background subtraction, and discrete wavelet transform (DWT) methods for moving objects. The performance evaluation of the methods is done based on simulation response time, accuracy, sensitivity, and specificity doe several images in different environments. The DWT has shown optimal behavior in terms of the response time of 0.27 seconds, accuracy of 95.34 %, selectivity of 95.96 %, and specificity of 94.68 %. © 2025, Intelektual Pustaka Media Utama. All rights reserved.
KW  - Discrete wavelet transform
KW  - Image subtraction
KW  - Moving objects
KW  - Optical modeling
KW  - Signal detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - In, H.
AU  - Kweon, J.
AU  - Moon, C.
TI  - Squeeze-EnGAN: Memory Efficient and Unsupervised Low-Light Image Enhancement for Intelligent Vehicles
PY  - 2025
T2  - Sensors
DO  - 10.3390/s25061825
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000966064&doi=10.3390%2fs25061825&partnerID=40&md5=306ffac4d298cd54a0243b3f6307de3e
AB  - Intelligent vehicles, such as autonomous cars, drones, and robots, rely on sensors to gather environmental information and respond accordingly. RGB cameras are commonly used due to their low cost and high resolution but are limited in low-light conditions. While employing LiDAR or specialized cameras can address this issue, these solutions often incur high costs. Deep learning-based low-light image enhancement (LLIE) methods offer an alternative, but existing models struggle to adapt to road scenes. Furthermore, most LLIE models rely on supervised training but are heavily constrained by the lack of low-light and normal-light paired datasets. In particular, obtaining paired datasets for driving scenes is extremely challenging. To address these issues, this paper proposes Squeeze-EnGAN, a memory-efficient, GAN-based LLIE method capable of unsupervised learning without paired image datasets. Squeeze-EnGAN incorporates a fire module into a U-net architecture, substantially reducing the number of parameters and Multiply-Accumulate Operations (MACs) compared to its base model, EnlightenGAN. Additionally, Squeeze-EnGAN achieves real-time performance on devices like Jetson Xavier (0.061 s). Significantly, enhanced images improve object detection performance over original images, demonstrating the model’s potential to aid high-level vision tasks in intelligent vehicles. © 2025 by the authors.
KW  - autonomous driving
KW  - generative adversarial network
KW  - low-light image enhancement
KW  - unsupervised learning
KW  - Health risks
KW  - Intelligent robots
KW  - Laser beams
KW  - Magnetic levitation vehicles
KW  - Risk assessment
KW  - SLAM robotics
KW  - Unsupervised learning
KW  - Adversarial networks
KW  - Autonomous car
KW  - Autonomous driving
KW  - Environmental information
KW  - Low-costs
KW  - Low-high
KW  - Low-light image enhancement
KW  - Low-light images
KW  - Memory efficient
KW  - RGB cameras
KW  - article
KW  - autonomous vehicle
KW  - camera
KW  - deep learning
KW  - diagnosis
KW  - drone
KW  - generative adversarial network
KW  - human
KW  - image enhancement
KW  - learning
KW  - light
KW  - memory
KW  - sensor
KW  - Generative adversarial networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Yan, C.
AU  - Xue, S.
AU  - Han, H.
AU  - Li, M.
TI  - Research on the appearance inspection technology of screw assembly quality based on machine vision
PY  - 2025
T2  - Proceedings of 2024 International Conference on Artificial Intelligence and Future Education, AIFE 2024
DO  - 10.1145/3708394.3708430
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218185910&doi=10.1145%2f3708394.3708430&partnerID=40&md5=7c617dcba050693990b8c4a157b5c297
AB  - Aiming at the demand for screw visual inspection in the appearance inspection of a certain complex product assembly quality, a real-Time multi-scale small object detection and classification model (RMSDC) is studied. RMSDC uses YOLOv8n as the base network structure. SPD-Conv consisting of a space-To-depth (SPD) layer and a non-strided convolution layer is used to improve the performance of the model in low-pixel images and small target detection. A convolution attention mixing (CAMixing) block is introduced in the middle of the backbone network and the neck network, which combines the attention mechanism with the multi-scale convolution to enhance the multi-scale feature extraction capability and improve the model accuracy. Choose an ultra-lightweight and effective dynamic upsampler (DySample), which is lightweight and can preserve the high-level feature semantic information of small targets as much as possible. The deformable convolution v4 (DCNv4) is fused to the dynamic head (Dyhead) to improve the generalization ability of the model in terms of shape transformation. Through comparative experiments, RMSDC in this paper improves the recall (R), mAP0.5 and mAP0.5:0.95 by 3.7%, 1.5%, and 1.8% compared to the baseline model on the experimental dataset, and mAP0.5 reaches 98.5%. The excellent performance of RMSDC has also been demonstrated on the ROSD dataset.  © 2024 Copyright held by the owner/author(s).
KW  - CAMixing
KW  - DCNv4
KW  - Screw Detection
KW  - SPD
KW  - YOLOv8
KW  - Cams
KW  - Image enhancement
KW  - Inspection equipment
KW  - Machine vision
KW  - Convolution attention mixing
KW  - Deformable convolution v4
KW  - Detection models
KW  - Multi-scales
KW  - Object classification
KW  - Real- time
KW  - Screw detection
KW  - Small object detection
KW  - Space-to-depth
KW  - YOLOv8
KW  - Screws
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Himeur, Y.
AU  - Aburaed, N.
AU  - Elharrouss, O.
AU  - Varlamis, I.
AU  - Atalla, S.
AU  - Mansoor, W.
AU  - Al-Ahmad, H.
TI  - Applications of knowledge distillation in remote sensing: A survey
PY  - 2025
T2  - Information Fusion
DO  - 10.1016/j.inffus.2024.102742
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207881950&doi=10.1016%2fj.inffus.2024.102742&partnerID=40&md5=70eb0c6b55ed0474e44620594ec7ed83
AB  - With the ever-growing complexity of models in the field of remote sensing (RS), there is an increasing demand for solutions that balance model accuracy with computational efficiency. Knowledge distillation (KD) has emerged as a powerful tool to meet this need, enabling the transfer of knowledge from large, complex models to smaller, more efficient ones without significant loss in performance. This review article provides an extensive examination of KD and its innovative applications in RS. KD, a technique developed to transfer knowledge from a complex, often cumbersome model (teacher) to a more compact and efficient model (student), has seen significant evolution and application across various domains. Initially, we introduce the fundamental concepts and historical progression of KD methods. The advantages of employing KD are highlighted, particularly in terms of model compression, enhanced computational efficiency, and improved performance, which are pivotal for practical deployments in RS scenarios. The article provides a comprehensive taxonomy of KD techniques, where each category is critically analyzed to demonstrate the breadth and depth of the alternative options, and illustrates specific case studies that showcase the practical implementation of KD methods in RS tasks, such as instance segmentation and object detection. Further, the review discusses the challenges and limitations of KD in RS, including practical constraints and prospective future directions, providing a comprehensive overview for researchers and practitioners in the field of RS. Through this organization, the paper not only elucidates the current state of research in KD but also sets the stage for future research opportunities, thereby contributing significantly to both academic research and real-world applications. © 2024 Elsevier B.V.
KW  - Knowledge distillation
KW  - Model and data distillation
KW  - Model compression
KW  - Remote sensing
KW  - Urban planning and precision agriculture
KW  - Balance model
KW  - Distillation method
KW  - Knowledge distillation
KW  - Model and data distillation
KW  - Model compression
KW  - Modeling accuracy
KW  - Performance
KW  - Precision Agriculture
KW  - Remote-sensing
KW  - Urban planning and precision agriculture
M3  - Short survey
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xiang, X.
AU  - Zhang, G.
AU  - Huang, L.
AU  - Zheng, Y.
AU  - Xie, Z.
AU  - Sun, S.
AU  - Yuan, T.
AU  - Chen, X.
TI  - Research on infrared small target pedestrian and vehicle detection algorithm based on multi-scale feature fusion
PY  - 2025
T2  - Journal of Real-Time Image Processing
DO  - 10.1007/s11554-024-01607-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212828304&doi=10.1007%2fs11554-024-01607-5&partnerID=40&md5=697b12176d51795b2c5d51a0fd6ac72e
AB  - Infrared imaging technology relies on detecting the electromagnetic waves emitted by an object's spontaneous thermal radiation for imaging. It can overcome the adverse effects of complex lighting conditions on the detection of pedestrians and vehicles on the road. To address the issues of low accuracy and missed detection in visual detection under complex traffic conditions, such as during rain, snow, or at night, a pedestrian and vehicle detection model using infrared imaging has been proposed. This model improves the neck network and incorporates an attention mechanism. First, by adding a multi-scale feature fusion small-object detection layer to the model's neck, enhancing the capture of detailed information about small infrared objects and reducing missed detections. Second, a novel dual-layer routing attention mechanism is designed, allowing the model to focus on the most relevant feature areas and improving the detection accuracy of small infrared objects. Next, the CARAFE upsampling method is used for adaptive upsampling and context information fusion, which enhances the model's ability to reorganize features and capture details. Finally, a lightweight CSPPC module is constructed using partial convolutions to replace the C2f module in the neck network, which improves the model's frame rate. Experimental results show that, compared to the baseline model, BCC-YOLOv8n improves precision, recall, mAP@0.5, and mAP@0.5:0.95 by 1.4%, 4.8%, 5.3%, and 4.5%, respectively, while reducing the number of parameters by approximately 7%. Additionally, a frame rate of 70.8 FPS was achieved, satisfying the requirements for real-time detection. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Double layer routing attention
KW  - Infrared small target detection
KW  - Model lightweight
KW  - Pedestrian and vehicle detection
KW  - Small target detection layer
KW  - Image coding
KW  - Image enhancement
KW  - Spontaneous emission
KW  - Thermography (imaging)
KW  - Double layer routing attention
KW  - Double layers
KW  - Infrared small target detection
KW  - Infrared small targets
KW  - Model lightweight
KW  - Pedestrian detection
KW  - Routings
KW  - Small target detection
KW  - Small target detection layer
KW  - Vehicles detection
KW  - Infrared radiation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Patil, O.C.
AU  - Nashimath, S.S.
AU  - Nadaf, T.
AU  - Patil, P.
AU  - Kallimani, R.
TI  - Development and Execution of an AI-Driven Adaptive Cruise Control with Collision Detection and Speed Locking Mechanism
PY  - 2025
T2  - 4th IEEE International Conference on Distributed Computing and Electrical Circuits and Electronics, ICDCECE 2025
DO  - 10.1109/ICDCECE65353.2025.11035305
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009595426&doi=10.1109%2fICDCECE65353.2025.11035305&partnerID=40&md5=e321a42107bd81d8ac80e4d2c5b68902
AB  - By reducing the need for constant driver intervention, adaptive cruise control (ACC) systems increase driving comfort and safety. This paper introduces an AI-based ACC system that combines dynamic speed adjustment and real-time obstacle detection. The system, which is based on YOLOv3 object detection and voice recognition technology, allows the car to respond to its surroundings and autonomously maintain a predetermined speed. To prevent collisions, the system applies the brakes or lowers speed when it detects obstacles. For BLDC motor control, the architecture combines mathematical modelling, feature extraction, and image processing. Reinforcement learning with real-time feedback optimizes speed regulation. High detection accuracy 91.4%, low response latency 0.82s, and effective speed locking under various road conditions are demonstrated by the experimental results. The system presented provides an intelligent, safe, and intuitive cruise control system, advancing semi-autonomous vehicle technologies. Future improvements and limitations are discussed to enhance real-world applicability.  © 2025 IEEE.
KW  - cruise control
KW  - image and voice recognition
KW  - ml algorithms
KW  - obstacle detection
KW  - speed control
KW  - YOLO model
KW  - Adaptive control systems
KW  - Adaptive cruise control
KW  - Computer control
KW  - Computer control systems
KW  - Locks (fasteners)
KW  - Object detection
KW  - Object recognition
KW  - Speed control
KW  - Speed regulators
KW  - Adaptive cruise control systems
KW  - Collision detection
KW  - Collision speed
KW  - Driving comfort
KW  - Driving safety
KW  - Dynamic speed
KW  - Locking mechanism
KW  - Ml algorithms
KW  - Obstacles detection
KW  - YOLO model
KW  - Obstacle detectors
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Dong, Y.
AU  - Hu, C.
AU  - Quaye, J.A.
AU  - Lu, N.
AU  - Zhao, L.
TI  - Intelligent identification of carbonate components based on deep learning
PY  - 2025
T2  - Facies
DO  - 10.1007/s10347-024-00694-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217795712&doi=10.1007%2fs10347-024-00694-x&partnerID=40&md5=ed94a64fb3492ccae122c9afd64c9abe
AB  - Many applications in carbonate facies analysis require a compositional analysis of the grain, cement, and pore types. Image analysis based on deep learning models can help automatically extract features for identifying objects and interpreting carbonate thin sections. However, small objects are detected with a lower average precision than medium and large objects due to the loss of information during the deep convolution operation. Existing object detection algorithms cannot simultaneously achieve a high detection accuracy and detection speed, which hinders the further study of petroliferous carbonate successions. You Only Look Once version 5 (YOLOv5) is an advanced, fast, and accurate detector. In this study, the applicability and performance of YOLOv5-based object detection approaches were assessed by conducting a carbonate compositional analysis. The training data comprised more than 6800 individually labeled objects from 1000 carbonate petrographic images. The dataset was grouped into nine different classes for the object detection tasks. Even with a small amount of training, the YOLOv5 could achieve a precision of 99.0%, a recall rate of 98.4%, and a mean average precision of 91.6% for object detection by combining the scale sequence feature pyramid network. The study not only meets the accuracy requirements of identifying multi-scale objects, particularly small objects, but also meets the detection speed requirement, with a significant application potential in identifying carbonate components. © Springer-Verlag GmbH Germany, part of Springer Nature 2025.
KW  - Artificial intelligence
KW  - Carbonate rocks
KW  - Deep learning
KW  - Intelligent identification
KW  - Object detection
KW  - algorithm
KW  - carbonate rock
KW  - chemical composition
KW  - facies analysis
KW  - machine learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bai, R.
AU  - Song, G.
AU  - Wang, Q.
TI  - YOLORemote: Advancing Remote Sensing Object Detection by Integrating YOLOv8 With the CE-WA-CS Feature Fusion Approach
PY  - 2025
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
DO  - 10.1109/JSTARS.2025.3543951
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003183428&doi=10.1109%2fJSTARS.2025.3543951&partnerID=40&md5=77260eea5c1c44649b03cd24aa05d899
AB  - The rapid development iteration of YOLO models has spurred extensive research into specialized adaptations tailored for remote sensing object detection (RSOD). Typically, these adaptations involve modifying specific YOLO versions to excel in dataset-specific benchmarks. Such efforts primarily focus on enhancing model performance on specific datasets, a trend that, while beneficial for achieving high leaderboard standings, frequently overlooks the critical aspects of generalizability, scalability, and transferability. Consequently, there is a significant gap in the research concerning the development of universally applicable modules that are easy to adapt across various baseline YOLO models—an essential factor for practical deployment and widespread adoption in real-world applications. This study addresses these shortcomings by introducing a novel feature fusion method, context enhancement (CE), weight adjustment-based spatial attention (WA), and channel shuffling (CS)—collectively termed CE-WA-CS, specifically designed for RSOD. This research rigorously tests the proposed feature fusion method across various YOLO models, substantiating its versatility and effectiveness in enhancing RSOD. Notably, in comparative performance evaluations involving several YOLO architectures, integrating our feature fusion method with YOLOv8 emerged as the most balanced in terms of accuracy, computational complexity, and inference speed. This optimal combination has been designated as YOLORemote. YOLORemote not only exemplifies a significant improvement in performance metrics but also demonstrates a practical balance that is crucial for real-time applications. © 2008-2012 IEEE.
KW  - and channel shuffling (CE-WA-CS)
KW  - Context enhancement
KW  - remote sensing object detection (RSOD)
KW  - weight adjustment-based spatial attention
KW  - YOLORemote
KW  - Benchmarking
KW  - Cerium alloys
KW  - Cesium alloys
KW  - Object detection
KW  - Remote sensing
KW  - Context enhancement-WA-channel shuffling
KW  - Excel
KW  - Feature fusion method
KW  - Features fusions
KW  - Modeling performance
KW  - Objects detection
KW  - Real-world
KW  - Remote sensing object detection
KW  - Remote-sensing
KW  - Yoloremote
KW  - benchmarking
KW  - comparative study
KW  - data set
KW  - numerical model
KW  - performance assessment
KW  - remote sensing
KW  - Proximity sensors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Hartmann, N.
AU  - Rüter, J.
AU  - Jünger, F.
TI  - Ensuring Safety of Deep Learning Components Using Improved Image-Level Property Selection for Monitoring
PY  - 2025
T2  - AIAA Science and Technology Forum and Exposition, AIAA SciTech Forum 2025
DO  - 10.2514/6.2025-2512
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001290345&doi=10.2514%2f6.2025-2512&partnerID=40&md5=beaaee834f7f75e8124f59a370254279
AB  - Environment perception will play an important role for autonomous aircraft, e.g., to be able to prevent mid-air collisions or to find emergency landing spots. Deep Learning (DL) based approaches for computer vision often give state-of-the-art results but are currently not certifiable for aviation because of their data driven training process and their black-box character. Runtime monitoring of the model input could mitigate this problem by ensuring that the model output is only considered when the input is deemed to be suitable. On the one hand, this could be achieved by monitoring operational parameters described by an Operational Design Domain (ODD) as suggested by the European Union Aviation Safety Agency (EASA). On the other hand, unsafe input data might be rejected based on its direct impact on the model performance using Out-of-Model-Scope (OMS) detection. However, performing either ODD monitoring or OMS detection for high-dimensional input data such as camera images is a non-trivial task as it is unclear which properties of an input image should be monitored. In this work, we describe a process to derive a set of suitable low-level image properties that can be used to monitor the input of a DL component. We show that the features selected by the process can be used by a runtime monitor to improve the safety of a DL component by filtering images that violate the ODD boundaries or are OMS. © 2025, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
KW  - Aircraft landing
KW  - Black-box testing
KW  - Civil aviation
KW  - Input output programs
KW  - Network security
KW  - Training aircraft
KW  - Autonomous aircraft
KW  - Data driven
KW  - Design domains
KW  - Emergency landing
KW  - Environment perceptions
KW  - Learning-based approach
KW  - Mid-air collisions
KW  - Operational design
KW  - Property
KW  - State of the art
KW  - Aircraft accidents
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Van Lier, M.
AU  - Van Leeuwen, M.
AU  - Van Manen, B.
AU  - Kampmeijer, L.
AU  - Boehrer, N.
TI  - Evaluation of Spatio-Temporal Small Object Detection in Real-World Adverse Weather Conditions
PY  - 2025
T2  - Proceedings - 2025 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops, WACVW 2025
DO  - 10.1109/WACVW65960.2025.00094
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005027292&doi=10.1109%2fWACVW65960.2025.00094&partnerID=40&md5=73278bf860b1e0f932a7ee34fd839f90
AB  - Deep learning-based object detection methods, such as YOLO, are promising for surveillance applications. However, detecting small objects in large-scale scenes with cluttered backgrounds and adverse weather remains challenging. Recent advancements leverage spatio-temporal information to enhance small object detection, yet the impact of (temporal) adverse weather conditions on such methods remains largely unexplored due to the lack of comprehensive evaluation datasets. This paper evaluates the performance of spatio-temporal YOLOv8 (TYOLOv8) for detecting small objects in real-world adverse weather conditions, comparing it to spatial YOLOv8 and the 3FN moving object detection method. Additionally, we propose haze augmentation to improve object detection performance in challenging hazy weather. Due to the lack of suitable datasets for evaluation, this paper introduces a novel real-world video dataset for small object detection, referred to as Nano-VID-weather, with an average object size of 16.42 pixels, consisting of a Tiny Objects subset and three challenging weather subsets: Wind, Rain and Haze. Our findings reveal that TYOLOv8 is resilient to real-world adversarial weather conditions, like wind, rain, and haze. Notably, on average TYOLOv8 outperformed both 3FN and YOLOv8 with +0.21mAP across all our subsets. These results demonstrate that TYOLOv8 can enhance surveillance capabilities for small object detection under real-world adverse weather conditions.  © 2025 IEEE.
KW  - adverse weather
KW  - dataset
KW  - object detection
KW  - small object detection
KW  - temporal object detection
KW  - yolov8
KW  - Deep learning
KW  - Object recognition
KW  - Rain
KW  - Snow
KW  - Adverse weather
KW  - Condition
KW  - Dataset
KW  - Objects detection
KW  - Real-world
KW  - Small object detection
KW  - Temporal object detection
KW  - Temporal objects
KW  - Yolov8
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, F.
AU  - Chen, C.
AU  - Shang, Z.
AU  - Ma, K.-K.
AU  - Wu, Q.
AU  - Lin, Z.
AU  - Zhan, J.
AU  - Shi, Y.
TI  - Deep Multi-Modal Ship Detection and Classification Network
PY  - 2025
T2  - IEEE Transactions on Circuits and Systems for Video Technology
DO  - 10.1109/TCSVT.2024.3519569
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212780154&doi=10.1109%2fTCSVT.2024.3519569&partnerID=40&md5=4904e0d89df95892112b4c5404b8b882
AB  - While a majority of single-modal ship detectors solely rely on RGB images, a novel multi-modal real-time transformer-based ship detection and classification method, called the MM-ShipNet, is proposed in this paper that integrates the data acquired from three modalities—i.e., RGB camera, radar, and automatic identification system (AIS). First, a bounding box is generated based on the position information from radar and ship’s actual size information from AIS. This physical information are fused and projected onto the camera-acquired RGB image frame. Each bounding box is then possibly weighted depending on the ship size presented on the image. The generated weighted ship masks (WSMs) will be exploited for facilitating ship classification task. In the second stage of MM-ShipNet, multi-modal detection transformer (MM-DETR) introduces an multi-modal cross-scale encoder (MCE) for improving ship detection and classification performance. Our MCE exploits a dual-flow structure to fuse the features extracted from the WSMs and the RGB images under different scales. Since our method is the first work entailing three aforementioned modalities, no such dataset with all modalities can be found in the open source. Thus, we construct a multi-modal ship dataset, termed MMShips, as another contribution. Our MMShips dataset comprises 9,513 camera-acquired real-life maritime RGB images and their aligned ship masks generated from radar and AIS. Experimental results clearly demonstrate that our MM-ShipNet significantly outperforms multiple state-of-the-art single-modal and multi-modal ship detectors. © 1991-2012 IEEE.
KW  - automatic identification system
KW  - multi-modality
KW  - radar
KW  - Ship detection and classification
KW  - transformer
KW  - Image coding
KW  - Photointerpretation
KW  - Radar imaging
KW  - Remote sensing
KW  - Scales (weighing instruments)
KW  - Ships
KW  - Automatic identification system
KW  - Bounding-box
KW  - Detection networks
KW  - Multi-modal
KW  - Multi-modality
KW  - RGB images
KW  - Ship classification
KW  - Ship detection
KW  - Single-modal
KW  - Transformer
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, X.
AU  - Yan, P.
AU  - Zheng, S.
AU  - Du, Q.
AU  - Wang, D.
TI  - YOLOv8-WTDD: multi-scale defect detection algorithm for wind turbines
PY  - 2025
T2  - Journal of Supercomputing
DO  - 10.1007/s11227-024-06487-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207046338&doi=10.1007%2fs11227-024-06487-x&partnerID=40&md5=49af27c3103a9f20e92c3f1f4fee074f
AB  - In addressing the challenges of wind turbine defect detection, such as different defect scales in UAV aerial photography, interference from different lighting conditions, and small-sized target defects leading to low detection accuracy and inaccurate localization, a YOLOv8-WTBB model based on YOLOv8 is proposed. Firstly, the Diverse Branch Block is designed to enhance multi-scale feature fusion capabilities. Next, the Receptive-Field Attention Convolution is introduced to focus on the spatial features of the receptive field, increasing the distinction between target features and the surrounding environment. Finally, introducing the Minimum Point Distance Intersection over the Union bounding box regression loss function notably improves localization accuracy in object detection and accelerates model convergence. Experimental results demonstrate that the proposed algorithm significantly outperforms the baseline network, with a 4.3% improvement in mean average precision, achieving 89.1%, and a 7.4% increase in mean average recall, reaching 84.8%. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
KW  - Deep learning
KW  - Renewable energy
KW  - Wind power generation
KW  - Wind turbine defect detection
KW  - YOLO
KW  - Aerial photography
KW  - Aircraft detection
KW  - Deep learning
KW  - Object detection
KW  - Wind power
KW  - Wind turbines
KW  - Deep learning
KW  - Defect detection
KW  - Defect detection algorithm
KW  - Lighting conditions
KW  - Multi-scales
KW  - Receptive fields
KW  - Renewable energies
KW  - Wind power generation
KW  - Wind turbine defect detection
KW  - YOLO
KW  - Windmill
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, H.
AU  - Qian, H.
TI  - SR-DAYOLOv8: cross-domain adaptive object detection based on super-resolution domain classifier
PY  - 2025
T2  - Multimedia Systems
DO  - 10.1007/s00530-024-01594-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212781889&doi=10.1007%2fs00530-024-01594-4&partnerID=40&md5=c1535077cdfe06a158e84e2326651373
AB  - Object detection is a fundamental task of environment perception in traffic road scenarios, and its accurate detection results are of great significance for improving the reliability of autonomous driving, optimizing traffic flow management, and enhancing road safety. However, the problem of domain offset between different traffic road scenarios leads to a poor generalization of the target detector. To address this challenge, we propose a new unsupervised domain adaptation object detection algorithm, SR-DAYOLOv8. Specifically, the algorithm contains three effective components. First, we use the source and target domains to train an unpaired image-to-image translator to generate a target-like domain, using the target-like domain as input to compensate for image-level differences. Second, to correct cross-domain discrepancies, we add a new detector for the target-like domain, enabling it to conduct supervised learning training, just like the source domain. Finally, we design a super-resolution domain classifier to obtain domain adaptive feature maps. Domain-invariant features are extracted through image-level adaptation and instance-level adaptation, and consistency regularization is employed to optimize the overall alignment effect. We conducted experiments on Cityscape, Foggy Cityscape, KITTI, SIM10K, and BDD100K datasets for different domain offset scenarios. Experimental results show that our method can improve target detection performance in different domain offset scenarios and outperform other state-of-the-art algorithms. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Domain offset
KW  - Object detection
KW  - Super-resolution domain classifier
KW  - Unsupervised domain adaptation
KW  - Advanced traffic management systems
KW  - Highway accidents
KW  - Highway administration
KW  - Highway traffic control
KW  - Image enhancement
KW  - Motor transportation
KW  - Object detection
KW  - Street traffic control
KW  - Autonomous driving
KW  - Cross-domain
KW  - Different domains
KW  - Domain adaptation
KW  - Domain offset
KW  - Environment perceptions
KW  - Objects detection
KW  - Super-resolution domain classifier
KW  - Superresolution
KW  - Unsupervised domain adaptation
KW  - Supervised learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, L.
AU  - Jin, X.
AU  - Han, J.
AU  - Yao, J.
TI  - Pedestrian Re-Identification Algorithm Based on Unmanned Aerial Vehicle Imagery
PY  - 2025
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app15031256
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217526239&doi=10.3390%2fapp15031256&partnerID=40&md5=12a223dcf6ddd7d1785d0f2476dfc102
AB  - Pedestrian re-identification in complex scenarios is often hindered by challenges such as viewpoint diversity, background interference, and behavioral complexity, which traditional methods struggle to address effectively in wide-area surveillance. Unmanned Aerial Vehicles (UAVs) offer a promising solution to this problem due to their flexibility and extensive coverage capabilities. However, UAV aerial images introduce additional challenges, including significant viewpoint variations and the complexity of pedestrian behaviors. To address these issues, this paper proposes a Transformer-based model that integrates a multi-scale graph convolution network (MU-GCN) with a non-local attention mechanism to address these challenges. A MU-GCN enhances feature extraction by employing graph convolutional networks to improve feature representation after extracting detailed features at various scales through multi-scale convolution kernels. This strengthens the model’s focus on local information. Meanwhile, the non-local attention mechanism enhances the model’s capacity to capture global contextual information by modeling dependencies between distant regions in the image. This approach is better suited for the unique characteristics of UAV aerial imagery. Experimental results demonstrate that, compared to the baseline model, the proposed method achieves improvements of 9.5% in mean average precision (mAP) and 4.9% in Rank-1 accuracy, validating the effectiveness of the model. © 2025 by the authors.
KW  - aerial imagery
KW  - graph convolutional network
KW  - pedestrian re-identification
KW  - transformer
KW  - Image enhancement
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial imagery
KW  - Aerial vehicle
KW  - Attention mechanisms
KW  - Convolutional networks
KW  - Graph convolutional network
KW  - Identification algorithms
KW  - Multi-scales
KW  - Nonlocal
KW  - Pedestrian re-identification
KW  - Transformer
KW  - Aerial photography
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Yu, H.
AU  - Zhang, R.
AU  - Sun, H.
AU  - Cao, Z.
AU  - Yang, B.
AU  - Zhang, J.
AU  - Liu, G.
TI  - RADCI: A Synchronized Radar-RGBT Object Detecting-Tracking Dataset And A Benchmark
PY  - 2025
T2  - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings
DO  - 10.1109/ICASSP49660.2025.10890097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009586674&doi=10.1109%2fICASSP49660.2025.10890097&partnerID=40&md5=393a3c510e4033028d03a5c0c51c8eb7
AB  - High-quality perception is crucial in autonomous driving and monitoring systems, where millimeter-wave radar and infrared cameras play important roles due to their robustness and reliability under harsh conditions. Both technologies can serve as low-cost supplements to optical image detection, improving overall system robustness. However, there is currently a lack of widely applicable feature-level fusion methods and multimodal datasets to effectively integrate visible light with these two heterogeneous data types for multiple tasks. In this work, we collect a new multimodal dataset, RADCI8, which synchronizes data from a camera, an infrared camera, and a radar for target detection and tracking. The dataset includes 2D image annotations, radar RAD tensor data with distance, angle, and Doppler information, as well as target ID annotations in both data formats. In addition, to address the incomplete use of radar data in previous fusion algorithms, we propose a detection method that fuses image and radar features using feature concatenation and an attention mechanism. Our proposed algorithm achieves 51.5% AP with an IOU of 50:95 on 2D bounding box prediction, significantly improving average detection accuracy over vision-based methods and maintaining robustness even when a single sensor degrades. © 2025 IEEE.
KW  - Dataset
KW  - Multimodal
KW  - Radar Processing
KW  - Sensor Fusion
KW  - Benchmarking
KW  - Cameras
KW  - Feature extraction
KW  - Infrared devices
KW  - Infrared radiation
KW  - Object detection
KW  - Radar imaging
KW  - Radar target recognition
KW  - Radar tracking
KW  - Sensor data fusion
KW  - Target tracking
KW  - Tracking radar
KW  - Dataset
KW  - High quality
KW  - Infra-red cameras
KW  - Infrared cameras
KW  - Multi-modal
KW  - Multi-modal dataset
KW  - Object detecting
KW  - Quality perceptions
KW  - Radar processing
KW  - Sensor fusion
KW  - Geometrical optics
KW  - Image enhancement
KW  - Millimeter waves
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, P.
AU  - Luo, Y.
AU  - Zhu, Z.
TI  - FDI-YOLO: Feature disentanglement and interaction network based on YOLO for SAR object detection
PY  - 2025
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2024.125442
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205029086&doi=10.1016%2fj.eswa.2024.125442&partnerID=40&md5=d1358dae817be331fc97dea277711a64
AB  - Synthetic Aperture Radar (SAR) object detection is one of the key measures to ensure maritime traffic and safety. However, SAR images contain a large amount of speckle noise, which poses a challenge to traditional deep learning methods for feature extraction and processing. Therefore, we propose a YOLO-based feature disentanglement and interaction network for SAR object detection (FDI-YOLO). First, FDI-YOLO proposes a reversible cross stage partial network (RCSPNet) as the backbone. The RCSPNet uses reversible transformations to retain more complete feature information for feature extraction and decompose it into feature maps of different dimensions. Then, we propose a structure with cross-scale depth feature interaction (CDFI), which captures the local texture and global semantic information of the in-scale features using crossover frequency semantic perception (CFSP), and then strengthens the linking of the cross-scale features through bidirectional information interaction. Finally, we use an adaptive object detection head and a bounding box regression loss with a dynamic focusing mechanism to further improve the detection capability of FDI-YOLO for SAR images. We conducted experiments on three publicly available SAR datasets, SSDD, ISDD, and HRSID. On these datasets, we achieve F1 scores of 98.1%/88.6%/88.5%, AP50 scores of 98.7%/90.3%/90.9%, and AP50-95 scores of 71.0%/42.4%/64.3%, respectively. The experimental results show that FDI-YOLO is able to perform the task of SAR object detection well with less computational resources. © 2024 Elsevier Ltd
KW  - Attention mechanism
KW  - Object detection
KW  - Reversible column network
KW  - SAR
KW  - YOLOv8
KW  - Deep learning
KW  - Image enhancement
KW  - Object detection
KW  - Radar target recognition
KW  - Attention mechanisms
KW  - Features extraction
KW  - Interaction networks
KW  - Maritime safety
KW  - Network-based
KW  - Objects detection
KW  - Radar objects
KW  - Reversible column network
KW  - Synthetic aperture radar images
KW  - YOLOv8
KW  - Object recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, G.
AU  - Yao, S.
AU  - Zhou, Y.
AU  - Liu, D.
AU  - Chang, B.
TI  - Boundary-Guided Global–Local Feature Fusion Network for Polyp Segmentation
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3551578
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000367740&doi=10.1109%2fTIM.2025.3551578&partnerID=40&md5=972b45641fbd753a1be7c76965fe7f20
AB  - Colon polyp segmentation can assist physicians in screening colonoscopy images, which is crucial in preventing colorectal cancer. Due to the limited area occupied by small object polyp objects in images, there is a high risk of overlooking them, which makes it one of the more challenging aspects to address. Additionally, the current segmentation of small object polyps also faces difficulties such as boundary blurring, diverse lesion shapes, and uneven image brightness. While deep learning methods based on convolutional neural networks (CNNs) have been successfully applied to polyp segmentation tasks, three significant challenges persist: 1) limited ability to extract boundary information; 2) inadequate robustness capture of global context information; and 3) insufficient ability for integrating global and local information. To address the issues mentioned above, we propose the boundary-guided global–local feature fusion network for small polyp segmentation (BGGL-Net), with the following contributions: 1) the local information encoder (LIE) and boundary feature extraction module leverage convolutional blocks and Laplacian operators to mine feature boundaries and fine-grained detail features; 2) we design a global information fusion module to enhance the model’s representational capacity and acquire rich and accurate global information; and 3) boundary-guided module (BGM) using a cross-attention mechanism, capturing the inherent relationship between the feature and establishes long-range dependencies between global- and low-level features. We enhance boundary accuracy by employing local boundary features to guide the global features, facilitating the effective fusion of global and local information. In the experiment, we compared ten state-of-the-art (SOTA) networks with BGGL-Net. The BGGL-Net achieves the highest segmentation accuracy on small object polyp datasets. Concerning generalization performance, the BGGL-Net outperforms CaraNet by up to 12.4% in the mDice metric on the ETIS-LaribPolypDB* dataset. © 2025 IEEE. All rights reserved.
KW  - Convolutional neural networks (CNNs)
KW  - feature fusion
KW  - Laplacian operators
KW  - small-object polyp segmentation
KW  - Transformer
KW  - Data fusion
KW  - Deep neural networks
KW  - Image coding
KW  - Image enhancement
KW  - Image segmentation
KW  - Laplace transforms
KW  - Convolutional neural network
KW  - Features fusions
KW  - Global and local informations
KW  - Global-local
KW  - Laplacian operator
KW  - Local feature
KW  - Polyp segmentation
KW  - Small object polyp segmentation
KW  - Small objects
KW  - Transformer
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kiranmai, R.
AU  - Deeptha, R.
AU  - Purnachand, K.
TI  - A Review on Object Detection in Traffic Scenarios Using Deep Learning Models
PY  - 2025
T2  - Proceedings of International Conference on Visual Analytics and Data Visualization, ICVADV 2025
DO  - 10.1109/ICVADV63329.2025.10961569
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004409395&doi=10.1109%2fICVADV63329.2025.10961569&partnerID=40&md5=98ff0c00e33fd1a00a8f3d3827ba73fc
AB  - Intelligent vehicle visual perception technology can assist automated driving systems in traffic scenarios by helping to recognize complicated situations quickly and accurately, which is necessary for safe and collision-free driving. Deep learning (DL)-based object identification has been a major advancement in automated driving. A survey is done in this paper on object detection in traffic scenes. This article reviews 30 research works with varied contributions. This work reviews the elected databases and analyses the varied object detection models in each paper. It further reviews the performance of each work and analyzes its maximal performance as well. In addition, varied datasets adopted in each considered work are examined. Finally, research gaps in object detection in traffic scenes are determined, which should be undertaken in future.  © 2025 IEEE.
KW  - Accuracy
KW  - Computer vision
KW  - Intelligent vehicle
KW  - Object Detection
KW  - YOLO
KW  - Automatic identification
KW  - Object detection
KW  - Object recognition
KW  - Accuracy
KW  - Automated driving systems
KW  - Collision-free
KW  - Learning models
KW  - Object identification
KW  - Objects detection
KW  - Performance
KW  - Traffic scene
KW  - Visual perception
KW  - YOLO
KW  - Deep learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, Z.
AU  - Yang, K.
AU  - Wu, Y.
AU  - Yang, H.
AU  - Tang, X.
TI  - HCLT-YOLO: A Hybrid CNN and Lightweight Transformer Architecture for Object Detection in Complex Traffic Scenes
PY  - 2025
T2  - IEEE Transactions on Vehicular Technology
DO  - 10.1109/TVT.2024.3496513
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001061677&doi=10.1109%2fTVT.2024.3496513&partnerID=40&md5=c4305cda1ee05ea791339028e9ec5543
AB  - The swift and accurate detection of traffic signs in traffic scenes is a pivotal aspect of environmental perception technology in autonomous driving systems. Traffic signs provide essential road information and regulatory instructions, which are critical to ensuring road safety. This paper presents the HCLT-YOLO model to address the challenges of false alarms and missed detections in complex traffic environments. Specifically, we propose a novel hybrid CNN-transformer network architecture that efficiently integrates both local and global features, thereby improving traffic sign feature representation. To further enhance the modelâs sensitivity to small traffic signs, we optimize the structure by introducing a dedicated small-object detection layer through upsampling and by leveraging SIoU to improve detection accuracy and computational efficiency. However, the addition of the small object detection layer and the Transformer module increases the overall computational complexity and parameter count, potentially affecting real-time performance. To address this issue, we introduce the DG-C2f module, which employs linear transformations for feature mapping, streamlining the convolution process and enhancing real-time feasibility. Experimental evaluations on the GTSDB and TT100K datasets demonstrate that the proposed model improves detection accuracy by 2.5% and 6.8%, respectively, compared to YOLOv8s models. Notably, the detection accuracy for small traffic signs improved significantly, by 6.9% and 11.7%, respectively. Additionally, processor-in-the-loop experiments on the NVIDIA Jetson AGX Orin show that the model achieves an inference speed of 46 FPS, meeting the real-time requirements for in-vehicle applications. © 2024 IEEE.
KW  - Autonomous driving
KW  - deep learning
KW  - lightweight transformer
KW  - traffic sign detection
KW  - Image coding
KW  - Image compression
KW  - Image segmentation
KW  - Linear transformations
KW  - Autonomous driving
KW  - Deep learning
KW  - Detection accuracy
KW  - Driving systems
KW  - Environmental perceptions
KW  - Lightweight transformer
KW  - Objects detection
KW  - Small object detection
KW  - Traffic scene
KW  - Traffic sign detection
KW  - Traffic signs
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mao, G.
AU  - Wang, K.
AU  - Du, H.
AU  - Huang, B.
AU  - Ren, X.
AU  - Fu, T.
AU  - Zhang, Z.
TI  - SRS-YOLO: Improved YOLOv8-Based Smart Road Stud Detection
PY  - 2025
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2025.3545942
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000442475&doi=10.1109%2fTITS.2025.3545942&partnerID=40&md5=9a3615a7baff831cfcd146685ccca2bc
AB  - Smart road studs have been extensively deployed as road safety and data collection devices. Accurate and reliable detection of smart road studs and its further integration into the perception and control modules of connected and autonomous vehicles (CAVs) undoubtedly benefit road boundary detection, localization of CAVs and augument the safety of CAVs’ driving. This work investigates real-time, accurate and reliable detection of smart road studs, which is a challenging task for CAVs because existing methods fail to achieve accurate and real-time smart road stud detection, especially in harsh road environment. To address these challenges, we first build a real-world smart road stud dataset, and then propose and validate a lightweight and efficient smart road stud detection model based on the you only look once 8th version (YOLOv8), called SRS-YOLO. First, a Squeezeand-Excitation (SE) attention module is used to improve the coarse-to-fine (C2F) module to differentiate the channel importance of feature maps and improve the detection accuracy of smart road studs. Second, a novel downsampling module (DownS) that integrates the average pooling and the max pooling is designed to reduce the number of parameters and minimize information loss during the downsampling process. Third, the loss function is replaced with the Normalized Wasserstein Distance (NWD) loss to alleviate the sensitivity to location deviations when computing the loss for small targets. The experimental results demonstrate that the proposed SRS-YOLO outperforms other state-of-the-art methods, and achieves a 87.92% mean average precision at a real-time speed of 78 frames. © 2000-2011 IEEE.
KW  - attention mechanism
KW  - connected and autonomous vehicle
KW  - real-time detection system
KW  - smart road stud detection
KW  - SRS-YOLO
KW  - Image coding
KW  - Studs (structural members)
KW  - Attention mechanisms
KW  - Autonomous Vehicles
KW  - Connected and autonomous vehicle
KW  - Detection system
KW  - Real- time
KW  - Real-time detection
KW  - Real-time detection system
KW  - Reliable detection
KW  - Smart road stud detection
KW  - SRS-YOLO
KW  - Studs (fasteners)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kaixuan, L.
AU  - Xiaofeng, L.
AU  - Qiang, C.
AU  - Zejiang, Z.
TI  - YOLOv8-GAIS: improved object detection algorithm for UAV aerial photography
ST  - YOLOv8-GAIS：一种改进的无人机航拍目标检测算法
PY  - 2025
T2  - Guangdian Gongcheng/Opto-Electronic Engineering
DO  - 10.12086/oee.2025.240295
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004051009&doi=10.12086%2foee.2025.240295&partnerID=40&md5=2a6d108edf68c604405d4b5a33f198f3
AB  - To address the issue of complex backgrounds in dim scenes, which cause object edge blurring and obscure small objects, leading to misdetection and omission, an improved YOLOv8-GAIS algorithm is proposed. The FAMFF (four-head adaptive multi-dimensional feature fusion) strategy is designed to achieve spatial filtering of conflicting information. A small object detection head is incorporated to address the issue of large object scale variation in aerial views. The SEAM (spatially enhanced attention mechanism) is introduced to enhance the network's attention and capture ability for occluded parts in low illumination situations. The InnerSIoU loss function is adopted to emphasize the core regions, thereby improving the detection performance of occluded objects. Field scenes are collected to expand the VisDrone2021 dataset, and the Gamma and SAHI (slicing aided hyper inference) algorithms are applied for preprocessing. This helps balance the distribution of different object types in low-illumination scenarios, optimizing the model's generalization ability and detection accuracy. Comparative experiments show that the improved model reduces the number of parameters by 1.53 MB, and increases mAP50 by 6.9%, mAP50-95 by 5.6%, and model computation by 7.2 GFLOPs compared to the baseline model. In addition, field experiments were conducted in Dagu South Road, Jinnan District, Tianjin City, China, to determine the optimal altitude for image acquisition by UAVs. The results show that, at a flight altitude of 60 m, the model achieves the detection accuracy of 77.8% mAP50. © 2025 Chinese Academy of Sciences. All rights reserved.
KW  - four-head adaptive multi-dimensional feature fusion
KW  - low-brightness image
KW  - object detection
KW  - unmanned aerial vehicle
KW  - YOLOv8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gong, P.
AU  - Zheng, K.
AU  - Jiang, Y.
AU  - Zhao, H.
AU  - Liang, X.
AU  - Feng, Z.
AU  - Huang, W.
TI  - Spatial Orientation Relation Recognition for Water Surface Targets
PY  - 2025
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse13030482
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001112842&doi=10.3390%2fjmse13030482&partnerID=40&md5=3e762ab13b6fefec73847a240c2728fb
AB  - Recently, extensive research efforts have concentrated on comprehending the semantic features of images in the field of computer vision. In order to address the spatial orientation relations among water surface targets (WSTs) in an image, which is a fundamental semantic feature, this paper focused on the recognition of spatial orientation relations. We first developed the water surface target spatial orientation vector field (WST-SOVF) algorithm, a novel end-to-end methodology, to recognize these spatial orientation relations among WSTs in an image. The WST-SOVF algorithm encodes the spatial orientation relation into the learning framework of a new deep convolutional neural network model, which comprises two distinct branches: the T-branch and the S-branch, both designed for the spatial feature extraction. The T-branch employs keypoint estimation to identify central points and classify the WST categories, while the S-branch constructs a spatial orientation vector field between WSTs, where each pixel in the field encodes the spatial orientation angle between two separated WSTs and collectively determines the category of spatial orientation. A fusion module was also designed to integrate the spatial feature obtained from both branches, thereby generating a comprehensive triple list that provides not only all the WSTs and their spatial orientation relations, but also their associated confidence levels. We performed a comparative evaluation of our WST-SOVF algorithm based on Huawei’s “Typical Surface/Underwater Target Recognition” dataset and the results demonstrated the outstanding performance of WST-SOVF algorithm. © 2025 by the authors.
KW  - deep convolutional neural network
KW  - recognition
KW  - spatial orientation relation
KW  - spatial orientation vector field
KW  - water surface target
KW  - Deep neural networks
KW  - Image coding
KW  - Vectors
KW  - Water content
KW  - Convolutional neural network
KW  - Orientation relations
KW  - Orientation vector fields
KW  - Recognition
KW  - Spatial orientation relation
KW  - Spatial orientation vector field
KW  - Spatial orientations
KW  - Surface target
KW  - Water surface
KW  - Water surface target
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xie, B.
AU  - Li, H.
AU  - Luan, Z.
AU  - Lei, Z.
AU  - Li, X.
AU  - Li, Z.
TI  - Lightweight coal miners and manned vehicles detection model based on deep learning and model compression techniques: A case study of coal mines in Guizhou region
ST  - 基于深度学习和模型压缩技术的轻量级煤矿人车检测模型− 以贵州地区煤矿为例
PY  - 2025
T2  - Meitan Xuebao/Journal of the China Coal Society
DO  - 10.13225/j.cnki.jccs.2024.0459
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219428264&doi=10.13225%2fj.cnki.jccs.2024.0459&partnerID=40&md5=db531f7cbf2e68160dade33b7ab46fe4
AB  - Intelligent recognition of coal mine workers and manned vehicles (coal mine pedestrian-vehicles) is an important component of video surveillance systems and a key task in the development of coal mine intelligence. However, the detection scene of coal mine pedestrian-vehicles is complex, and deploying large pedestrian-vehicle detection models on limited computing devices is challenging. Balancing between model detection performance and efficiency poses many challenges. This paper proposes a lightweight coal mine pedestrian detection model based on deep learning and model compression techniques. Taking the coal mine video surveillance dataset in Guizhou region as an example. The model accurately and in real-time completes the task of detecting coal mine pedestrian-vehicles, achieving a balance between model detection performance and efficiency. Specifically, in the network model design phase, a lightweight detection model named FCW-YOLO is proposed based on YOLOv8s as the baseline. Faster-Block and coordinate attention are integrated into the feature extraction module of the network, designing a novel C2f-Faster-CA lightweight architecture to reduce redundant channels of the network while adaptively capturing global key information. Furthermore, the WIOU boundary regression loss function is employed to increase the model's focus on common quality samples, addressing issues such as regression errors caused by imbalanced training samples. In the model compression phase, the proposed FCW-YOLO model undergoes channel-level sparsity through a collaborative pruning algorithm, automatically identifying unimportant channels and reducing them, resulting in the FCWP-YOLO model, achieving secondary lightweight design of the coal mine pedestrian-vehicle detection model. Results on a self-built coal mine pedestrian-vehicle detection dataset show that the proposed model has parameters, computational load, and model size of 2.3 M, 4.0 GFLOPs, and 6.0 MB, respectively, achieving compression ratios of 4.9 times, 4.7 times, and 4.4 times compared to the baseline model. The average detection accuracy is 88.7%, an improvement of 1.1%, with a processing speed of only 5.6ms per image. Compared to various lightweight architectures and advanced detection models, this method demonstrates excellent accuracy, lower computational costs, and better real-time performance, providing a feasible coal mine pedestrian-vehicle detection method for resource-constrained coal mine scenarios, meeting the deployment requirements of coal mine video surveillance and enabling real-time alerts for intelligent inspection of coal mine pedestrian-vehicles. © 2025 China Coal Society. All rights reserved.
KW  - coal manned vehicles detection
KW  - coal miners detection
KW  - deep learning
KW  - lightweight architecture
KW  - model compression
KW  - Chemical sensors
KW  - Data compression ratio
KW  - Image coding
KW  - Image compression
KW  - Image enhancement
KW  - Image segmentation
KW  - Miners
KW  - Optical flows
KW  - Partial pressure sensors
KW  - Photointerpretation
KW  - Population dynamics
KW  - Pressure sensors
KW  - Special effects
KW  - Temperature sensors
KW  - Coal manned vehicle detection
KW  - Coal miner detection
KW  - Coal miners
KW  - Deep learning
KW  - Detection models
KW  - Lightweight architecture
KW  - Manned vehicles
KW  - Model compression
KW  - Model-based OPC
KW  - Vehicles detection
KW  - Coal mines
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, X.
AU  - Teng, Q.
AU  - Yang, H.
AU  - He, X.
AU  - Qing, L.
AU  - Wang, P.
AU  - Chen, H.
TI  - CRKD-YOLO: Cross-Resolution Knowledge Distillation for Low-Resolution Remote Sensing Image Object Detection
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3559616
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003700202&doi=10.1109%2fTIM.2025.3559616&partnerID=40&md5=6c3aca5bd0f68faffbf54ebca068af6e
AB  - The majority of advanced remote sensing object detection technologies excel in accurately detecting objects from high-resolution images. However, in practical scenarios, it is often necessary to detect objects in images of varying resolutions due to differences in imaging equipment. When dealing with lower-resolution images, the limited detailed information and blurry boundaries lead to a noticeable decrease in detection accuracy. To address this problem, we propose an efficient object detection method for low-resolution remote sensing images based on the YOLO detector, named CRKD-YOLO. The method constructs a cross-resolution knowledge distillation (CRKD) framework to resolve the issue of feature mismatch, enabling the model with low-resolution inputs to learn more refined feature representations from high-resolution images. Furthermore, to effectively leverage the limited detailed information in low-resolution images, we propose the backbone augment feature pyramid network (BAFPN). It enhances detection accuracy for low-resolution remote sensing images while making the model more lightweight. Massive experiments on DOTA, DIOR, NWPU VHR-10, DroneVehicle, and VEDAI demonstrate that our CRKD-YOLO achieves significant improvements, even achieving higher accuracy compare to training and testing high-resolution images with baseline. © 1963-2012 IEEE.
KW  - Cross-resolution knowledge distillation (CRKD)
KW  - feature enhancement
KW  - object detection
KW  - remote sensing images
KW  - Distillation equipment
KW  - Optical remote sensing
KW  - Photographic equipment
KW  - Cross-resolution knowledge distillation
KW  - Detection accuracy
KW  - Feature enhancement
KW  - High-resolution images
KW  - Image object detection
KW  - Low resolution images
KW  - Lower resolution
KW  - Objects detection
KW  - Remote sensing images
KW  - Remote-sensing
KW  - Proximity sensors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Vincze, Z.
AU  - Rovid, A.
TI  - Aggregated Time Series Features in a Voxel-Based Network Architecture
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3535151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216850264&doi=10.1109%2fACCESS.2025.3535151&partnerID=40&md5=0afa597b1c8e653eb77ed0d90f90c91b
AB  - Using point cloud sequences is a popular way to harness the additional information represented in the time domain in order to enhance the performance of 3D object detector neural networks. However, it is not trivial to decide which abstraction level should the additional information presented to the network, or what is the point in the architecture, where aggregating the additional information is most beneficial. In this article, the authors propose various voxel-based networks and analyze their performance in relation to the abstraction level of the time series data. During the evaluation, the authors examine the object detection performance of a popular voxel-based neural network with its original architecture and several variants where the time domain related features were propagated through the network and aggregated at different stages of processing. Based on the evaluation results, a conclusion is drawn regarding the abstraction level at which the time-series aggregation step is performed in order to improve the performance of the baseline voxel-based detector. © 2025 The Authors.
KW  - LiDAR point cloud
KW  - Neural networks
KW  - object detection
KW  - point cloud sequence
KW  - time series
KW  - Cloud platforms
KW  - Abstraction level
KW  - LiDAR point cloud
KW  - Neural-networks
KW  - Objects detection
KW  - Performance
KW  - Point cloud sequence
KW  - Point-clouds
KW  - Time domain
KW  - Times series
KW  - Cloud computing architecture
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rehman, A.U.
AU  - Jiao, W.
AU  - Jiang, Y.
AU  - Wei, J.
AU  - Sohaib, M.
AU  - Sun, J.
AU  - E, S.
AU  - Rehman, K.U.
AU  - Chi, Y.
TI  - Deep learning in industrial machinery: A critical review of bearing fault classification methods
PY  - 2025
T2  - Applied Soft Computing
DO  - 10.1016/j.asoc.2025.112785
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216766102&doi=10.1016%2fj.asoc.2025.112785&partnerID=40&md5=ae7437e2d684a284ae3bd68d9ac404f7
AB  - The review provides an overview of the state-of-the-art in Deep Learning (DL) algorithms for rolling bearing fault classification which remains vital in industrial sectors including transportation, energy, manufacturing, and so forth. Even though they experience a variety of faults, rolling bearings are very crucial in ensuring machine efficiency. This prompts the review of the DL application, which is continuously growing, for intelligently detecting these faults. It comprehensively analyses DL models including Convolutional Neural Networks (CNNs), Auto-Encoders (AEs), Deep Belief Neural Networks (DBNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), and some advanced networks i.e., Transfer Learning (TL), Transformer Neural Network (TNN), Self-Supervised Learning (SSL), Federated Learning (FL), Meta-learning and Interpreting Neural Networks assessing their effectiveness and limitations in fault classification. Thus, the current review is unique among available literature at present since it bridges this crucial gap by including all forms of advanced networks and gives an insight into the potential and challenges. Besides, it emphasizes the importance of different sensing techniques and key datasets in the field to show the contribution towards advancements of DL applications. Finally, referring to current challenges and recommendations for future research directions encompassing environmental adaption, sensor deployment, data preprocessing, model training enhancements, algorithm selection, classifier development, and systematic documentation frame the conclusive part of the paper. This review will serve as an important source for diligent researchers in legitimacy approaches of machinery reliability improvement by means of DL-based techniques for rolling bearing fault classification. © 2025 Elsevier B.V.
KW  - Deep learning algorithms
KW  - Fault classification methods
KW  - Industrial machinery
KW  - Rolling Bearing
KW  - Convolutional neural networks
KW  - Recurrent neural networks
KW  - Roller bearings
KW  - 'current
KW  - Advanced networks
KW  - Bearing fault
KW  - Classification methods
KW  - Deep learning algorithm
KW  - Fault classification
KW  - Fault classification method
KW  - Industrial machinery
KW  - Neural-networks
KW  - Rolling bearings
KW  - Self-supervised learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Qian, Y.
AU  - Liu, Y.
AU  - Zhang, Z.
AU  - Dang, J.
TI  - Traffic lights detection based on enhanced YOLOv5
PY  - 2025
T2  - Proceedings of 2024 8th International Conference on Electronic Information Technology and Computer Engineering, EITCE 2024
DO  - 10.1145/3711129.3711192
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001862238&doi=10.1145%2f3711129.3711192&partnerID=40&md5=d0b02de91f9ab75b185e1c2652a1856e
AB  - This study introduces a refined YOLOv5 model aimed at overcoming challenges associated with the detection of diminutive traffic signal lights in intricate environmental settings. The Normalized Wasserstein Distance (NWD) metric is employed to counteract the Intersection over Union (IoU) metric’s susceptibility to positional discrepancies in the detection of small targets. An Efficient Multi-scale Attention (EMA) module is integrated to boost the model’s comprehension of pixel-level interactions and overall contextual information. The implementation of the Dynamic Head (DyHead) enhances the representational capacity of the object detection head. These optimizations enhance the model’s capability to capture complex features across different hierarchical levels and spatial locations. The model is trained on the S2TLD dataset, and the issue of class imbalance is addressed by data augmentation. The experimental findings indicate that the introduced algorithm markedly enhances the model’s capacity to identify small traffic lights within complex environments, resulting in a 1.7% improvement in mAP@0.5 over the baseline algorithm. © 2024 Copyright held by the owner/author(s).
KW  - Deep learning
KW  - Object detection
KW  - Traffic lights
KW  - YOLOv5
KW  - Deep learning
KW  - Object detection
KW  - Object recognition
KW  - Deep learning
KW  - Distance metrics
KW  - Light detection
KW  - Multi-scales
KW  - Objects detection
KW  - Signal light
KW  - Small targets
KW  - Traffic light
KW  - Wasserstein distance
KW  - YOLOv5
KW  - Traffic signals
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Raina, S.
AU  - Challagundla, J.
AU  - Acharya, S.
AU  - Singh, M.
TI  - Improving Small Object Detection with Area-Scaled and Dynamic Focal Loss
PY  - 2025
T2  - 2025 IEEE 15th Annual Computing and Communication Workshop and Conference, CCWC 2025
DO  - 10.1109/CCWC62904.2025.10903720
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001129668&doi=10.1109%2fCCWC62904.2025.10903720&partnerID=40&md5=ce6046136f6ca60ff8eb2fe4e85d7d0f
AB  - Detecting small objects remains a persistent challenge for state-of-the-art object detection models, which excel at identifying larger objects but struggle with smaller ones due to limited pixel representation and susceptibility to noise and occlusion. This paper introduces two innovative loss-scaling strategies to address these challenges: Inverse Quadratic Scaling, which adjusts loss inversely proportional to the square root of object area, and Dynamic Focal Loss, which incorporates object size into the focal loss framework. These methods aim to improve training dynamics for small objects while preserving performance for larger objects. We evaluate our strategies on the CPPES dataset using a ResNet-50-based DETR model, characterized by significant object size variations. Experimental results demonstrate substantial improvements in small object detection, achieving F1 score increases of up to 274% in some categories compared to the baseline. Notably, Inverse Quadratic Scaling excels in enhancing underrepresented objects like goggles, while Dynamic Focal Loss offers balanced performance across sizes. These findings underscore the potential of tailored loss functions to mitigate small object detection challenges and provide a foundation for real-world applications such as medical imaging, autonomous vehicles, and surveillance systems. © 2025 IEEE.
KW  - deep learning
KW  - DETR
KW  - loss scaling
KW  - object detection
KW  - ResNet
KW  - small objects
KW  - Goggles
KW  - Inverse problems
KW  - Deep learning
KW  - DETR
KW  - Excel
KW  - Loss scaling
KW  - Object size
KW  - Objects detection
KW  - Performance
KW  - Scalings
KW  - Small object detection
KW  - Small objects
KW  - Medical imaging
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shao, Z.
AU  - Wang, H.
AU  - Cai, Y.
AU  - Chen, L.
AU  - Li, Y.
TI  - UA-Fusion: Uncertainty-Aware Multimodal Data Fusion Framework for 3-D Object Detection of Autonomous Vehicles
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3548184
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001087204&doi=10.1109%2fTIM.2025.3548184&partnerID=40&md5=ea7b2e7a34af25a2889154e7b6c0d0bb
AB  - In dynamic traffic scenarios, uncertainties such as occlusions, small objects, as well as unpredictable adverse weather conditions, prevent current environment perception methods from achieving the necessary levels of accuracy, reliability, and safety. In this article, an uncertainty-aware multimodal data fusion framework named UA-Fusion is proposed for 3-D multiobject detection (3D MOD). This framework aims to improve comprehensive and reliable perception capabilities in uncertainty-aware scenarios. Specifically, an uncertainty-aware fusion (UAF) decoder based on a probabilistic cross-modal attention mechanism (PCAM) is presented. This strategy explicitly models and leverages uncertainties in object prediction. It also allows for adaptive and complementary fusion of multimodal data, thereby addressing both aleatoric and epistemic uncertainty. Furthermore, an uncertainty-reduced object query initialization (UOQI) strategy is proposed. This approach fully utilizes the advantages of 2-D object detection in identifying small objects as priors to generate high-quality 3-D queries. Finally, a robustness optimization strategy for training based on query denoising (QD-ROST) is proposed to improve training robustness and convergence in the presence of uncertainty factors. Extensive experiments are conducted on the real-world dataset nuScenes. Notably, UA-Fusion effectively addresses challenges related to uncertainty. Additionally, experiments on the Argoverse 2 (AV2) and RADIATE datasets further validate the generalizability and effectiveness of the proposed method. © 1963-2012 IEEE.
KW  - 3-D object detection
KW  - autonomous vehicle (AV)
KW  - camera
KW  - deep learning
KW  - LiDAR
KW  - multimodal data fusion
KW  - Deep learning
KW  - Unmanned aerial vehicles (UAV)
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - Dynamic traffic
KW  - LiDAR
KW  - Multimodal data fusion
KW  - Objects detection
KW  - Small objects
KW  - Uncertainty
KW  - Data fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, Y.
AU  - Yang, D.
AU  - Song, T.
AU  - Ye, Y.
AU  - Zhang, X.
TI  - YOLO-SSP: an object detection model based on pyramid spatial attention and improved downsampling strategy for remote sensing images
PY  - 2025
T2  - Visual Computer
DO  - 10.1007/s00371-024-03434-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193788810&doi=10.1007%2fs00371-024-03434-y&partnerID=40&md5=8c330c96f199ac557fa77ca71746eb8e
AB  - Object detection is an essential task in remote sensing image processing. However, the remote sensing images are characterized by large range of object sizes and complex object backgrounds, which results in challenges in the object detection task. Moreover, the detection effect of existing object detectors on remote sensing images is still not satisfactory. In order to tackle the above problems, an object detection model named YOLO-SSP for remote sensing images is proposed based on the YOLOv8m model in this paper. To begin with, the original downsampling layers are replaced with the proposed lightweight SPD-Conv module, which performs downsampling without loss of fine-grained information and improves the ability of the network to learn the feature representation. In addition, to adapt the large number of small objects in remote sensing images, a small object detection layer is added and achieves the expected results. Finally, a pyramid spatial attention mechanism is proposed to obtain the weights of different spatial positions through hierarchical pooling operations. It effectively improves the detection performance of small objects and those with complex backgrounds. We conducted ablation experiments on the DIOR dataset and compared the YOLO-SSP model with other state-of-the-art models. YOLO-SSP obtains 64.7% of mAP, which is an improvement of 2.3% relative to the baseline model. To demonstrate the generalizability and robustness of the improved model, the comparison experiments are also performed on the TGRS-HRRSD dataset and SIMD dataset with mAP of 77.2 and 64.9%, respectively. The code will be available at https://github.com/YongliLiu/SSP. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Attention mechanism
KW  - Object detection
KW  - Remote sensing images
KW  - Small object
KW  - Complex networks
KW  - Image enhancement
KW  - Object recognition
KW  - Remote sensing
KW  - Signal sampling
KW  - Attention mechanisms
KW  - Detection models
KW  - Down sampling
KW  - Model-based OPC
KW  - Object size
KW  - Objects detection
KW  - Remote sensing image processing
KW  - Remote sensing images
KW  - Small objects
KW  - Spatial attention
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sivanandham, S.
AU  - Gunaseelan, D.
TI  - Enhanced YOLOv5s Model for Improved Multi-Sized Object Detection in Road Scenes
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3582136
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009285398&doi=10.1109%2fACCESS.2025.3582136&partnerID=40&md5=c6047e0c9fd31c2ac1f15a9e3921a83a
AB  - Detecting objects in complex driving environments is crucial for autonomous vehicles to navigate safely. However, this task becomes challenging when addressing scale variations, occlusions and diverse backgrounds. This paper proposes an enhanced YOLOv5s model for handling varying object sizes from small pedestrians and traffic signs to larger vehicles in road scenes. The proposed enhancement begins by refining the default anchor boxes using the percentile-based quantile method on the distribution of the bounding boxes and the adjustments to the convolution layers for enhanced feature extraction. Smaller kernel sizes and fewer channels are employed in the initial layers to capture fine-grained details, while in deeper layers, the number of channels is progressively increased to capture broader information that better represents larger objects. Furthermore, an efficient channel attention (ECA) mechanism is integrated into the backbone to prioritize key feature channels, thereby enhancing the model’s ability to detect overlapping and small objects. To improve the feature fusion process, a Multi-scale BiFPN block is integrated into the neck of the model. This combines fine-grained spatial details from the shallow layers with more abstract semantic information from deeper layers, enabling the detection of objects across varying scales. Experimental evaluations carried out on the IDD dataset reveal that the enhanced YOLOv5s model achieves a significant gain in prediction accuracy when compared with the original YOLOv5s. To mitigate the effect of class imbalance and improve generalization across varying object sizes, CutMix data augmentation is employed during training. It shows a 48% increase in mean average precision (mAP@0.5) and a 44% and 49% rise in precision and recall, respectively, with an inference time of 14.6ms compared to the baseline model. These improvements underscore the effectiveness of the proposed enhancements in addressing the challenges of detecting multi-sized objects in complex road environments. © 2013 IEEE.
KW  - Anchor box refinement
KW  - attention mechanism
KW  - CutMix data augmentation
KW  - feature fusion
KW  - multi-sized object detection
KW  - road scenes
KW  - YOLOv5
KW  - Automobile drivers
KW  - Feature Selection
KW  - Object recognition
KW  - Road vehicles
KW  - Roads and streets
KW  - Semantics
KW  - Traffic signs
KW  - Anchor box refinement
KW  - Anchor-box
KW  - Attention mechanisms
KW  - Cutmix data augmentation
KW  - Data augmentation
KW  - Features fusions
KW  - Multi-sized object detection
KW  - Objects detection
KW  - Road scene
KW  - YOLOv5
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Guo, H.
AU  - He, G.
AU  - Fang, M.
AU  - Xie, S.
AU  - Ge, J.
TI  - RDW-YOLOv8n: a deeply focused algorithm for small object traffic sign detection under complex weather conditions
PY  - 2025
T2  - Journal of Electronic Imaging
DO  - 10.1117/1.JEI.34.2.023030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005305948&doi=10.1117%2f1.JEI.34.2.023030&partnerID=40&md5=472df3a851908adf7393ae5343b310df
AB  - In autonomous driving technology, accurate detection of traffic signs is crucial for vehicles to perceive their surrounding road conditions. However, the detection of small traffic signs is often affected by changes in lighting and obstruction from obstacles under complex weather conditions, resulting in inaccurate localization and missed detections. To address these challenges, we introduce a new optimized algorithm - RDW-YOLOv8n. We utilize an enhanced small object layer integrated with the ResCBAM module to replace the original large object layer. Unlike traditional methods of introducing small object layers, this method improves detection accuracy by enhancing the network's perception of small objects through residual attention while effectively addressing the issue of missed detections present in the original algorithm and significantly reducing the number of model parameters. In addition, lightweight depthwise convolution (DWConv) is introduced to further streamline the model without sacrificing accuracy. Finally, the original loss function is replaced with the WIoU loss function to enable the model to focus on the average quality of anchor boxes, balancing the learning of high- and low-quality samples. Experiments conducted on the CCTSDB 2021 dataset show that the RDW-YOLOv8n algorithm achieves a mean average precision (mAP50) of 84.7% and a recall rate (R) of 75.3%, which represent improvements of 6.9 and 5.2 percentage points over the baseline model, respectively. Meanwhile, the model parameter count is reduced by 68.11%, and the F1 score increases by 3.1%. These results effectively demonstrate the detection performance of this algorithm under complex weather. When compared with recent advanced algorithms, RDW-YOLOv8n achieves the highest mAP50 with the smallest number of parameters, highlighting the strong competitiveness and superiority of this algorithm. © 2025 SPIE and IS&T.
KW  - complex weather
KW  - enhanced small object layer
KW  - small object detection
KW  - traffic sign
KW  - YOLOv8n
KW  - Image segmentation
KW  - Variable message signs
KW  - Vehicle detection
KW  - Complex weather
KW  - Condition
KW  - Enhanced small object layer
KW  - Loss functions
KW  - Missed detections
KW  - Modeling parameters
KW  - Small object detection
KW  - Small objects
KW  - Traffic sign detection
KW  - YOLOv8n
KW  - Road and street markings
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, Y.
AU  - Wang, F.
TI  - D-TLDetector: Advancing Traffic Light Detection With a Lightweight Deep Learning Model
PY  - 2025
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2024.3522195
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001060297&doi=10.1109%2fTITS.2024.3522195&partnerID=40&md5=6bf09ab731f8fc6f3350d44faa4f98e3
AB  - Traffic signal light detection poses significant challenges in the intelligent driving sector, with high precision and efficiency being crucial for system safety. Advances in deep learning have led to significant improvements in image object detection. However, existing methods continue to struggle with balancing detection speed and accuracy. We propose a lightweight model for traffic light detection that uses a streamlined backbone network and a Low-GD neck architecture. The model's backbone employs structured reparameterization and lightweight Vision Transformers, using multi-branch and Feed-Forward Network structures to boost informational richness and positional awareness, respectively. The Neck network utilizes the Low-GD structure to enhance the aggregation and integration of multi-scale features, reducing information loss during cross-layer exchanges. We introduce a data augmentation strategy using Stable Diffusion to expand our traffic light dataset in complex weather conditions like fog, rain, and snow, improving model generalization. Our method excels on the YCTL2024 traffic light dataset, achieving a detection speed of 135 FPS and 98.23% accuracy, with only 1.3M model parameters. Testing on the Bosch Small Traffic Lights Dataset confirms the method's strong generalization capabilities. This suggests that our proposed method can effectively provide accurate and real-time traffic light detection. © 2000-2011 IEEE.
KW  - deep learning
KW  - Intelligent transportation system
KW  - lightweight network
KW  - stable diffusion
KW  - traffic light detection
KW  - Infrared absorption
KW  - Intelligent systems
KW  - Laser beams
KW  - Motor transportation
KW  - Deep learning
KW  - Detection speed
KW  - Intelligent transportation systems
KW  - Learning models
KW  - Light detection
KW  - Lightweight network
KW  - Signal light
KW  - Stable diffusion
KW  - Traffic light
KW  - Traffic light detection
KW  - Traffic signals
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yuan, C.
AU  - Liu, J.
AU  - Wang, H.
AU  - Yang, Q.
TI  - Object Detection in Complex Traffic Scenes Based on Environmental Perception Attention and Three-Scale Feature Fusion
PY  - 2025
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app15063163
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000878142&doi=10.3390%2fapp15063163&partnerID=40&md5=2b7a27068a3b6cc2ce4330c66d86b378
AB  - With the recent advancements in automated driving technology, object detection algorithms that can effectively respond to complex and diverse road traffic scenarios are especially important for driving safety during real driving. In this context, we conduct an in-depth study on object detection algorithms for diverse scenarios in autonomous driving. For diverse and changing backgrounds and multi-scale targets, we propose environmental perception attention (EPA) and the three-scale fusion module (TSFM) to improve the accuracy of object detection algorithms in complex traffic scenes. Environmental perception attention effectively improves the model’s ability to perceive the object by modeling long-range information and inter-channel relationships to direct the model’s attention to important task-related regions and important features in the image. The three-scale fusion module mixes features from different scales while introducing low-level feature map information, enabling the model to take into account the features of objects at different scales. In our experiments, we apply the proposed method to the YOLOv8 model for validation. The results show that compared to the performance of the baseline model on the BDD100K automated driving domain dataset with diverse and complex backgrounds, the mAP@0.5 metric of the improved model is increased by 1.3%. This makes the YOLOv8 more accurate and effective for the detection of different objects in the scenario, and it can better adapt to the different traffic scenarios and environmental changes. © 2025 by the authors.
KW  - environmental perception attention
KW  - three-scale fusion module
KW  - traffic scene target detection
KW  - Automated driving
KW  - Environmental perception attention
KW  - Environmental perceptions
KW  - Fusion modules
KW  - Object detection algorithms
KW  - Objects detection
KW  - Targets detection
KW  - Three-scale fusion module
KW  - Traffic scene
KW  - Traffic scene target detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, Y.
AU  - Cui, S.
AU  - Xiang, X.
AU  - Bai, Y.
AU  - Zang, L.
AU  - Ding, H.
TI  - Research on Improved YOLOv7 for Traffic Obstacle Detection
PY  - 2025
T2  - World Electric Vehicle Journal
DO  - 10.3390/wevj16010001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215765210&doi=10.3390%2fwevj16010001&partnerID=40&md5=598024a7cabedc8273e269b7420780a2
AB  - Object detection and recognition algorithms are widely used in applications such as real-time monitoring and autonomous driving. However, there is limited research on traffic obstacle detection in complex scenarios involving road construction and sudden accidents. This gap results in low accuracy and difficulties in recognizing occluded targets, thereby hindering the further development and widespread adoption of intelligent transportation systems. To address these issues, this paper proposes an improved algorithm based on YOLOv7, incorporating a lightweight coordinate attention mechanism to focus on small objects at long distances and capture target location information. The use of a high receptive field enhances the feature hierarchy within the detection network. Additionally, we introduce the focal efficient intersection over union loss function to address sample imbalance, which accelerates the model’s convergence speed, reduces loss values, and improves overall model stability. Our model achieved a detection accuracy of 98.1%, reflecting a 1.4% increase, while also enhancing detection speed and minimizing missed detections. These advancements significantly bolster the model’s performance, demonstrating advantages for real-world applications. © 2024 by the authors.
KW  - attention mechanism
KW  - intelligent transportation
KW  - loss function
KW  - object detection
KW  - traffic obstacles
KW  - YOLOv7
KW  - Feature extraction
KW  - Highway accidents
KW  - Intelligent systems
KW  - Motor transportation
KW  - Object detection
KW  - Object recognition
KW  - Attention mechanisms
KW  - Intelligent transportation
KW  - Loss functions
KW  - Object detection algorithms
KW  - Object detection and recognition
KW  - Object recognition algorithm
KW  - Objects detection
KW  - Obstacles detection
KW  - Traffic obstacle
KW  - YOLOv7
KW  - Obstacle detectors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, Y.
AU  - Shi, Y.
TI  - VRU-YOLO: A Small Object Detection Algorithm for Vulnerable Road Users in Complex Scenes
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3534321
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216978355&doi=10.1109%2fACCESS.2025.3534321&partnerID=40&md5=c5eff3e28c50aa7788b6df9b71fe8468
AB  - Accurate detection of vulnerable road users (VRUs) is critical for enhancing traffic safety and advancing autonomous driving systems. However, due to their small size and unpredictable movements, existing detection methods struggle to provide stable and accurate results under real-Time conditions. To overcome these challenges, this paper proposes an improved VRU detection algorithm based on YOLOv8, named VRU-YOLO. First, we redesign the neck structure and construct a Detail Enhancement Feature Pyramid Network (DEFPN) to enhance the extraction and fusion capabilities of small target features. Second, the YOLOv8 network's Spatial Pyramid Pooling Fast (SPPF) module is replaced with a novel Feature Pyramid Convolution Fast (FPCF) module based on dilated convolution, effectively mitigating feature loss in small target processing. Additionally, a lightweight Optimized Shared Detection Head (OSDH-Head) is introduced, reducing computational complexity while improving detection efficiency. Finally, to alleviate the deficiencies of traditional loss functions in shape matching and computational efficiency, we propose the Wise-Powerful Intersection over Union (WPIoU) loss function, which further optimizes the regression of target bounding boxes. Experimental results on a custom-built multi-source VRU dataset show that the proposed model enhances precision, recall, mAP50, and mAP50:95 by 1.3%, 3.4%, 3.3%, and 1.8%, respectively, in comparison to the baseline model. Moreover, in a generalization test conducted on the remote sensing small target dataset VisDrone2019, the VRU-YOLO model achieved an mAP50 of 31%. This study demonstrates that the improved model offers more efficient performance in small object detection scenarios, making it well-suited for VRU detection in complex road environments.  © 2013 IEEE.
KW  - feature fusion
KW  - shared convolution
KW  - small object detection
KW  - Vulnerable road users
KW  - YOLOv8
KW  - Motor transportation
KW  - Remote sensing
KW  - Feature pyramid
KW  - Features fusions
KW  - Loss functions
KW  - Road users
KW  - Shared convolution
KW  - Small object detection
KW  - Small targets
KW  - User detection
KW  - Vulnerable road user
KW  - YOLOv8
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, W.
AU  - Chen, M.
AU  - Zhang, L.
AU  - Tian, J.
TI  - SR-NET: a lightweight enhanced feature extraction network for fabric defect detection
PY  - 2025
T2  - Textile Research Journal 
DO  - 10.1177/00405175241293349
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007601583&doi=10.1177%2f00405175241293349&partnerID=40&md5=6bd3f0d45e39e2a52be7bda3209c97c1
AB  - In the textile industry, detecting defects in fabrics featuring intricate patterns and small imperfections poses a persistent challenge. There is an urgent need for defect detection systems characterized by high accuracy and ease of deployment. To address these needs, a fabric defect detection algorithm, SR-NET, has been proposed in this paper, featuring lightweight and enhanced multiple feature extraction capabilities. The Spatial Receptive-field Convolution (SRConv) module was introduced as a convolutional module, effectively optimizing the performance of the convolutional neural network and improving overall model detection performance. The Region-Semantic Residual Module (RSR) module was proposed as an attention mechanism, which was fused with the C2f module to form the C2f_RSR module. This enhancement significantly enhanced the modeling ability to extract features pertinent to small defects. To reduce the sensitivity of the model to deviations in the position of small objects, The Gaussian Wasserstein Distance (GWD) metric was introduced into LGWD as a loss function. This adjustment has demonstrated outstanding performance in small target detection. Experimental results demonstrated that SR-NET enhanced mAP by 5.6% compared with baseline model. Simultaneously, SR-NET kept parameters and FLOPs almost unchanged. SR-NET proved to be well suited for practical production applications, and is able to meet real-time detection requirements. © The Author(s) 2025.
KW  - computer vision
KW  - deep learning
KW  - defect detection
KW  - Fabric defect
KW  - feature extraction
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Fabrics
KW  - Feature Selection
KW  - Leak detection
KW  - Machine vision
KW  - Deep learning
KW  - Defect detection
KW  - Defect-detection systems
KW  - Detecting defects
KW  - Fabric defect detection
KW  - Fabric defects
KW  - Features extraction
KW  - Gaussians
KW  - Performance
KW  - Wasserstein distance
KW  - Textile industry
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, Y.
TI  - Fusion-based modeling of an intelligent algorithm for enhanced object detection using a Deep Learning Approach on radar and camera data
PY  - 2025
T2  - Information Fusion
DO  - 10.1016/j.inffus.2024.102647
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202055723&doi=10.1016%2fj.inffus.2024.102647&partnerID=40&md5=eb8f028df4e89a75991504872ddebd3f
AB  - Object detection, the process of detecting and classifying objects within a given environment, forms the foundational element. Multisensory fusion incorporates data from diverse sensors, like radar and cameras, to refine the reliability and accuracy of detection. Further, Radar and camera data fusion refine this process by integrating the unique strength of both technologies, which leverage the radar's proficiency in adverse weather conditions and the camera's high-resolution imaging. This incorporation enhances the object detection systems, which enables them to effectively operate across the spectrum of scenarios, from autonomous vehicles navigating challenging weather to surveillance systems monitoring critical infrastructure. Deep learning (DL), a branch of machine learning (ML), empowers this system with the capability to learn complex representations and patterns directly from the data, which enables them to generalize and adapt to new situations. By integrating the advanced methodology, we can develop strong perception system capable of interpreting and detecting objects accurately in dynamic and diverse environments, from autonomous vehicles navigating urban landscapes to surveillance systems monitoring complex environments. This study designs an Intelligent Algorithm for Enhanced Object Detection Using Deep Learning Approach on the Radar and Camera Data Fusion (IAEOD-DLRCDF) technique. The presented IAEOD-DLRCDF technique uses multi-angle joint calibration where the spatial sparse alignment of the heterogeneous data of the camera and Radar is realized with image falsification disregarded. Besides, the IAEOD-DLRCDF technique applies YOLOv8 object detector for radar and camera target detection individually which are then integrated with the image plane. Moreover, the detected objects are then classified via the bidirectional long short-term memory (BiLSTM) model. Furthermore, the Adam optimizer is used for the optimum hyperparameter selection of the BiLSTM network which results in a better recognition rate. The performance assessment of the IAEOD-DLRCDF method is tested under benchmark dataset. The empirical analysis stated that the IAEOD-DLRCDF method gains better performance over other models. © 2024
KW  - Adam optimizer
KW  - Data fusion
KW  - Deep learning
KW  - Machine learning
KW  - Object detection
KW  - Radar
KW  - YOLOv8
KW  - Aircraft detection
KW  - Critical infrastructures
KW  - Sensor data fusion
KW  - Adam optimizer
KW  - Autonomous Vehicles
KW  - Data fusion technique
KW  - Deep learning
KW  - Intelligent Algorithms
KW  - Learning approach
KW  - Machine-learning
KW  - Objects detection
KW  - Optimizers
KW  - YOLOv8
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Aljagoub, D.
AU  - Na, R.
AU  - Cheng, C.
TI  - Delamination detection in concrete decks using numerical simulation and UAV-based infrared thermography with deep learning
PY  - 2025
T2  - Automation in Construction
DO  - 10.1016/j.autcon.2024.105940
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212427296&doi=10.1016%2fj.autcon.2024.105940&partnerID=40&md5=bf792a8be9effcd1250bd50135dc6c8b
AB  - The potential of concrete bridge delamination detection using infrared thermography (IRT) has grown with technological advancements. However, most current studies require an external input (subjective threshold), reducing the detection's objectivity and accuracy. Deep learning enables automation and streamlines data processing, potentially enhancing accuracy. Yet, data scarcity poses a challenge to deep learning applications, hindering their performance. This paper aims to develop a deep learning approach using supervised learning object detection models with extended data from real and simulated images. The numerical simulation image supplementation seeks to eliminate the limited data barrier by creating a comprehensive dataset, potentially improving model performance and robustness. Mask R-CNN and YOLOv5 were tested across various training data and model parameter combinations to develop an optimal detection model. Lastly, when tested, the model showed a remarkable ability to detect delamination of varying properties accurately compared to currently employed IRT techniques. © 2024
KW  - Augmentation
KW  - Deep learning
KW  - Delamination
KW  - Detection automation
KW  - Image processing
KW  - Infrared thermography (IRT)
KW  - Instance segmentation
KW  - Mask R-CNN
KW  - Unmanned arterial vehicle (UAV)
KW  - YOLOv5
KW  - Digital elevation model
KW  - Image enhancement
KW  - Image segmentation
KW  - Network security
KW  - Self-supervised learning
KW  - Thermography (imaging)
KW  - Augmentation
KW  - Deep learning
KW  - Delaminations detection
KW  - Detection automation
KW  - Images processing
KW  - Infrared thermography
KW  - Instance segmentation
KW  - Mask R-CNN
KW  - Unmanned arterial vehicle
KW  - YOLOv5
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kong, H.
AU  - Huang, C.
AU  - Yu, J.
AU  - Shen, X.
TI  - A Survey of mmWave Radar-Based Sensing in Autonomous Vehicles, Smart Homes and Industry
PY  - 2025
T2  - IEEE Communications Surveys and Tutorials
DO  - 10.1109/COMST.2024.3409556
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196065476&doi=10.1109%2fCOMST.2024.3409556&partnerID=40&md5=c7a557496e16e5999474f9fdf7abab1c
AB  - Sensing technology plays a crucial role in bridging the physical and digital worlds. By transforming a multitude of physical phenomena into digital data, it significantly enhances our understanding of the environment and is instrumental in a wide range of applications. Given the wide bandwidth and short wavelength characteristics, millimeter wave (mmWave) radar sensing is considered one of the most promising sensing techniques beyond mmWave communication. In this paper, we provide a comprehensive survey of mmWave radar-based sensing techniques and applications in autonomous vehicles, smart homes, and industry. Specifically, we first review widely exploited mmWave radar techniques and signal processing techniques from the perspective of dedicated radars and communication integration, which are the basis of mmWave radar sensing. Then, we introduce mainstream machine learning techniques, especially the latest deep learning techniques for designing applications with mmWave signals. Related hardware devices, available public datasets, and evaluation metrics are also presented. Afterward, we provide a taxonomy of emerging mmWave radar sensing applications, and review the developments in object detection, ego-motion estimation, simultaneous localization and mapping, activity recognition, pose estimation, gesture recognition, speech recognition, vital sign monitoring, user authentication, indoor positioning, industrial imaging, industrial measurement, environmental monitoring, etc. We conclude the paper by discussing challenges and potential future research directions. © 1998-2012 IEEE.
KW  - autonomous vehicle
KW  - deep learning
KW  - industry
KW  - Millimeter wave radar
KW  - radar signal processing
KW  - smart home
KW  - wireless sensing
KW  - Deep learning
KW  - Learning algorithms
KW  - Metadata
KW  - Millimeter waves
KW  - Motion estimation
KW  - Object detection
KW  - Object recognition
KW  - Signal detection
KW  - Speech recognition
KW  - Tracking radar
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - Millimeter-wave radar
KW  - Millimeterwave communications
KW  - Millimetre-wave radar
KW  - Radar detection
KW  - Radar sensing
KW  - Smart homes
KW  - Wireless sensing
KW  - Automation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, X.
AU  - He, H.
AU  - Huang, J.
AU  - Liu, R.
AU  - Qian, T.
AU  - Hon, C.
TI  - ASFM-AFD: Multi-Modal Fusion of AFD-Optimized LiDAR and Camera Data for Paper Defect Detection
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3580902
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008660562&doi=10.1109%2fTIM.2025.3580902&partnerID=40&md5=ab472c6d3c60fdc6c56e5075ecfb3f2f
AB  - Current paper defect detection methods often struggle to satisfy the critical demands for precision and adaptability in complex industrial environments. This study developed a novel multi-sensor platform combining low-precision LiDAR and high-resolution camera, along with an Adaptive Sensor Fusion Module - Adaptive Fourier Decomposition (ASFM-AFD) framework for processing multi-modal data. This integration substantially improves defect detection capabilities. The AFD-based optimization approach effectively enhances the quality of low-precision LiDAR data, achieving performance comparable to high-precision LiDAR while providing cost-effective solution for industrial inspection applications. Comparative studies with Empirical Mode Decomposition and Variational Mode Decomposition demonstrates that AFD exhibits superior noise reduction and resolution enhancement while maintaining stable performance under various challenging conditions. Experimental results show that the proposed framework achieves an average precision of 97.5% for paper defect detection in high-speed production environments, significantly improving detection accuracy and demonstrating robustness across various operating conditions. © 1963-2012 IEEE.
KW  - Adaptive Fourier Decomposition
KW  - Lidar
KW  - Multi-Modal Fusion
KW  - Point Cloud Data
KW  - Signal Processing
KW  - Small Object Detection
KW  - Cost effectiveness
KW  - Data accuracy
KW  - Data handling
KW  - Defects
KW  - Empirical mode decomposition
KW  - Modal analysis
KW  - Noise abatement
KW  - Object detection
KW  - Paper
KW  - Variational mode decomposition
KW  - 'current
KW  - Adaptive fourier decompositions
KW  - Adaptive sensor fusion
KW  - Fusion modules
KW  - Lower precision
KW  - Multi-modal fusion
KW  - Paper defect detections
KW  - Point cloud data
KW  - Signal-processing
KW  - Small object detection
KW  - Optical radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wei, J.
AU  - As'arry, A.
AU  - Anas Md Rezali, K.
AU  - Zuhri Mohamed Yusoff, M.
AU  - Ma, H.
AU  - Zhang, K.
TI  - A Review of YOLO Algorithm and Its Applications in Autonomous Driving Object Detection
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3573376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006931401&doi=10.1109%2fACCESS.2025.3573376&partnerID=40&md5=681c8ccacee7157ee140b3de7613abef
AB  - Object detection in autonomous driving scenarios represents a significant research direction within artificial intelligence. Real-time and accurate object detection and recognition are crucial in ensuring autonomous vehicles’ safe and stable operation. In recent years, the continuous introduction of the YOLO series of algorithms and their enhanced models has led to remarkable performance in autonomous driving object detection. From YOLOv1 to YOLOv12, detection accuracy has improved significantly, with mAP increasing from approximately 63.4% to over 80% and inference speed exceeding 100 FPS in lightweight versions such as YOLOv8n and YOLOv10. This paper reviews the YOLO algorithm and its application in object detection in autonomous driving scenarios. Firstly, the development and distinctions among the YOLO series of detection algorithms are explained, and their performance is analyzed. Secondly, the strategies for improving YOLO-based models across the input, feature extraction, and prediction stages are summarized. Thirdly, the research status and application of the YOLO algorithm in autonomous driving object detection are elaborated upon from the perspectives of traffic vehicles, pedestrians, traffic signs, traffic lights, and lane lines, with comparisons and analyses of performance metrics such as accuracy and real-time performance. Finally, considering the current challenges in autonomous driving object detection, the development trajectory and prospects of the YOLO algorithm are summarized and discussed. © 2013 IEEE.
KW  - applications
KW  - Autonomous driving
KW  - object detection
KW  - YOLO algorithm
KW  - Feature extraction
KW  - Object recognition
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - ITS applications
KW  - Object detection and recognition
KW  - Objects detection
KW  - Performance
KW  - Real- time
KW  - Safe operation
KW  - Stable operation
KW  - YOLO algorithm
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gallagher, J.E.
AU  - Oughton, E.J.
TI  - Surveying You Only Look Once (YOLO) Multispectral Object Detection Advancements, Applications, and Challenges
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3526458
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214897758&doi=10.1109%2fACCESS.2025.3526458&partnerID=40&md5=8a849b1c9c301494ca74c35f58ab999a
AB  - Multispectral imaging and deep learning have emerged as powerful tools supporting diverse use cases from autonomous vehicles to agriculture, infrastructure monitoring and environmental assessment. The combination of these technologies has led to significant advancements in object detection, classification, and segmentation tasks in the non-visible light spectrum. This paper considers 400 total papers, reviewing 200 in detail to provide an authoritative meta-review of multispectral imaging technologies, deep learning models, and their applications, considering the evolution and adaptation of you only look once (YOLO). Ground-based collection is the most prevalent approach, totaling 63% of the papers reviewed, although uncrewed aerial systems (UAS) for YOLO-multispectral applications have doubled since 2020. The most prevalent sensor fusion is red-green-blue (RGB) with long-wave infrared (LWIR), comprising 39% of the literature. YOLOv5 remains the most used variant for adaption to multispectral applications, consisting of 33% of all modified YOLO models reviewed. Future research needs to focus on: 1) developing adaptive YOLO architectures capable of handling diverse spectral inputs that do not require extensive architectural modifications; 2) exploring methods to generate large synthetic multispectral datasets; 3) advancing multispectral YOLO transfer learning techniques to address dataset scarcity; and 4) innovating fusion research with other sensor types beyond RGB and LWIR.  © 2013 IEEE.
KW  - convolutional neural networks (CNN)
KW  - deep learning
KW  - HSI
KW  - LWIR
KW  - MSI
KW  - Multispectral object detection
KW  - NIR
KW  - RGB
KW  - SAR
KW  - you only look once (YOLO)
KW  - Antenna grounds
KW  - Deep neural networks
KW  - Image segmentation
KW  - Sensor data fusion
KW  - Thermography (imaging)
KW  - Convolutional neural network
KW  - Deep learning
KW  - HSI
KW  - Longwave infrared
KW  - MSI
KW  - Multi-spectral
KW  - Multispectral object detection
KW  - NIR
KW  - Objects detection
KW  - Red green blues
KW  - You only look once
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Liu, S.
TI  - Vehicle detection in different traffic scenarios based on YOLOv5
PY  - 2025
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.3057990
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216424648&doi=10.1117%2f12.3057990&partnerID=40&md5=a2d918174bc6d4316793cdf06d48dfa5
AB  - Urban traffic environments pose significant challenges for automated vehicle detection, including fluctuating lighting, adverse weather, and complex road conditions. Visibility issues from fog, rain, and low light, alongside the prevalence of small vehicles in dense traffic, hinder detection accuracy. This study proposes an enhanced YOLOv5-based model for improved vehicle detection in complex urban traffic scenarios. Key contributions include integrating BiFPN for robust multi-scale feature fusion, adding an FFA module to boost detection under low-visibility conditions, and incorporating Image Adjustment Techniques (IAT) for preprocessing. Additionally, select YOLOv5 modules were upgraded to YOLOv8 components, yielding notable performance gains over the baseline model. © 2025 SPIE.
KW  - D2City dataset
KW  - Defogging
KW  - Image enhancement
KW  - Small object detection
KW  - Vehicle detection
KW  - YOLOv5
KW  - Laser beams
KW  - Urban transportation
KW  - Adverse weather
KW  - Automated vehicle detection
KW  - D2city dataset
KW  - Defogging
KW  - Scenario-based
KW  - Small object detection
KW  - Traffic environment
KW  - Urban traffic
KW  - Vehicles detection
KW  - YOLOv5
KW  - Vehicle detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, X.
AU  - Miao, H.
AU  - Liang, J.
AU  - Li, K.
AU  - Tan, J.
AU  - Luo, R.
AU  - Jiang, Y.
TI  - Multi-Dimensional Research and Progress in Parking Space Detection Techniques
PY  - 2025
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics14040748
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218872793&doi=10.3390%2felectronics14040748&partnerID=40&md5=254be4865c75b9926e4da5e9b3186ad6
AB  - Due to the increase in the number of vehicles and the complexity of parking spaces, parking space detection technology has emerged. It is capable of automatically identifying vacant parking spaces in parking lots or on streets, and delivering this information to drivers or parking management systems in real time, which has a significant impact on improving urban parking efficiency, alleviating traffic congestion, optimizing driving experience, and promoting the development of intelligent transportation systems. This paper firstly describes the research significance of parking space detection technology and its research background, and then systematically reviews different types of parking spaces and detection technologies, covering a variety of technical means such as ultrasonic sensors, infrared sensors, magnetic sensors, other sensors, methods based on traditional computer vision, and methods based on deep learning. At the end of the paper, the article summarizes the current research progress in parking space detection technology, analyzes the existing challenges, and provides an outlook on future research directions. © 2025 by the authors.
KW  - computer vision
KW  - deep learning
KW  - parking space detection
KW  - sensors
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Guo, F.
AU  - Wu, J.
AU  - Zhang, Q.
TI  - Dual-Domain Feature-Guided Task Alignment for Enhanced Small Object Detection
PY  - 2025
T2  - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings
DO  - 10.1109/ICASSP49660.2025.10888564
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003877954&doi=10.1109%2fICASSP49660.2025.10888564&partnerID=40&md5=e284205d749f12584d4c50a76effd74b
AB  - Small object detection is a critical challenge in Unmanned Aerial Vehicles (UAVs) due to the limited pixel representation of small objects and the impact of successive pooling operations, which frequently results in the disappearance of small objects within intricate backgrounds. To tackle this issue, we propose the Small Object Enhancement Pyramid (SOEP) module, which first transforms feature representations (i.e., in the spatial domain) into the frequency domain to better capture small objects typically characterized by high-frequency components. These feature representations are then fused in the spatial domain using a frequency-based attention map, enhancing small object representations by integrating information from both complementary domains. Furthermore, we introduce a Task Aligned Head (TAH) that integrates classification and localization tasks interactively, reducing the misalignment that occurs when these tasks are learned independently, particularly in the context of small objects. Experimental results on the Visdrone dataset verify that our proposed method (D2FTA) outperforms the baseline method by 12.7%, 14.19% on mAP0.5 and mAP0.5:0.95. © 2025 IEEE.
KW  - frequency domain
KW  - small object detection
KW  - spatial domain
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tang, P.
AU  - Lv, M.
AU  - Ding, Z.
AU  - Xu, W.
AU  - Jiang, M.
TI  - Pothole detection-you only look once: Deformable convolution based road pothole detection
PY  - 2025
T2  - IET Image Processing
DO  - 10.1049/ipr2.13300
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211079719&doi=10.1049%2fipr2.13300&partnerID=40&md5=3b28abffc61b4c6869434e4feabd817d
AB  - The detection of road potholes plays a crucial role in ensuring passenger comfort and the structural safety of vehicles. To address the challenges of pothole detection in complex road environments, this paper proposes a model focusing on shape features (pothole detection you only look once, PD-YOLO). The model aims to overcome the limitations of multi-scale feature learning caused by the use of fixed convolutional kernels in the baseline model, by constructing a feature extraction module that better adapts to variations in the shape of potholes. Subsequently, a cross-stage partial network was designed using a one-time aggregation method, simplifying the model while enabling the network to fuse information between feature maps at different stages. Additionally, a dynamic sparse attention mechanism is introduced to select relevant features, reducing redundancy and suppressing background noise. Experiments conducted on the VOC2007 and GRDDC2020_Pothole datasets reveal that compared to the baseline model YOLOv8, PD-YOLO achieves improvements of 3.9% and 2.8% in mean average precision, with a frame rate of approximately 290 frames per second, effectively meeting the accuracy and real-time requirements for pothole detection. The code and dataset for this paper are located at: https://github.com/woyijiankou/PD-YOLO. © 2024 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
KW  - image capture
KW  - image classification
KW  - image sampling
KW  - Baseline models
KW  - Feature learning
KW  - Image captures
KW  - Images classification
KW  - Multi-scale features
KW  - Passengers comfort
KW  - Road environment
KW  - Safety of vehicles
KW  - Shape features
KW  - Structural safety
KW  - Image sampling
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Y.
AU  - Mao, Y.
AU  - Wang, H.
AU  - Yu, Z.
AU  - Guo, S.
AU  - Zhang, J.
AU  - Wang, L.
AU  - Guo, B.
TI  - Orchestrating Joint Offloading and Scheduling for Low-Latency Edge SLAM
PY  - 2025
T2  - IEEE Transactions on Mobile Computing
DO  - 10.1109/TMC.2025.3547256
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000458312&doi=10.1109%2fTMC.2025.3547256&partnerID=40&md5=d2561a12f8099b2e50449cfd337a3d6d
AB  - Visual Simultaneous Localization and Mapping (vSLAM) is a prevailing technology for many emerging robotic applications. Achieving real-time SLAM on mobile robotic systems with limited computational resources is challenging because the complexity of SLAM algorithms increases over time. This restriction can be lifted by offloading computations to edge servers, forming the emerging paradigm of edge-assisted SLAM. Nevertheless, the exogenous and stochastic input processes affect the dynamics of the edge-assisted SLAM system. Moreover, the requirements of clients on SLAM metrics change over time, exerting implicit and time-varying effects on the system. In this paper, we aim to push the limit beyond existing edge-assist SLAM by proposing a new architecture that can handle the input-driven processes and also satisfy clients' implicit and time-varying requirements. The key innovations of our work involve a regional feature prediction method for importance-aware local data processing, a configuration adaptation policy that integrates data compression/decompression and task offloading, and an input-dependent learning framework for task scheduling with constraint satisfaction. Extensive experiments prove that our architecture improves pose estimation accuracy and saves up to 47\% of communication costs compared with a popular edge-assisted SLAM system, as well as effectively satisfies the clients' requirements. (Figure presented). © 2025 IEEE. All rights reserved.
KW  - and constrained reinforcement learning
KW  - mobile edge computing (MEC)
KW  - Simultaneous localization and mapping (SLAM)
KW  - task offloading
KW  - task scheduling
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bai, T.
AU  - Zhao, H.
AU  - Huang, L.
AU  - Wang, Z.
AU  - Kim, D.I.
AU  - Nallanathan, A.
TI  - A Decade of Video Analytics at Edge: Training, Deployment, Orchestration, and Platforms
PY  - 2025
T2  - IEEE Communications Surveys and Tutorials
DO  - 10.1109/COMST.2025.3563377
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003595638&doi=10.1109%2fCOMST.2025.3563377&partnerID=40&md5=1f10a7c7a226c907b56695d38913a2de
AB  - Video analytics (VA), capable of autonomously understanding events in video content, has demonstrated significant potential across various applications, from surveillance to self-driving cars and industrial automation. However, traditional VA, relying on either end-device or cloud-based solutions, faces limitations such as restricted on-device computing power and network congestion at cloud centers. Edge computing offers a promising solution, enabling low-latency, high-accuracy, and bandwidth-efficient performance, thus supporting the rapid growth of VA deployment. This article provides a comprehensive review of VA at the edge, examining aspects of model training, deployment, end-edge-cloud orchestration, and VA platforms. Specifically, we explore model training approaches conducted in the cloud, at the edge, and in hybrid cloud-edge configurations. We also discuss various model deployment techniques, including quantization and network pruning. Furthermore, the article surveys end-edge-cloud orchestration strategies, categorized into VA query offloading and query scheduling. We evaluate practical deployments and review the literature on VA platforms. Finally, we outline several promising future research directions for advancing this field.  © 1998-2012 IEEE.
KW  - computer vision
KW  - deep learning
KW  - Edge computing
KW  - edge intelligence
KW  - video analytics
KW  - Mobile edge computing
KW  - Deep learning
KW  - Edge clouds
KW  - Edge computing
KW  - Edge intelligence
KW  - End-devices
KW  - Industrial automation
KW  - Model training
KW  - Self drivings
KW  - Video analytics
KW  - Video contents
KW  - Computation offloading
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gan, Z.
AU  - Li, J.
AU  - Wu, P.
AU  - Bai, Y.
AU  - Xiong, B.
AU  - Zeng, N.
AU  - Zou, F.
AU  - He, D.
AU  - Ni, W.
TI  - A Novel Equipment Based on Improved YOLOv5s for Automated Electrical Cover Handling
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2024.3500043
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000373946&doi=10.1109%2fTIM.2024.3500043&partnerID=40&md5=3b8f5bf0d0d96d9432e572874496b7e2
AB  - With the rapid development of the power industry, the number of power cables has increased significantly, making the improvement of maintenance efficiency a critical issue. Reducing accidents caused by the manual handling of heavy electrical cover plates has become a pressing concern. However, the maintenance of power cables often requires precise positioning and efficient movement of these heavy cover plates in energy-poor environments. Existing object detection models typically not only require high-performance computing but also have with high cost and large volume, rendering them quite unsuitable for deployment in the resource-constrained system. To address these challenges, we propose an automated solution that integrates negative pressure adsorption and computer vision techniques. The core module of this method is Cover Plates Edge Line-YOLO (CPEL-YOLO), a lightweight model based on You Only Look Once (YOLO). In CPEL-YOLO, it leverages MobileNetV3 with efficient multiscale attention (EMA) and C3-Faster with EMA to focus the model on precise edge lines with reduced computational complexity. Additionally, Scylla-IoU (SIoU) is incorporated to optimize the training process. A dataset of 1009 cover plate images was constructed for training and evaluation. Experimental results show that CPEL-YOLO achieves a mAP@0.5 of 88.1%, comparable to YOLOv5s, while reducing model size by 42% and improving precision by 4.4%.  © 1963-2012 IEEE.
KW  - Cover plate handling
KW  - edge line detection
KW  - electrical power maintenance
KW  - multiscale attention
KW  - YOLOv5s
KW  - Cover plate
KW  - Cover plate handling
KW  - Edge line detection
KW  - Edge lines
KW  - Electrical power
KW  - Electrical power maintenance
KW  - Line detection
KW  - Multiscale attention
KW  - Plate edges
KW  - YOLOv5
KW  - Network security
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, L.
AU  - Fu, L.
AU  - Jia, X.
AU  - Cui, B.
AU  - Zhu, X.
AU  - Jin, J.
TI  - YOLO-BOS: An Emerging Approach for Vehicle Detection with a Novel BRSA Mechanism
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24248126
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213081735&doi=10.3390%2fs24248126&partnerID=40&md5=b030f94a7dacc27707e985df5adae3c1
AB  - In intelligent transportation systems, accurate vehicle target recognition within road scenarios is crucial for achieving intelligent traffic management. Addressing the challenges posed by complex environments and severe vehicle occlusion in such scenarios, this paper proposes a novel vehicle-detection method, YOLO-BOS. First, to bolster the feature-extraction capabilities of the backbone network, we propose a novel Bi-level Routing Spatial Attention (BRSA) mechanism, which selectively filters features based on task requirements and adjusts the importance of spatial locations to more accurately enhance relevant features. Second, we incorporate Omni-directional Dynamic Convolution (ODConv) into the head network, which is capable of simultaneously learning complementary attention across the four dimensions of the kernel space, therefore facilitating the capture of multifaceted features from the input data. Lastly, we introduce Shape-IOU, a new loss function that significantly enhances the accuracy and robustness of detection results for vehicles of varying sizes. Experimental evaluations conducted on the UA-DETRAC dataset demonstrate that our model achieves improvements of 4.7 and 4.4 percentage points in mAP@0.5 and mAP@0.5:0.95, respectively, compared to the baseline model. Furthermore, comparative experiments on the SODA10M dataset corroborate the superiority of our method in terms of precision and accuracy. © 2024 by the authors.
KW  - BRSA
KW  - ODConv
KW  - Shape-IOU
KW  - vehicle detection
KW  - YOLO-BOS
KW  - Advanced traffic management systems
KW  - Air traffic control
KW  - Highway administration
KW  - Highway traffic control
KW  - Intelligent systems
KW  - Magnetic levitation vehicles
KW  - Motor transportation
KW  - Street traffic control
KW  - Vehicle locating systems
KW  - Attention mechanisms
KW  - Bi-level routing spatial attention
KW  - Directional dynamics
KW  - Omni-directional
KW  - Omni-directional dynamic convolution
KW  - Routings
KW  - Shape-IOU
KW  - Spatial attention
KW  - Vehicles detection
KW  - YOLO-BOS
KW  - article
KW  - bronchiolitis obliterans
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - spatial attention
KW  - Vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Z.
AU  - Xiang, J.
AU  - Duan, J.
TI  - A low illumination target detection method based on a dynamic gradient gain allocation strategy
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-80265-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210018417&doi=10.1038%2fs41598-024-80265-w&partnerID=40&md5=96d78d0b625f2c9192aa3f6a63c4e319
AB  - Current target detection methods perform well under normal lighting conditions; however, they encounter challenges in effectively extracting features, leading to false detections and missed detections in low illumination environments. To address these issues, this study introduces an efficient target detection method for low illumination, named DimNet. This method optimizes the model through enhancements in multi-scale feature fusion, feature extraction, detection head, and loss function. Firstly, efficient multi-scale feature fusion is performed by using a new neck structure in the original model so that it can fully exchange high-level semantic information and low-level spatial information. Secondly, by designing a new feature aggregation module, it can simultaneously fuse channel and spatial information as well as local and global information to improve the representation of the network. Subsequently, to achieve more accurate target recognition, a new detection head is designed by replacing the original convolutional layer and utilizing the reparameterization technique, which enhances recognition performance in complex scenes. Additionally, the size of the improved detection head is reduced by adopting a parameter-sharing approach, thereby balancing detection accuracy with computational efficiency. Finally, to solve the fuzzy boundary problem caused by the target boundary being similar to the surrounding background due to insufficient illumination under low illumination conditions, a new loss function is designed in this paper, which pays more attention to the center of the target and weakly considers the aspect ratio of the target prediction frame, and at the same time, the new loss function employs a dynamic gradient gain assignment strategy to reduce the effect of the low-quality anchor frames and to improve the target localization Accuracy. The experimental results show that DimNet achieves a mAP50 of 75.60% on the ExDark dataset, which is an improvement of 3.77% over the baseline model and 2.25% over the state-of-the-art (SOTA) model. DimNet outperforms the previous and current SOTA methods in terms of detection accuracy and other aspects of performance, which is a clear advantage. © The Author(s) 2024.
KW  - Feature extraction
KW  - Feature fusion
KW  - Loss function
KW  - Low-light image
KW  - Target detection
KW  - article
KW  - diagnosis
KW  - feature extraction
KW  - human
KW  - illumination
KW  - neck
KW  - prediction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, D.
AU  - Solihin, M.I.
AU  - Ardiyanto, I.
AU  - Zhao, Y.
AU  - Li, W.
AU  - Cai, B.
AU  - Chen, C.
TI  - A streamlined approach for intelligent ship object detection using EL-YOLO algorithm
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-64225-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197383728&doi=10.1038%2fs41598-024-64225-y&partnerID=40&md5=a5e16c6b7ff61229ea0f091fb4489135
AB  - Maritime objects frequently exhibit low-quality and insufficient feature information, particularly in complex maritime environments characterized by challenges such as small objects, waves, and reflections. This situation poses significant challenges to the development of reliable object detection including the strategies of loss function and the feature understanding capabilities in common YOLOv8 (You Only Look Once) detectors. Furthermore, the widespread adoption and unmanned operation of intelligent ships have generated increasing demands on the computational efficiency and cost of object detection hardware, necessitating the development of more lightweight network architectures. This study proposes the EL-YOLO (Efficient Lightweight You Only Look Once) algorithm based on YOLOv8, designed specifically for intelligent ship object detection. EL-YOLO incorporates novel features, including adequate wise IoU (AWIoU) for improved bounding box regression, shortcut multi-fuse neck (SMFN) for a comprehensive analysis of features, and greedy-driven filter pruning (GDFP) to achieve a streamlined and lightweight network design. The findings of this study demonstrate notable advancements in both detection accuracy and lightweight characteristics across diverse maritime scenarios. EL-YOLO exhibits superior performance in intelligent ship object detection using RGB cameras, showcasing a significant improvement compared to standard YOLOv8 models. © The Author(s) 2024.
KW  - Improved YOLOv8
KW  - Intelligent ship
KW  - Lightweight YOLOv8
KW  - Modified bounding box regression
KW  - Object detection
KW  - algorithm
KW  - article
KW  - camera
KW  - controlled study
KW  - diagnosis
KW  - nonhuman
KW  - ship
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Qiu, C.
AU  - Tang, H.
AU  - Yang, Y.
AU  - Wan, X.
AU  - Xu, X.
AU  - Lin, S.
AU  - Lin, Z.
AU  - Meng, M.
AU  - Zha, C.
TI  - Machine vision-based autonomous road hazard avoidance system for self-driving vehicles
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-62629-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194864716&doi=10.1038%2fs41598-024-62629-4&partnerID=40&md5=b42c1e212551e5c761a12fd8364c95f8
AB  - The resolution of traffic congestion and personal safety issues holds paramount importance for human’s life. The ability of an autonomous driving system to navigate complex road conditions is crucial. Deep learning has greatly facilitated machine vision perception in autonomous driving. Aiming at the problem of small target detection in traditional YOLOv5s, this paper proposes an optimized target detection algorithm. The C3 module on the algorithm’s backbone is upgraded to the CBAMC3 module, introducing a novel GELU activation function and EfficiCIoU loss function, which accelerate convergence on position loss lbox, confidence loss lobj, and classification loss lcls, enhance image learning capabilities and address the issue of inaccurate detection of small targets by improving the algorithm. Testing with a vehicle-mounted camera on a predefined route effectively identifies road vehicles and analyzes depth position information. The avoidance model, combined with Pure Pursuit and MPC control algorithms, exhibits more stable variations in vehicle speed, front-wheel steering angle, lateral acceleration, etc., compared to the non-optimized version. The robustness of the driving system's visual avoidance functionality is enhanced, further ameliorating congestion issues and ensuring personal safety. © The Author(s) 2024.
KW  - Control algorithm
KW  - Deep learning
KW  - Machine vision
KW  - Risk avoidance
KW  - Self-Driving
KW  - YOLOv5s
KW  - acceleration
KW  - algorithm
KW  - article
KW  - autonomous vehicle
KW  - camera
KW  - controlled study
KW  - deep learning
KW  - detection algorithm
KW  - human
KW  - learning
KW  - risk aversion
KW  - velocity
KW  - vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mo, C.
AU  - Hu, Z.
AU  - Wang, J.
AU  - Xiao, X.
TI  - SGT-YOLO: A Lightweight Method for PCB Defect Detection
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3563011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003674825&doi=10.1109%2fTIM.2025.3563011&partnerID=40&md5=5281187f599329bfd29b6629630839ef
AB  - Ensuring the quality of the printed circuit board (PCB) is vital. Most of the current fault detection algorithms perform well in PCB defect detection. However, these methods involve too many parameters or computations, which make them unfriendly to devices with limited computational capability and small memory capacity. Additionally, problems such as missed or false detection may occur due to the complex background environment and small defect size. Thus, this article proposes an improved model based on YOLOv5s named SE-ENv2 GC-Neck TSCODE (SGT)-you only look once (YOLO) to strike a better trade-off between accuracy and model complexity. First, an SE-ENv2 backbone derived from EfficientNetv2(ENv2) is proposed, which retains more detail and position information about tiny defects and emphasizes the critical features while maintaining a small model size. Second, the P4 and P5 detection heads were removed from YOLOv5s to decrease the model’s parameters, allowing the model to focus more on small PCB defects. Moreover, the task-specific context decoupling (TSCODE) head is introduced to extract the location and category information about defects separately, strengthening the model’s learning ability. Finally, a GSConv CAA-neck (GC-Neck) consisting of GSConv and C3-GC is proposed, which enhances the model’s capability to extract tiny defect features while reducing parameters. The experimental results show that SGT-YOLO reduced the baseline parameters and floating-point operations by 79% and 35%, respectively. Furthermore, SGT-YOLO improves the mean average precision (mAP) and mAP0.5 by 2.7% and 6.4% on A challenging dataset for PCB defects detection and classification (HRIPCB) datasets, indicating its lightweight and accuracy in PCB defect detection. © 1963-2012 IEEE.
KW  - Deep learning
KW  - defect detection
KW  - printed circuit board (PCB)
KW  - YOLOv5
KW  - Digital arithmetic
KW  - 'current
KW  - Circuit boards
KW  - Computational capability
KW  - Deep learning
KW  - Defect detection
KW  - Fault detection algorithm
KW  - Memory capacity
KW  - Missed detections
KW  - Printed circuit board
KW  - YOLOv5
KW  - Leak detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cai, W.
AU  - Chen, H.
AU  - Zhang, M.
TI  - A survey on collaborative hunting with robotic swarm: Key technologies and application scenarios
PY  - 2024
T2  - Neurocomputing
DO  - 10.1016/j.neucom.2024.128008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196256992&doi=10.1016%2fj.neucom.2024.128008&partnerID=40&md5=f019d1343d73ebe61e5ff585f5510926
AB  - Compared with a single robot, robotic swarm realizes multi-agent cooperative operations through information interaction, and has such characteristics of wide combat monitoring range, flexible combat organization and strong reconfiguration. As the intelligence of robotic swarm, collaborative hunting tasks have been used in the fields of intrusion countermeasures, target elimination, scientific research, target rescue and etc. A comprehensive overview of current state of researches on collaborative hunting tasks with robotic swarm would be beneficial to researchers. Therefore, we provide an overview of research progress related to collaborative hunting tasks of robotic swarm in this paper. Specifically, we analyze key technologies in the collaborative hunting tasks from the perspectives of target searching, hunting task allocation and tracking path planning. Moreover, we summarize the differences of hunting strategies in different scenarios (i.e., air scenario, sea scenario and ground scenario), so it has important guiding significance for future researches. Finally, we discuss future research directions for collaborative hunting tasks of robotic swarm. © 2024 Elsevier B.V.
KW  - Collaborative hunting task
KW  - Hunting task allocation
KW  - Robotic swarm
KW  - Target searching
KW  - Tracking path planning
KW  - Intelligent robots
KW  - Multi agent systems
KW  - Robot programming
KW  - Swarm intelligence
KW  - Target tracking
KW  - Application scenario
KW  - Collaborative hunting task
KW  - Hunting task allocation
KW  - Key technologies
KW  - Robotic swarms
KW  - Target searching
KW  - Task allocation
KW  - Technologies and applications
KW  - Technology scenarios
KW  - Tracking path planning
KW  - animal hunting
KW  - female
KW  - human
KW  - intelligence
KW  - short survey
KW  - Motion planning
M3  - Short survey
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, O.T.-C.
AU  - Chang, Y.-X.
AU  - Chung, C.-Y.
AU  - Cheng, Y.-Y.
AU  - Ha, M.-H.
TI  - Hardware-Aware Iterative One-Shot Neural Architecture Search With Adaptable Knowledge Distillation for Efficient Edge Computing
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3554185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002269481&doi=10.1109%2fACCESS.2025.3554185&partnerID=40&md5=18ba3c58e76f0b1f9b6e73bde1002798
AB  - The growing demand for edge applications calls for efficient and optimized deep neural network models. Neural Architecture Search (NAS) is instrumental in designing such models, but achieving optimal architectures quickly remains a key challenge. To address this, we propose Hardware-aware Iterative One-shot NAS (HIO-NAS), a highly efficient approach that iteratively explores architectures across predefined search spaces for depth, filter, and width, all while respecting hardware constraints. HIO-NAS operates in four main steps: full training, random search, hardware verification, and retraining with adaptable knowledge distillation, repeated for each search space. The hardware-aware mechanism incorporates a lookup table to evaluate and filter subnetworks sampled during random search, ensuring only the most promising candidates proceed to hardware verification. The top-performing subnetwork(s) are then deployed on the target hardware for further validation. The adaptable knowledge distillation technique dynamically adjusts the teacher model’s influence based on deviations in the cost function during training. By progressively refining search spaces, HIO-NAS reduces computational overhead and avoids convergence issues. Its hypernetwork design emphasizes chain-like structures, where layers maintain connectivity. Applied to MobileNet v2 on CIFAR-100 and YOLOv7 on BDD100K, HIO-NAS delivered significant improvements: a 2.1% accuracy gain and 20.8% reduction in latency for MobileNet v2, and better performance over YOLOv7 and its variants. Interestingly, the findings highlight that unit-sharing topologies excel in depth searches, whereas unit-non-sharing topologies perform better in filter and width searches. Overall, HIO-NAS showcases a robust capability for efficiently discovering high-performance, hardware-optimized architectures, making it ideal for edge applications. © 2013 IEEE.
KW  - Edge computing
KW  - Hardware awareness
KW  - Iterative search
KW  - Knowledge distillation
KW  - Light-weight model
KW  - Neural architecture search
KW  - One-shot NAS
KW  - Deep neural networks
KW  - Personnel training
KW  - Table lookup
KW  - Teaching
KW  - Edge computing
KW  - Hardware awareness
KW  - Iterative search
KW  - Knowledge distillation
KW  - Light weight
KW  - Light-weight model
KW  - Neural architecture search
KW  - Neural architectures
KW  - One-shot neural architecture search
KW  - Cost functions
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liang, K.
AU  - Zhao, J.
AU  - Zhang, Z.
AU  - Guan, W.
AU  - Pan, M.
AU  - Li, M.
TI  - Data-driven AI algorithms for construction machinery
PY  - 2024
T2  - Automation in Construction
DO  - 10.1016/j.autcon.2024.105648
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201695100&doi=10.1016%2fj.autcon.2024.105648&partnerID=40&md5=93ce09843c6782270c3ce72a87f3d0c1
AB  - Based on the transition to Industry 4.0, construction operations are gradually moving towards large-scale and high-efficiency development. However, excessive manual labor is becoming a problem, affecting construction industry progress, and causing significant safety hazards. As the continuous development of artificial intelligence and big date technologies, intelligent construction machinery with data-driven methods is considered the best solution for enhancing construction safety and efficiency, which are mainly reflected in prognostic and health management, environment perception and automation control. Therefore, this paper reviews the widespread research on semi-automatic or even fully automatic construction methods reported in the literature. Firstly, it introduces several widely-used artificial intelligence algorithms and their variations. Secondly, three main topics were covered: prognostic and health management applications in experimental and real-world settings, environmental perception systems, and automation control methods for construction machinery. Finally, several research prospects and challenges were presented. © 2024
KW  - Artificial intelligence
KW  - Automation control
KW  - Construction machinery
KW  - Data-driven methods
KW  - Environment perception
KW  - Prognostic and health management
KW  - Diagnosis
KW  - AI algorithms
KW  - Automation controls
KW  - Construction machinery
KW  - Construction operations
KW  - Data driven
KW  - Data-driven methods
KW  - Environment perceptions
KW  - Higher efficiency
KW  - Large-scales
KW  - Prognostic and health management
KW  - Efficiency
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, S.
AU  - Lu, X.
AU  - Guo, C.
TI  - YOLOv5_mamba: unmanned aerial vehicle object detection based on bidirectional dense feedback network and adaptive gate feature fusion
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-73241-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205276103&doi=10.1038%2fs41598-024-73241-x&partnerID=40&md5=c1ea6697fc9d958c77c54e4fbade7dce
AB  - Addressing the problem that the object size in Unmanned Aerial Vehicles (UAVs) aerial images is too small and contains limited feature information, leading to existing detection algorithms having less than ideal performance in small object detection, we propose a UAV aerial object detection system named YOLv5_mamba based on bidirectional dense feedback network and adaptive gate feature fusion. This paper improves the You Only Look Once Version 5 (YOLOv5) algorithm by firstly introducing the Faster Implementation of CSP Bottleneck with 2 convolutions (C2f) module from YOLOv8 into the backbone network to enhance the feature extraction capability of the backbone network. Furthermore, the mamba module and C2f module are introduced to construct a bidirectional dense feedback network to enhance the transfer of contextual information in the neck part. Thirdly, an adaptive gate feature fusion network is proposed to improve the head part of YOLOv5 and enhance its final detection capability. Experimental results on the public UAV aerial dataset VisDrone2019 demonstrate that the proposed algorithm improves the detection accuracy by 9.3% compared to the original YOLOv5 baseline network, showing better detection performance for small objects. For the UCAS_AOD dataset, the proposed algorithm outperforms YOLOv5-s by 9%. In the case of the DIOR dataset, the proposed algorithm exceeds YOLOv5-s by 12%. © The Author(s) 2024.
KW  - Adaptive gate feature fusion
KW  - Mamba
KW  - Object detection
KW  - UAV
KW  - YOLOv5
KW  - algorithm
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - diagnosis
KW  - feature extraction
KW  - microcatheter
KW  - neck
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ali, M.L.
AU  - Zhang, Z.
TI  - The YOLO Framework: A Comprehensive Review of Evolution, Applications, and Benchmarks in Object Detection
PY  - 2024
T2  - Computers
DO  - 10.3390/computers13120336
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213275573&doi=10.3390%2fcomputers13120336&partnerID=40&md5=bcf406e52c10aa1fd6af3dc57b4d9f4b
AB  - This paper provides a comprehensive review of the YOLO (You Only Look Once) framework up to its latest version, YOLO 11. As a state-of-the-art model for object detection, YOLO has revolutionized the field by achieving an optimal balance between speed and accuracy. The review traces the evolution of YOLO variants, highlighting key architectural improvements, performance benchmarks, and applications in domains such as healthcare, autonomous vehicles, and robotics. It also evaluates the framework’s strengths and limitations in practical scenarios, addressing challenges like small object detection, environmental variability, and computational constraints. By synthesizing findings from recent research, this work identifies critical gaps in the literature and outlines future directions to enhance YOLO’s adaptability, robustness, and integration into emerging technologies. This review provides researchers and practitioners with valuable insights to drive innovation in object detection and related applications. © 2024 by the authors.
KW  - deep neural network
KW  - performance evaluation
KW  - real-time object detection
KW  - single stage detection
KW  - YOLO
KW  - YOLOv10
KW  - YOLOv11
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chi, P.
AU  - Wang, Z.
AU  - Liao, H.
AU  - Li, T.
AU  - Wu, X.
AU  - Zhang, Q.
TI  - Towards new-generation of intelligent welding manufacturing: A systematic review on 3D vision measurement and path planning of humanoid welding robots
PY  - 2025
T2  - Measurement: Journal of the International Measurement Confederation
DO  - 10.1016/j.measurement.2024.116065
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207885828&doi=10.1016%2fj.measurement.2024.116065&partnerID=40&md5=ab1ad8eed8f47f9c726d345ae2d22d08
AB  - In recent years, intelligent welding technology has emerged as a prominent focus within the welding domain, amalgamating a diverse array of sophisticated technologies, including robotics, computer vision, artificial intelligence, and sensor systems. This integration heralds unprecedented levels of automation in welding processes, endowing them with heightened efficiency, precision, and cognitive capabilities. Notably, scholarly attention has been dedicated to the realms of intelligent welding manufacturing and welding robotics. However, a discernible lacuna exists in the form of a comprehensive review elucidating the pivotal technologies underpinning welding robots, while research about autonomous mobile welding robots appears to have encountered a developmental impasse. In response, this study undertakes a systematic literature review to scrutinize the core technologies of humanoid welding robots (HWR), positing their elevated research prospects within the milieu of next-generation intelligent welding manufacturing. This study explores the hardware of humanoid welding robots as an emerging technology, drawing on current advancements in humanoid robotics. The key technologies relevant to both humanoid and welding robots are also examined, highlighting their integration and potential applications. Initially, the discourse delves into hand-eye calibration methodologies, delineating a multifaceted approach predicated upon a multi-coordinate system tailored to HWR. Subsequently, the significance of visual-based pose estimation and three-dimensional (3D) reconstruction techniques is underscored, given their instrumental role in furnishing HWR with environmental cognition, a discourse expounded meticulously. Additionally, meticulous scrutiny is accorded to mobile robot path planning and dual-robot trajectory planning methodologies, pivotal for orchestrating welding operation sequences tailored to HWR. To assess the job completion and potential applications of HWR, this paper evaluates welding quality judgment and explores the humanoid robot's utility in non-welding scenes. Conclusively, this paper identifies the exigencies confronting HWR and proffers strategic directives delineating avenues for seminal research and practical application within this burgeoning domain. © 2024 Elsevier Ltd
KW  - 3D target measurement
KW  - Hand-eye calibration
KW  - Humanoid welding robot
KW  - Trajectory planning
KW  - Visual-based 3D reconstruction
KW  - Industrial robots
KW  - Intelligent robots
KW  - Mobile robots
KW  - Motion planning
KW  - Robot programming
KW  - Smart manufacturing
KW  - Hand/eye calibration
KW  - Humanoid robot
KW  - Humanoid welding robot
KW  - Intelligent welding
KW  - Systematic Review
KW  - Three-dimensional reconstruction
KW  - Three-dimensional target measurement
KW  - Trajectory Planning
KW  - Visual-based three-dimensional reconstruction
KW  - Welding robots
KW  - Anthropomorphic robots
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lu, E.H.-C.
AU  - Hsieh, Y.-C.
TI  - Cross-Field Road Markings Detection Based on Inverse Perspective Mapping
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24248080
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213264177&doi=10.3390%2fs24248080&partnerID=40&md5=7895108278eb8fd9b6bc836c02ef9818
AB  - With the rapid development of the autonomous vehicles industry, there has been a dramatic proliferation of research concerned with related works, where road markings detection is an important issue. When there is no public open data in a field, we must collect road markings data and label them by ourselves manually, which is huge labor work and takes lots of time. Moreover, object detection often encounters the problem of small object detection. The detection accuracy often decreases when the detection distance increases. This is primarily because distant objects on the road take up few pixels in the image and object scales vary depending on different distances and perspectives. For the sake of solving the issues mentioned above, this paper utilizes a virtual dataset and open dataset to train the object detection model and cross-field testing in the field of Taiwan roads. In order to make the model more robust and stable, the data augmentation method is employed to generate more data. Therefore, the data are increased through the data augmentation method and homography transformation of images in the limited dataset. Additionally, Inverse Perspective Mapping is performed on the input images to transform them into the bird’s eye view, which solves the “small objects at far distance” problem and the “perspective distortion of objects” problem so that the model can clearly recognize the objects on the road. The model testing on the front-view images and bird’s eye view images also shows a remarkable improvement of accuracy by 18.62%. © 2024 by the authors.
KW  - cross-field
KW  - deep learning
KW  - inverse perspective mapping
KW  - object detection
KW  - road markings
KW  - Inverse transforms
KW  - Road and street markings
KW  - Augmentation methods
KW  - Autonomous Vehicles
KW  - Cross-field
KW  - Data augmentation
KW  - Deep learning
KW  - Inverse perspective mappings
KW  - Objects detection
KW  - Related works
KW  - Road marking
KW  - Vehicle industry
KW  - adult
KW  - article
KW  - autonomous vehicle
KW  - deep learning
KW  - diagnosis
KW  - human
KW  - sake
KW  - Taiwan
KW  - Mapping
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ranjbarzadeh, R.
AU  - Crane, M.
AU  - Bendechache, M.
TI  - The impact of backbone selection in YOLOv8 models on brain tumor localization
PY  - 2025
T2  - Iran Journal of Computer Science
DO  - 10.1007/s42044-025-00258-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001482005&doi=10.1007%2fs42044-025-00258-4&partnerID=40&md5=ef2b933df98768ffcc7f8b88aac3e830
AB  - This study investigates the utilization of the You Only Look Once (YOLOv8) deep learning framework for accurately identifying the location of brain tumors in medical imaging. We investigate the effects of model size and pretraining on the accuracy and computational efficiency of tumor detection by utilizing different setups of the YOLOv8 model. These setups include various configurations, ranging from very small to large, and can be pretrained on the COCO dataset or not. The experimental results, carried out on Google Colaboratory using NVIDIA Tesla T4 GPUs, show that pretrained models often achieve better performance by utilizing the extensive feature representations learned from the COCO dataset, resulting in increased precision in tumor location. For instance, the YOLOv8-XS model pretrained on COCO achieves an IoU of 0.278 and a Dice coefficient of 0.435, whereas its non-pretrained counterpart attains only 0.241 IoU and 0.388 Dice, indicating a 15% improvement in tumor localization accuracy. Similarly, pretrained YOLOv8-L achieves 0.269 IoU, outperforming standard object detection models such as Mask R-CNN (IoU: 0.212) and Faster R-CNN (IoU: 0.228). These results highlight the impact of pretraining on model performance, particularly for lightweight architectures, while also revealing diminishing returns for larger models. The research uncovers a subtle connection between the complexity of the model, pretraining, and the time required for training. It emphasizes the possible advantages and constraints of pretraining for various sizes of models. © The Author(s) 2025.
KW  - Brain tumor localization
KW  - COCO dataset
KW  - Deep learning
KW  - Object detection
KW  - Transfer learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gong, J.
AU  - Fu, W.
AU  - Liu, N.
TI  - Design of SAR image target contour enhancement preprocessing module
ST  - SAR 图像目标轮廓增强预处理模块设计
PY  - 2024
T2  - Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics
DO  - 10.12305/j.issn.1001-506X.2024.12.09
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213713971&doi=10.12305%2fj.issn.1001-506X.2024.12.09&partnerID=40&md5=eb3ea5e8e2a8e23fa2ad33d8adfdfec7
AB  - In response to the issue that the every Channel's data in three Channels of synthetic aperture radar (SAR) images is the same which may cause Channel Information redundancy when deep learning-based target detection network detects the targets of SAR, a Channel expansion preprocessing algorithm module based on smoothing and sharpening filtering is proposed, which is then named as ORLM (Original, Roberts, Laplace, Mean) block. The proposed algorithm in this article can be encapsulated, integrated, and applied to the data reading program of the target detection algorithm. It can extend the Channel of SAR images with the same data in each Channel, and ensure that the expanded Channel data fully contains the contour information of the target. Through training, testing and comparative experiments of the target detection network with and without the proposed preprocessing algorithm on different ship target detection datasets, the experimental results show that the preprocessing algorithm proposed can be applied to various target detection algorithms and can improve detection accuracy without significantly reducing of the real-time detection Performance. © 2024 Chinese Institute of Electronics. All rights reserved.
KW  - deep learning
KW  - image processing
KW  - synthetic aperture radar (SAR)
KW  - target detection
KW  - Data encapsulation
KW  - Deep learning
KW  - Laplace transforms
KW  - Radar target recognition
KW  - Synthetic aperture radar
KW  - Deep learning
KW  - Detection networks
KW  - Images processing
KW  - Pre-processing algorithms
KW  - Preprocessing modules
KW  - Synthetic aperture radar
KW  - Synthetic aperture radar images
KW  - Target detection algorithm
KW  - Targets detection
KW  - Three channel
KW  - Ship testing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Rigatos, G.
AU  - Abbaszadeh, M.
AU  - Siano, P.
TI  - Nonlinear optimal and flatness-based control methods and applications for complex dynamical systems
PY  - 2025
T2  - Nonlinear Optimal and Flatness-based Control Methods and Applications for Complex Dynamical Systems
DO  - 10.1049/PBCE136E
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009517601&doi=10.1049%2fPBCE136E&partnerID=40&md5=1f44a64eecbc09235144ef33ab8a5831
AB  - Robotics, mechatronics and autonomous systems can exhibit complex nonlinear dynamics which can lead to unsatisfactory transients and deviation from setpoints or even to instability. A standard approach in the control of these systems had been the concept of diffeomorphisms to bring a system into a linear form. However, these methods are not straightforward and result in complicated state-space model transformations. In this monograph, new methods have been investigated which are not constrained by the shortcomings of global linearization-based control schemes. They can be implemented in a computationally simple manner, are followed by global stability proofs, and perform better than previous optimal control approaches for a wider class of nonlinear dynamical systems and applications. In this monograph, the authors present two main proven control methods: the nonlinear optimal (H-infinity) control method, and the flatness-based control approach. These methods have shown to be better suited than previous standard approaches in solving control issues, and can be used in a wide class of dynamical systems. They can have a broad range of applications in mechatronics, industrial robotics, space robotics, robotic cranes and pendulums, autonomous vehicles, aerospace systems and satellites, power electronics, biosystems and financial systems. This very comprehensive monograph is a valuable resource for academic researchers and engineers working on control systems and estimation methods, and university staff and graduate students in the fields of control and automation, robotics and mechatronics, electrical engineering, electric power systems and power electronics, biosystems, computer science, financial systems, and physics. The monograph is also a very useful reference for skilled technical professionals developing real world applications. © The Institution of Engineering and Technology and its licensors 2025. All rights reserved.
KW  - Dynamics
KW  - Electric machine control
KW  - Engineering education
KW  - Mechatronics
KW  - Nonlinear dynamical systems
KW  - Optimal control systems
KW  - Power electronics
KW  - Robotics
KW  - State space methods
KW  - Students
KW  - Bio-systems
KW  - Complex dynamical systems
KW  - Complex nonlinear dynamics
KW  - Control applications
KW  - Control approach
KW  - Control methods
KW  - Financial system
KW  - Flatness-based control
KW  - Nonlinear optimal
KW  - Setpoints
KW  - Dynamical systems
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Arsenos, A.
AU  - Petrongonas, E.
AU  - Filippopoulos, O.
AU  - Skliros, C.
AU  - Kollias, D.
AU  - Kollias, S.
TI  - NEFELI: A deep-learning detection and tracking pipeline for enhancing autonomy in advanced air mobility
PY  - 2024
T2  - Aerospace Science and Technology
DO  - 10.1016/j.ast.2024.109613
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206639436&doi=10.1016%2fj.ast.2024.109613&partnerID=40&md5=3547d23c6cd4285cf818834eae0be6b4
AB  - Efficient detection and accurate collision estimation for non-cooperative aerial vehicles are crucial for the realization of fully autonomous aircraft and Advanced Air Mobility (AAM). This paper introduces NEFELI, a machine learning software utilizing optical sensors to detect and track non-cooperative aerial vehicles. NEFELI's detector employs an enhanced YOLOv5-large model, strengthened with a sliced inference step to enhance detection capabilities for distant, diminutive objects. Furthermore, NEFELI introduces several innovations in its tracking component. A key advancement lies in the creation and utilization of the first large-scale re-identification (Re-ID) dataset of aerial objects. This dataset is used to train the deep learning appearance (Re-ID) model of the tracking module and integrates appearance information into the detection and tracking pipeline, resulting in more robust and reliable tracking performance. Moreover, the tracking model combines the deep learning appearance model with a Kalman Filter-based motion model to address the challenge of precisely tracking distant aerial objects. Notably, an extensive comparative analysis that was conducted showed that NEFELI outperforms state-of-the-art detection and tracking models in terms of Higher Order Tracking Accuracy (HOTA) metric, ID switches, and Association Accuracy (AssA) by a wide margin. A crucial aspect of this work is NEFELI's software architecture design, which enables efficient implementation on a low SWaP (Size, Weight, and Power) edge Graphic Processing Unit (GPU). To further showcase NEFELI's generalization capabilities and edge implementation performance, real-world flight experiments with small UAVs were carried out. The experimental results demonstrate NEFELI's ability to detect and track small UAVs at distances of up to 145 m in real-time speed of 6.7 fps. © 2024 Elsevier Masson SAS
KW  - Advanced air mobility (AAM)
KW  - Autonomous aircraft
KW  - Autonomy
KW  - Detect and avoid (DAA)
KW  - Detection
KW  - Edge computing
KW  - Non-cooperative traffic management
KW  - Onboard computing (OBC)
KW  - Re-identification
KW  - Sense and avoid
KW  - Tracking
KW  - Unmanned aerial vehicle (UAV)
KW  - Air navigation
KW  - Air traffic control
KW  - Air transportation
KW  - Aircraft detection
KW  - Benchmarking
KW  - Control towers
KW  - Edge computing
KW  - Image coding
KW  - Image segmentation
KW  - Image texture
KW  - Image thinning
KW  - Kalman filters
KW  - Object tracking
KW  - Railroad traffic control
KW  - Street traffic control
KW  - Time switches
KW  - Unmanned aerial vehicles (UAV)
KW  - Advanced air mobility
KW  - Aerial vehicle
KW  - Autonomous aircraft
KW  - Autonomy
KW  - Detect and avoid
KW  - Detection
KW  - Edge computing
KW  - Non-cooperative
KW  - Non-cooperative traffic management
KW  - Onboard computing
KW  - Re identifications
KW  - Sense and avoid
KW  - Tracking
KW  - Traffic management
KW  - Unmanned aerial vehicle
KW  - Aircraft accidents
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hussein, M.
AU  - Zhu, W.-X.
TI  - A real-time ghost machine learning model built on YOLOv8 for traffic road signs detection and classification in Germany
PY  - 2024
T2  - Multimedia Systems
DO  - 10.1007/s00530-024-01527-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209582294&doi=10.1007%2fs00530-024-01527-1&partnerID=40&md5=30e8a2d47d94c473632f407bd816aaee
AB  - Identifying traffic signs is an essential part of traffic safety and self-driving systems. In real life, the driving environment is changing, making detecting traffic signs wisely and economically vital. The traffic sign detection problem has several small objects and complex ambient interference. The detecting situation also requires a practical and lightweight detection model. This study proposes a new lightweight model, the enhanced Ghost-YOLOv8, based on lightweight modules GhostConv and C3Ghost, based on the YOLOv8 model. It used a light method to extract the features, significantly speeding up inference. In addition to small, medium, and large objects, the head was expanded to include a new multi-scale feature extraction module layer focused on x-small. The experiment results show that when using the German Traffic Sign Detection Benchmark (GTSDB) dataset with three classes, the enhanced Ghost-YOLOv8 has mAP (0.50) of 99.4%and has fewer computations than the YOLOv8 model by 155.2 GFLOPs and has 18.6 Mparameters, which represents only 27.3% from the parameters used in the base model. Also, we suggested a new dataset called the GTSDB-43 dataset, which expanded the number of classes on the GTSDB dataset from three or four main classes to 43 classes and mentioned their main category type simultaneously. Compared with notable algorithms, this method's accuracy and speed are competitive. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - Lightweight
KW  - Object detection
KW  - Real-time
KW  - The enhanced Ghost-YOLOv8
KW  - YOLOv8 model
KW  - Adversarial machine learning
KW  - Inference engines
KW  - Machine learning
KW  - Motor transportation
KW  - Object detection
KW  - Benchmark datasets
KW  - Lightweight
KW  - Machine learning models
KW  - Objects detection
KW  - Real- time
KW  - Road sign classifications
KW  - Road sign detection
KW  - The enhanced ghost-YOLOv8
KW  - Traffic sign detection
KW  - YOLOv8 model
KW  - Traffic signs
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gao, W.
AU  - Gu, W.
AU  - Yin, Y.
AU  - Li, T.
AU  - Dong, P.
TI  - ODCS-YOLO detection algorithm for rail surface defects based on omni-dimensional dynamic convolution and context augmentation module
PY  - 2024
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/ad5dd5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198605669&doi=10.1088%2f1361-6501%2fad5dd5&partnerID=40&md5=3d3f29c77852c3ce5522be9477cbf085
AB  - To solve the problems of easy miss and false detection on rail surface defects caused by small size, dense target, and high similarity between features and background, this paper proposed an improved detection algorithm in complex background. First, the conventional convolution of YOLOv5 backbone network is replaced with omni-dimensional dynamic convolution (ODConv), which improves the feature extraction capability of the network without increasing the computational cost; second, to improve the model’s performance in detecting tiny objects, a two-layer context augmentation module (CAM) is introduced into the path aggregation network (PAN) structure; finally, the traditional non-maximum suppression (NMS) algorithm is replaced by the Soft-NMS algorithm in the network post-processing to reduce the false-alarm and miss-rate. The experimental results on the Railway Track Fault Detection public dataset show that the OD-YOLO (OD stands for ODConv) and C-PAN (CAM module is introduced into PAN) structures could achieve better performance in the same type of improved algorithms; compared with the baseline algorithm YOLOv5, the ODCS-YOLO (OD stands for ODConv, C stands for CAM and S stands for Soft-NMS) algorithm improves the precision by 12.4%, the recall by 3.6%, the map50 by 8.6% and the GFLOPs is reduced by 0.6. Compared with seven classical object detection algorithms, the ODCS-YOLO algorithm achieves the highest detection accuracy, which makes it able to meet the real-time detection requirements of rail surface defects in real working conditions. The ODCS-YOLO model provides certain technical support for the defects detection and a new method for the detection of dense small objects. © 2024 IOP Publishing Ltd.
KW  - CAM
KW  - dense small object
KW  - ODConv
KW  - ODCS-YOLO
KW  - rail defects detection
KW  - Soft-NMS
KW  - C (programming language)
KW  - Fault detection
KW  - Feature extraction
KW  - Object detection
KW  - Signal detection
KW  - Surface defects
KW  - Context augmentation module
KW  - Defect detection
KW  - Dense small object
KW  - Dimensional dynamics
KW  - Non-maximum suppression
KW  - ODCS-YOLO
KW  - Omni-dimensional dynamic convolution
KW  - Rail defect detection
KW  - Rail defects
KW  - Small objects
KW  - Soft-non-maximum suppression
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, L.
AU  - Jiang, F.
AU  - Zhu, F.
AU  - Ren, L.
TI  - Enhanced Multi-Target Detection in Complex Traffic Using an Improved YOLOv8 with SE Attention, DCN_C2f, and SIoU
PY  - 2024
T2  - World Electric Vehicle Journal
DO  - 10.3390/wevj15120586
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213485561&doi=10.3390%2fwevj15120586&partnerID=40&md5=da91324e462fdbb674ee71b53a8ada9b
AB  - This paper presents an enhanced YOLOv8 model designed to address multi-target detection challenges in complex traffic scenarios. The model integrates the Squeeze-and-Excitation attention mechanism, the deformable convolution C2f module, and the smooth IoU loss function, achieving significant improvements in detection accuracy and robustness in various complex environments. Experimental results show that the enhanced YOLOv8 model outperforms existing YOLO solutions across multiple metrics, particularly in precision and recall. Specifically, the enhanced model achieves 83.8% precision and 82.7% recall, improving 1.05 times in precision and 1.1 times in recall compared to the average precision (79.7%) and recall (75.4%) of other YOLO series models. In terms of mAP_0.5, the enhanced model achieves 89%, representing a 1.05-fold improvement over the average mAP_0.5 (84.4%) of YOLO series models. For mAP_0.5:0.95, the enhanced model reaches 76.5%, which is a 1.1-fold improvement over the average mAP_0.5:0.95 (69.7%) of YOLO series models. These improvements demonstrate the superior performance of the proposed model in multi-scale and complex scenarios, providing strong support for intelligent transportation systems and autonomous driving. © 2024 by the authors.
KW  - complex traffic environment
KW  - deformable convolution C2f module
KW  - intelligent transportation
KW  - multi-target detection
KW  - object detection
KW  - SE attention mechanism
KW  - smooth IoU loss function
KW  - YOLOv8
KW  - Object detection
KW  - Object recognition
KW  - Attention mechanisms
KW  - Complex traffic environment
KW  - Deformable convolution c2f module
KW  - Intelligent transportation
KW  - Loss functions
KW  - Multi-target detection
KW  - Objects detection
KW  - SE attention mechanism
KW  - Smooth IoU loss function
KW  - Traffic environment
KW  - YOLOv8
KW  - Intelligent systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jing, X.
AU  - Wang, Y.
AU  - Li, D.
AU  - Pan, W.
TI  - Melon ripeness detection by an improved object detection algorithm for resource constrained environments
PY  - 2024
T2  - Plant Methods
DO  - 10.1186/s13007-024-01259-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201394021&doi=10.1186%2fs13007-024-01259-3&partnerID=40&md5=f10a0ba5c0f6d1267e0297080467f23a
AB  - Background: Ripeness is a phenotype that significantly impacts the quality of fruits, constituting a crucial factor in the cultivation and harvesting processes. Manual detection methods and experimental analysis, however, are inefficient and costly. Results: In this study, we propose a lightweight and efficient melon ripeness detection method, MRD-YOLO, based on an improved object detection algorithm. The method combines a lightweight backbone network, MobileNetV3, a design paradigm Slim-neck, and a Coordinate Attention mechanism. Additionally, we have created a large-scale melon dataset sourced from a greenhouse based on ripeness. This dataset contains common complexities encountered in the field environment, such as occlusions, overlapping, and varying light intensities. MRD-YOLO achieves a mean Average Precision of 97.4% on this dataset, achieving accurate and reliable melon ripeness detection. Moreover, the method demands only 4.8 G FLOPs and 2.06 M parameters, representing 58.5% and 68.4% of the baseline YOLOv8n model, respectively. It comprehensively outperforms existing methods in terms of balanced accuracy and computational efficiency. Furthermore, it maintains real-time inference capability in GPU environments and demonstrates exceptional inference speed in CPU environments. The lightweight design of MRD-YOLO is anticipated to be deployed in various resource constrained mobile and edge devices, such as picking robots. Particularly noteworthy is its performance when tested on two melon datasets obtained from the Roboflow platform, achieving a mean Average Precision of 85.9%. This underscores its excellent generalization ability on untrained data. Conclusions: This study presents an efficient method for melon ripeness detection, and the dataset utilized in this study, alongside the detection method, will provide a valuable reference for ripeness detection across various types of fruits. © The Author(s) 2024.
KW  - Deep learning
KW  - Melon
KW  - Object detection
KW  - Ripeness detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, X.
AU  - Liu, J.
AU  - Zhao, G.
AU  - Liu, L.
AU  - Zhang, W.
AU  - Hu, X.
AU  - Cheng, S.
TI  - High precision single-photon object detection via deep neural networks
PY  - 2024
T2  - Optics Express
DO  - 10.1364/OE.533032
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206007733&doi=10.1364%2fOE.533032&partnerID=40&md5=c4936353b267e98d6cb787e95fbcfea3
AB  - Single-photon imaging is an emerging technology in sensing that is capable of imaging and identifying remote objects under extreme conditions. However, it faces several challenges, such as low resolution and high noise, to do the task of object detection. In this work, we propose an enhanced You Only Look Once network to identify and localize objects within images generated by single-photon sensing. We then experimentally test the proposed network on both the self-built single-photon dataset and the VisDrone2019 public dataset. Our results show that our network achieves a higher detection accuracy than the baseline models. Moreover, it admits a higher average precision in detecting small single-photon objects. Our work is expected to aid significant progress in exploring practical applications of single-photon sensing. © 2024 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.
KW  - Atomic beams
KW  - Electron beams
KW  - Open access
KW  - Photons
KW  - Emerging technologies
KW  - Extreme conditions
KW  - High-precision
KW  - Low-high
KW  - Lower resolution
KW  - Neural-networks
KW  - Objects detection
KW  - Remote object
KW  - Single photons
KW  - Single-photon imaging
KW  - accuracy
KW  - article
KW  - controlled study
KW  - deep neural network
KW  - diagnosis
KW  - female
KW  - noise
KW  - photon
KW  - Deep neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kong, L.
AU  - Wang, Y.
AU  - Chang, D.
AU  - Zhao, Y.
TI  - Temporal-Enhanced Radar and Camera Fusion for Object Detection
PY  - 2024
T2  - ACM Transactions on Multimedia Computing, Communications and Applications
DO  - 10.1145/3700442
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215566371&doi=10.1145%2f3700442&partnerID=40&md5=7dc28762cbfab52891f45135c787c9f2
AB  - Recently, object detection methods based on multi-modal fusion have gained widespread adoption in autonomous driving, proving to be valuable for detecting objects in dynamic environments. Among them, millimeter wave (mmWave) radar is commonly utilized as an effective complement to cameras, as it is almost unaffected by harsh weather conditions. However, current approaches that fuse mmWave radar and camera often overlook the correlation between the two modalities, failing to fully exploit their complementary features. To address this, we propose a temporal-enhanced radar and camera fusion network to explore the correlation between these two modalities and learn a comprehensive representation for object detection. In our model, a temporal fusion model is introduced to fuse mmWave radar features from different moments, thus mitigating the problem of mmWave radar point-object mismatch due to object movement. Moreover, a new correlation-based fusion strategy using the dedicated mask cross-attention is proposed to fuse mmWave radar and vision features more effectively. Finally, we design a gate feature pyramid network that selects shallow texture information based on deep semantic information to obtain more representative features. The experimental results on the nuScenes benchmark demonstrate the effectiveness of our proposed method. © 2024 Copyright held by the owner/author(s).
KW  - Automatic Driving
KW  - Cross Attention
KW  - Ensemble
KW  - Benchmarking
KW  - Monolithic microwave integrated circuits
KW  - Object recognition
KW  - Automatic driving
KW  - Autonomous driving
KW  - Cross attention
KW  - Detecting objects
KW  - Ensemble
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Multi-modal fusion
KW  - Object detection method
KW  - Objects detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nguyen, V.-T.
AU  - Nguyen, P.-T.
AU  - Su, S.-F.
AU  - Tan, P.X.
AU  - Bui, T.-L.
TI  - Vision-Based Pick and Place Control System for Industrial Robots Using an Eye-in-Hand Camera
PY  - 2025
T2  - IEEE Access
DO  - 10.1109/ACCESS.2025.3536496
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217026346&doi=10.1109%2fACCESS.2025.3536496&partnerID=40&md5=3e24fabda40d0dfb5fe74136beffa31e
AB  - In this paper, we present a vision-based pick-and-place control system for industrial robots using an eye-in-hand camera. In industry, using robots with cameras greatly improves efficiency and performance. Previous studies have focused on utilizing robotic arms for the pick-and-place process in simulated environments. The challenge when experimenting with real systems lies in aligning the coordinate systems between the robot and the camera, as well as ensuring high data accuracy during experimentation. To address this issue, our research focuses on utilizing a low-cost 2D camera combined with deep learning algorithms mounted on the end-effector of the robotic arm. This study is evaluated in both simulation and real-world experiments. We propose a novel approach that combines the YOLOv7 (You Only Look Once V7) deep learning network with GAN (Generative Adversarial Networks) to achieve fast and accurate object recognition. This system uses deep learning to process camera data to extract object positions for the robot in real-time. Due to its advantages of fast inference and high accuracy, YOLO is applied as the baseline for research. By training the deep learning model on diverse objects, it effectively recognizes and detects any object in the robot's workspace. Through experimental results, we demonstrate the feasibility and effectiveness of our vision-based pick-and-place system. Our research contributes an important advancement in the field of industrial robots by showcasing the potential of using a 2D camera and an integrated deep learning system for object manipulation.  © 2013 IEEE.
KW  - calibration vision
KW  - object detection
KW  - robot real-time
KW  - Robotic arm
KW  - vision
KW  - Deep learning
KW  - End effectors
KW  - Generative adversarial networks
KW  - Machine vision
KW  - Robot learning
KW  - Robot vision
KW  - Robotic arms
KW  - SLAM robotics
KW  - Calibration vision
KW  - Efficiency and performance
KW  - Eye-in-hand
KW  - Objects detection
KW  - Pick and place
KW  - Real systems
KW  - Real- time
KW  - Robot real-time
KW  - Simulated environment
KW  - Vision based
KW  - Industrial robots
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sun, F.
AU  - He, N.
AU  - Wang, X.
AU  - Liu, H.
AU  - Zou, Y.
TI  - YOLOv7-P: a lighter and more effective UAV aerial photography object detection algorithm
PY  - 2024
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-024-03476-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201712675&doi=10.1007%2fs11760-024-03476-8&partnerID=40&md5=890950ddde3068b3376b3f5c64ad39f4
AB  - Because of the special way an unmanned aerial vehicle (UAV) acquires aerial photography, UAV images have the characteristics of large coverage area, complex background, and a large proportion of small targets, which exacerbate the difficulty of object detection. Additionally, UAV-based aerial image detection needs to meet lightweight and real-time capabilities. To address these issues, this paper proposes a lightweight model YOLOv7-P that is based on YOLOv7 but has a stronger detection capability for small targets. First, partial convolution (PConv) is used to reduce redundant parameters and computation in YOLOv7. Second, an optimal combination of detection heads is determined that can significantly improve the detection performance of small objects. Third, a novel lightweight convolution called PConv-wide is proposed to replace RepConv in the network, thus simplifying the network without affecting detection accuracy. Finally, the normalized wasserstein distance loss is reasonably combined with the complete intersection over union loss to further improve the sensitivity of the network to small targets. The proposed YOLOv7-P model strikes a delicate balance between precision and parameter count. Compared with the baseline YOLOv7 network, it reduces parameter count by 47.1% without increasing computational complexity and boosts AP50 by 8% and mAP by 5.4% on the VisDrone dataset. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.
KW  - Lightweight
KW  - Small object detection
KW  - UAV aerial imaging
KW  - YOLOv7
KW  - Aircraft detection
KW  - Image enhancement
KW  - Photographic equipment
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial imaging
KW  - Aerial vehicle
KW  - Coverage area
KW  - Lightweight
KW  - Object detection algorithms
KW  - Small object detection
KW  - Small targets
KW  - Unmanned aerial vehicle aerial imaging
KW  - Vehicle images
KW  - YOLOv7
KW  - Aerial photography
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jing, Y.
AU  - Sun, Y.
AU  - Wang, Q.
TI  - Lightweight Single-Stage Network for Gas Leak Detection Based on Infrared Imaging
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3561424
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002786786&doi=10.1109%2fTIM.2025.3561424&partnerID=40&md5=88b5090d5bf4452fa3cdb1c420b8fb5a
AB  - Gas leak detection is essential for real-time monitoring and safety early warning of industrial production, manufacturing, and transportation processes. For many years, infrared optical gas imaging (IOGI) has been widely used in the field of gas leak monitoring, but the task still faces great challenges due to the limitations of infrared imaging principle and system technology, as well as the characteristics of insubstantial gas objects. First, a dataset containing 66950 infrared images is built, which covers gas leak samples with different scales, shapes, and blurring levels. Second, a single-stage gas leak detection network model named dual layer focus aggregation network (DLFANet) was designed. Specifically, a lightweight feature extraction cross-stage partially efficient two-layer aggregation network cross stage partial-efficient dual layer aggregation network (CSP-EDLAN) module is designed to enhance the transmission of gradient flow information and cross-channel information interaction, where dual convolution (DualConv) is utilized to reduce the computational consumption of feature extraction. A focal modulation module is introduced into the backbone network to realize the focus of the gas target by integrating the characteristic information of different scales. In addition, the wise intersection shape intersection over union (Wise-Shape-IoU) loss function with a dynamic non-monotonic mechanism and shape constraint capability is designed to prevent low-quality samples from generating harmful gradients, which makes the bounding box regression (BBR) of gas targets with greater accuracy. Finally, extensive experimental results on the constructed dataset show that the proposed DLFANet strikes a better balance between detection accuracy (map) and speed frame per second (FPS) while predicting the BBR of gaseous objects more accurately compared to state-of-the-art models. © 2025 IEEE.
KW  - Deep learning
KW  - industrial gas leakage
KW  - infrared imaging
KW  - insubstantial gas object detection
KW  - Critical path analysis
KW  - Information leakage
KW  - Network security
KW  - Thermography (imaging)
KW  - Aggregation network
KW  - Deep learning
KW  - Gas leak detection
KW  - Gas leakages
KW  - Gas leaks
KW  - Industrial gas
KW  - Industrial gas leakage
KW  - Insubstantial gas object detection
KW  - Objects detection
KW  - Single stage
KW  - Leak detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nasir, F.A.
AU  - Liaquat, S.
AU  - Naqvi, I.H.
AU  - Khurshid, K.
AU  - Mahyuddin, N.M.
TI  - Improved You Only Look Once (YOLOv5)-based Passive Missile Detection using Simulated Solar Blind Ultraviolet Signatures
PY  - 2025
T2  - IEEE Aerospace and Electronic Systems Magazine
DO  - 10.1109/MAES.2025.3555249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001527622&doi=10.1109%2fMAES.2025.3555249&partnerID=40&md5=d7bfc4606b6e52de9e21802e71bba60a
AB  - Civilian and military aircraft are increasingly vulnerable to passive missile threats, such as short-range or within-visual-range air-to-air missiles (SRAAMs/WVRAAMs) and man-portable air defense systems (MANPADS). These missiles evade detection by an aircraft's radar warning receiver (RWR) due to their lack of radio frequency (RF) emissions. This paper presents a novel deep learning-based passive missile detection algorithm using simulated solar blind ultraviolet (SBUV) signatures, which offer unique advantages over traditional infrared (IR) signatures. The algorithm is built on an improved YOLOv5 (You Only Look Once) framework, capable of detecting and classifying UV signatures in real-time from sequential image data. The architecture of YOLOv5 has been modified for improved SBUV detection by recalculation of anchor box sizes, non-maximum suppression (NMS) threshold re-adjustment, SBUV specific data augmentations and increased resolution in detection heads. To overcome the challenge of limited training data, we employ advanced data synthesis techniques to create realistic training datasets derived from 3D missile and aircraft combat scenario simulations in the SBUV spectrum. These simulations incorporate diverse parameters and conditions to closely replicate real-world scenarios, ensuring high fidelity and robustness. Performance evaluation against real-world scenarios revealed an F1-score of 95% and a mean average precision (mAP) of 95% for synthetic data, compared to 88.36% and 85%, respectively, for real-world data. The algorithm, achieved a detection accuracy of 92% on synthetic data and 86% on real-world data, demonstrating its reliability and robustness. Validation under varying environmental conditions demonstrated optimal performance under clear skies, achieving benchmark results. Minimal performance degradation was observed under low-light conditions, such as dusk or dawn, as well as fog. However, overcast conditions posed the most significant challenge, reducing detection accuracy from 86% to 79% and the F1-score from 88.36% to 80%. The processing speed of the proposed algorithm satisfies the real-time requirements of modern missile warning systems, making it a potential candidate for operational deployment. © 1986-2012 IEEE.
KW  - Detection accuracy
KW  - F1-score
KW  - Infrared (IR) signatures
KW  - Passive missile detection
KW  - Solar blind ultraviolet (SBUV) signatures
KW  - YOLOv5.
KW  - Air to surface missiles
KW  - Aircraft detection
KW  - Benchmarking
KW  - Building codes
KW  - Chemical sensors
KW  - Fighter aircraft
KW  - Radar warning systems
KW  - Remote sensing
KW  - Solar irradiance
KW  - Surface to air missiles
KW  - Temperature sensors
KW  - Tracking radar
KW  - Condition
KW  - Detection accuracy
KW  - F1 scores
KW  - Infrared signature
KW  - Passive missile detection
KW  - Real-world scenario
KW  - Solar blind ultraviolet
KW  - Solar blind ultraviolet  signature
KW  - Synthetic data
KW  - YOLOv5.
KW  - Air to air missiles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sánchez Pedroche, D.
AU  - Amigo, D.
AU  - García, J.
AU  - Molina, J.M.
AU  - Zubasti, P.
TI  - Drone Swarm for Distributed Video Surveillance of Roads and Car Tracking
PY  - 2024
T2  - Drones
DO  - 10.3390/drones8110695
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210318621&doi=10.3390%2fdrones8110695&partnerID=40&md5=f6a805c14205d31c37545bc4fa566790
AB  - This study proposes a swarm-based Unmanned Aerial Vehicle (UAV) system designed for surveillance tasks, specifically for detecting and tracking ground vehicles. The proposal is to assess how a system consisting of multiple cooperating UAVs can enhance performance by utilizing fast detection algorithms. Within the study, the differences in one-stage and two-stage detection models have been considered, revealing that while two-stage models offer improved accuracy, their increased computation time renders them impractical for real-time applications. Consequently, faster one-stage models, such as the tested YOLOv8 architectures, appear to be a more viable option for real-time operations. Notably, the swarm-based approach enables these faster algorithms to achieve an accuracy level comparable to that of slower models. Overall, the experimentation analysis demonstrates how larger YOLO architectures exhibit longer processing times in exchange for superior tracking success rates. However, the inclusion of additional UAVs introduced in the system outweighed the choice of the tracking algorithm if the mission is correctly configured, thus demonstrating that the swarm-based approach facilitates the use of faster algorithms while maintaining performance levels comparable to slower alternatives. However, the perspectives provided by the included UAVs hold additional significance, as they are essential for achieving enhanced results. © 2024 by the authors.
KW  - UAV surveillance
KW  - UAV swarm configuration
KW  - vehicle detection and tracking
KW  - Drones
KW  - Security systems
KW  - Swarm intelligence
KW  - Aerial vehicle
KW  - Detection and tracking
KW  - Distributed video surveillances
KW  - Fast algorithms
KW  - Swarm-based approach
KW  - Unmanned aerial vehicle surveillance
KW  - Unmanned aerial vehicle swarm configuration
KW  - Vehicle surveillances
KW  - Vehicles detection
KW  - Aircraft detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, X.
AU  - Jiang, T.
AU  - Zhu, Y.
AU  - Li, L.
AU  - Fan, F.
AU  - Jin, X.
TI  - FEL-YoloV8: A New Algorithm for Accurate Monitoring Soybean Seedling Emergence Rates and Growth Uniformity
PY  - 2025
T2  - IEEE Transactions on Geoscience and Remote Sensing
DO  - 10.1109/TGRS.2025.3578800
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007896170&doi=10.1109%2fTGRS.2025.3578800&partnerID=40&md5=1ecdb6446a4825355d55878bdcb4a188
AB  - Effective monitoring of soybean emergence rates and growth uniformity is crucial for soybean breeding evaluation and field management. Although Unmanned Aerial Vehicles (UAVs) have improved image acquisition efficiency, detecting soybean seedlings during the vegetative emergence (VE) stage remains challenging due to their small size, low contrast, and insufficient information. Existing studies often neglect emergence rate and growth uniformity quantification. This study proposes a fully automated method for monitoring soybean emergence rate and growth uniformity, applicable to both ground-based and UAV platforms. The FEL-YoloV8 model was constructed by enhancing the feature extraction module, improving the feature fusion module, and incorporating model lightweight modules. Based on the detection results from the FEL-YoloV8 model, the missing seedling locations, counts, and growth uniformity of soybeans were estimated. The study shows that the feature enhancement and fusion modules improved the model’s performance by 2.10%. Under comparable computational complexity, the performance of the FEL-YoloV8 model (AP = 0.979) surpasses current advanced models (e.g., Faster R-CNN, RT-DETR, YoloV8n, s, m, l, x, YoloV9, YoloV10, L-FFCA-Yolo, and TPH-YoloV5). The detection results from the FEL-YoloV8 model enabled the estimation of missing seedling locations and quantities in soybeans. The proposed method enables fully automated monitoring of soybean emergence rates and growth uniformity. This approach lays the foundation for accurate multi-platform evaluation of soybean emergence rates and growth uniformity, guiding soybean breeding evaluation and field management. © 1980-2012 IEEE.
KW  - Emergence rates
KW  - Feature fusion module
KW  - Growth uniformity
KW  - Soybean
KW  - YoloV8
KW  - Aircraft detection
KW  - Antennas
KW  - Automation
KW  - Feature extraction
KW  - Nitrogen fixation
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Emergence rate
KW  - Feature fusion module
KW  - Features fusions
KW  - Field management
KW  - Fusion modules
KW  - Growth uniformity
KW  - Soybean
KW  - Soybean seedlings
KW  - Yolov8
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, B.
AU  - Zhu, Z.
AU  - Chen, Y.
AU  - Wang, J.
AU  - Gao, K.
AU  - Qian, X.
TI  - A Diffusion model-based intelligent optimization method of rural road environments
PY  - 2025
T2  - International Journal of Transportation Science and Technology
DO  - 10.1016/j.ijtst.2025.01.014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217924747&doi=10.1016%2fj.ijtst.2025.01.014&partnerID=40&md5=8cdcf3c0289cf3243df8d615499ea863
AB  - Well-designed rural road environments can guide drivers to adopt reasonable driving behaviors, thereby significantly improving the driving experience and ensuring road safety. Existing methods for optimizing rural road environments mainly rely on expert knowledge, have low automation degrees, and are limited in efficiency and accuracy. Therefore, this study aims to propose an intelligent optimization method for rural road environments by using image generation technology. Using environment images from a naturalistic driving dataset, the area and location information of semantic components (e.g., lane markings, vegetation, guardrails, traffic signs, etc.) in rural road environments are extracted, and their impacts on driving speed is analyzed based on explainable machine learning (XGBoost and SHAP). These impacts are then utilized to determine how to adjust and optimize the road environment components at appropriate locations (i.e., obtain the optimization scheme). Then, a novel image generation technique, Diffusion model, is employed to establish an intelligent optimization method, which can directly generate optimized images of rural road environments. Compared to traditional manual mapping or other popular image generation algorithms such as CycleGAN, the method proposed in this study has the advantages of high efficiency, labor saving, and better image generation quality. This study can facilitate the design and optimization of rural road environments and enhance rural road safety in a more intelligent way. © 2025 Tongji University and Tongji University Press
KW  - Diffusion model
KW  - Explainable machine learning
KW  - Image generation technology
KW  - Intelligent optimization
KW  - Rural road environments
KW  - Diffusion model
KW  - Explainable machine learning
KW  - Generation technologies
KW  - Image generation technology
KW  - Image generations
KW  - Intelligent optimization
KW  - Intelligent optimization method
KW  - Machine-learning
KW  - Road environment
KW  - Rural road environment
KW  - Road and street markings
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, H.
AU  - Peng, T.
AU  - Qiao, N.
AU  - Guan, Z.
AU  - Feng, X.
AU  - Guo, P.
AU  - Duan, T.
AU  - Gong, J.
TI  - CrackTinyNet: A novel deep learning model specifically designed for superior performance in tiny road surface crack detection
PY  - 2024
T2  - IET Intelligent Transport Systems
DO  - 10.1049/itr2.12497
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186546798&doi=10.1049%2fitr2.12497&partnerID=40&md5=73f51dc4af152782cc70e355d4204095
AB  - With the rapid advancement of highway construction, the maintenance of highway infrastructure has become particularly vital. During highway maintenance, the effective detection of tiny road surface cracks helps to extend the lifespan of roads and enhance traffic efficiency and safety. To elevate the performance of existing road detection models, the CrackTinyNet (CrTNet) algorithm is specifically proposed for detecting tiny road surface cracks. This algorithm utilizes the novel BiFormer general visual transformer, designed expressly for tiny objects, and optimizes the loss function to a normalized Wasserstein distance loss function. It replaces traditional downsampling with Space-to-Depth Conv to prevent the excessive loss of tiny object information in the network structure. To highlight the model's advantage in detecting tiny road cracks, ablation experiments and comparison trials were conducted with mainstream deep learning models for crack detection. The results of the ablation experiments show that, compared to the baseline, CrTNet improved the Mean Average Precision (MAP) by 0.22. When compared to other network models suitable for road detection, these results exhibited an improvement of over 8.9%. In conclusion, the CrTNet proposed in this study enables a more accurate detection of tiny road cracks, playing a significant role in the advancement of intelligent traffic management. © 2024 The Author(s). IET Intelligent Transport Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
KW  - crack detection
KW  - object detection
KW  - road safety
KW  - road traffic
KW  - Ablation
KW  - Crack detection
KW  - Deep learning
KW  - Highway administration
KW  - Highway planning
KW  - Highway traffic control
KW  - Intelligent systems
KW  - Learning systems
KW  - Motor transportation
KW  - Roads and streets
KW  - Surface defects
KW  - Learning models
KW  - Loss functions
KW  - Objects detection
KW  - Performance
KW  - Road cracks
KW  - Road detection
KW  - Road safety
KW  - Road surfaces
KW  - Road traffic
KW  - Surface cracks
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xing, Z.
AU  - Meng, Z.
AU  - Zheng, G.
AU  - Ma, G.
AU  - Yang, L.
AU  - Guo, X.
AU  - Tan, L.
AU  - Jiang, Y.
AU  - Wu, H.
TI  - Intelligent rehabilitation in an aging population: empowering human-machine interaction for hand function rehabilitation through 3D deep learning and point cloud
PY  - 2025
T2  - Frontiers in Computational Neuroscience
DO  - 10.3389/fncom.2025.1543643
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005275255&doi=10.3389%2ffncom.2025.1543643&partnerID=40&md5=93a029ba51dfa0ab16d4dba35aadcef4
AB  - Human-machine interaction and computational neuroscience have brought unprecedented application prospects to the field of medical rehabilitation, especially for the elderly population, where the decline and recovery of hand function have become a significant concern. Responding to the special needs under the context of normalized epidemic prevention and control and the aging trend of the population, this research proposes a method based on a 3D deep learning model to process laser sensor point cloud data, aiming to achieve non-contact gesture surface feature analysis for application in the field of intelligent rehabilitation of human-machine interaction hand functions. By integrating key technologies such as the collection of hand surface point clouds, local feature extraction, and abstraction and enhancement of dimensional information, this research has constructed an accurate gesture surface feature analysis system. In terms of experimental results, this research validated the superior performance of the proposed model in recognizing hand surface point clouds, with an average accuracy of 88.72%. The research findings are of significant importance for promoting the development of non-contact intelligent rehabilitation technology for hand functions and enhancing the safe and comfortable interaction methods for the elderly and rehabilitation patients. Copyright © 2025 Xing, Meng, Zheng, Ma, Yang, Guo, Tan, Jiang and Wu.
KW  - 3D perception
KW  - deep learning
KW  - human-machine interaction
KW  - neural network
KW  - non-contact rehabilitation
KW  - Assistive technology
KW  - Computational neuroscience
KW  - Deep neural networks
KW  - Functional electric stimulation
KW  - Functional neural stimulation
KW  - Human rehabilitation equipment
KW  - Patient monitoring
KW  - 3D perception
KW  - Deep learning
KW  - Hand function
KW  - Human machine interaction
KW  - Neural-networks
KW  - Non-contact
KW  - Non-contact rehabilitation
KW  - Point-clouds
KW  - Surface feature analysis
KW  - Surface points
KW  - accuracy
KW  - aging
KW  - algorithm
KW  - Article
KW  - artificial intelligence
KW  - artificial neural network
KW  - cloud
KW  - confusion
KW  - deep learning
KW  - epidemic
KW  - feature extraction
KW  - gesture
KW  - hand function
KW  - human
KW  - human machine interaction
KW  - intelligent rehabilitation
KW  - learning algorithm
KW  - machine learning
KW  - nerve cell network
KW  - neuroscience
KW  - non contact rehabilitation
KW  - perception
KW  - rehabilitation patient
KW  - theoretical neuroscience
KW  - training
KW  - Functional assessment
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nguyen, A.V.
AU  - Hoang, V.T.
AU  - Tran, T.H.
TI  - Construction of Robotics and Application of the Optical-Flow Algorithm in Determining Robot Motions
PY  - 2024
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app14209342
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207335312&doi=10.3390%2fapp14209342&partnerID=40&md5=9a6568d9b91bc7ccc32f8b63a01de0a9
AB  - This article presents the research results in the application of image processing in determining the position, direction, and moving speed of different objects when they move in the free space in the field of view of measuring cameras. The research includes developing an algorithm to detect, identify, and locate objects and an algorithm to calculate the movement direction and instantaneous velocity of the object. Two robots with measurement systems were designed for testing, one carries the investigating object, and the other the camera. These robots can communicate with a computer system using a tele-wireless system. A program was also built for capturing images and analyzing the state of the model automatically. Experimental results show that the position, angle, and velocity of different objects can be captured well. The average error in determining the direction of movement is an average of 1.25°, and the error of the moving speed is less than 0.5 m/s. The research results provide a potential tool for designing robots for highly effective detection. The algorithm and the measurement system are simple and inexpensive but highly effective, and they can be used in the initial process of determining an object. A detailed description of the algorithm, robot system, and testing will be presented in this study. © 2024 by the authors.
KW  - computer vision
KW  - controlling robot
KW  - image processing
KW  - optical flow
KW  - robot
KW  - Industrial robots
KW  - Machine vision
KW  - Microrobots
KW  - Object detection
KW  - Optical data processing
KW  - Robot applications
KW  - Robot vision
KW  - Controling robot
KW  - Field of views
KW  - Free spaces
KW  - Images processing
KW  - Measurement system
KW  - Moving speed
KW  - Optical flow algorithm
KW  - Optical-
KW  - Research results
KW  - Robot motion
KW  - Optical flows
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, X.
AU  - Li, H.
TI  - A study on UAV target detection and 3D positioning methods based on the improved deformable DETR model and multi-view geometry
PY  - 2025
T2  - Advances in Mechanical Engineering
DO  - 10.1177/16878132251315505
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216127423&doi=10.1177%2f16878132251315505&partnerID=40&md5=ba1cb2360dd200bffcb92be4b3699991
AB  - This paper addresses critical challenges in Unmanned Aerial Vehicle (UAV) target detection and 3D positioning, specifically inaccuracies in localization and lack of robustness in complex environments. The objective of this research is to improve UAV detection and positioning accuracy by proposing an enhanced Deformable DETR (Detection Transformer) model integrated with multi-view geometry theory. To achieve this, the study first preprocesses UAV-collected data, then optimizes the convolutional layers of the original DETR model to better handle object occlusion and scale variations. Furthermore, the research incorporates multi-view geometric modeling and multimodal fusion strategies to enhance detection accuracy during the target recognition process. Experimental results demonstrate that the proposed approach achieves over 70% detection accuracy, significantly outperforming traditional methods. The findings underscore the effectiveness of combining the improved Deformable DETR model with multi-view geometry for high-precision detection and 3D localization in complex environments. This research has significant implications for UAV-based applications, such as autonomous navigation, surveillance, and search-and-rescue missions, where precise target detection and 3D positioning are critical for successful operation. © The Author(s) 2025.
KW  - image denoising
KW  - improved deformable DETR model
KW  - Multiple unmanned aerial vehicle object detection
KW  - multiple view geometry
KW  - three-dimensional positioning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lin, T.
AU  - Ren, Z.
AU  - Zhu, L.
AU  - Zhu, Y.
AU  - Feng, K.
AU  - Ding, W.
AU  - Yan, K.
AU  - Beer, M.
TI  - A Systematic Review of Multi-Sensor Information Fusion for Equipment Fault Diagnosis
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3529577
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215662454&doi=10.1109%2fTIM.2025.3529577&partnerID=40&md5=55588458aebc2fafe5989cd27d81f5ed
AB  - In contrast to fault diagnosis relying solely on a single sensor, the method of multi-sensor information fusion for fault diagnosis (MSIFFD) broadens the spectrum of available information sources. It is renowned for its high accuracy and reliability, traits that have attracted growing attention within the research community and led to the generation of a substantial body of publications. However, there is currently a lack of a comprehensive and systematic review in this domain. Thereby, this review aims to thoroughly explore all research achievements in the field of MSIFFD. At the outset, an analysis is undertaken to delineate the fusion level of multi-sensor information, with a specific focus on its location and types of inputs and outputs within the information flow. Subsequently, an examination of the six primary fundamental operations for amalgamating multi-sensor information is undertaken to clarify the motivations and processes involved in information fusion across various operations. Following this, factors influencing fusion diagnostics and strategies aimed at improving their performance are examined to explore approaches for enhancing fusion accuracy. The subsequent sections delve into the analysis of sensor types and application scenarios, providing a reference guide for practical applications. Finally, this review outlines potential future challenges in the field of MSIFFD and provides a range of recommendations and possible solutions for consideration.  © 1963-2012 IEEE.
KW  - Equipment Fault Diagnosis
KW  - Fusion Level
KW  - Fusion Operation
KW  - Fusion Strategy
KW  - Information Fusion
KW  - Multi-Sensor
KW  - Signal Type
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Butler, J.
AU  - Leung, H.
TI  - A Heatmap-Supplemented R-CNN Trained Using an Inflated IoU for Small Object Detection
PY  - 2024
T2  - Remote Sensing
DO  - 10.3390/rs16214065
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208600801&doi=10.3390%2frs16214065&partnerID=40&md5=0f9190d99ef6ef061fa049ecac550c27
AB  - Object detection architectures struggle to detect small objects across applications including remote sensing and autonomous vehicles. Specifically, for unmanned aerial vehicles, poor detection of small objects directly limits this technology’s applicability. Objects both appear smaller than they are in large-scale images captured in aerial imagery and are represented by reduced information in high-altitude imagery. This paper presents a new architecture, CR-CNN, which predicts independent regions of interest from two unique prediction branches within the first stage of the network: a conventional R-CNN convolutional backbone and an hourglass backbone. Utilizing two independent sources within the first stage, our approach leads to an increase in successful predictions of regions that contain smaller objects. Anchor-based methods such as R-CNNs also utilize less than half the number of small objects compared to larger ones during training due to the poor intersection over union (IoU) scores between the generated anchors and the groundtruth—further reducing their performance on small objects. Therefore, we also propose artificially inflating the IoU of smaller objects during training using a simple, size-based Gaussian multiplier—leading to an increase in the quantity of small objects seen per training cycle based on an increase in the number of anchor–object pairs during training. This architecture and training strategy led to improved detection overall on two challenging aerial-based datasets heavily composed of small objects while predicting fewer false positives compared to Mask R-CNN. These results suggest that while new and unique architectures will continue to play a part in advancing the field of object detection, the training methodologies and strategies used will also play a valuable role. © 2024 by the authors.
KW  - convolutional neural network
KW  - Mask R-CNN
KW  - object detection
KW  - UAV
KW  - Aerial photography
KW  - Aircraft detection
KW  - Antenna grounds
KW  - Convolutional neural networks
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Autonomous Vehicles
KW  - Convolutional neural network
KW  - Heatmaps
KW  - Large-scales
KW  - Mask R-CNN
KW  - Objects detection
KW  - Remote-sensing
KW  - Small object detection
KW  - Small objects
KW  - Remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, W.
AU  - Zheng, R.
AU  - Jiang, J.
AU  - Tian, Z.
AU  - Zhang, F.
AU  - Liu, Y.
TI  - EDSD: efficient driving scenes detection based on Swin Transformer
PY  - 2024
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-024-19622-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198952366&doi=10.1007%2fs11042-024-19622-w&partnerID=40&md5=30588cffbe7f98d7aad829e95e6f638c
AB  - In the field of autonomous driving, the detection of targets such as vehicles, bicycles, and pedestrians in complex road conditions is of great importance. Through extensive experimentation, we have found that various vehicle targets generally occupy large sizes in the image but are easily occluded, while small targets such as pedestrians usually appear densely. The detection of targets of different sizes is an important challenge for the performance of current detectors. To address this issue, we proposed a novel hierarchical feature pyramid network structure. This structure comprises a series of CNN-Transformer variant layers, each of which is a superposition of CST neural network modules and Swin Transformer modules. In addition, considering that the huge computation of the global self-attention mechanism is difficult to be applied in the field of autonomous driving, we adopted the shifted window method in SwinFM, which effectively accelerates the inference process by replacing the traditional method by using the self-attention mechanism within the window. This study uses the Swin Transformer as a baseline. Compared to the baseline, our EDSD model improves the average accuracy by 1.8% and 3.1% on the BDD100K dataset and the KITTI dataset, respectively. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
KW  - Autonomous driving
KW  - Convolutional neural network
KW  - Object detection
KW  - Small object
KW  - Swin transformer
KW  - Autonomous vehicles
KW  - Convolutional neural networks
KW  - Multilayer neural networks
KW  - Pedestrian safety
KW  - Attention mechanisms
KW  - Autonomous driving
KW  - Convolutional neural network
KW  - Objects detection
KW  - Road condition
KW  - Scene detection
KW  - Small objects
KW  - Small targets
KW  - Swin transformer
KW  - Vehicle targets
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lin, N.
AU  - Zhang, L.
AU  - Wu, T.
AU  - Hawbani, A.
AU  - Zhou, H.
AU  - Zhao, L.
TI  - Surface Multiple Object Tracking: An Accurate HAT-YOLOv8-ADT Tracking Model
PY  - 2025
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2025.3539852
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217537826&doi=10.1109%2fJIOT.2025.3539852&partnerID=40&md5=27ea0b4d816f334cd507aa71146397a2
AB  - With the development of artificial intelligence technology, Autonomous aerial vehicles (AAV) have the ability to sense the environment. multiple object tracking (MOT) in AAV video is a very important vision task with a wide variety of applications. However, there are still many challenges in MOT in AAV video. First, the movement of the onboard camera in the three-dimensional (3-D) direction during the tracking process, as well as the unpredictable measurement noise characteristics of AAVs flying at high speeds, can lead to significant deviations in the prediction of the object’s position. Second, the applicability of the traditional detection algorithm decreases when the object is small and dense in the AAV viewpoint during detection. Finally, the traditional intersection over union (IoU) matching approach does not take into account the effects of the height and width of the box, and the matching results are inaccurate for the prediction and detection box. In order to address these challenges, we recommend the adaptive DeepSort (ADT) algorithm to reduce the prediction bias due to camera movement and difficulty in predetermining measurement noise characteristics, the hybrid attention transformer-YOLOv8 (HAT-YOLOv8) algorithm to enhance the detection capability of tiny objects, and the IoU of height and width (HWIoU) matching algorithm, which improves the matching accuracy and thus the tracking accuracy. Experimental results show that our proposed solution outperforms the baseline solution. It outperforms the current mainstream StrongSort in MOTA, HOTA and IDF1 by 2.86%, 0.9%, and 9.36%. © 2014 IEEE.
KW  - Autonomous aerial vehicles (AAV)
KW  - deep simple online and realtime tracking (DeepSort)
KW  - multiple object tracking (MOT)
KW  - small object detection
KW  - YOLOv8
KW  - Aircraft detection
KW  - Time difference of arrival
KW  - Aerial vehicle
KW  - Deepsort
KW  - Matchings
KW  - Measurement Noise
KW  - Multiple object tracking
KW  - Noise characteristic
KW  - Small object detection
KW  - Surface multiples
KW  - Tracking models
KW  - YOLOv8
KW  - Object tracking
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, D.
AU  - Yu, S.
AU  - Yang, H.
TI  - Progress on Environmental Perception Technology of Foreign Unmanned Surface Vehicles
ST  - 国外水面无人艇环境感知技术前沿进展
PY  - 2024
T2  - Binggong Xuebao/Acta Armamentarii
DO  - 10.12382/bgxb.2024.0860
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212582490&doi=10.12382%2fbgxb.2024.0860&partnerID=40&md5=f3e851a42603670b5e833d21935800a8
AB  - Unmanned surface vehicles (USVs) have high mobility, strong concealment, and extensive operational range, making them highly suitable for performing a wide array of tasks such as reconnaissance,anti-submarine warfare,search and rescue. Environmental perception technology,crucial for the operation of USVs, has attracted considerable attention. This paper conducts a survey on the development status of environmental perception technology for USVs at abroad,and define and analyzed the challenges in USV environmental perception through specific case studies. The current state of research on USV environmental perception technology is analyzed from the perspectives of both unimodal and multimodal perception,considering the sensory equipment utilized by USVs. Finally,the unresolved challenges in USV environmental perception technology are summarized,and its future development is prospected. © 2024 China Ordnance Industry Corporation. All rights reserved.
KW  - multi-modal perception
KW  - perception technology
KW  - radar
KW  - unmanned surface vehicle
KW  - Unmanned underwater vehicles
KW  - Antisubmarine warfare
KW  - Case-studies
KW  - Development status
KW  - Environmental perceptions
KW  - High mobility
KW  - Multi modal perceptions
KW  - Operational range
KW  - Perception technology
KW  - Search and rescue
KW  - Surface vehicles
KW  - Unmanned surface vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hakani, R.
AU  - Rawat, A.
TI  - Edge Computing-Driven Real-Time Drone Detection Using YOLOv9 and NVIDIA Jetson Nano
PY  - 2024
T2  - Drones
DO  - 10.3390/drones8110680
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210319320&doi=10.3390%2fdrones8110680&partnerID=40&md5=bd3d68f95a8b7591ef4bfa2cbe91cb80
AB  - Drones, with their ability to vertically take off and land with their stable hovering performance, are becoming favorable in both civilian and military domains. However, this introduces risks of its misuse, which may include security threats to airports, institutes of national importance, VIP security, drug trafficking, privacy breaches, etc. To address these issues, automated drone detection systems are essential for preventing unauthorized drone activities. Real-time detection requires high-performance devices such as GPUs. For our experiments, we utilized the NVIDIA Jetson Nano to support YOLOv9-based drone detection. The performance evaluation of YOLOv9 to detect drones is based on metrics like mean average precision (mAP), frames per second (FPS), precision, recall, and F1-score. Experimental data revealed significant improvements over previous models, with a mAP of 95.7%, a precision of 0.946, a recall of 0.864, and an F1-score of 0.903, marking a 4.6% enhancement over YOLOv8. This paper utilizes YOLOv9, optimized with pre-trained weights and transfer learning, achieving significant accuracy in real-time drone detection. Integrated with the NVIDIA Jetson Nano, the system effectively identifies drones at altitudes ranging from 15 feet to 110 feet while adapting to various environmental conditions. The model’s precision and adaptability make it particularly suitable for deployment in security-sensitive areas, where quick and accurate detection is crucial. This research establishes a solid foundation for future counter-drone applications and shows great promise for enhancing situational awareness in critical, high-risk environments. © 2024 by the authors.
KW  - computer vision
KW  - deep learning
KW  - drone detection
KW  - NVIDIA Jetson Nano
KW  - YOLOv9
KW  - You Only Look Once (YOLO)
KW  - Aircraft detection
KW  - Deep learning
KW  - Drones
KW  - Edge computing
KW  - Military airports
KW  - Nanorobots
KW  - Network security
KW  - Target drones
KW  - Deep learning
KW  - Drone detection
KW  - Edge computing
KW  - F1 scores
KW  - Hovering performance
KW  - NVIDIA jetson nano
KW  - Real- time
KW  - Take off
KW  - YOLOv9
KW  - You only look once
KW  - Airport security
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bai, L.
AU  - Song, H.
AU  - Feng, T.
AU  - Fu, T.
AU  - Yu, Q.
AU  - Yang, J.
TI  - Revisiting class-incremental object detection: An efficient approach via intrinsic characteristics alignment and task decoupling
PY  - 2024
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2024.125057
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201508312&doi=10.1016%2fj.eswa.2024.125057&partnerID=40&md5=901bb2324a9ecdf0fbb21b3aa204e6bb
AB  - In real-world settings, object detectors frequently encounter continuously emerging object instances from new classes. Incremental Object Detection (IOD) addresses this challenge by incrementally training an object detector with instances from new classes while retaining knowledge acquired from previously learned classes. Despite recent advancements, existing studies reveal a critical gap: they diverge from the inherent characteristics of dense detectors, leaving considerable room for improving incremental learning efficiency. To address this challenge, we propose a novel and efficient IOD approach that aligns more closely with the intrinsic properties of dense detectors. Specifically, our approach introduces a learning-aligned mechanism, comprising tailored knowledge distillation and task alignment learning, to achieve more efficient incremental learning. Additionally, we propose expanding the classification network through task decoupling to alleviate performance limitations stemming from different optimization goals in the incremental learning process of the classification branch. Extensive experiments conducted on the MS COCO and PASCAL VOC datasets demonstrate the effectiveness of our method, achieving state-of-the-art performance across various one-step and multi-step incremental scenarios. In multi-step incremental scenarios, our approach demonstrates a significant improvement of up to 12.9% in Average Precision (AP) compared to the previous method ERD. 1 © 2024 Elsevier Ltd
KW  - Incremental learning
KW  - Incremental object detection
KW  - Intrinsic characteristics alignment
KW  - Knowledge distillation
KW  - Task decoupling
KW  - Adversarial machine learning
KW  - Federated learning
KW  - Multi-task learning
KW  - Decouplings
KW  - Incremental learning
KW  - Incremental object detection
KW  - Intrinsic characteristic alignment
KW  - Intrinsic characteristics
KW  - Knowledge distillation
KW  - Multisteps
KW  - Object detectors
KW  - Objects detection
KW  - Task decoupling
KW  - Contrastive Learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Dai, W.
AU  - Li, Z.
AU  - Xu, X.
AU  - Chen, X.
AU  - Zeng, H.
AU  - Hu, R.
TI  - Enhanced Cross Layer Refinement Network for robust lane detection across diverse lighting and road conditions
PY  - 2025
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2024.109473
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207018329&doi=10.1016%2fj.engappai.2024.109473&partnerID=40&md5=ac2c792238bf0419cbc50b92c2f581be
AB  - With the rapid development of autonomous driving technology, lane detection, a key component of intelligent vehicle systems, is crucial for ensuring road safety and efficient vehicle navigation. In this paper, a new lane detection method is proposed to address the problem of degraded performance of existing lane detection methods when dealing with complex road environments. The proposed method evolves from the original Cross Layer Refinement Network (CLRNet) by incorporating two of our carefully designed core components: the Global Feature Optimizer (GFO) and the Adaptive Lane Geometry Aggregator (ALGA). The GFO is a multi-scale attention mechanism that mimics the human visual focusing ability, effectively filtering out unimportant information and focusing on the image regions most relevant to the task. The ALGA is a shape feature-aware aggregation module that utilizes the shape prior of lanes to enhance the correlation of anchor points in an image, better fusing global and local information. By integrating both components into CLRNet, an enhanced version called Enhanced CLRNet (E-CLRNet) is presented, which exhibits higher performance stability in complex roadway scenarios. Experiments on the CULane dataset reveal that E-CLRNet demonstrates superior performance stability over the original CLRNet in complex scenarios, including curves, shadows, missing lines, and dazzling light conditions. In particular, in the curves, the F1 score of E-CLRNet is improved by almost 3% over the original CLRNet. This study not only improves the accuracy and performance stability of lane detection but also provides a new solution for the application of autonomous driving technology in complex environments, which promotes the development of intelligent vehicle systems. © 2024 Elsevier Ltd
KW  - Autonomous driving
KW  - Intelligent vehicle systems
KW  - Lane detection
KW  - Multi-scale attention mechanism
KW  - Road safety
KW  - Autonomous vehicles
KW  - Vehicle detection
KW  - Attention mechanisms
KW  - Autonomous driving
KW  - Cross layer
KW  - Detection methods
KW  - Intelligent vehicle systems
KW  - Lane detection
KW  - Multi-scale attention mechanism
KW  - Multi-scales
KW  - Performance stability
KW  - Road safety
KW  - Vehicle safety
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tian, D.
AU  - Yan, X.
AU  - Zhou, D.
AU  - Wang, C.
AU  - Zhang, W.
TI  - IV-YOLO: A Lightweight Dual-Branch Object Detection Network
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24196181
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206282940&doi=10.3390%2fs24196181&partnerID=40&md5=92f0a2b738d36c34c5453a015ffbb5f7
AB  - With the rapid growth in demand for security surveillance, assisted driving, and remote sensing, object detection networks with robust environmental perception and high detection accuracy have become a research focus. However, single-modality image detection technologies face limitations in environmental adaptability, often affected by factors such as lighting conditions, fog, rain, and obstacles like vegetation, leading to information loss and reduced detection accuracy. We propose an object detection network that integrates features from visible light and infrared images—IV-YOLO—to address these challenges. This network is based on YOLOv8 (You Only Look Once v8) and employs a dual-branch fusion structure that leverages the complementary features of infrared and visible light images for target detection. We designed a Bidirectional Pyramid Feature Fusion structure (Bi-Fusion) to effectively integrate multimodal features, reducing errors from feature redundancy and extracting fine-grained features for small object detection. Additionally, we developed a Shuffle-SPP structure that combines channel and spatial attention to enhance the focus on deep features and extract richer information through upsampling. Regarding model optimization, we designed a loss function tailored for multi-scale object detection, accelerating the convergence speed of the network during training. Compared with the current state-of-the-art Dual-YOLO model, IV-YOLO achieves mAP improvements of 2.8%, 1.1%, and 2.2% on the Drone Vehicle, FLIR, and KAIST datasets, respectively. On the Drone Vehicle and FLIR datasets, IV-YOLO has a parameter count of 4.31 M and achieves a frame rate of 203.2 fps, significantly outperforming YOLOv8n (5.92 M parameters, 188.6 fps on the Drone Vehicle dataset) and YOLO-FIR (7.1 M parameters, 83.3 fps on the FLIR dataset), which had previously achieved the best performance on these datasets. This demonstrates that IV-YOLO achieves higher real-time detection performance while maintaining lower parameter complexity, making it highly promising for applications in autonomous driving, public safety, and beyond. © 2024 by the authors.
KW  - attention mechanism
KW  - bi-directional pyramid feature fusion
KW  - dual-branch image object detection
KW  - IV-YOLO
KW  - small target detection
KW  - Aircraft detection
KW  - Image coding
KW  - Image enhancement
KW  - Image fusion
KW  - Laser beams
KW  - Photointerpretation
KW  - Proximity sensors
KW  - Remote sensing
KW  - Thermography (imaging)
KW  - Attention mechanisms
KW  - Bi-directional
KW  - Bi-directional pyramid feature fusion
KW  - Detection networks
KW  - Dual-branch image object detection
KW  - Features fusions
KW  - Image object detection
KW  - IV-YOLO
KW  - Objects detection
KW  - Small target detection
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - human
KW  - illumination
KW  - male
KW  - rain
KW  - remote sensing
KW  - spatial attention
KW  - unmanned aerial vehicle
KW  - vegetation
KW  - velocity
KW  - Drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, S.
AU  - Li, J.
AU  - Shi, L.
AU  - Ding, M.
AU  - Nguyen, D.C.
AU  - Chen, W.
AU  - Han, Z.
TI  - Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends
PY  - 2025
T2  - IEEE Communications Surveys and Tutorials
DO  - 10.1109/COMST.2025.3563919
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003588117&doi=10.1109%2fCOMST.2025.3563919&partnerID=40&md5=52c0ad10ac4b5534d496aa6a99f867ea
AB  - As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. The Industrial Metaverse is used for product design, production operations, quality inspection, and testing. However, there is limited understanding of the enabling technologies associated with it, including the specific industrial scenarios targeted by each technology and the potential migration of technologies from other domains to the industrial sector. This paper provides a comprehensive survey of the latest literature on the Industrial Metaverse. We first analyze its advantages for industrial production, then review key enabling technologies such as blockchain (BC), privacy-preserving computing (PPC), digital twin (DT), fifth/sixth generation mobile communication technology (5G/6G), XR, and artificial intelligence (AI), and explore how these technologies support different aspects of industrial production. We also present major challenges in the Industrial Metaverse, including privacy and security concerns, resource limitations, and interoperability constraints, along with existing solutions. Finally, we outline several open issues and future research directions. © 1998-2012 IEEE.
KW  - artificial intelligence
KW  - blockchain
KW  - digital twin
KW  - extended reality
KW  - fifth/sixth generation mobile communication technology
KW  - industrial
KW  - Metaverse
KW  - privacy-preserving computing
KW  - Block-chain
KW  - Enabling technologies
KW  - Extended reality
KW  - Fifth/sixth generation mobile communication technology
KW  - Industrial
KW  - Industrial production
KW  - Metaverses
KW  - Mobile communication technology
KW  - Privacy preserving
KW  - Privacy-preserving computing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yan, H.
AU  - Gao, F.
AU  - Zhao, J.
AU  - Zhang, X.
TI  - MRT-YOLO: A Fine-Grained Feature-Based Method for Object Detection
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13234687
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211925263&doi=10.3390%2felectronics13234687&partnerID=40&md5=943c46c0af385d8f324314b353bf8f50
AB  - Object detection is an essential component of autonomous driving, unmanned aerial vehicle (UAV) reconnaissance, and other domains. It equips drones and vehicles with the capability to perceive and comprehend their surrounding environment, making it a crucial technology for achieving safe and reliable autonomous driving as well as UAV spot reconnaissance. This paper proposes an end-to-end, high-precision, multi-scale real-time detection algorithm called MRT-YOLO based on YOLOv8. Firstly, in the feature downsampling process of the backbone network, we extend the channel depth to enhance the model’s learning capability for fine-grained features and thereby improve its performance in retaining feature information. Secondly, we enhance the cross-stage partial layer version 2 (C2f) module in YOLOv8 by incorporating a channel self-attention mechanism within it, which optimizes performance through effective feature interaction and integration. Simultaneously, we also employ an improved bidirectional feature pyramid network (BiFPN) and introduce the proposed multi-scale feature learning (MFL) module to further enhance the model’s feature extraction ability. In this study, we fuse the feature maps (C2, C3, C4, and C5) from the backbone network to generate a new feature map C6, thus increasing cross-connections between low-level and high-level features. Lastly, a multi-scale small object detection structure is designed to enhance recognition sensitivity toward densely distributed small objects. The proposed algorithm’s effectiveness and superiority are demonstrated through experiments conducted on two datasets: VisDrone (UAV vision dataset) and BDD100K (automatic driving dataset). © 2024 by the authors.
KW  - channel attention
KW  - feature extraction
KW  - MRT-YOLO
KW  - multi-scale feature fusion
KW  - object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xie, Y.
AU  - Du, D.
AU  - Bi, M.
TI  - YOLO-ACE: A Vehicle and Pedestrian Detection Algorithm for Autonomous Driving Scenarios Based on Knowledge Distillation of YOLOv10
PY  - 2025
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2025.3569735
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005375362&doi=10.1109%2fJIOT.2025.3569735&partnerID=40&md5=cab5bc4c9ff412f41a859398ecd21b60
AB  - Vehicle and pedestrian detection are critical tasks in autonomous driving, and fast and accurate detection algorithms are of great significance for improving the safety and reliability of autonomous driving systems. This paper proposes an improved YOLOv10 algorithm, YOLO-ACE, based on knowledge distillation for vehicle and pedestrian detection in autonomous driving scenarios. First, a new Add-CGLU (Additive-Convolutional Gated Linear Unit) architecture is developed to replace the original C2f module in the backbone part. Then, a new FPSC (Feature Pyramid Shared Conv) module is proposed to optimize the original SPPF module. After that, the neck part is redesigned to propose a new EMBS (Efficient Multi-Branch Scale) pyramid network. Finally, a new DD (Double Distillation) strategy is customized to perform knowledge distillation on the overall model. Experimental results on the public dataset BDD100K show that the computational parameters of YOLO-ACE are reduced by 21.6%, FLOPs are reduced by 20.0%, and the model size is reduced by 19.5%. At the same time, the F1 Score increased by 4.9%, the mAP increased by 4.5%, and the running speed reached 70.9 FPS. YOLO-ACE provides a more efficient vehicle and pedestrian detection solution in autonomous driving scenarios, promoting further development of autonomous driving systems.  © 2014 IEEE.
KW  - Additive-Convolutional Gated Linear Unit module
KW  - Efficient Multi-Branch Scale pyramid network
KW  - Feature Pyramid Shared Conv module
KW  - knowledge distillation
KW  - vehicle and pedestrian detection
KW  - YOLOv10
KW  - Autonomous vehicles
KW  - Boolean functions
KW  - Additive-convolutional gated linear unit module
KW  - Efficient multi-branch scale pyramid network
KW  - Feature pyramid
KW  - Feature pyramid shared conv module
KW  - Knowledge distillation
KW  - Linear units
KW  - Pedestrian detection
KW  - Pyramid network
KW  - Vehicles detection
KW  - YOLOv10
KW  - Pedestrian safety
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xing, Z.
AU  - Ma, G.
AU  - Wang, L.
AU  - Yang, L.
AU  - Guo, X.
AU  - Chen, S.
TI  - Toward Visual Interaction: Hand Segmentation by Combining 3-D Graph Deep Learning and Laser Point Cloud for Intelligent Rehabilitation
PY  - 2025
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2025.3546874
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219537245&doi=10.1109%2fJIOT.2025.3546874&partnerID=40&md5=9854f6c2ac4196e72e9c55b1e9e3c2d9
AB  - Against the backdrop of the increasing trend of aging population in China and even globally, the demand for hand function rehabilitation is growing day by day, and human-machine interaction virtual rehabilitation systems have become a research hotspot. Currently, 3-D vision has shown great potential in morphological analysis, but the complexity and irregularity of hand surfaces pose challenges for accurate segmentation. This study has proposed a hand surface segmentation network (HSSN) for intelligent hand function rehabilitation in the virtual reality, which combines 3-D graph deep learning and laser point cloud. HSSN integrates a series of methods, with edge convolution layers effectively addressing the complex morphology of hand surfaces, multiscale edge convolution solving the problem of missing or redundant local features, multidensity processing enhancing the robustness of the model to point cloud density, and normal vector feature enhancement solving the problem of insufficient geometric features of actual hand surface point clouds. Through the comprehensive application of these methods, HSSN has demonstrated excellent performance in comparative experiments. This study is of great significance for promoting the personalized and precise development of intelligent rehabilitation in the virtual reality environment. More importantly, this achievement has provided a new perspective for interdisciplinary research in fields, such as rehabilitation engineering and human-machine interaction. © 2014 IEEE.
KW  - 3-D vision
KW  - graph deep learning
KW  - human-machine interaction
KW  - intelligent and precise rehabilitation
KW  - point cloud processing
KW  - Deep learning
KW  - 3-D vision
KW  - 3d graphs
KW  - Cloud processing
KW  - Graph deep learning
KW  - Human machine interaction
KW  - Intelligent and precise rehabilitation
KW  - Laser point
KW  - Point cloud processing
KW  - Point-clouds
KW  - Surface segmentation
KW  - Virtual environments
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yan, J.
AU  - Cheng, Y.
AU  - Zhang, F.
AU  - Zhou, N.
AU  - Wang, H.
AU  - Jin, B.
AU  - Wang, M.
AU  - Zhang, W.
TI  - Multimodal Imitation Learning for Arc Detection in Complex Railway Environments
PY  - 2025
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2025.3556896
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002985474&doi=10.1109%2fTIM.2025.3556896&partnerID=40&md5=08fbfd7abfdb314ea07ffef5d365e06d
AB  - The pantograph-catenary system (PCS) is a critical component of railway vehicles, and its performance directly affects current collection quality. The arc rate serves as an essential measurement indicator for monitoring the PCS state. However, in complex railway environments—where arc sizes and shapes can vary significantly and are further influenced by factors such as reflected light, glare, and adverse weather—the traditional arc detection methods are easily affected by unstable current collection and power fluctuations, resulting in increased false detection rates and reduced measurement accuracy. Deep learning methods, while promising, also face limitations when dealing with such diverse arc morphologies and strong external interference. To address these challenges, this article proposes a multimodal imitation learning-based arc detection network (MILADNet). First, the measurement system fuses infrared and visible-light image features to enhance arc feature extraction in scenarios with strong glare or reflective interference, thereby mitigating false alarms caused by relying on a single sensor. Second, to overcome the lack of information on small arcs, an online imitation learning framework is introduced to improve the system’s detection sensitivity for small arcs. Finally, to address data bias arising from uneven arc distributions, an unsupervised transferable representation learning method is employed to reduce dependence on labeled data and enhance model generalization. Experimental results show that MILADNet exhibits outstanding detection performance for arcs of various sizes and in complex environments, demonstrating both high efficiency and accuracy during measurement and data processing. Beyond improving the precision and reliability of arc detection, this method offers a novel solution for the instrumentation and measurement field and shows significant potential for condition monitoring and anomaly detection in railway systems. © 1963-2012 IEEE.
KW  - Arc detection
KW  - deep learning
KW  - optimal transport theory
KW  - pantograph-catenary system (PCS)
KW  - transfer learning
KW  - Anomaly detection
KW  - Electric current collection
KW  - Glare effects
KW  - Health risks
KW  - Image enhancement
KW  - Labeled data
KW  - Luminescent devices
KW  - Network security
KW  - Arc detection
KW  - Current collection
KW  - Deep learning
KW  - Imitation learning
KW  - Multi-modal
KW  - Optimal transport
KW  - Optimal transport theory
KW  - Pantograph catenary system
KW  - Transfer learning
KW  - Transport theory
KW  - Railroads
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhong, M.
AU  - Jiang, B.
TI  - Enhancing Target Detection and Recognition in Advanced Driver Assistance Systems Using Infrared Thermal Imaging and the YOLOv5 Algorithm
PY  - 2024
T2  - International Journal of Heat and Technology
DO  - 10.18280/ijht.420530
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209232697&doi=10.18280%2fijht.420530&partnerID=40&md5=365acfe594d9e49504d088deba5ca13c
AB  - The potential of Advanced Driving Assistance Systems (ADAS) to enhance road safety is considerable; however, the reliability of ADAS in detecting and classifying road entities under varied environmental conditions remains a critical challenge. Conventional ADAS sensors often encounter limitations in adverse weather and low-visibility conditions, such as nighttime, rain, snow, and haze, reducing their capacity to detect vehicles and pedestrians effectively. To address these limitations, this study explores the integration of infrared thermal imaging technology into standard automotive sensor kits to enhance target detection capabilities. The YOLOv5 deep learning algorithm is applied to infrared thermal imaging data, aiming to improve the detection and classification of road targets, including pedestrians and motor vehicles, across diverse driving scenarios. Experimental results demonstrate that the proposed approach significantly enhances target detection, maintaining a balance between detection accuracy and real-time performance, particularly under challenging visibility conditions. These findings indicate that the integration of infrared thermal imaging with YOLOv5 in ADAS could reduce accident risks and improve road safety by providing more reliable scene analysis under adverse conditions. ©2024 The authors.
KW  - Advanced Driving Assistance System (ADAS)
KW  - automotive sensor
KW  - deep learning
KW  - infrared thermal imaging
KW  - low-visibility condition
KW  - target detection
KW  - YOLOv5 algorithm
KW  - Highway accidents
KW  - Motor transportation
KW  - Pedestrian safety
KW  - Road vehicles
KW  - Satellites
KW  - Thermography (imaging)
KW  - Vehicle detection
KW  - Vehicle safety
KW  - Advanced driving assistance system
KW  - Automotive sensors
KW  - Deep learning
KW  - Driving assistance systems
KW  - Infrared thermal imaging
KW  - Low visibility conditions
KW  - Road safety
KW  - Target detection and recognition
KW  - Targets detection
KW  - YOLOv5 algorithm
KW  - Advanced driver assistance systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Singh, S.
AU  - Lamba, N.
AU  - Khosla, A.
TI  - A closer look at single object tracking under variable haze
PY  - 2024
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-024-19997-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202618322&doi=10.1007%2fs11042-024-19997-w&partnerID=40&md5=ba3543b7da6b1754fdb93edf09626e67
AB  - The task of monitoring the object's path as it travels within a scene has consistently been challenging. When a specific level of haze is introduced to the environment, the endeavor becomes more difficult. The most recent tracking algorithms claim to be capable of accurately monitoring objects in typical visual conditions. However, it is imperative to conduct a comprehensive analysis of their functionality in hazy conditions, as haze is a meteorological adversity that is frequently encountered and has the potential to result in severe consequences. The primary objective of this investigation is to evaluate the efficacy of prominent tracking algorithms in the presence or absence of haze. Additionally, the performance was assessed by examining it in a variety of hazy conditions that were generated using the monocular depth information of the original image. The comparison between the authentic hazy photographs and the artificially created hazy photos has also been demonstrated. Furthermore, several novel relative parameters have been developed for object tracking under obscured vision conditions. These parameters can be employed to maintain the relative tracking performances under both normal and varying hazy conditions.To emphasize the effects of haze, the results have been obtained by using the help of state-of-the-art tracking algorithms. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
KW  - Monocular depth estimation
KW  - Object Tracking
KW  - Robustness
KW  - SORT
KW  - Synthetic haze generation
KW  - Tracking Transformer
KW  - Haze pollution
KW  - Object detection
KW  - Object recognition
KW  - Object tracking
KW  - Target tracking
KW  - Condition
KW  - Depth Estimation
KW  - Monocular depth estimation
KW  - Object Tracking
KW  - Robustness
KW  - Single object
KW  - SORT
KW  - Synthetic haze generation
KW  - Tracking algorithm
KW  - Tracking transformer
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ning, Y.
AU  - Chen, J.
TI  - Target Detection Algorithm for UAV Aerial Images Based on BMGS-YOLOv8
PY  - 2025
T2  - 2025 8th International Conference on Advanced Algorithms and Control Engineering, ICAACE 2025
DO  - 10.1109/ICAACE65325.2025.11019851
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009121845&doi=10.1109%2fICAACE65325.2025.11019851&partnerID=40&md5=2a7d243350cfc55aad404262d0fbef91
AB  - Aiming at the problem of detection difficulties caused by the small size of targets in UAV aerial images, which are easy to be occluded, as well as the densely populated targets such as pedestrians and vehicles, an improved BMGS-YOLOv8 algorithm is proposed in this paper. The algorithm first introduces the Biformer attention mechanism in the backbone network, which captures both global and local features by filtering key key-value pairs, and improves the efficiency of small target detection while reducing the amount of computation. Secondly, GSConv convolution is used to replace the standard convolution in YOLOv8, which effectively reduces the number of model parameters. Experimental results on the VisDrone2019 dataset show that compared with the benchmark model YOLOv8n, BMGS-YOLOv8 improves the mAP50 and mAP50-95 metrics by 0.8% and 0.2%, respectively, while the amount of model parameters is reduced by 7%. © 2025 IEEE.
KW  - aerial imagery
KW  - attention mechanisms
KW  - drones
KW  - small targets
KW  - YOLOv8
KW  - Aerial photography
KW  - Aircraft detection
KW  - Antennas
KW  - Big data
KW  - Convolution
KW  - Image enhancement
KW  - Radar target recognition
KW  - Remote sensing
KW  - Robotics
KW  - Signal detection
KW  - Target drones
KW  - Aerial imagery
KW  - Aerial images
KW  - Attention mechanisms
KW  - Back-bone network
KW  - Global feature
KW  - Image-based
KW  - Modeling parameters
KW  - Small targets
KW  - Target detection algorithm
KW  - YOLOv8
KW  - Drones
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, H.
AU  - Min, B.-W.
AU  - Zhang, H.
TI  - A study on a target detection model for autonomous driving tasks
PY  - 2024
T2  - IET Image Processing
DO  - 10.1049/ipr2.13185
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198381038&doi=10.1049%2fipr2.13185&partnerID=40&md5=7e034e941533aac6906105e0705070e1
AB  - Target detection in autonomous driving tasks presents a complex and critical challenge due to the diversity of targets and the intricacy of the environment. To address this issue, this paper proposes an enhanced YOLOv8 model. Firstly, the original large target detection head is removed and replaced with a detection head tailored for small targets and high-level semantic details. Secondly, an adaptive feature fusion method is proposed, where input feature maps are processed using dilated convolutions with different dilation rates, followed by adaptive feature fusion to generate adaptive weights. Finally, an improved attention mechanism is incorporated to enhance the model's focus on target regions. Additionally, the impact of Group Shuffle Convolution (GSConv) on the model's detection speed is investigated. Validated on two public datasets, the model achieves a mean Average Precision (mAP) of 53.7% and 53.5%. Although introducing GSConv results in a slight decrease in mAP, it significantly improves frames per second. These findings underscore the effectiveness of the proposed model in autonomous driving tasks. © 2024 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
KW  - image classification
KW  - image processing
KW  - learning (artificial intelligence)
KW  - Autonomous vehicles
KW  - Convolution
KW  - Semantics
KW  - Adaptive features
KW  - Autonomous driving
KW  - Critical challenges
KW  - Detection models
KW  - Driving tasks
KW  - Images classification
KW  - Images processing
KW  - Learning (artificial intelligence)
KW  - Small targets
KW  - Targets detection
KW  - Image classification
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Park, J.
AU  - Lee, J.
AU  - Park, Y.
AU  - Lim, Y.
TI  - Deep Learning-Based Stopped Vehicle Detection Method Utilizing In-Vehicle Dashcams
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13204097
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207677839&doi=10.3390%2felectronics13204097&partnerID=40&md5=669fab42e14d57c4cfeed66ec7ed8859
AB  - In complex urban road conditions, stationary or illegally parked vehicles present a considerable risk to the overall traffic system. In safety-critical applications like autonomous driving, the detection of stopped vehicles is of utmost importance. Previous methods for detecting stopped vehicles have been designed for stationary viewpoints, such as security cameras, which consistently monitor fixed locations. However, these methods for detecting stopped vehicles based on stationary views cannot address blind spots and are not applicable from driving vehicles. To address these limitations, we propose a novel deep learning-based framework for detecting stopped vehicles in dynamic environments, particularly those recorded by dashcams. The proposed framework integrates a deep learning-based object detector and tracker, along with movement estimation using the dense optical flow method. We also introduced additional centerline detection and inter-vehicle distance measurement. The experimental results demonstrate that the proposed framework can effectively identify stopped vehicles under real-world road conditions. © 2024 by the authors.
KW  - multiple object tracking
KW  - object detection
KW  - optical flow
KW  - stopped vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Huang, Y.
AU  - Tao, Q.
TI  - Improving real-time object detection in Internet-of-Things smart city traffic with YOLOv8-DSAF method
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-68115-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199810002&doi=10.1038%2fs41598-024-68115-1&partnerID=40&md5=aafccb3cc980ada0a8ba9af775709420
AB  - With the rise of global smart city construction, target detection technology plays a crucial role in optimizing urban functions and improving the quality of life. However, existing target detection technologies still have shortcomings in terms of accuracy, real-time performance, and adaptability. To address this challenge, this study proposes an innovative target detection model. Our model adopts the structure of YOLOv8-DSAF, comprising three key modules: depthwise separable convolution (DSConv), dual-path attention gate module (DPAG), and feature enhancement module (FEM). Firstly, DSConv technology optimizes computational complexity, enabling real-time target detection within limited hardware resources. Secondly, the DPAG module introduces a dual-channel attention mechanism, allowing the model to selectively focus on crucial areas, thereby improving detection accuracy in high-dynamic traffic scenarios. Finally, the FEM module highlights crucial features to prevent their loss, further enhancing detection accuracy. Additionally, we propose an Internet of Things smart city framework consisting of four main layers: the application domain, the Internet of Things infrastructure layer, the edge layer, and the cloud layer. The proposed algorithm utilizes the Internet of Things infrastructure layer, edge layer, and cloud layer to collect and process data in real-time, achieving faster response times. Experimental results on the KITTI V and Cityscapes datasets indicate that our model outperforms the YOLOv8 model. This suggests that in complex urban traffic scenarios, our model exhibits superior performance with higher detection accuracy and adaptability. We believe that this innovative model will significantly propel the development of smart cities and advance target detection technology. © The Author(s) 2024.
KW  - DPAG
KW  - FEM
KW  - Internet of Things
KW  - Smart city construction
KW  - Target detection technology
KW  - YOLOv8
KW  - algorithm
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - internet of things
KW  - quality of life
KW  - reaction time
KW  - traffic
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Babu, H.
AU  - Velmurugan, J.
TI  - An effective framework to detect the vehicle with improved accuracy using you only look once over Haar cascade
PY  - 2024
T2  - AIP Conference Proceedings
DO  - 10.1063/5.0228868
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205354752&doi=10.1063%2f5.0228868&partnerID=40&md5=bf7654c3b8c3797c6de34cf4a33432f2
AB  - So that you may find out how well You Only Look Once works with OpenCV for speed-based car detection. This investigation involves two groups; one of them is the YOLO over OpenCV group. In each group, there are 10 participants, and the study settings for Glower are (α=0.05) and (power=0.85), both set simultaneously. When it comes to car detection, YOLO produces results that are 91% better than OpenCV's 84% accuracy. The accuracy of the two methods differs by a statistically significant amount of p=0.639 when assessed with two tails. When it comes to identifying new Vehicle Detection, the SVM performs far better than the You Only Look Once model. One may argue that it's the top choice for vehicle detection as well. © 2024 Author(s).
KW  - Haar cascade approach
KW  - Intelligent Transportation System
KW  - Linear Regression
KW  - OpenCV
KW  - YOLO
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pawłowski, P.
AU  - Piniarski, K.
TI  - Efficient Lossy Compression of Video Sequences of Automotive High-Dynamic Range Image Sensors for Advanced Driver-Assistance Systems and Autonomous Vehicles
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13183651
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205054052&doi=10.3390%2felectronics13183651&partnerID=40&md5=ecfbe03fc4df92da10927932b328c4a1
AB  - In this paper, we introduce an efficient lossy coding procedure specifically tailored for handling video sequences of automotive high-dynamic range (HDR) image sensors in advanced driver-assistance systems (ADASs) for autonomous vehicles. Nowadays, mainly for security reasons, lossless compression is used in the automotive industry. However, it offers very low compression rates. To obtain higher compression rates, we suggest using lossy codecs, especially when testing image processing algorithms in software in-the-loop (SiL) or hardware-in-the-loop (HiL) conditions. Our approach leverages the high-quality VP9 codec, operating in two distinct modes: grayscale image compression for automatic image analysis and color (in RGB format) image compression for manual analysis. In both modes, images are acquired from the automotive-specific RCCC (red, clear, clear, clear) image sensor. The codec is designed to achieve a controlled image quality and state-of-the-art compression ratios while maintaining real-time feasibility. In automotive applications, the inherent data loss poses challenges associated with lossy codecs, particularly in rapidly changing scenes with intricate details. To address this, we propose configuring the lossy codecs in variable bitrate (VBR) mode with a constrained quality (CQ) parameter. By adjusting the quantization parameter, users can tailor the codec behavior to their specific application requirements. In this context, a detailed analysis of the quality of lossy compressed images in terms of the structural similarity index metric (SSIM) and the peak signal-to-noise ratio (PSNR) metrics is presented. With this analysis, we extracted some codec parameters, which have an important impact on preservation of video quality and compression ratio. The proposed compression settings are very efficient: the compression ratios vary from 51 to 7765 for grayscale image mode and from 4.51 to 602.6 for RGB image mode, depending on the specified output image quality settings. We reached 129 frames per second (fps) for compression and 315 fps for decompression in grayscale mode and 102 fps for compression and 121 fps for decompression in the RGB mode. These make it possible to achieve a much higher compression ratio compared to lossless compression while maintaining control over image quality. © 2024 by the authors.
KW  - ADAS
KW  - autonomous vehicles
KW  - group of pictures
KW  - high-dynamic range imaging
KW  - lossy compression
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, X.
AU  - Duan, J.
AU  - Yang, L.
AU  - Duan, S.
TI  - Intelligent cotter pins defect detection for electrified railway based on improved faster R-CNN and dilated convolution
PY  - 2024
T2  - Computers in Industry
DO  - 10.1016/j.compind.2024.104146
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201387987&doi=10.1016%2fj.compind.2024.104146&partnerID=40&md5=1fcebc3b1c1817b0334990adc5d42d5f
AB  - The cotter pin (CP) is a vital fastener for the catenary support components (CSCs) of high-speed electrified railways. Due to the vibration and excitation caused by the passing of railway vehicles, some CPs may be broken or fallen off over time, which poses a significant safety hazard to the railway systems. Currently, the CP defect detection is primarily conducted by humans, which is inefficient and inconsistent. Therefore, there is an urgent need for automatic CP defect detection to ensure railway safety. However, this task is very challenging as it requires covering hundreds or thousands of miles in limited times when the railway stops running. To this end, we first design a traffic track intelligent imaging device to capture catenary images at various angles at high speed. Then, inspired by the success of deep learning-based object detection, we develop a CP detection model based on an improved Faster R-CNN with a multi-scale region proposal network (MS-RPN) and propose the positive sample adaptive loss function (PSALF) to enhance detection accuracy. Finally, we propose a module to recognize the CP defect based on dilated convolution. The experimental results show that our method can effectively detect the CP defect in the catenary image, achieving 99.05 % precision and 98.40 % recall rate on CP defect detection. Furthermore, CP detection method and CP defect detection are significantly faster than baseline method, with FPS improvements of 2.76 and 24.67, respectively, thus making it more suitable for real-time applications in railway systems. © 2024 Elsevier B.V.
KW  - Catenary Support Components (CSCs)
KW  - Cotter Pin (CP)
KW  - Defect detection
KW  - Dilated Convolution
KW  - Faster R-CNN
KW  - High-speed electrified railway
KW  - Electric railroads
KW  - Image enhancement
KW  - Image segmentation
KW  - Locks (fasteners)
KW  - Railroad accidents
KW  - Railroad tracks
KW  - Railroad yards and terminals
KW  - Thermography (imaging)
KW  - Catenary support component
KW  - Cotte pin
KW  - Defect detection
KW  - Dilated convolution
KW  - Electrified railways
KW  - Fast R-CNN
KW  - High Speed
KW  - High-speed electrified railway
KW  - Railway system
KW  - Railway vehicles
KW  - Railroads
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xiao, Y.
AU  - Di, N.
TI  - SOD-YOLO: A lightweight small object detection framework
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-77513-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208082125&doi=10.1038%2fs41598-024-77513-4&partnerID=40&md5=0eab8809e29f18d6de93bc1a205b325e
AB  - Currently, lightweight small object detection algorithms for unmanned aerial vehicles (UAVs) often employ group convolutions, resulting in high Memory Access Cost (MAC) and rendering them unsuitable for edge devices that rely on parallel computing. To address this issue, we propose the SOD-YOLO model based on YOLOv7, which incorporates a DSDM-LFIM backbone network and includes a small object detection branch. The DSDM-LFIM backbone network, which combines Deep-Shallow Downsampling Modules (DSD Modules) and Lightweight Feature Integration Modules (LFI Modules), avoids excessive use of group convolutions and element-wise operations. The DSD Module focuses on extracting both deep and shallow features from feature maps using fewer parameters to obtain richer feature representations. The LFI Module, is a dual-branch feature integration module designed to consolidate feature information. Experimental results demonstrate that the SOD-YOLO model achieves an AP50 of 50.7% and a FPS of 72.5 on the VisDrone validation set. Compared to YOLOv7, our model reduces computational costs by 20.25% and decreases the number of parameters by 17.89%. After scaling the number of channels in the model, it achieves an AP50 of 33.4% with an inference time of 27.3ms on the Atlas 200I DK A2. These experimental results indicate that the SOD-YOLO model can effectively perform small object detection tasks in a large number of aerial images captured by UAVs. © The Author(s) 2024.
KW  - Lightweight
KW  - Object Detection
KW  - SOD-YOLO
KW  - UAV Image
KW  - article
KW  - detection algorithm
KW  - diagnosis
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Khalili, B.
AU  - Smyth, A.W.
TI  - SOD-YOLOv8—Enhancing YOLOv8 for Small Object Detection in Aerial Imagery and Traffic Scenes
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24196209
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206476633&doi=10.3390%2fs24196209&partnerID=40&md5=043e9c8a97d69204b3921bec785580ee
AB  - Object detection, as a crucial aspect of computer vision, plays a vital role in traffic management, emergency response, autonomous vehicles, and smart cities. Despite the significant advancements in object detection, detecting small objects in images captured by high-altitude cameras remains challenging, due to factors such as object size, distance from the camera, varied shapes, and cluttered backgrounds. To address these challenges, we propose small object detection YOLOv8 (SOD-YOLOv8), a novel model specifically designed for scenarios involving numerous small objects. Inspired by efficient generalized feature pyramid networks (GFPNs), we enhance multi-path fusion within YOLOv8 to integrate features across different levels, preserving details from shallower layers and improving small object detection accuracy. Additionally, we introduce a fourth detection layer to effectively utilize high-resolution spatial information. The efficient multi-scale attention module (EMA) in the C2f-EMA module further enhances feature extraction by redistributing weights and prioritizing relevant features. We introduce powerful-IoU (PIoU) as a replacement for CIoU, focusing on moderate quality anchor boxes and adding a penalty based on differences between predicted and ground truth bounding box corners. This approach simplifies calculations, speeds up convergence, and enhances detection accuracy. SOD-YOLOv8 significantly improves small object detection, surpassing widely used models across various metrics, without substantially increasing the computational cost or latency compared to YOLOv8s. Specifically, it increased recall from 40.1% to 43.9%, precision from 51.2% to 53.9%, mAP0.5 from 40.6% to 45.1%, and mAP0.5:0.95 from 24% to 26.6%. Furthermore, experiments conducted in dynamic real-world traffic scenes illustrated SOD-YOLOv8’s significant enhancements across diverse environmental conditions, highlighting its reliability and effective object detection capabilities in challenging scenarios. © 2024 by the authors.
KW  - attention mechanism
KW  - bounding box regression
KW  - feature pyramid network
KW  - small object detection
KW  - YOLOv8
KW  - Antenna grounds
KW  - Emergency traffic control
KW  - Image enhancement
KW  - Risk management
KW  - Attention mechanisms
KW  - Bounding box regression
KW  - Bounding-box
KW  - Feature pyramid
KW  - Feature pyramid network
KW  - Objects detection
KW  - Pyramid network
KW  - Small object detection
KW  - Traffic scene
KW  - YOLOv8
KW  - altitude
KW  - article
KW  - autonomous vehicle
KW  - benchmarking
KW  - camera
KW  - computer vision
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - female
KW  - human
KW  - imagery
KW  - male
KW  - punishment
KW  - reliability
KW  - traffic
KW  - Aerial photography
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Xu, Y.
TI  - MI-YOLO: An Improved Traffic Sign Detection Algorithm Based on YOLOv8
PY  - 2024
T2  - Engineering Letters
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211641922&partnerID=40&md5=d7db3a866721da10627da09f7af63079
AB  - Traffic sign detection plays an essential role in the technology of self-driving vehicles. Recently, deep learning methods have significantly advanced the field of traffic sign recognition. Nevertheless, faced with increasingly complex traffic scenarios, practical applications of traffic sign detection still encounter challenges, including false detections, missed detections, and reduced accuracy. To tackle these challenges, we introduce an enhanced algorithm for traffic sign detection built on the YOLOv8 model, aimed at improving performance and accuracy. Firstly, a Multi-Scale Convolutional Attention (MSCA) module is embedded into the backbone architecture to improve the model’s feature extraction capabilities at multiple scales, enhancing its focus on target areas. Furthermore, a small object detection layer is added during the detection phase, effectively reducing the false positive and missed detection rates for small objects. Finally, we present the Inner-WIoU loss function for bounding boxes, which integrates a dynamic non-monotonic focusing mechanism with auxiliary boxes. This boosts the model’s capability to identify objects and enhances overall detection performance. The findings from the experiments demonstrate that the enhanced algorithm obtains an mAP0.5 value of 83.8% on the TT100K dataset, indicating a 7.8% increase compared to the baseline YOLOv8 algorithm. When compared to existing algorithms, the proposed method demonstrates competitive performance. © 2024, International Association of Engineers. All rights reserved.
KW  - Bounding Box Loss
KW  - Multi-Scale Attention
KW  - Small Object Detection
KW  - Traffic Sign Detection
KW  - YOLOv8
KW  - Adaptive boosting
KW  - Deep learning
KW  - Object detection
KW  - Object recognition
KW  - Bounding box loss
KW  - Bounding-box
KW  - Detection algorithm
KW  - Missed detections
KW  - Multi-scale attention
KW  - Multi-scales
KW  - Self drivings
KW  - Small object detection
KW  - Traffic sign detection
KW  - YOLOv8
KW  - Traffic signs
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, Z.
AU  - Li, X.
AU  - Du, C.
AU  - Chen, A.
AU  - Han, Y.
AU  - Li, J.
TI  - YOLO⁃v8 with Multidimensional Attention and Upsampling Fusion for Small Air Target Detection in Radar Images
ST  - 基于多维注意和上采样融合的 YOLO‑v8 雷达图像空中小目标检测
PY  - 2024
T2  - Transactions of Nanjing University of Aeronautics and Astronautics
DO  - 10.16356/j.1005-1120.2024.06.004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003733647&doi=10.16356%2fj.1005-1120.2024.06.004&partnerID=40&md5=67f2b86a202bccaf05f208c54c8c8def
AB  - This study presents an innovative approach to improving the performance of YOLO-v8 model for small object detection in radar images. Initially, a local histogram equalization technique was applied to the original images, resulting in a notable enhancement in both contrast and detail representation. Subsequently, the YOLO-v8 backbone network was augmented by incorporating convolutional kernels based on a multidimensional attention mechanism and a parallel processing strategy, which facilitated more effective feature information fusion. At the model's head, an upsampling layer was added, along with the fusion of outputs from the shallow network, and a detection head specifically tailored for small object detection, thereby further improving accuracy. Additionally, the loss function was modified to incorporate focal-intersection over union (IoU) in conjunction with scaled-IoU, which enhanced the model's performance. A weighting strategy was also introduced, effectively improving detection accuracy for small targets. Experimental results demonstrate that the customized model outperforms traditional approaches across various evaluation metrics, including recall, precision, F1-score, and the receiver operating characteristic (ROC) curve, validating its efficacy and innovation in small object detection within radar imagery. The results indicate a substantial improvement in accuracy compared to conventional methods such as image segmentation and standard convolutional neural networks. © 2024 Nanjing University of Aeronautics an Astronautics. All rights reserved.
KW  - machine learning
KW  - object detection
KW  - radar images
KW  - YOLO
KW  - Convolutional neural networks
KW  - Image enhancement
KW  - Object tracking
KW  - Photointerpretation
KW  - Radar tracking
KW  - Air target detections
KW  - Innovative approaches
KW  - Local histogram equalizations
KW  - Machine-learning
KW  - Objects detection
KW  - Performance
KW  - Radar image
KW  - Small object detection
KW  - Upsampling
KW  - YOLO
KW  - Image segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Singh, A.
AU  - Dass, S.
TI  - Advanced Traffic Conflict Detection and Risk Assessment Using Multi-Scale Video Analysis: A YOLOv8 Modified and Attention-Enhanced Safety Metrics Evaluation
PY  - 2025
T2  - International Journal of Intelligent Transportation Systems Research
DO  - 10.1007/s13177-025-00508-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008348739&doi=10.1007%2fs13177-025-00508-6&partnerID=40&md5=a0ad227f5fc7ff55398a43fa58b7d3dd
AB  - This work proposed an Adaptive Temporal-Visual Hybrid Network (ATVHN) suited for precise conflict detection and threat estimation in harsh road scenarios for traffic safety. Conventional traffic systems are sometimes unable to trace the subtle relations between cars and pedestrians, particularly under difficult environments like poor light, occlusion, and anarchic motion. The envisioned framework combines a pre-trained YOLOv8 model with multi-scale video analysis and adds two new attention modules: Adaptive Visual-Attention Embedding Network (AVAENet) and Hybrid Temporal Dynamics Attention Network (HyTeDANet). These modules extract key spatial and temporal features from HWID12 video and time-series data, enriched through preprocessing and fusion techniques. Experimental performance indicates that ATVHN performs better than current models, such as Bi-LSTM and RNN, with high accuracy (0.98), precision (0.97), and sensitivity (0.97). Strong performance notwithstanding, the model is limited in effectiveness by sensitivity to high-quality video input and computational burden. Possible applications include integration within intelligent traffic systems for real-time surveillance, risk prediction alerts, and adaptive city traffic planning. © The Author(s), under exclusive licence to Intelligent Transportation Systems Japan 2025.
KW  - Attention mechanisms
KW  - Intelligent transportation systems
KW  - Multi-scale video analysis
KW  - Risk assessment
KW  - Traffic conflicts
KW  - Traffic safety
KW  - YOLOv8
KW  - Accident prevention
KW  - Advanced traffic management systems
KW  - Highway planning
KW  - Intelligent systems
KW  - Motor transportation
KW  - Real time systems
KW  - Risk analysis
KW  - Risk perception
KW  - Security systems
KW  - Traffic control
KW  - Video analysis
KW  - Attention mechanisms
KW  - Conflict detection
KW  - Intelligent transportation systems
KW  - Multi-scale video analyze
KW  - Multi-scales
KW  - Risks assessments
KW  - Traffic conflicts
KW  - Traffic safety
KW  - Video analysis
KW  - YOLOv8
KW  - Risk assessment
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, H.
AU  - Li, Z.
AU  - Wang, C.
TI  - YOLO-Dynamic: A Detection Algorithm for Spaceborne Dynamic Objects
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24237684
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211797698&doi=10.3390%2fs24237684&partnerID=40&md5=e6e199792e390dcd79452faf0853d8d7
AB  - Ground-based detection of spaceborne dynamic objects, such as near-Earth asteroids and space debris, is essential for ensuring the safety of space operations. This paper presents YOLO-Dynamic, a novel detection algorithm aimed at addressing the limitations of existing models, particularly in complex environments and small-object detection. The proposed algorithm introduces two newly designed modules: the SC_Block_C2f and the LASF_Neck. SC_Block_C2f, developed in this study, integrates StarNet and Convolutional Gated Linear Unit (CGLU) operations, improving small-object recognition and feature extraction. Meanwhile, LASF_Neck employs a lightweight multi-scale architecture for optimized feature fusion and faster detection. The YOLO-Dynamic algorithm’s performance was validated on real-world images captured at Antarctic observatory sites. Compared to the baseline YOLOv8s model, YOLO-Dynamic achieved a 7% increase in mAP@0.5 and a 10.3% improvement in mAP@0.5:0.95. Additionally, the number of parameters was reduced by 1.48 M, and floating-point operations decreased by 3.8 G. These results confirm that YOLO-Dynamic not only delivers superior detection accuracy but also maintains computational efficiency, making it well suited for real-world applications requiring reliable and efficient spaceborne object detection. © 2024 by the authors.
KW  - LASF_Neck
KW  - multi-scale feature fusion
KW  - SC_Block_C2f
KW  - spaceborne dynamic object detection
KW  - YOLOv8
KW  - Asteroids
KW  - Object detection
KW  - Object tracking
KW  - Space applications
KW  - Tropics
KW  - Dynamic objects
KW  - Features fusions
KW  - LASF_neck
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Objects detection
KW  - SC_block_c2f
KW  - Space-borne
KW  - Spaceborne dynamic object detection
KW  - YOLOv8
KW  - aged
KW  - algorithm
KW  - Antarctica
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - diagnosis
KW  - diagnostic test accuracy study
KW  - feature extraction
KW  - space debris
KW  - Space debris
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, F.
AU  - Wang, J.
AU  - Jiao, L.
AU  - Zhang, J.
AU  - Wang, H.
AU  - Li, S.
AU  - Li, L.
AU  - Chen, P.
AU  - Liu, X.
AU  - Ma, W.
AU  - Wang, S.
AU  - Yang, S.
AU  - Zhang, X.
AU  - Du, Y.
AU  - Bao, Q.
AU  - Sun, L.
AU  - Hou, B.
TI  - Remote Sensing Video Tracking: Current Status, Challenges, and Future
PY  - 2025
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
DO  - 10.1109/JSTARS.2025.3573572
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006606002&doi=10.1109%2fJSTARS.2025.3573572&partnerID=40&md5=94040ffa95cbd621baab7d7c01de1507
AB  - With the rapid advancement of remote sensing technology, the acquisition and processing of remote sensing video data, including high-resolution satellite, hyperspectral, and synthetic aperture radar video, have become key research areas in remote sensing. As a crucial branch of video analysis, remote sensing video tracking enables continuous monitoring and dynamic analysis of ground targets, with widespread applications in military operations, disaster monitoring, environmental protection, and urban management. This review systematically examines the current state of remote sensing video tracking research, exploring the technological advancements of traditional and modern methods, including model-based approaches, feature-based approaches, and deep-learning-based methods. The work also examines evaluation metrics and benchmark testing methodologies in remote sensing video tracking. Finally, this review highlights ten unresolved challenges in the field and offers insights into the future of next-generation artificial intelligence technologies. Through a comprehensive overview and in-depth analysis of existing technologies, this work aims to provide researchers with a systematic reference, promoting further development and innovation in remote sensing video tracking. © 2008-2012 IEEE.
KW  - Evaluation metrics and benchmark
KW  - remote sensing video tracking
KW  - ten unresolved challenges
KW  - Ground penetrating radar systems
KW  - Military aviation
KW  - Military communications
KW  - Military data processing
KW  - Military electronic countermeasures
KW  - Naval warfare
KW  - Space surveillance
KW  - Current status
KW  - Evaluation metric and benchmark
KW  - Evaluation metrics
KW  - High resolution satellites
KW  - Remote sensing technology
KW  - Remote sensing video tracking
KW  - Remote-sensing
KW  - Ten unresolved challenge
KW  - Video data
KW  - Video-tracking
KW  - artificial intelligence
KW  - data acquisition
KW  - disaster management
KW  - environmental protection
KW  - future prospect
KW  - image resolution
KW  - military application
KW  - monitoring system
KW  - remote sensing
KW  - satellite imagery
KW  - synthetic aperture radar
KW  - technology adoption
KW  - tracking
KW  - urban planning
KW  - videography
KW  - Ground operations
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tang, J.
AU  - Ye, C.
AU  - Zhou, X.
AU  - Xu, L.
TI  - YOLO-Fusion and Internet of Things: Advancing object detection in smart transportation
PY  - 2024
T2  - Alexandria Engineering Journal
DO  - 10.1016/j.aej.2024.09.012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204065135&doi=10.1016%2fj.aej.2024.09.012&partnerID=40&md5=a421fc39675f254b3a9dd9b54e5eac44
AB  - In intelligent transportation systems, traditional object detection algorithms struggle to handle complex environments and varying lighting conditions, particularly when detecting small targets and processing multimodal data. Furthermore, existing IoT frameworks are limited in their efficiency for real-time data collection and processing, leading to data transmission delays and increased resource consumption, which constrains the overall performance of intelligent transportation systems. To address these issues, this paper proposes a novel deep learning model, YOLO-Fusion. Based on the YOLOv8 architecture, this model innovatively integrates infrared and visible-light images, utilizing FusionAttention and Dynamic Fusion modules to optimize the fusion of multimodal information. To further enhance detection performance, this paper designs a Fusion-Dynamic Loss, improving the model's performance in complex intelligent transportation scenarios. To support the efficient operation of YOLO-Fusion, this paper also introduces an IoT framework that uses intelligent sensors and edge computing technology to achieve real-time collection, transmission and processing of traffic data, significantly improving data timeliness and accuracy. Experimental results demonstrate that YOLO-Fusion significantly outperforms traditional methods on the DroneVehicle and FLIR datasets, showcasing its broad application potential in intelligent traffic monitoring and management. © 2024 Faculty of Engineering, Alexandria University
KW  - Internet of Things
KW  - Multimodal data fusion
KW  - Smart city
KW  - Smart transportation
KW  - YOLO-Fusion
KW  - Data handling
KW  - Data transfer
KW  - Deep learning
KW  - Multimodal transportation
KW  - Complex environments
KW  - Environment lighting
KW  - Intelligent transportation systems
KW  - Lighting conditions
KW  - Multimodal data fusion
KW  - Object detection algorithms
KW  - Objects detection
KW  - Smart transportation
KW  - Varying lighting
KW  - YOLO-fusion
KW  - Data fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Yang, H.
AU  - Wu, M.
AU  - Wang, S.
AU  - Cao, Y.
AU  - Hu, S.
AU  - Shao, J.
AU  - Zeng, C.
TI  - UR-YOLO: an urban road small object detection algorithm
PY  - 2024
T2  - Pattern Analysis and Applications
DO  - 10.1007/s10044-024-01324-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204876357&doi=10.1007%2fs10044-024-01324-6&partnerID=40&md5=443def860c5ea67b67986a3b06d0c308
AB  - The autonomous driving system heavily depends on perception algorithms to gather crucial information about the surrounding urban environment. However, detecting small objects on busy urban roads poses a significant challenge. To overcome this obstacle, we present UR-YOLO (Urban Roads-YOLO), a novel small object detection algorithm tailored for urban roads, which builds upon the enhanced YOLOv9 framework. UR-YOLO comprises three key enhancements. Firstly, to mitigate the high background ratio in small object datasets, we employ SCRConv to replace selected standard convolutions in the backbone network. The reduction in spatial redundancy sharpens the perception of vital features. Secondly, to address the sparse distribution of small objects, we incorporate SPPELANBRA, a refined version of SPPELAN, to enhance the model’s sensitivity towards small objects, thereby improving its overall accuracy. Lastly, to address the issue of overlapping small objects, we upgrade the bounding box loss function by substituting the original SIoU loss with the Inner-MPDIoU loss. It not only improves the detection accuracy for small objects but also accelerates the convergence of the training process. To validate the effectiveness of UR-YOLO, we conducted comprehensive ablation and comparative experiments on the 2023 CICVAC dataset. The experimental results reveal that our proposed improvements have boosted the YOLOv9 model’s mAP, precision, and recall by significant margins of 6.02%, 6.63%, and 4.81% respectively. Furthermore, when compared to prior YOLO series and two-stage detection models, UR-YOLO exhibits superior accuracy, higher frames per second, and greater robustness, making it a robust solution for diverse weather conditions on urban roads. Code is available at https://github.com/Ranghao/UR_YOLO. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.
KW  - Autonomous driving
KW  - Feature extraction
KW  - Loss function
KW  - Small object detection
KW  - UR-YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Yang, X.
AU  - Lu, R.
AU  - Su, S.
AU  - Tang, B.
AU  - Zhang, T.
AU  - Zhu, Z.
TI  - TPDTNet: Two-Phase Distillation Training for Visible-to-Infrared Unsupervised Domain Adaptive Object Detection
PY  - 2025
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
DO  - 10.1109/JSTARS.2025.3528057
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214791468&doi=10.1109%2fJSTARS.2025.3528057&partnerID=40&md5=1cdf838c09a21e35804529c6be4aaaaf
AB  - In remote sensing target detection cases, great challenges are faced when migrating detection models from the visible domain to the infrared domain. Cross-domain migration suffers from problems such as a lack of data annotations in the infrared domain and interdomain feature differences. To improve the detection accuracy attained for infrared images, we propose a novel two-phase distillation training network (TPDTNet). Specifically, in the first phase, we incorporate a contrastive learning framework to maximize the mutual information between the source and target domains. In addition, we construct a generative model that learns only a unidirectional modality conversion mapping, thereby capturing the associations between their visual contents. The source-domain image is converted to an image with the style of the target domain, thereby achieving image-level domain alignment. The generated image is combined with the source-domain image to form an enhanced domain for cross-modal training. Enhanced domain data are fed into the teacher network to initialize the weights and produce pseudolabels. Next, to address small remote sensing target detection tasks, we construct a multidimensional progressive feature fusion detection framework, which initially fuses two adjacent low-level feature maps and then progressively incorporates high-level features to enhance the quality of fusing nonadjacent layer features. Subsequently, a spatial-dimension convolution is integrated into the backbone network. This convolutional operation is embedded following standard convolution to mitigate the loss of detailed features. Finally, a distillation training strategy that utilizes pseudodetection labels to calculate target information. By minimizing the Kullback-Leibler divergence between the probability maps of the teacher and student networks, the channel activations are transformed into probability distributions, thereby achieving knowledge distillation. The training weights are transferred from the teacher network to the student network to maximize the detection accuracy. Extensive experiments are conducted on three optical-to-infrared datasets, and the experimental results show that our TPDTNet method achieves state-of-the-art results relative to those of the baseline model.  © 2008-2012 IEEE.
KW  - Distillation training
KW  - object detection
KW  - remote sensing
KW  - unsupervised domain adaptation (UDA)
KW  - Health risks
KW  - Image annotation
KW  - Image fusion
KW  - Infrared imaging
KW  - Optical remote sensing
KW  - Personnel training
KW  - Photomapping
KW  - Proximity sensors
KW  - Students
KW  - Distillation training
KW  - Domain adaptation
KW  - Infrared domains
KW  - Objects detection
KW  - Remote-sensing
KW  - Targets detection
KW  - Teachers'
KW  - Training network
KW  - Two phase
KW  - Unsupervised domain adaptation
KW  - detection method
KW  - image analysis
KW  - remote sensing
KW  - training
KW  - Teaching
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, N.
AU  - Li, G.
AU  - Wang, S.
AU  - Wei, Z.
AU  - Ren, H.
AU  - Zhang, X.
AU  - Pei, Y.
TI  - SS-YOLO: A Lightweight Deep Learning Model Focused on Side-Scan Sonar Target Detection
PY  - 2025
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse13010066
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216219965&doi=10.3390%2fjmse13010066&partnerID=40&md5=9a58c9a3607a9d4b2cc05e4ff2997082
AB  - As seabed exploration activities increase, side-scan sonar (SSS) is being used more widely. However, distortion and noise during the acoustic pulse’s travel through water can blur target details and cause feature loss in images, making target recognition more challenging. In this paper, we improve the YOLO model in two aspects: lightweight design and accuracy enhancement. The lightweight design is essential for reducing computational complexity and resource consumption, allowing the model to be more efficient on edge devices with limited processing power and storage. Thus, meeting our need to deploy SSS target detection algorithms on unmanned surface vessel (USV) for real-time target detection. Firstly, we replace the original complex convolutional method in the C2f module with a combination of partial convolution (PConv) and pointwise convolution (PWConv), reducing redundant computations and memory access while maintaining high accuracy. In addition, we add an adaptive scale spatial fusion (ASSF) module using 3D convolution to combine feature maps of different sizes, maximizing the extraction of invariant features across various scales. Finally, we use an improved multi-head self-attention (MHSA) mechanism in the detection head, replacing the original complex convolution structure, to enhance the model’s ability to focus on important features with low computational load. To validate the detection performance of the model, we conducted experiments on the combined side-scan sonar dataset (SSSD). The results show that our proposed SS-YOLO model achieves average accuracies of 92.4% (mAP 0.5) and 64.7% (mAP 0.5:0.95), outperforming the original YOLOv8 model by 4.4% and 3%, respectively. In terms of model complexity, the improved SS-YOLO model has 2.55 M of parameters and 6.4 G of FLOPs, significantly lower than those of the original YOLOv8 model and similar detection models. © 2025 by the authors.
KW  - feature fusion
KW  - lightweight design
KW  - multi-head self-attention
KW  - partial convolution
KW  - side-scan sonar (SSS)
KW  - YOLOv8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Breckner, K.
AU  - Neumayr, T.
AU  - Mara, M.
AU  - Streit, M.
AU  - Augstein, M.
TI  - The Changing Nature of Human-AI Relations: A Scoping Review on Terminology and Evolvement in the Scientific Literature
PY  - 2025
T2  - International Journal of Human-Computer Interaction
DO  - 10.1080/10447318.2025.2482742
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009551578&doi=10.1080%2f10447318.2025.2482742&partnerID=40&md5=52aad23dc05187e7457a7f9aec21cf6e
AB  - Recent years have brought immense progress in the development of AI technology. This broadened its application fields but also led to a surge of interest in many research domains and increasing significance of human-AI relations for the development of AI technology. This rapid growth and evolvement is reflected by the establishment of a great variety of terms, potentially leading to what is known as jingle and jangle fallacies. With our scoping review of the terminology used in scientific literature to describe human-AI relations and its evolvement over time (with 803 records screened, 658 finally included), we capture the variety and development of human-AI terminology in accordance with the shift from interaction to collaboration between humans and AI. We aim to raise awareness of these developments spanning over different research communities and provide a solid basis for future researchers and practitioners conducting complementary, cross-domain research. Our review comprises terminological, bibliometric and thematic analyses, e.g., reporting on the historical development of terms and term composition patterns, but also identifying key authors and publications, geographic distribution of relevant research, and elaborating on term conception and usage, and co-occurrences throughout the literature. © 2025 The Author(s). Published with license by Taylor & Francis Group, LLC.
KW  - artificial intelligence
KW  - human-ai relations
KW  - human-centered ai
KW  - Scoping review
KW  - Terminology
KW  - AI Technologies
KW  - Application fields
KW  - Human-ai relation
KW  - Human-centered ai
KW  - ITS applications
KW  - Rapid growth
KW  - Research communities
KW  - Research domains
KW  - Scientific literature
KW  - Scoping review
KW  - Artificial intelligence
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pal, O.K.
AU  - Shovon, M.D.S.H.
AU  - Mridha, M.F.
AU  - Shin, J.
TI  - In-depth review of AI-enabled unmanned aerial vehicles: trends, vision, and challenges
PY  - 2024
T2  - Discover Artificial Intelligence
DO  - 10.1007/s44163-024-00209-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211111138&doi=10.1007%2fs44163-024-00209-1&partnerID=40&md5=290ac0b7767144557c29aee7d1e74f2f
AB  - In recent times, AI and UAV have progressed significantly in several applications. This article analyzes applications of UAV with modern green computing in various sectors. It addresses cutting-edge technologies such as green computing, generative AI, future scope, and related concerns in UAV. The research investigates the role of green computing and generative AI in combination with UAVs for navigation, object recognition and tracking, wildlife monitoring, precision agriculture, rescue operations, surveillance, and UAV communication. This study examines how modern computing technologies and UAVs are being applied in agriculture, surveillance, disaster management, and other areas. The ethics of UAV and AI applications, including safety, legal frameworks, and other issues, are thoroughly investigated. This research examines AI-based UAV applications across different disciplines, using open-source data and current advancements for future growth in this domain. This investigation will aid future researchers in their exploration of UAVs using cutting-edge computing technologies. © The Author(s) 2024.
KW  - Aerial vehicles
KW  - Agriculture surveillance
KW  - CNN
KW  - Generative AI
KW  - Green computing
KW  - Traffic monitoring
KW  - YOLO
KW  - Aircraft detection
KW  - Fertilizers
KW  - Aerial vehicle
KW  - Agriculture surveillance
KW  - Computing technology
KW  - Cutting edge technology
KW  - Generative AI
KW  - Object Tracking
KW  - Objects recognition
KW  - Traffic monitoring
KW  - Wildlife monitoring
KW  - YOLO
KW  - Unmanned aerial vehicles (UAV)
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Deng, F.
AU  - Qiao, B.
AU  - Li, K.
AU  - Zhao, L.
AU  - Chen, X.
AU  - Li, J.
AU  - Liu, J.
AU  - Sun, Y.
TI  - Wind turbine detection based on high spatial resolution four-band reflectance images
PY  - 2025
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.3057549
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217421906&doi=10.1117%2f12.3057549&partnerID=40&md5=1263264b7e8165a282fcdec9b4b844b3
AB  - Wind turbine detection is essential for the power industry and regulatory agencies to efficiently and accurately determine their number and spatial distribution. Under complex underlying surface conditions, the detection accuracy of conventional methods is low, especially when dealing with multi-scale wind turbine targets. In addition, the underutilization of the near-infrared (NIR) band limits the capability of optical remote sensing sensors in wind turbine detection. To address the above problems, this paper proposes a wind turbine detection algorithm YOLOv8m-BD based on high spatial resolution four-band reflectance images using YOLOv8m as the baseline model. Firstly, the four-band image is input through channel adaptation modification. Next, the CSPDarknet53 to 2-stage FPN (C2f) module is enhanced by introducing deformable convolution (DCN) to expand the receptive field. Finally, the effectiveness of our design was validated through extensive experiments on a self-built wind turbine dataset. Compared to the YOLOv8m baseline model that only inputs RGB three bands, the improved YOLOv8m-BD achieves an accuracy of 96.4% and a false alarm rate of 2.0%, an increase of 3.4% in accuracy and a decrease of 2.5% in false alarm rate. © 2025 SPIE.
KW  - RGB+NIR four band
KW  - Wind turbine detection
KW  - YOLOv8
KW  - Geological surveys
KW  - Image resolution
KW  - Optical remote sensing
KW  - Thermography (imaging)
KW  - Unattended sensors
KW  - Wind turbines
KW  - Baseline models
KW  - False alarm rate
KW  - High spatial resolution
KW  - Near Infrared
KW  - Near-infrared
KW  - Power industry
KW  - Reflectance images
KW  - RGB+near-infrared four band
KW  - Wind turbine detection
KW  - YOLOv8
KW  - Proximity sensors
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Choudhary, A.
AU  - Kumar Mishra, R.
AU  - Fatima, S.
AU  - Panigrahi, B.K.
TI  - Multimodal Fusion-Based Fault Diagnosis of Electric Vehicle Motor for Sustainable Transportation
PY  - 2025
T2  - IEEE Transactions on Transportation Electrification
DO  - 10.1109/TTE.2024.3502466
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001573355&doi=10.1109%2fTTE.2024.3502466&partnerID=40&md5=a1c7489966b0ef30890af83d002d96ad
AB  - Electric vehicles (EVs) are essential for sustainable transportation, and various ecofriendly vehicles are being manufactured. In EVs, the traction motor is a crucial prime mover for propelling the vehicle forward. However, traction motors are susceptible to faults like any other motors which can compromise their performance, safety, and longevity. This study proposes a reliable fault diagnosis strategy by using information fusion of vibration and current sensor data. Initially, vibration and current signals fusion-based diagnostic methods have been developed in the laboratory environment for induction motors (IMs) having seven fault conditions. This developed method involved wavelet synchrosqueezing transform (WSST) for the decomposition of the acquired vibration and current signature and further converted into a time-frequency spectrum. Thereafter, a multi-input fusion network (MiFN) has been designed for the fusion of vibration and current information. Finally, the developed fault diagnosis method has been extended and validated on an electric two-wheeler for diagnosing the faults in the brushless direct current motor (BLDC) hub motor. The suggested approach demonstrated significantly better classification accuracy than the signature of each sensor across a range of different speed situations. The achieved accuracies are in the range of 97.50%–98.35% in the laboratory environment and 90%–95% in the electric two-wheeler. The experimental results demonstrate that the suggested diagnosis methodology is highly accurate and remarkably reliable for pragmatic working conditions of EVs. © 2015 IEEE.
KW  - Electric vehicles (EVs)
KW  - fault diagnosis
KW  - information fusion
KW  - multi-input fusion network (MiFN)
KW  - sensor fusion
KW  - wavelet synchrosqueezing transform (WSST)
KW  - Data fusion
KW  - Electric fault location
KW  - Traction motors
KW  - Faults diagnosis
KW  - Laboratory environment
KW  - Multi-input fusion network
KW  - Multi-modal fusion
KW  - Multiinput
KW  - Sensor fusion
KW  - Sustainable transportation
KW  - Synchrosqueezing
KW  - Two wheelers
KW  - Wavelet synchrosqueezing transform
KW  - Wavelet decomposition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, Y.
AU  - Han, D.
AU  - Han, B.
AU  - Wu, Z.
TI  - ADV-YOLO: improved SAR ship detection model based on YOLOv8
PY  - 2025
T2  - Journal of Supercomputing
DO  - 10.1007/s11227-024-06527-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207086360&doi=10.1007%2fs11227-024-06527-6&partnerID=40&md5=7e665d732085eb12893fd6be497bec77
AB  - Synthetic aperture radar (SAR) ship detection plays a crucial role in supporting maritime traffic control, sea rescue, and marine environmental protection. Despite its importance, SAR ship detection confronts several challenges, including the small size of ship targets, unclear contours, complex background noise, and variable scales of ships. To address these challenges, this paper introduces an enhanced SAR ship detection model, termed ADV-YOLO, which builds upon the YOLOv8 framework. The proposed model incorporates space-to-depth building blocks to improve detection accuracy for low-resolution images and small objects. Additionally, a dilation-wise residual module replaces the C2f module in the network’s neck, augmenting the model’s capability to discern multi-scale targets and enrich feature representation. Furthermore, the WIoU loss function is adopted to replace the conventional CIoU loss, enhancing model accuracy, particularly for low-quality sample bounding boxes. Extensive experiments conducted on the HRSID and SSDD datasets demonstrate the robustness and reliability of ADV-YOLO. Compared to YOLOv8n, there is a significant performance improvement: the proposed method achieves an AP50-95 of 70% on the HRSID dataset, with an improvement of 4.5%. Additionally, it improves by 3.1% for AP50 and 5.7% for AP75. On the SSDD dataset, the AP50-75 improves by 0.9%, AP50 by 1.1%, and AP75 by 0.9%. This advancement underscores the potential of ADV-YOLO in enhancing real-time maritime surveillance and safety applications. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
KW  - Dilation-wise residual
KW  - Space-to-depth
KW  - Synthetic aperture radar (SAR) ship detection
KW  - YOLOv8
KW  - Air traffic control
KW  - Marine radar
KW  - Radar target recognition
KW  - Ships
KW  - Waterway transportation
KW  - Complex background
KW  - Detection models
KW  - Dilation-wise residual
KW  - Maritime traffic
KW  - Model-based OPC
KW  - Ship detection
KW  - Ship targets
KW  - Space-to-depth
KW  - Synthetic aperture radar  ship detection
KW  - YOLOv8
KW  - Synthetic aperture radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, Z.
AU  - Zhu, C.
AU  - Tao, H.
AU  - Zhang, Q.
AU  - Pu, C.
AU  - Xiao, J.
TI  - Robot map construction based on topology and hierarchical fusion in unknown environment
PY  - 2025
T2  - Journal of Physics: Conference Series
DO  - 10.1088/1742-6596/2999/1/012022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004199712&doi=10.1088%2f1742-6596%2f2999%2f1%2f012022&partnerID=40&md5=f971317d1d9bcf455713238faec0f3e5
AB  - Map construction has always been crucial in exploring enclosed unknown environments. Traditional mapping methods have problems such as low accuracy or long time consumption. Due to the indoor and enclosed nature of unknown environments, inspection robots often encounter challenges in mapping and path planning. To address similar issues, we proposed an optimized indoor mapping solution based on topology technology and hierarchical fusion, and conducted experiments using robots in a library simulation scenario in an unknown environment. Figure 1 shows the map that we have obtained by layering and blending.By utilizing topology techniques and hierarchical fusion mapping methods, we have effectively simplified the mapping process, reducing computational complexity and cost while obtaining spatial information and this method allows for the consideration of the geometric characteristics of robots when planning safe and accurate passable paths. © 2025 Institute of Physics Publishing. All rights reserved.
KW  - Geometry
KW  - Maps
KW  - Mathematical morphology
KW  - Photomapping
KW  - Hierarchical fusions
KW  - Inspection robots
KW  - Map constructions
KW  - Mapping method
KW  - Mapping process
KW  - Robot maps
KW  - Spatial informations
KW  - Technology fusion
KW  - Time consumption
KW  - Unknown environments
KW  - Topology
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Aldoğan, C.F.
AU  - Aksu, K.
AU  - Demirel, H.
TI  - Enhancement of Sentinel-2A Images for Ship Detection via Real-ESRGAN Model
PY  - 2024
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app142411988
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213221644&doi=10.3390%2fapp142411988&partnerID=40&md5=17c007838f705c15730cb9f2de5e141f
AB  - Ship detection holds great value regarding port management, logistics operations, ship security, and other crucial issues concerning surveillance and safety. Recently, ship detection from optical satellite imagery has gained popularity among the research community because optical images are easily accessible with little or no cost. However, these images’ quality and quantity of feature details are bound to their spatial resolution, which often comes in medium-low spatial resolution. Accurately detecting ships requires images with richer texture and resolution. Super-resolution is used to recover features in medium-low resolution images, which can help leverage accuracy in ship detection. In this regard, this paper quantitatively and visually investigates the effectiveness of super-resolution in enabling more accurate ship detection in medium spatial resolution images by comparing Sentinel-2A images and enhanced Sentinel-2A images. A collection of Sentinel-2A images was enhanced four times with a Real-ESRGAN model that trained PlanetScope images with high spatial resolution. Separate ship detections with YOLOv10 were implemented for Sentinel-2A images and enhanced Sentinel-2A images. The visual and metric results of both detections were compared to demonstrate the contributory effect of enhancement on the ships’ detection accuracy. Ship detection on enhanced Sentinel-2A images has a mAP50 and mAP50-95 value of 87.5% and 68.5%. These results outperformed the training process on Sentinel-2A images with a mAP value increase of 2.6% for both mAP50 and mAP50-95, demonstrating the positive contribution of super-resolution. © 2024 by the authors.
KW  - object detection
KW  - PlanetScope
KW  - Real-ESRGAN
KW  - satellite images
KW  - Sentinel-2A
KW  - ship detection
KW  - super-resolution
KW  - YOLO
KW  - Image enhancement
KW  - Image texture
KW  - Satellite imagery
KW  - Objects detection
KW  - Planetscope
KW  - Port management
KW  - Real-ESRGAN
KW  - Satellite images
KW  - Sentinel-2a
KW  - Ship detection
KW  - Spatial resolution
KW  - Superresolution
KW  - YOLO
KW  - Image resolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, R.
AU  - Chen, L.
AU  - Huang, Z.
AU  - Zhang, W.
AU  - Wu, S.
TI  - A Review on the High-Efficiency Detection and Precision Positioning Technology Application of Agricultural Robots
PY  - 2024
T2  - Processes
DO  - 10.3390/pr12091833
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205318004&doi=10.3390%2fpr12091833&partnerID=40&md5=9fc89c10e17734a0e21586f7ce1099ac
AB  - The advancement of agricultural technology has increasingly positioned robotic detection and localization techniques at the forefront, ensuring critical support for agricultural development through their accuracy and reliability. This paper provides an in-depth analysis of various methods used in detection and localization, including UWB, deep learning, SLAM, and multi-sensor fusion. In the domain of detection, the application of deep algorithms in assessing crop maturity and pest analysis is discussed. For localization, the accuracy of different methods in target positioning is examined. Additionally, the integration of convolutional neural networks and multi-sensor fusion with deep algorithms in agriculture is reviewed. The current methodologies effectively mitigate environmental interference, significantly enhancing the accuracy and reliability of agricultural robots. This study offers directional insights into the development of robotic detection and localization in agriculture, clarifying the future trajectory of this field and promoting the advancement of related technologies. © 2024 by the authors.
KW  - convolutional neural networks (CNN)
KW  - deep learning
KW  - detection and localization
KW  - multi-sensor fusion
KW  - simultaneous localization and mapping (SLAM)
KW  - ultra-wideband (UWB) technology
KW  - Agricultural robots
KW  - Deep neural networks
KW  - Precision agriculture
KW  - SLAM robotics
KW  - Agricultural robot
KW  - Convolutional neural network
KW  - Deep learning
KW  - Detection and localization
KW  - Higher efficiency
KW  - Multi-sensor fusion
KW  - Simultaneous localization and mapping
KW  - Ultra-wideband technology
KW  - Convolutional neural networks
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Zheng, H.
AU  - Cui, Z.
AU  - Huang, Z.
AU  - Liang, Y.
AU  - Li, P.
AU  - Liu, P.
TI  - Intelligent detection method with 3D ranging for external force damage monitoring of power transmission lines
PY  - 2024
T2  - Applied Energy
DO  - 10.1016/j.apenergy.2024.123983
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199888118&doi=10.1016%2fj.apenergy.2024.123983&partnerID=40&md5=f051514f9f3c9e3b665806bfb6adedc5
AB  - The rapid development of smart grid requires more and more reliable power supply. The external force damage to transmission lines can significantly impact the safety and stability of the power grid, potentially causing electric shock accidents. Existing methods for monitoring external force damage in transmission corridors lack the ability to provide effective warnings based on the real-time distance between potential hazards and power lines. Additionally, limited computational power and storage capacity at edge terminals restrict the efficient deployment of high-precision (high complexity) visual algorithms. This study presents, for the first time, a lightweight intelligent detection method integrating detection and three-dimensional (3D) ranging. A regression loss optimized for small objects is introduced to compensate for the shortcomings of lightweight networks in detection accuracy. Simultaneously, based on the influence of convolutions in various modules of the baseline model on performance, lightweight improvements are made to the detector architecture using Omni-Dimensional Dynamic Convolution and Distribution Shifting Convolution. Finally, a 3D ranging module is integrated into the detector, involving operations such as 2D–3D information matching and back-projection transformation. This method innovatively achieves automated ranging and hierarchical warning. Its effectiveness is validated in transmission corridor scenarios under various weather conditions and surveillance video. The results demonstrate that our method outperforms other algorithms in hazard detection accuracy and lightweight performance. Moreover, the distance prediction error rate is below 1.6%. The hierarchical warning solutions can be applied to more scenarios. © 2024 Elsevier Ltd
KW  - Coordinate transformation
KW  - Lightweight deep learning
KW  - Monocular vision
KW  - Prevention of external force damage
KW  - Smart power transmission
KW  - Computational complexity
KW  - Computational efficiency
KW  - Damage detection
KW  - Deep neural networks
KW  - Electric lines
KW  - Electric power distribution
KW  - Electric power transmission
KW  - Electric power transmission networks
KW  - Hazards
KW  - Power transmission
KW  - Security systems
KW  - Smart power grids
KW  - 3-D ranging
KW  - Coordinate transformations
KW  - External force
KW  - Intelligent detection methods
KW  - Lightweight deep learning
KW  - Monocular vision
KW  - Power-transmission
KW  - Prevention of external force damage
KW  - Smart power
KW  - Smart power transmission
KW  - damage mechanics
KW  - detection method
KW  - electricity supply
KW  - energy storage
KW  - machine learning
KW  - power line
KW  - prediction
KW  - smart grid
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Jia, M.
AU  - Li, B.
AU  - Meng, L.
AU  - Zhu, L.
TI  - Multi-Grade Road Distress Detection Strategy Based on Enhanced YOLOv8 Model
PY  - 2024
T2  - Buildings
DO  - 10.3390/buildings14123832
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213260692&doi=10.3390%2fbuildings14123832&partnerID=40&md5=3671e38cca3f0bf9bb8baf56d5a7e20c
AB  - The total mileage of the road network in China has been growing rapidly during the last twenty years. With the development of deep learning, the automatic road distr ess detection method is more accurate and effective than manual detection. However, the road are classified into five grades according to the Chinese road standard and each grade has its own characteristics. A single model cannot effectively identify multi-grade roads with different materials and levels of road distress. This study proposes a YOLOv8-based road distress detection strategy adapted for multiple road grades. The improved URetinex-Net network is used to enhance the spatial features and scenario diversity of the road distress datasets. Compared to the base YOLOv8 model, the enhancements have led to a 12% increase in accuracy for cement roads, a 22.3% improvement in detection speed, a 5.5% increase in accuracy for ordinary asphalt roads, a 7.5% increase in recognition accuracy for highways, and a 9.3% improvement in detection speed, with significant effects. This study refines the classification of roads based on their grades and matches them with corresponding artificial intelligence training strategies, providing guidance for road inspection and maintenance. © 2024 by the authors.
KW  - data enhancement
KW  - deep learning
KW  - distress detection
KW  - YOLOv8
KW  - Classifieds
KW  - Data enhancement
KW  - Deep learning
KW  - Detection methods
KW  - Detection speed
KW  - Distress detection
KW  - Road grades
KW  - Road network
KW  - Single models
KW  - YOLOv8
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tang, D.
AU  - Tang, S.
AU  - Fan, Z.
TI  - LCFF-Net: A lightweight cross-scale feature fusion network for tiny target detection in UAV aerial imagery
PY  - 2024
T2  - PLoS ONE
DO  - 10.1371/journal.pone.0315267
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213336340&doi=10.1371%2fjournal.pone.0315267&partnerID=40&md5=36c9f9c135bf7ac6fd0dfa7160fe34ef
AB  - In the field of UAV aerial image processing, ensuring accurate detection of tiny targets is essential. Current UAV aerial image target detection algorithms face challenges such as low computational demands, high accuracy, and fast detection speeds. To address these issues, we propose an improved, lightweight algorithm: LCFF-Net. First, we propose the LFERELAN module, designed to enhance the extraction of tiny target features and optimize the use of computational resources. Second, a lightweight cross-scale feature pyramid network (LC-FPN) is employed to further enrich feature information, integrate multi-level feature maps, and provide more comprehensive semantic information. Finally, to increase model training speed and achieve greater efficiency, we propose a lightweight, detail-enhanced, shared convolution detection head (LDSCD-Head) to optimize the original detection head. Moreover, we present different scale versions of the LCFF-Net algorithm to suit various deployment environments. Empirical assessments conducted on the VisDrone dataset validate the efficacy of the algorithm proposed. Compared to the baseline-s model, the LCFF-Net-n model outperforms baseline-s by achieving a 2.8% increase in the mAP50 metric and a 3.9% improvement in the mAP50–95 metric, while reducing parameters by 89.7%, FLOPs by 50.5%, and computation delay by 24.7%. Thus, LCFF-Net offers high accuracy and fast detection speeds for tiny target detection in UAV aerial images, providing an effective lightweight solution. © 2024 Tang et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
KW  - Algorithms
KW  - Image Processing, Computer-Assisted
KW  - Neural Networks, Computer
KW  - Unmanned Aerial Devices
KW  - algorithm
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - diagnosis
KW  - human
KW  - image processing
KW  - imagery
KW  - algorithm
KW  - artificial neural network
KW  - image processing
KW  - procedures
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhuang, J.
AU  - Wang, N.
AU  - Zhuang, Y.
AU  - Hao, Y.
TI  - Frame Extraction Person Retrieval Framework Based on Improved YOLOv8s and the Stage-Wise Clustering Person Re-Identification
PY  - 2025
T2  - IET Image Processing
DO  - 10.1049/ipr2.70046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000439590&doi=10.1049%2fipr2.70046&partnerID=40&md5=6f53bfc07c7ae7b2e5796aee24d4cd2d
AB  - Person re-identification (Re-ID), a crucial research area in smart city security, faces challenges due to person posture changes, object occlusion and other factors, making it difficult for existing methods to accurately retrieving target person in video surveillance. To resolve this problem, we propose a person retrieval framework that integrates YOLOv8s and person Re-ID. Improved YOLOv8s is employed to extract person categories from the video on a frame-by-frame basis, and when combined with the stage-wise clustering person Re-ID network (SCPN), it enables collaborative person retrieval across multiple cameras. Notably, a feature precision (FP) module is added in the YOLOv8s network to form FP-YOLOv8s, and SCPN incorporates innovative enhancements including the stage-wise learning rate scheduler, centralized clustering loss and adaptive representation joint attention module into the person Re-ID baseline model. Comprehensive experiments on COCO, Market-1501 and DukeMTMC-ReID datasets demonstrate that our proposed framework outperforms several other leading methods. Given the scarcity of image-video person Re-ID datasets, we also provide an extended image-video person (EIVP) dataset, which contains 102 videos and 814 bounding boxes of 57 identities captured by 8 cameras. The video reasoning detection score of this framework reaches 78.8% on this dataset, indicating a 3.2% increase compared to conventional models. © 2025 The Author(s). IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
KW  - feature representation
KW  - frame extraction
KW  - person re-identification
KW  - person retrieval
KW  - YOLOv8s
KW  - Image retrieval
KW  - City securities
KW  - Clusterings
KW  - Feature representation
KW  - Frame extraction
KW  - Object occlusion
KW  - Person re identifications
KW  - Person retrieval
KW  - Research areas
KW  - Retrieval frameworks
KW  - YOLOv8
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, X.
AU  - Li, X.
TI  - Research on surface defect detection algorithm of pipeline weld based on YOLOv7
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-52451-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182849727&doi=10.1038%2fs41598-024-52451-3&partnerID=40&md5=788e800d173f40afc444941779df116c
AB  - Aiming at the problems of low target detection accuracy and high leakage rate of the current traditional weld surface defect detection methods and existing detection models, an improved YOLOv7 pipeline weld surface defect detection model is proposed to improve detection results. In the improved model, a Le-HorBlock module is designed, and it is introduced into the back of fourth CBS module of the backbone network, which preserves the characteristics of high-order information by realizing second-order spatial interaction, thus enhancing the ability of the network to extract features in weld defect images. The coordinate attention (CoordAtt) block is introduced to enhance the representation ability of target features, suppress interference. The CIoU loss function in YOLOv7 network model is replaced by the SIoU, so as to optimize the loss function, reduce the freedom of the loss function, and accelerate convergence. And a new large-scale pipeline weld surface defect dataset containing 2000 images of pipeline welds with weld defects is used in the proposed model. In the experimental comparison, the improved YOLOv7 network model has greatly improved the missed detection rate compared with the original network. The experimental results show that the improved YOLOv7 network model mAP@80.5 can reach 78.6%, which is 15.9% higher than the original model, and the detection effect is better than the original network and other classical target detection networks. © 2024, The Author(s).
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - human
KW  - pipeline
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - An, R.
AU  - Zhang, X.
AU  - Sun, M.
AU  - Wang, G.
TI  - GC-YOLOv9: Innovative smart city traffic monitoring solution
PY  - 2024
T2  - Alexandria Engineering Journal
DO  - 10.1016/j.aej.2024.07.004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198272213&doi=10.1016%2fj.aej.2024.07.004&partnerID=40&md5=dc1be1f8c003f0ccb5a05354fa14fb61
AB  - In urban smart city environments, traffic hazards can lead to catastrophic outcomes, including significant property losses and severe threats to public safety. Conventional traffic monitoring systems are limited in terms of accuracy and speed, presenting significant challenges for real-time traffic surveillance. To tackle these challenges, this paper introduces the GC-YOLOv9 algorithm. Specifically, we have enhanced the YOLOv9 model by incorporating Ghost Convolution, markedly improving the model's perceptual abilities and detection accuracy. Furthermore, this study designed an integrated smart city framework that includes layers for service applications, the Internet of Things, edge processing, and data centers. By deploying the enhanced YOLOv9 model within this framework, our method achieved mAP@0.5 scores of 77.15 and 74.95 on the BDD100K and Cityscapes datasets, respectively, surpassing existing technologies. Additionally, the potential applications of this method in public area fire safety management, forest fire monitoring, and intelligent security systems further underscore its significant value in improving the safety and efficiency of smart cities. © 2024 Faculty of Engineering, Alexandria University
KW  - Ghost convolution
KW  - Internet of Things
KW  - Real-time data processing
KW  - Smart cities
KW  - Traffic monitoring
KW  - YOLOv9
KW  - Convolution
KW  - Deforestation
KW  - Network security
KW  - Real time systems
KW  - Smart city
KW  - City traffic
KW  - Ghost convolution
KW  - Property loss
KW  - Public safety
KW  - Real-time data processing
KW  - Real-time traffic surveillances
KW  - Traffic hazards
KW  - Traffic monitoring
KW  - Traffic monitoring systems
KW  - YOLOv9
KW  - Internet of things
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Yan, J.
AU  - Liu, Y.
AU  - Gao, Z.
TI  - LRS²-DM: Small Ship Target Detection in Low-Resolution Remote Sensing Images Based on Diffusion Models
PY  - 2025
T2  - IEEE Transactions on Geoscience and Remote Sensing
DO  - 10.1109/TGRS.2025.3580609
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008648975&doi=10.1109%2fTGRS.2025.3580609&partnerID=40&md5=d6f21df4a6916fdbec84a2d2703f9677
AB  - With advancements in remote sensing technology, ship detection has emerged as a pivotal component in marine environmental protection and maritime traffic management. However, the significant distance of satellite imaging results in ship targets appearing as small-scale objects in the images. Current detection algorithms face challenges in accurately identifying the features of small ship targets in low-resolution settings. Therefore, this article proposes a small ship target detection model for low-resolution remote sensing images based on diffusion models (DMs). In the first stage, cognitive conditions are used as inputs. A low-level super-resolution (L2SR) module enhances image clarity and facilitates the extraction of richer ship target features. The second stage employs a spatial refinement module (SRM) to effectively enhance textures, edges, and other fine-grained features of small targets. Finally, an optimized loss function is designed to mitigate uncertainties arising from noise in the DM for remote sensing images. The experimental results demonstrate that the proposed method achieves superior performance on the DOTA-v2.0-Ship and S-Ship datasets, attaining average precision (AP) values of 95.34% and 96.12%, respectively. Moreover, it sustains a high frame-per-second (FPS) rate, striking an optimal balance between detection accuracy and computational efficiency. © 1980-2012 IEEE.
KW  - Diffusion models (DMs)
KW  - low resolution
KW  - ship inspection
KW  - small goals
KW  - Diffusion
KW  - Image enhancement
KW  - Remote sensing
KW  - Satellite imagery
KW  - Ships
KW  - Small satellites
KW  - Textures
KW  - Waterway transportation
KW  - Diffusion model
KW  - Image-based
KW  - Lower resolution
KW  - Remote sensing images
KW  - Remote sensing technology
KW  - Ship detection
KW  - Ship inspection
KW  - Ship targets
KW  - Small goal
KW  - Targets detection
KW  - detection method
KW  - environmental protection
KW  - image resolution
KW  - remote sensing
KW  - satellite imagery
KW  - traffic management
KW  - Computational efficiency
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, F.
AU  - Shen, T.
AU  - Bäck, T.
AU  - Chen, J.
AU  - Huang, G.
AU  - Jin, Y.
AU  - Kuang, K.
AU  - Li, M.
AU  - Lu, C.
AU  - Miao, J.
AU  - Wang, Y.
AU  - Wei, Y.
AU  - Wu, F.
AU  - Yan, J.
AU  - Yang, H.
AU  - Yang, Y.
AU  - Zhang, S.
AU  - Zhao, Z.
AU  - Zhuang, Y.
AU  - Pan, Y.
TI  - Knowledge-Empowered, Collaborative, and Co-Evolving AI Models: The Post-LLM Roadmap
PY  - 2025
T2  - Engineering
DO  - 10.1016/j.eng.2024.12.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215941211&doi=10.1016%2fj.eng.2024.12.008&partnerID=40&md5=4b22ff775e65fbde0ff1cd79d3e74e4d
AB  - Large language models (LLMs) have significantly advanced artificial intelligence (AI) by excelling in tasks such as understanding, generation, and reasoning across multiple modalities. Despite these achievements, LLMs have inherent limitations including outdated information, hallucinations, inefficiency, lack of interpretability, and challenges in domain-specific accuracy. To address these issues, this survey explores three promising directions in the post-LLM era: knowledge empowerment, model collaboration, and model co-evolution. First, we examine methods of integrating external knowledge into LLMs to enhance factual accuracy, reasoning capabilities, and interpretability, including incorporating knowledge into training objectives, instruction tuning, retrieval-augmented inference, and knowledge prompting. Second, we discuss model collaboration strategies that leverage the complementary strengths of LLMs and smaller models to improve efficiency and domain-specific performance through techniques such as model merging, functional model collaboration, and knowledge injection. Third, we delve into model co-evolution, in which multiple models collaboratively evolve by sharing knowledge, parameters, and learning strategies to adapt to dynamic environments and tasks, thereby enhancing their adaptability and continual learning. We illustrate how the integration of these techniques advances AI capabilities in science, engineering, and society—particularly in hypothesis development, problem formulation, problem-solving, and interpretability across various domains. We conclude by outlining future pathways for further advancement and applications. © 2024 THE AUTHORS
KW  - Artificial intelligence
KW  - Knowledge empowerment
KW  - Large language models
KW  - Model co-evolution
KW  - Model collaboration
KW  - Adversarial machine learning
KW  - Co-evolution
KW  - Domain specific
KW  - Intelligence models
KW  - Interpretability
KW  - Knowledge empowerment
KW  - Language model
KW  - Large language model
KW  - Model co-evolution
KW  - Model collaboration
KW  - Roadmap
KW  - Collaborative learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, R.
AU  - Tang, S.H.
AU  - Shen, J.
AU  - Supeni, E.E.B.
AU  - Rahim, S.A.
TI  - Enhancing autonomous driving safety: A robust traffic sign detection and recognition model TSD-YOLO
PY  - 2024
T2  - Signal Processing
DO  - 10.1016/j.sigpro.2024.109619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199342177&doi=10.1016%2fj.sigpro.2024.109619&partnerID=40&md5=31a273fa41ffffb4f250478847fb7397
AB  - As autonomous driving technology rapidly advances, Traffic Sign Detection and Recognition (TSDR) has become pivotal in ensuring the safety and regulatory compliance of autonomous vehicles. Despite progress, existing technologies struggle under challenging conditions such as adverse weather and complex roadway environments. To overcome these obstacles, we introduce a novel model, TSD-YOLO, which leverages Mamba and YOLO technologies to enhance the accuracy and robustness of traffic sign detection. Our innovative YOLO-MAM dual-branch module merges convolutional layer-based local feature extraction with the long-distance dependency capabilities of the State Space Models (SSMs). We conducted experimental validations using the Tsinghua-Tencent 100K (TT-100K) dataset and the Mapillary Traffic Sign Detection (MTSD) dataset, demonstrating our model's efficacy across various datasets. Furthermore, cross-dataset validations affirm the model's exceptional generalization and robustness across diverse environments. This study not only bolsters traffic sign detection and recognition in autonomous driving systems but also paves the way for future advancements in autonomous driving technology. © 2024 Elsevier B.V.
KW  - Autonomous driving
KW  - Mamba
KW  - Traffic sign detection
KW  - TSD-YOLO
KW  - YOLOv8
KW  - Regulatory compliance
KW  - State space methods
KW  - Traffic signs
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Detection models
KW  - Driving safety
KW  - Mamba
KW  - Recognition models
KW  - Traffic sign detection
KW  - Traffic sign detection and recognition
KW  - TSD-YOLO
KW  - YOLOv8
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, Z.
AU  - Zhang, Y.
AU  - Wang, X.
AU  - Li, H.
AU  - Sun, Y.
AU  - Wang, G.
TI  - Algorithm for detecting surface defects in wind turbines based on a lightweight YOLO model
PY  - 2024
T2  - Scientific Reports
DO  - 10.1038/s41598-024-74798-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206873013&doi=10.1038%2fs41598-024-74798-3&partnerID=40&md5=4fe1a1c1c32cc525379a1d7d057a5961
AB  - Improving wind power generation efficiency and lowering maintenance and operational costs are possible through the early and efficient diagnosis and repair of surface defects in wind turbines. To solve the lightweight deployment difficulty and insufficient accuracy issues of the traditional detection methods, this paper proposes a high-precision PC-EMA block based on YOLOv8 using partial convolution (PConv) combined with an efficient multiscale attention (EMA) channel attention mechanism, which replaces the bottleneck layer of the YOLOv8 backbone network to improve the extraction of target feature information from each layer of the network. In the feature fusion phase, GSConv, which can retain more channel information, is introduced to balance the model’s complexity and accuracy. Finally, by merging two branches and designing the PConv head with a low-latency PConv rather than a regular convolution, we are able to effectively reduce the complexity of the model while maintaining accuracy in the detection head. We use the WIoUv3 as the regression loss for the improved model, which improves the average accuracy by 5.07% and compresses the model size by 32.5% compared to the original YOLOv8 model. Deployed on Jetson Nano, the FPS increased by 11 frames/s after a TensorRT acceleration. © The Author(s) 2024.
KW  - Channel attention mechanism
KW  - GSConv
KW  - Partial convolution
KW  - Wind turbines
KW  - YOLOv8
KW  - acceleration
KW  - algorithm
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - human
KW  - latent period
KW  - wind
KW  - wind power
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bao, T.
AU  - Lin, D.
AU  - Zhang, X.
AU  - Zhou, Z.
AU  - Wang, K.
TI  - Pedestrian safety alarm system based on binocular distance measurement for trucks using recognition feature analysis
PY  - 2024
T2  - Autonomous Intelligent Systems
DO  - 10.1007/s43684-024-00080-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209215767&doi=10.1007%2fs43684-024-00080-y&partnerID=40&md5=3e81d3beccf939222506692bad79ad12
AB  - As an essential part of modern smart manufacturing, road transport with large and heavy trucks has in-creased dramatically. Due to the inside wheel difference in the process of turning, there is a considerable safety hazard in the blind area of the inside wheel difference. In this paper, multiple cameras combined with deep learning algorithms are introduced to detect pedestrians in the blind area of wheel error. A scheme of vehicle-pedestrian safety alarm detection system is developed via the integration of YOLOv5 and an improved binocular distance measurement method. The system accurately measures the distance between the truck and nearby pedestrians by utilizing multiple cameras and PP Human recognition, providing real-time safety alerts. The experimental results show that this method significantly reduces distance measurement errors, improves the reliability of pedestrian detection, achieves high accuracy and real-time performance, and thus enhances the safety of trucks in complex traffic environments. © The Author(s) 2024.
KW  - Feature recognition
KW  - Human distance measurement
KW  - PP-human attribute identification
KW  - Security alarm
KW  - Alarm systems
KW  - Automobiles
KW  - Binoculars
KW  - Deep learning
KW  - Magnetic levitation vehicles
KW  - Pedestrian safety
KW  - Smart manufacturing
KW  - Truck transportation
KW  - Trucks
KW  - Feature analysis
KW  - Features recognition
KW  - Human attributes
KW  - Human distance measurement
KW  - Multiple cameras
KW  - PP-human attribute identification
KW  - Recognition features
KW  - Safety alarm system
KW  - Security alarm
KW  - Smart manufacturing
KW  - Wheels
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nie, H.
AU  - Zhang, G.
AU  - Li, D.
AU  - He, Y.
TI  - Environment Perception and Motion Planning for Multi-rotors: A Review
ST  - 多旋翼无人机的环境感知与运动规划方法综述
PY  - 2025
T2  - Information and Control
DO  - 10.13976/j.cnki.xk.2024.4751
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009060795&doi=10.13976%2fj.cnki.xk.2024.4751&partnerID=40&md5=711396f92f72837f9aaad85db245f597
AB  - Recently, there has been growing interest among researchers in developing multi-rotors with advanced autonomous flight capabilities. Multi-rotors are capable of performing complex and hazardous tasks in place of humans, such as search and rescue, inspection, and exploration missions. However, they face numerous challenges when operating in the diverse and dynamic real-world environments. To effectively accomplish these tasks, multi-rotors require a robust, safe, and efficient autonomous flight system. As we know, autonomous flight systems involve multiple complex technical aspects, including mapping, state estimation, and motion planning. We offer an in-depth discussion, comparison, and comprehensive review of the strengths and limitations of these submodules. Furthermore, we highlight the current limitations and unresolved challenges of autonomous flight in various scenarios, providing valuable insights for researchers seeking to bridge the gap between theory and practical applications. Finally, we summarize the future challenges and emerging trends in the development of autonomous flight systems. © 2025 Science Press. All rights reserved.
KW  - autonomous flight
KW  - motion planning
KW  - state estimation
KW  - trajectory optimization
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, L.
AU  - Huang, Z.A.
AU  - Shi, C.
AU  - Ma, H.
AU  - Li, X.
AU  - Wu, X.
TI  - MFPIDet: improved YOLOV7 architecture based on multi-scale feature fusion for prohibited item detection in complex environment
PY  - 2024
T2  - Complex and Intelligent Systems
DO  - 10.1007/s40747-024-01580-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201219749&doi=10.1007%2fs40747-024-01580-3&partnerID=40&md5=e392ce19cee1c34ad08b1069e5d44338
AB  - Prohibited item detection is crucial for the safety of public places. Deep learning, one of the mainstream methods in prohibited item detection tasks, has shown superior performance far beyond traditional prohibited item detection methods. However, most neural network architectures in deep learning still lack sufficient local feature representation ability for overlapping and small targets, and ignore the problem of semantic conflicts caused by direct feature fusion. In this paper, we propose MFPIDet, a novel prohibited item detection neural network architecture based on improved YOLOV7 to achieve reliable prohibited item detection in complex environments. Specifically, a multi-scale attention module (MAM) backbone is proposed to filter the redundant information of target regions and further applied to enhance the local feature representation ability of overlapping objects. Here, to reduce the redundant information of target regions, a squeeze-excitation (SE) block is used to filter the background. Then, aiming at enhancing the feature expression ability of overlapping objects, a multi-scale feature extraction module (MFEM) is designed for local feature representation. In addition, to obtain richer context information, We design an adaptive fusion feature pyramid network (AF-FPN) to combine the adaptive context information fusion module (ACIFM) with the feature fusion module (FFM) to improve the neck structure of YOLOV7. The proposed method is validated on the PIDray dataset, and the tested results showed that our method obtained the highest mAP (68.7%), which is improved by 3.5% than YOLOV7 methods. Our approach provides a new design pattern for prohibited item detection in complex environments and shows the development potential of deep learning in related fields. © The Author(s) 2024.
KW  - Adaptive context information fusion
KW  - Attention mechanism
KW  - MFPIDet
KW  - Multi-scale feature extraction
KW  - Prohibited item detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, X.
AU  - Hou, W.
AU  - Li, X.
TI  - Detection method of small size defects on pipeline weld surface based on improved YOLOv7
PY  - 2024
T2  - PLoS ONE
DO  - 10.1371/journal.pone.0313348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212048266&doi=10.1371%2fjournal.pone.0313348&partnerID=40&md5=ac55c2ea6d0a7e65790b95402f5d2556
AB  - The background of pipeline weld surface defect image is complex, and the defect size is small. Aiming at the small defect size in the weld image, which is easy to cause missed detection and false detection, a lightweight target detection algorithm based on improved YOLOv7 is proposed. Firstly, in the feature fusion network of YOLOv7, the detection ability of the algorithm to detect small and medium-sized targets in defect images is enhanced by adding a 160*160 small target detection head. Then, the convolution module in the backbone network and the feature fusion network is replaced by the depthwise separable convolution with less computational overhead, so as to effectively reduce the network calculation, parameter quantity and model volume. Finally, the loss function CIoU of YOLOv7 is optimized to EIoU loss function to accelerate the convergence speed of the model. The experimental results show that the defect detection mAP@0.5 based on the improved YOLOv7 algorithm can reach 72.2%, which is 11% higher than that of YOLOv7, and the model calculation amount and parameter amount are reduced by 75.6% and 60.3%, respectively. It can completely detect the small size defects and has a high degree of confidence, which can be effectively applied to the detection of small size defects on the surface of pipeline weld. Copyright: © 2024 Xu et al.
KW  - Algorithms
KW  - Image Processing, Computer-Assisted
KW  - Surface Properties
KW  - adult
KW  - algorithm
KW  - article
KW  - controlled study
KW  - detection algorithm
KW  - diagnosis
KW  - human
KW  - pipeline
KW  - velocity
KW  - algorithm
KW  - image processing
KW  - procedures
KW  - surface property
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Qian, R.
AU  - Ding, Y.
TI  - An Efficient UAV Image Object Detection Algorithm Based on Global Attention and Multi-Scale Feature Fusion
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13203989
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207677243&doi=10.3390%2felectronics13203989&partnerID=40&md5=9007bf74c99499f0e552980c23b8cffb
AB  - Object detection technology holds significant promise in unmanned aerial vehicle (UAV) applications. However, traditional methods face challenges in detecting denser, smaller, and more complex targets within UAV aerial images. To address issues such as target occlusion and dense small objects, this paper proposes a multi-scale object detection algorithm based on YOLOv5s. A novel feature extraction module, DCNCSPELAN4, which combines CSPNet and ELAN, is introduced to enhance the receptive field of feature extraction while maintaining network efficiency. Additionally, a lightweight Vision Transformer module, the CloFormer Block, is integrated to provide the network with a global receptive field. Moreover, the algorithm incorporates a three-scale feature fusion (TFE) module and a scale sequence feature fusion (SSFF) module in the neck network to effectively leverage multi-scale spatial information across different feature maps. To address dense small objects, an additional small object detection head was added to the detection layer. The original large object detection head was removed to reduce computational load. The proposed algorithm has been evaluated through ablation experiments and compared with other state-of-the-art methods on the VisDrone2019 and AU-AIR datasets. The results demonstrate that our algorithm outperforms other baseline methods in terms of both accuracy and speed. Compared to the YOLOv5s baseline model, the enhanced algorithm achieves improvements of 12.4% and 8.4% in AP50 and AP metrics, respectively, with only a marginal parameter increase of 0.3 M. These experiments validate the effectiveness of our algorithm for object detection in drone imagery. © 2024 by the authors.
KW  - feature fusion
KW  - global attention
KW  - object detection
KW  - UAV
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Wang, G.
AU  - Li, H.
AU  - Han, S.
AU  - Zhang, J.
TI  - Intelligent Construction Activity Identification for All-Weather Site Monitoring Using 4D Millimeter-Wave Technology
PY  - 2024
T2  - Journal of Construction Engineering and Management
DO  - 10.1061/JCEMD4.COENG-14875
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202340123&doi=10.1061%2fJCEMD4.COENG-14875&partnerID=40&md5=72a35f58eef889bfec1e5a6a0e499c04
AB  - Site monitoring is indispensable for modern construction management. Contact approaches, represented by wearable devices, have problems such as privacy leaks and hindering working. Vision-based noncontact methods depend highly on light and environmental conditions, and have poor three-dimensional perception ability. To propose an all-weather noncontact activity identification approach on construction sites, four-dimensional (4D) millimeter-wave (MMW) radar is adopted in this study for the first time because of its excellent abilities of motion sensing, spatial sensing, and penetration. First, a feature processing method is proposed to convert the MMW signal to a seven-dimensional point cloud, which consists of the shape information (x, y, and z) and four attributes (Doppler′, SNR′, H, and V), representing the information of velocity, signal-to-noise ratio, height, and volume, respectively. Second, a novel deep learning framework is developed, which contains (1) one shape subnetwork, driven by the PointNet++ model, to capture the shape information of objects; (2) four attribute subnetworks to fully utilize the additional attribute features; and (3) a two-layer fusion module to combine all the outputs of the subnetworks. With precision of 0.963, recall of 0.961, and an F1 score of 0.962, the results show that the proposed method can accurately identify construction activities under different environmental conditions. It also can facilitate further development of MMW radar-based solutions for construction site analysis. © 2024 American Society of Civil Engineers.
KW  - Activity identification
KW  - All-weather
KW  - Construction monitoring
KW  - Four-dimensional (4D) millimeter-wave radar
KW  - Gluing
KW  - Image coding
KW  - Image segmentation
KW  - Intelligent buildings
KW  - Motion sensors
KW  - Structural health monitoring
KW  - Time and motion study
KW  - Activity identification
KW  - All-weather
KW  - Construction activities
KW  - Construction monitoring
KW  - Environmental conditions
KW  - Four-dimensional  millimeter-wave radar
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Site monitoring
KW  - Subnetworks
KW  - Project management
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Katariya, V.
AU  - Jannat, F.-E.
AU  - Pazho, A.D.
AU  - Noghre, G.A.
AU  - Tabkhi, H.
TI  - VegaEdge: Edge AI confluence for real-time IoT-applications in highway safety
PY  - 2024
T2  - Internet of Things (Netherlands)
DO  - 10.1016/j.iot.2024.101268
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197759726&doi=10.1016%2fj.iot.2024.101268&partnerID=40&md5=4cb3dfa8451e59a0d41c7c519cb046b4
AB  - Traditional highway safety and monitoring solutions, reliant on surveillance cameras, face limitations due to their dependence on high-speed internet connectivity and the remote processing of Artificial Intelligence (AI) algorithms. This reliance introduces latency, undermining the real-time detection and analysis crucial for highway applications. The fusion of AI with the Internet of Things (IoT) opens new avenues for highway safety and surveillance innovation. Yet, most existing solutions are confined to vehicle detection and tracking, hindered by edge-IoT platforms’ limited power and processing capabilities. Addressing these limitations, this paper presents VegaEdge, an AI framework optimized for edge-IoT devices capable of real-time vehicle detection and tracking, trajectory forecasting, and identifying anomalous driving behaviors, such as road departures, sudden stops, and hazardous merges. A novel lightweight anomaly detection algorithm based on trajectory prediction is used for identifying hazardous driving on highways. VegaEdge demonstrates its versatility and efficiency across various traffic conditions and roadway configurations and has been evaluated on platforms like the Nvidia Jetson Orin and Xavier NX. The Nvidia Jetson Orin processes up to 738 trajectories per second and detects up to 140 vehicles in a single frame. Additionally, the Carolinas Anomaly Dataset (CAD) an extension of the Carolinas Highway Dataset (CHD) is introduced. While CHD consists of standard highway vehicle videos and trajectories, CAD includes video data of anomalous driving behaviors, providing a crucial resource for enhancing anomaly detection algorithms. CAD is available at https://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set. © 2024 Elsevier B.V.
KW  - Dataset
KW  - Deep learning
KW  - Edge
KW  - Embedded
KW  - Highway safety
KW  - IoT
KW  - Pipeline
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Liu, Z.
AU  - Liu, J.
AU  - Shi, Y.
AU  - Ren, W.
AU  - Yan, X.
AU  - Fan, J.
AU  - Li, F.
TI  - GIFF-AlgaeDet: An effective and lightweight deep learning method based on Global Information and Feature Fusion for microalgae detection
PY  - 2024
T2  - Algal Research
DO  - 10.1016/j.algal.2024.103815
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210413904&doi=10.1016%2fj.algal.2024.103815&partnerID=40&md5=8da15a9797a809d572075ddf7666aaf5
AB  - The identification and detection of microalgae are essential prerequisites for the development and utilization of microalgal resources. Traditional methods for the identification and detection of microalgae face the challenges of poor accuracy and time-consuming labor. Here is a method for microalgae identification and detection proposed in this paper, which utilizes Global Information and Feature Fusion (GIFF). Initially, to address the issue of low accuracy, the Coordinate Attention Group Shuffle Convolution (CAGS) is incorporated into the method to enhance the feature extraction capability. Furthermore, to address the issue of time-consuming labor, two small object detection heads for microalgae detection have been designed to effectively improve the training and detection speed. Additionally, the SCYLLA-IoU (SIoU) algorithm is employed to address the issue of unstable model convergence. To assess the efficacy of the method employed in this study, a dataset was intentionally created for the purpose of detecting microalgae. The experimental results indicate that, under the same experimental conditions, the proposed method has achieved significant improvements in terms of average precision, mAP@50, and mAP@95. Compared to the original method, it has increased by 3.1 %, 2 %, and 9.8 %, respectively. Moreover, this algorithm obtains a great improvement in detection speed and lightness, with a 29 % reduction in parameters and a single image detection time of 0.0219 s, which is significantly less than baseline. Location of the dataset and code: https://github.com/DjtuResearch/Microalgae_detection. © 2024
KW  - Deep learning
KW  - Microalgal detection
KW  - Microalgal resources
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kozłowski, M.
AU  - Racewicz, S.
AU  - Wierzbicki, S.
TI  - Image Analysis in Autonomous Vehicles: A Review of the Latest AI Solutions and Their Comparison
PY  - 2024
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app14188150
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205265639&doi=10.3390%2fapp14188150&partnerID=40&md5=3d0850dcf24d3abc98742a4bba055dcd
AB  - The integration of advanced image analysis using artificial intelligence (AI) is pivotal for the evolution of autonomous vehicles (AVs). This article provides a thorough review of the most significant datasets and latest state-of-the-art AI solutions employed in image analysis for AVs. Datasets such as Cityscapes, NuScenes, CARLA, and Talk2Car form the benchmarks for training and evaluating different AI models, with unique characteristics catering to various aspects of autonomous driving. Key AI methodologies, including Convolutional Neural Networks (CNNs), Transformer models, Generative Adversarial Networks (GANs), and Vision Language Models (VLMs), are discussed. The article also presents a comparative analysis of various AI techniques in real-world scenarios, focusing on semantic image segmentation, 3D object detection, vehicle control in virtual environments, and vehicle interaction using natural language. Simultaneously, the roles of multisensor datasets and simulation platforms like AirSim, TORCS, and SUMMIT in enriching the training data and testing environments for AVs are highlighted. By synthesizing information on datasets, AI solutions, and comparative performance evaluations, this article serves as a crucial resource for researchers, developers, and industry stakeholders, offering a clear view of the current landscape and future directions in autonomous vehicle image analysis technologies. © 2024 by the authors.
KW  - AI solutions
KW  - autonomous vehicles
KW  - image analysis
KW  - safety features
KW  - Magnetic levitation vehicles
KW  - Semantic Segmentation
KW  - Visual languages
KW  - Artificial intelligence solution
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Convolutional neural network
KW  - Image analyze
KW  - Image-analysis
KW  - Intelligence models
KW  - Safety features
KW  - State of the art
KW  - Transformer modeling
KW  - Convolutional neural networks
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, L.
AU  - Jin, Z.
AU  - Yu, X.
AU  - Wang, A.
TI  - Road Vehicle and Pedestrian Detection Based on YOLOv9 for Haar Wavelet Downsampling
ST  - Haar 小波下采样优化 YOLOv9 的道路车辆和行人检测
PY  - 2024
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2406-0204
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007353484&doi=10.3778%2fj.issn.1002-8331.2406-0204&partnerID=40&md5=5cc16a356d9b53cf5fecb78ceab42105
AB  - In the current background of intelligence and informatization, the YOLOv9 algorithm based on Haar wavelet downsampling (HWD) is proposed for vehicle and pedestrian target detection in complex environments with autonomous driving mode to intelligently collect pedestrian and vehicle targets on the road. The operation of Haar wavelet downsampling reduces the spatial resolution of feature maps and preserves detailed information such as edges and textures as much as possible, effectively reducing the uncertainty of information. By utilizing the sum of cross entropy loss and generalized dice loss as the loss function of the network, the difference between probability distributions can be effectively measured, and dice loss calculations can be performed pixel by pixel, making it easier to optimize the network. The experimental results show that the average accuracy of the proposed model reaches 95.86%, and the detection frame rate reaches 179 FPS on the KITTY dataset. Compared with YOLOv9, the improved algorithm can accurately identify vehicles and pedestrians of different scales on complex roads, which not only improves the redundancy of computational capacity and missed detection of small targets in the original detection algorithm, but also provides visual technology support for intelligent autonomous driving. © 2024 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
KW  - deep learning
KW  - Haar wavelet downsampling (HWD)
KW  - small object detection
KW  - vehicles and pedestrians
KW  - YOLOv9
KW  - Image coding
KW  - Image segmentation
KW  - Autonomous driving
KW  - Deep learning
KW  - Down sampling
KW  - Haar wavelet downsampling
KW  - Haar-wavelets
KW  - Road vehicles
KW  - Small object detection
KW  - Vehicle and pedestrian
KW  - Vehicles detection
KW  - YOLOv9
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Y.
AU  - Deng, J.
AU  - Liu, P.
AU  - Li, W.
AU  - Zhao, S.
TI  - Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network
PY  - 2025
T2  - IEEE Transactions on Automation Science and Engineering
DO  - 10.1109/TASE.2024.3370147
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187001436&doi=10.1109%2fTASE.2024.3370147&partnerID=40&md5=675e9b11286e7a2bd5fa0729b8b5e8e4
AB  - Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively. © 2004-2012 IEEE.
KW  - domain adaptation
KW  - MAV dataset
KW  - MAV detection
KW  - noise suppression
KW  - Benchmarking
KW  - Cameras
KW  - Job analysis
KW  - Micro air vehicle (MAV)
KW  - Noise abatement
KW  - Spurious signal noise
KW  - Statistical tests
KW  - Adaptation models
KW  - Benchmark testing
KW  - Domain adaptation
KW  - Features extraction
KW  - Micro air vehicle dataset
KW  - Micro air vehicle detection
KW  - Micro air-vehicles
KW  - Noise suppression
KW  - Task analysis
KW  - Vehicles detection
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chaudhry, R.
TI  - SD-YOLO-AWDNet: A hybrid approach for smart object detection in challenging weather for self-driving cars
PY  - 2024
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2024.124942
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200551348&doi=10.1016%2fj.eswa.2024.124942&partnerID=40&md5=e6ad1040eaa414e08c7dfc8962196371
AB  - Several deep learning algorithms are currently focused on object detection in adverse weather scenarios for autonomous driving systems. However, these algorithms face challenges in real-time scenarios, leading to a reduction in detection accuracy. To tackle these issues, this paper introduces a lightweight object detection model named Self Driving Cars You Only Look Once Adverse Weather Detection Network (SD-YOLO-AWDNet), derived from enhancements to the YOLOv5 algorithm. The model incorporates four progressive improvement levels within the YOLOv5 framework. This includes integrating C3Ghost and GhostConv modules in the backbone to enhance detection speed by reducing computational overhead during feature extraction. To address potential accuracy issues arising from these modules, Depthwise-Separable Dilated Convolutions (DSDC) are introduced, striking a balance between accuracy and parameter reduction. The model further incorporates a Coordinate Attention (CA) module in the GhostBottleneck to enhance feature extraction and eliminate unnecessary features, improving precision in object detection. Additionally, a novel “Focal Distribution Loss” replaces CIoU Loss, accelerating bounding box regression and loss reduction. Test dataset experiments demonstrate that SD-YOLO-AWDNet outperforms YOLOv5 with a 54% decrease in FLOPs, a 52.53% decrease in model parameters, a 2.24% increase in mAP, and a threefold improvement in detection speed. © 2024 Elsevier Ltd
KW  - Deep neural network
KW  - Object detection
KW  - Self driving cars
KW  - YOLO
KW  - Autonomous vehicles
KW  - Extraction
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Statistical tests
KW  - Adverse weather
KW  - Autonomous driving
KW  - Detection networks
KW  - Detection speed
KW  - Driving systems
KW  - Features extraction
KW  - Hybrid approach
KW  - Objects detection
KW  - Smart objects
KW  - YOLO
KW  - Deep neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pao, W.Y.
AU  - Carvalho, M.
AU  - Hosseinnouri, F.
AU  - Li, L.
AU  - Rouaix, C.
AU  - Agelin-Chaab, M.
AU  - Hangan, H.
AU  - Gultepe, I.
AU  - Komar, J.
TI  - Evaluating weather impact on vehicles: a systematic review of perceived precipitation dynamics and testing methodologies
PY  - 2024
T2  - Engineering Research Express
DO  - 10.1088/2631-8695/ad2033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183981658&doi=10.1088%2f2631-8695%2fad2033&partnerID=40&md5=ea67cb73e23daebc92ed2338d31fdf6a
AB  - The performance of road vehicles degrades when driving in adverse weather conditions. Weather testing for vehicles is important to understand the impacts of precipitation on vehicle performance, such as driver visibility, autonomous sensor signal, tire traction, and structural integrity due to corrosion, to ensure safety. This tutorial summarizes the essential elements for performing realistic testing by applying physical and meteorological rationale to vehicle applications. Three major topics are identified as crucial steps for precise quantitative studies, including understanding the natural precipitation characteristics, estimating the perceived precipitation experienced by a moving vehicle, and selecting data collection strategies. The methods used in current practices to investigate the effects of rain and snow on road vehicles at common facilities of outdoor test tracks, drive-through weather chambers, and climatic wind tunnels are summarized. The testing techniques and relevant instrumentations are also discussed, with considerations of factors that influence the measured data, such as particle size distribution, precipitation intensity, wind-induced droplet dynamic events, accumulation behaviour, etc. The goals of this paper are to provide a tutorial with guidelines on designing weather testing experiments for road vehicles and to promote the idea of establishing standardized methodologies for realistic vehicle testing that facilitates accurate prediction of vehicle performance in adverse weather conditions. © 2024 The Author(s). Published by IOP Publishing Ltd
KW  - adverse weather
KW  - perceived precipitation
KW  - rain
KW  - snow
KW  - vehicle testing
KW  - Corrosion
KW  - Digital storage
KW  - Particle size
KW  - Particle size analysis
KW  - Rain
KW  - Roads and streets
KW  - Safety testing
KW  - Vehicle performance
KW  - Adverse weather
KW  - Autonomous sensors
KW  - Condition
KW  - Perceived precipitation
KW  - Performance
KW  - Road vehicles
KW  - Systematic Review
KW  - Testing methodology
KW  - Vehicle testing
KW  - Weather impact
KW  - Snow
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bao, S.
AU  - Shi, W.
AU  - Yang, D.
AU  - Xiang, H.
AU  - Yu, Y.
TI  - Global principal planes aided LiDAR-based mobile mapping method in artificial environments
PY  - 2024
T2  - Advanced Engineering Informatics
DO  - 10.1016/j.aei.2024.102472
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189004189&doi=10.1016%2fj.aei.2024.102472&partnerID=40&md5=afba8f044381f6823f54e9fccbb590c3
AB  - 3-D mapping of buildings is crucial for urban renewal, but traditional LiDAR-based mapping methods are often less effective for buildings with narrow spaces and limited geometric features. Current methods attempt to overcome this by integrating additional sensors, such as cameras, which increases cost and complexity. This paper proposes a novel LiDAR-based mobile mapping framework using global principal planes (GPPs) to address this challenge without additional sensors. GPPs are defined as unlimited planes characterized by principal normal vectors (PNVs). GPPs can provide stronger constraints than traditional small planes extracted from one or certain LiDAR frames because they are little affected by the accumulative error from point cloud matching. A PNV estimation method is also proposed based on an inertial measurement unit and polar histogram, and PNVs are axes of the natural cartesian XYZ coordinate system. Point clouds are transformed into the PNVs coordinate system to extract robust edge and plane feature points and GPPs. The proposed framework is tested in various environments. It achieves about 3 cm accuracy in corridors and similar accuracy in stairwells. Compared to five state-of-the-art mapping methods (Cartographer, etc.), its accuracy improves by over 76%, increasing at least an order of magnitude. In the outdoor KITTI dataset, it shows a reduction in absolute pose errors by 4% to 20%. Extensive experiments demonstrate its accuracy, robustness, and generalizability. Ablation experiments further validate the efficacy of different components in the framework. © 2024
KW  - Feature extraction
KW  - Global principal plane
KW  - LiDAR
KW  - Mobile mapping
KW  - Principal normal vector
KW  - Optical radar
KW  - Artificial environments
KW  - Co-ordinate system
KW  - Features extraction
KW  - Global principal plane
KW  - LiDAR
KW  - Mapping method
KW  - Mobile mapping
KW  - Normal vector
KW  - Principal normal vector
KW  - Principal planes
KW  - Mapping
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Abu-raddaha, A.
AU  - El-Shair, Z.A.
AU  - Rawashdeh, S.
TI  - Leveraging Perspective Transformation for Enhanced Pothole Detection in Autonomous Vehicles
PY  - 2024
T2  - Journal of Imaging
DO  - 10.3390/jimaging10090227
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205060414&doi=10.3390%2fjimaging10090227&partnerID=40&md5=8483e5d6adae2cb32ccfdc3ba6baa384
AB  - Road conditions, often degraded by insufficient maintenance or adverse weather, significantly contribute to accidents, exacerbated by the limited human reaction time to sudden hazards like potholes. Early detection of distant potholes is crucial for timely corrective actions, such as reducing speed or avoiding obstacles, to mitigate vehicle damage and accidents. This paper introduces a novel approach that utilizes perspective transformation to enhance pothole detection at different distances, focusing particularly on distant potholes. Perspective transformation improves the visibility and clarity of potholes by virtually bringing them closer and enlarging their features, which is particularly beneficial given the fixed-size input requirement of object detection networks, typically significantly smaller than the raw image resolutions captured by cameras. Our method automatically identifies the region of interest (ROI)—the road area—and calculates the corner points to generate a perspective transformation matrix. This matrix is applied to all images and corresponding bounding box labels, enhancing the representation of potholes in the dataset. This approach significantly boosts detection performance when used with YOLOv5-small, achieving a 43% improvement in the average precision (AP) metric at intersection-over-union thresholds of 0.5 to 0.95 for single class evaluation, and notable improvements of 34%, 63%, and 194% for near, medium, and far potholes, respectively, after categorizing them based on their distance. To the best of our knowledge, this work is the first to employ perspective transformation specifically for enhancing the detection of distant potholes. © 2024 by the authors.
KW  - autonomous vehicles
KW  - computer vision
KW  - deep learning
KW  - mobile robotics
KW  - perspective transformation
KW  - pothole detection
KW  - Highway accidents
KW  - Mobile robots
KW  - Object detection
KW  - Adverse weather
KW  - Autonomous Vehicles
KW  - Avoiding obstacle
KW  - Corrective actions
KW  - Deep learning
KW  - Human reaction
KW  - Mobile robotic
KW  - Perspective transformation
KW  - Pothole detection
KW  - Road condition
KW  - Human reaction time
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Mei, L.
AU  - Yin, Z.
AU  - Li, H.
AU  - Liu, R.
AU  - Jiang, W.
AU  - Lu, C.X.
TI  - End-to-End Target Liveness Detection via mmWave Radar and Vision Fusion for Autonomous Vehicles
PY  - 2024
T2  - ACM Transactions on Sensor Networks
DO  - 10.1145/3628453
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199910727&doi=10.1145%2f3628453&partnerID=40&md5=20b618dcdfed4a43468d849bef855779
AB  - The successful operation of autonomous vehicles hinges on their ability to accurately identify objects in their vicinity, particularly living targets such as bikers and pedestrians. However, visual interference inherent in real-world environments, such as omnipresent billboards, poses substantial challenges to extant vision-based detection technologies. These visual interference exhibit similar visual attributes to living targets, leading to erroneous identification. We address this problem by harnessing the capabilities of mmWave radar, a vital sensor in autonomous vehicles, in combination with vision technology, thereby contributing a unique solution for liveness target detection. We propose a methodology that extracts features from the mmWave radar signal to achieve end-to-end liveness target detection by integrating the mmWave radar and vision technology. This proposed methodology is implemented and evaluated on the commodity mmWave radar IWR6843ISK-ODS and vision sensor Logitech camera. Our extensive evaluation reveals that the proposed method accomplishes liveness target detection with a mean average precision of 98.1%, surpassing the performance of existing studies. © 2024 Copyright held by the owner/author(s).
KW  - mmWave radar
KW  - Target liveness detection
KW  - Millimeter waves
KW  - Radar interference
KW  - Tracking radar
KW  - Autonomous Vehicles
KW  - End to end
KW  - Liveness
KW  - Liveness detection
KW  - Mm waves
KW  - Mmwave radar
KW  - Real world environments
KW  - Target liveness detection
KW  - Targets detection
KW  - Vision technology
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, T.
AU  - Zhu, S.
AU  - Gao, T.
AU  - Li, H.
AU  - Tu, H.
AU  - Li, Z.
TI  - Real-time Vehicle Detection Based on Adaptive Fusion
ST  - 基于自适应融合的实时车辆检测
PY  - 2024
T2  - Tongji Daxue Xuebao/Journal of Tongji University 
DO  - 10.11908/j.issn.0253-374x.23399
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191900879&doi=10.11908%2fj.issn.0253-374x.23399&partnerID=40&md5=0f6c0021a6ab69329c444a83564a3bf7
AB  - A traffic target detection algorithm，fusion attention adaptive pyramid network （FAAP-Net） ， is proposed to address the issues of slow speed and low accuracy in traditional vehicle detection techniques，significantly reducing the occurrence of traffic accidents. To mitigate computational complexity， a lightweight complementary pooling structure （CPS） is designed，employing two sets of different pooling combinations in width and height，which maintains a high precision while significantly reducing the floating point operations per second （GFLOPs） and the parameter count of the network. Addressing the information loss during intelligent traffic system feature map generation， the adaptive fusion feature pyramid network （AF-FPN）incorporates the adaptive attention module （AAM） and the feature enhancement module （FEM） to integrate shape features for vehicle detection. Lastly，to address the weak representation of vehicle detail features，a channel-wise grouped attention （SA） mechanism is introduced，enhancing the focus of the backbone network on various vehicle detection details and effectively extracting significant features. The experimental results on the BDD100K dataset demonstrate that the FAAP-Net algorithm achieves a notable improvement，increasing the average precision from 30.3 % to 43.7 %. © 2024 Science Press. All rights reserved.
KW  - adaptive fusion
KW  - complementary pooling
KW  - object detection
KW  - shuffle attention
KW  - vehicle detection
KW  - Accidents
KW  - Digital arithmetic
KW  - Feature extraction
KW  - Vehicles
KW  - Adaptive fusion
KW  - Adaptive pyramid
KW  - Algorithm fusion
KW  - Complementary pooling
KW  - Objects detection
KW  - Pyramid network
KW  - Real- time
KW  - Shuffle attention
KW  - Target detection algorithm
KW  - Vehicles detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Deng, P.
AU  - Zhou, L.
AU  - Chen, J.
TI  - PVC-SSD: Point-Voxel Dual-Channel Fusion With Cascade Point Estimation for Anchor-Free Single-Stage 3-D Object Detection
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3380898
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189535436&doi=10.1109%2fJSEN.2024.3380898&partnerID=40&md5=6958b810b8dae447467539c496cd0d81
AB  - Existing single-stage 3-D object detection algorithms, whether relying on point or voxel methodologies, face challenges in achieving high-performance detection across diverse object categories simultaneously. Moreover, current algorithms employing point-voxel approaches often fall short in fully leveraging the advantages offered by the two sparse point cloud feature extraction methods. Consequently, these methodologies inadequately capture both the local and global features of the object. In response to these challenges, we introduce a novel single-stage 3-D object detection algorithm called PVC-SSD. This algorithm adopts an anchor-free methodology and employs point-voxel dual-channel fusion (PVCF) encoding to effectively model both local and global features, thereby enhancing the overall performance of object detection. The proposed algorithm comprises three key components. First, the PVCF module is designed to seamlessly integrate both local and global features of the object. Second, the cascade candidate point estimation (CCPE) module focuses on improving the quality of candidate points. At last, the position encoding self-attention (PESA) module is dedicated to establishing pointwise correlations within sparse point clouds. And this module is instrumental in reinforcing foreground features and mitigating geometric differences within the same category induced by factors such as viewpoint and distance. Through extensive experiments conducted on the KITTI and Waymo large scale 3-D object detection datasets, we substantiate the robust competitiveness and efficiency of PVC-SSD in multicategory detection tasks. © 2001-2012 IEEE.
KW  - 3-D object detection
KW  - attention mechanism
KW  - autonomous vehicle
KW  - light detection and ranging (LiDAR)
KW  - point cloud
KW  - Autonomous vehicles
KW  - Encoding (symbols)
KW  - Extraction
KW  - Job analysis
KW  - Object detection
KW  - Object recognition
KW  - Optical radar
KW  - Signal detection
KW  - Signal encoding
KW  - Three dimensional displays
KW  - 3D object
KW  - 3d object detection
KW  - Attention mechanisms
KW  - Autonomous Vehicles
KW  - Features extraction
KW  - Light detection and ranging
KW  - Objects detection
KW  - Point cloud compression
KW  - Point-clouds
KW  - Task analysis
KW  - Three-dimensional display
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, C.
AU  - Ren, H.
AU  - Yang, H.
AU  - Huo, H.
AU  - Zhu, P.
AU  - Yao, Z.
AU  - Li, J.
AU  - Sun, M.
AU  - Yang, S.
TI  - M2FNet: Multi-modal fusion network for object detection from visible and thermal infrared images
PY  - 2024
T2  - International Journal of Applied Earth Observation and Geoinformation
DO  - 10.1016/j.jag.2024.103918
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193715729&doi=10.1016%2fj.jag.2024.103918&partnerID=40&md5=3e5da9fe239bc9bb2270151739c60497
AB  - Fusing multi-modal information from visible (VIS) and thermal infrared (TIR) images is crucial for object detection in fully adapting to varied lighting conditions. However, the existing models usually treat VIS and TIR images as independent information and extract corresponding features from separate networks due to the scarcity of training data with labeled instances from both VIS and TIR registration images. To fill this gap, a novel Multi-Modal Fusion NETwork (M2FNet) based on the Transformer architecture is proposed in this paper, which contains two effective modules: the Union-Modal Attention (UMA) and the Cross-Modal Attention (CMA). The UMA module aggregates multi-spectral features from VIS and TIR images and then extracts multi-modal features via a convolutional neural network (CNN) backbone. The CMA module is designed to learn cross-attention features from VIS and TIR pairwise features by Transformer architecture. Evaluation results by the mean average precision (mAP) metric show that the M2FNet method significantly advances the baseline methods trained using only VIS or TIR images by 10.71 % and 2.97 %, respectively. The increments in mAP are observed in the M2FNet method compared with the existing multi-modal methods on two public datasets. Sensitivity analysis of eight illumination thresholds shows that the M2FNet method presents robustness performance on varied illumination conditions and achieves the maximum increase in accuracy of 25.6 %. Moreover, this method is subsequently applied to a new testing dataset, VI2DA (Visible-Infrared paired Video and Image DAtaset), observed by diverse sensors and platforms for testing the generalization ability of object detectors, which will be publicly available at https://github.com/TIR-OD/Datasets. © 2024 The Author(s)
KW  - Low-light condition
KW  - Multi-modal fusion network
KW  - Object detection
KW  - Transformer architecture
KW  - Visible and thermal infrared images
KW  - accuracy assessment
KW  - artificial neural network
KW  - comparative study
KW  - data set
KW  - detection method
KW  - infrared imagery
KW  - light availability
KW  - precision
KW  - remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, S.
AU  - Wang, X.
AU  - Sun, Q.
AU  - Dong, K.
TI  - MWIRGas-YOLO: Gas Leakage Detection Based on Mid-Wave Infrared Imaging
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24134345
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198393297&doi=10.3390%2fs24134345&partnerID=40&md5=3a104ade3bc1dc5401f65fa455f479ca
AB  - The integration of visual algorithms with infrared imaging technology has become an effective tool for industrial gas leak detection. However, existing research has mostly focused on simple scenarios where a gas plume is clearly visible, with limited studies on detecting gas in complex scenes where target contours are blurred and contrast is low. This paper uses a cooled mid-wave infrared (MWIR) system to provide high sensitivity and fast response imaging and proposes the MWIRGas-YOLO network for detecting gas leaks in mid-wave infrared imaging. This network effectively detects low-contrast gas leakage and segments the gas plume within the scene. In MWIRGas-YOLO, it utilizes the global attention mechanism (GAM) to fully focus on gas plume targets during feature fusion, adds a small target detection layer to enhance information on small-sized targets, and employs transfer learning of similar features from visible light smoke to provide the model with prior knowledge of infrared gas features. Using a cooled mid-wave infrared imager to collect gas leak images, the experimental results show that the proposed algorithm significantly improves the performance over the original model. The segment mean average precision reached 96.1% (mAP50) and 47.6% (mAP50:95), respectively, outperforming the other mainstream algorithms. This can provide an effective reference for research on infrared imaging for gas leak detection. © 2024 by the authors.
KW  - gas leak detection
KW  - global attention mechanism
KW  - mid-wave infrared imaging
KW  - small target detection layer
KW  - Gases
KW  - Infrared radiation
KW  - Leak detection
KW  - Smoke
KW  - Attention mechanisms
KW  - Gas leak detection
KW  - Gas leakages
KW  - Gas leaks
KW  - Gas plumes
KW  - Global attention mechanism
KW  - Leakage detection
KW  - Mid-wave infrared imaging
KW  - Small target detection
KW  - Small target detection layer
KW  - adult
KW  - algorithm
KW  - article
KW  - diagnosis
KW  - gas
KW  - human
KW  - infrared radiation
KW  - male
KW  - plume
KW  - smoke
KW  - thermography
KW  - transfer of learning
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hsieh, C.-C.
AU  - Jia, H.-W.
AU  - Huang, W.-H.
AU  - Hsih, M.-H.
TI  - Deep Learning-Based Road Pavement Inspection by Integrating Visual Information and IMU
PY  - 2024
T2  - Information (Switzerland)
DO  - 10.3390/info15040239
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191662389&doi=10.3390%2finfo15040239&partnerID=40&md5=46762bfa34f8fc09e0a60acf8895a7c8
AB  - This study proposes a deep learning method for pavement defect detection, focusing on identifying potholes and cracks. A dataset comprising 10,828 images is collected, with 8662 allocated for training, 1083 for validation, and 1083 for testing. Vehicle attitude data are categorized based on three-axis acceleration and attitude change, with 6656 (64%) for training, 1664 (16%) for validation, and 2080 (20%) for testing. The Nvidia Jetson Nano serves as the vehicle-embedded system, transmitting IMU-acquired vehicle data and GoPro-captured images over a 5G network to the server. The server recognizes two damage categories, low-risk and high-risk, storing results in MongoDB. Severe damage triggers immediate alerts to maintenance personnel, while less severe issues are recorded for scheduled maintenance. The method selects YOLOv7 among various object detection models for pavement defect detection, achieving a mAP of 93.3%, a recall rate of 87.8%, a precision of 93.2%, and a processing speed of 30–40 FPS. Bi-LSTM is then chosen for vehicle vibration data processing, yielding 77% mAP, 94.9% recall rate, and 89.8% precision. Integration of the visual and vibration results, along with vehicle speed and travel distance, results in a final recall rate of 90.2% and precision of 83.7% after field testing. © 2024 by the authors.
KW  - deep learning
KW  - image recognition
KW  - intelligent inspection
KW  - pavement inspection
KW  - 5G mobile communication systems
KW  - Data handling
KW  - Deep learning
KW  - Defects
KW  - Inspection
KW  - Integration testing
KW  - Learning systems
KW  - Object detection
KW  - Pavements
KW  - Statistical tests
KW  - Vehicles
KW  - Vibrations (mechanical)
KW  - Well testing
KW  - Acceleration change
KW  - Deep learning
KW  - Defect detection
KW  - Intelligent inspection
KW  - Learning methods
KW  - Pavement inspections
KW  - Recall rate
KW  - Road pavements
KW  - Three axes
KW  - Visual information
KW  - Image recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Niu, S.
AU  - Nie, Z.
AU  - Li, G.
AU  - Zhu, W.
TI  - Early Drought Detection in Maize Using UAV Images and YOLOv8+
PY  - 2024
T2  - Drones
DO  - 10.3390/drones8050170
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194050631&doi=10.3390%2fdrones8050170&partnerID=40&md5=64752e390bc22719ec0137b7a628999f
AB  - The escalating global climate change significantly impacts the yield and quality of maize, a vital staple crop worldwide, especially during seedling stage droughts. Traditional detection methods are limited by their single-scenario approach, requiring substantial human labor and time, and lack accuracy in the real-time monitoring and precise assessment of drought severity. In this study, a novel early drought detection method for maize based on unmanned aerial vehicle (UAV) images and Yolov8+ is proposed. In the Backbone section, the C2F-Conv module is adopted to reduce model parameters and deployment costs, while incorporating the CA attention mechanism module to effectively capture tiny feature information in the images. The Neck section utilizes the BiFPN fusion architecture and spatial attention mechanism to enhance the model’s ability to recognize small and occluded targets. The Head section introduces an additional 10 × 10 output, integrates loss functions, and enhances accuracy by 1.46%, reduces training time by 30.2%, and improves robustness. The experimental results demonstrate that the improved Yolov8+ model achieves precision and recall rates of approximately 90.6% and 88.7%, respectively. The mAP@50 and mAP@50:95 reach 89.16% and 71.14%, respectively, representing respective increases of 3.9% and 3.3% compared to the original Yolov8. The UAV image detection speed of the model is up to 24.63 ms, with a model size of 13.76 MB, optimized by 31.6% and 28.8% compared to the original model, respectively. In comparison with the Yolov8, Yolov7, and Yolo5s models, the proposed method exhibits varying degrees of superiority in mAP@50, mAP@50:95, and other metrics, utilizing drone imagery and deep learning techniques to truly propel agricultural modernization. © 2024 by the authors.
KW  - maize drought
KW  - object detection
KW  - small targets
KW  - UAV
KW  - YOLOv8
KW  - Aircraft detection
KW  - Deep learning
KW  - Image enhancement
KW  - Aerial vehicle
KW  - Attention mechanisms
KW  - Detection methods
KW  - Drought detection
KW  - Maize drought
KW  - Objects detection
KW  - Small targets
KW  - Unmanned aerial vehicle
KW  - Vehicle images
KW  - YOLOv8
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Molina-Padrón, N.
AU  - Cabrera-Almeida, F.
AU  - Araña-Pulido, V.
AU  - Tovar, B.
TI  - Towards a Global Surveillance System for Lost Containers at Sea
PY  - 2024
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse12020299
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185964094&doi=10.3390%2fjmse12020299&partnerID=40&md5=aa14c836b75eebdf17e7aedd41eaeec8
AB  - Every year, more than 1500 containers are lost around the world. These accidents are increasingly more common due to the boom of the shipping industry, presenting serious consequences for marine ecosystems and maritime navigation. This problem has alerted various international organisms to regulate these catastrophes, incorporating new regulations that will force cargo ships to report the loss of containers during its voyages. However, the lack of technological means that support compliance with this regulation may lead to these accidents continuing to affect the maritime sector. This article analyzes different electronic technologies for the prevention of collisions with floating containers, as well as their monitoring at a global level. The analysis carried out provides a glimpse of the possibility of developing a global monitoring system for containers lost at sea. This analysis compares both the opportunities and limitations of each of the proposed technologies, demonstrating how the current state-of-the-art technology has sufficient means to address this problem. © 2024 by the authors.
KW  - container
KW  - detection
KW  - identification
KW  - location
KW  - lost
KW  - monitoring
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Swathi, P.
AU  - Tejaswi, D.S.
AU  - Khan, M.A.
AU  - Saishree, M.
AU  - Rachapudi, V.B.
AU  - Anguraj, D.K.
TI  - Real-Time Vehicle Detection for Traffic Monitoring: A Deep Learning Approach
ST  - Detección de vehículos en tiempo real para la monitorización del tráfico: Un enfoque de aprendizaje profundo
PY  - 2024
T2  - Data and Metadata
DO  - 10.56294/dm2024295
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195962652&doi=10.56294%2fdm2024295&partnerID=40&md5=3b1066a44b2b25cb9f5d8d8b2ca53603
AB  - Vehicle detection is an essential technology for intelligent transportation systems and autonomous vehicles. Reliable real-time detection allows for traffic monitoring, safety enhancements and navigation aids. However, vehicle detection is a challenging computer vision task, especially in complex urban settings. Traditional methods using hand-crafted features like HAAR cascades have limitations. Recent deep learning advances have enabled convolutional neural networks (CNNs) like Faster R-CNN, SSD and YOLO to be applied to vehicle detection with significantly improved accuracy. But each technique has tradeoffs between precision and processing speed. Two-stage detectors like Faster R-CNN are highly accurate but slow at 7 FPS. Single-shot detectors like SSD are faster at 22 FPS but less precise. YOLO is extremely fast at 45 FPS but has lower accuracy. This paper reviews prominent deep learning vehicle detectors. It proposes a new integrated method combining YOLOv3 detection, optical flow tracking and trajectory analysis to enhance both accuracy and speed. Results on highway and urban datasets show improved precision, recall and F1 scores compared to YOLOv3 alone. Optical flow helps filter noise and recover missed detections. Trajectory analysis enables consistent object IDs across frames. Compared to other CNN models, the proposed technique achieves a better balance of real-time performance and accuracy. Occlusion handling and small object detection remain open challenges. In summary, deep learning has enabled major progress but enhancements in model architecture, training data and occlusion handling are needed to realize the full potential for traffic management applications. The integrated method proposed offers improved performance over baseline detectors. We have achieved 99 % accuracy in our project. © 2024; Los autores.
KW  - Convolution Neural Network (CNN)
KW  - Deep Learning
KW  - Image Classification
KW  - Machine Learning
KW  - Multi Detecting Object Tracking
KW  - Traffic Detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Brescia, W.
AU  - Gomes, P.
AU  - Toni, L.
AU  - Mascolo, S.
AU  - De Cicco, L.
TI  - MilliNoise: a Millimeter-wave Radar Sparse Point Cloud Dataset in Indoor Scenarios
PY  - 2024
T2  - MMSys 2024 - Proceedings of the 2024 ACM Multimedia Systems Conference
DO  - 10.1145/3625468.3652189
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191976218&doi=10.1145%2f3625468.3652189&partnerID=40&md5=251bc5d899bdcedbdc1cfd3000be842f
AB  - Millimeter-wave (mmWave) radar sensors produce Point Clouds (PCs) that are much sparser and noisier than other PC data (e.g., Li-DAR), yet they are more robust in challenging conditions such as in the presence of fog, dust, smoke, or rain. This paper presents MilliNoise, a point cloud dataset captured in indoor scenarios through a mmWave radar sensor installed on a wheeled mobile robot. Each of the 12M points in the MilliNoise dataset is accurately labeled as true/noise point by leveraging known information of the scenes and a motion capture system to obtain the ground truth position of the moving robot. Each frame is carefully pre-processed to produce a fixed number of points for each cloud, enabling classification tools which require data with a fixed shape. Moreover, MilliNoise has been post-processed by labeling each point with the distance to its closest obstacle in the scene, which allows casting the denoising task into the regression framework. Along with the dataset, we provide researchers with the tools to visualize the data and prepare it for statistical and machine learning analysis. MilliNoise is available at: https://github.com/c3lab/MilliNoise  © 2024 Owner/Author.
KW  - Microcomputers
KW  - Millimeter waves
KW  - Radar equipment
KW  - Smartphones
KW  - Smoke
KW  - Condition
KW  - Ground truth
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Motion capture system
KW  - Moving robots
KW  - Point cloud data
KW  - Point-clouds
KW  - Radar sensors
KW  - Sparse point cloud
KW  - Mobile robots
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lan, J.
AU  - Zheng, M.
AU  - Chu, X.
AU  - Liu, C.
AU  - Ding, S.
TI  - A ship high-precision positioning method in the lock chamber based on LiDAR
PY  - 2024
T2  - Ocean Engineering
DO  - 10.1016/j.oceaneng.2024.118033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191944686&doi=10.1016%2fj.oceaneng.2024.118033&partnerID=40&md5=78af39af143aac359132b5cdb3b56623
AB  - Accurate perception of ship position can assist ships in safe navigation in the lock. However, due to the limited space in the lock chamber, the satellite positioning signal is unstable, making it difficult to accurately position the ship in real time. Therefore, a novel ship positioning method based on LiDAR is proposed. Firstly, the preprocessing of point cloud is realized through the steps including point cloud reflection intensity filtering, coordinate transformation, and ship point cloud clustering extraction. Then, different segmentation strategies are proposed to extract the point cloud of each ship with respect to the characteristics of point cloud changes during the ship's navigation upstream and downstream through the lock. Finally, real-time tracking of ships is realized by Kalman filter. The experiments of four ships navigating upstream and downstream through the lock were conducted respectively. The results show that the root mean square error (RMSE) of ship position is about 1 m, which is better than the BeiDou positioning accuracy. The overall processing time of each frame is controlled within 0.1 s, which meets the practical requirements. This present study provides a new solution to the ship positioning problem in locks. © 2024
KW  - LiDAR
KW  - Point cloud segmentation
KW  - Ship lock
KW  - Ship positioning
KW  - Kalman filters
KW  - Mean square error
KW  - Optical radar
KW  - Ships
KW  - Down-stream
KW  - High precision positioning
KW  - LiDAR
KW  - Limited space
KW  - Point cloud segmentation
KW  - Point-clouds
KW  - Positioning methods
KW  - Safe navigations
KW  - Ship lock
KW  - Ship positioning
KW  - accuracy assessment
KW  - BDS
KW  - lidar
KW  - positioning system
KW  - precision
KW  - real time
KW  - segmentation
KW  - ship technology
KW  - Locks (fasteners)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hosseinian, S.M.
AU  - Mirzahossein, H.
TI  - Efficiency and Safety of Traffic Networks Under the Effect of Autonomous Vehicles
PY  - 2024
T2  - Iranian Journal of Science and Technology - Transactions of Civil Engineering
DO  - 10.1007/s40996-023-01291-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179343296&doi=10.1007%2fs40996-023-01291-8&partnerID=40&md5=1d58b9d7038d57911c5d31954087cf8a
AB  - Efficiency and safety are two crucial aspects that have garnered significant attention in the context of traffic networks. The investigation of autonomous vehicles (AVs) in terms of efficiency and safety in a traffic network is a relatively new topic, and there have not been many practical developments reported so far. However, despite the availability of information, it is scattered across fragments within multiple resources dealing with efficiency and safety under the influence of AVs. Therefore, this study focused on the effect of efficiency and safety under the influence of AVs in a traffic network and provides an overview of the main research outcomes in this field. The review then delves into the challenges and opportunities associated with integrating AVs into existing traffic systems. The methodologies and approaches used in analyzing the effects of AVs are also explored. This review identifies critical gaps in knowledge and suggests future research directions to further enhance the understanding of how AVs can contribute to the efficiency and safety of traffic networks. Finally, this study offers valuable insights and guidance for policymakers, researchers, and practitioners involved in the planning, design, and management of transportation systems as they navigate the integration of AVs into traffic networks. © The Author(s), under exclusive licence to Shiraz University 2023.
KW  - Autonomous vehicles
KW  - Efficiency
KW  - Safety
KW  - Traffic network
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Z.
AU  - Yuan, T.
AU  - Ma, L.
AU  - Zhou, Y.
AU  - Peng, Y.
TI  - Target Detection for USVs by Radar-Vision Fusion with Swag-Robust Distance-Aware Probabilistic Multimodal Data Association
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3394703
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192181478&doi=10.1109%2fJSEN.2024.3394703&partnerID=40&md5=313a6c0a3eb3b1bb07d9f690b0c6d035
AB  - Unmanned surface vehicles (USVs) have been widely used for a wide range of tasks in the past decades. Accurate perception of the surrounding environment on the water surface under complex conditions is crucial for USVs to conduct effective operations. This article proposes a radar-vision fusion framework for USVs to accurately detect typical targets on the water surface. The modality difference between images and radar measurements, along with their perpendicular coordinates, presents challenges in the fusion process. The swaying of USVs on water and the extensive areas of perception enhance the difficulties of multisensor data association. To address these problems, we propose two modules to enhance multisensor fusion performance: a movement-compensated projection module and a distance-aware probabilistic data association module. The former effectively reduces projection bias during the alignment process of radar and camera signals by compensating for sensor movement using measured roll and pitch angles from the inertial measurement unit (IMU). The latter module models target regions guided by each radar measurement as a bivariate Gaussian distribution, with its covariance matrix adaptively derived based on the distance between targets and the camera. Consequently, the association of radar points and images is robust to projection errors and works well for multiscale objects. Features of radar points and images are subsequently extracted with two parallel backbones and fused at different levels to provide sufficient semantic information for robust object detection. The proposed framework achieves an average precision (AP) of 0.501 on the challenging real-world dataset established by us, outperforming state-of-the-art vision-only and radar-vision fusion methods.  © 2001-2012 IEEE.
KW  - Multimodal data association
KW  - object detection
KW  - radar-vision fusion
KW  - unmanned surface vehicles (USVs)
KW  - Cameras
KW  - Covariance matrix
KW  - Modal analysis
KW  - Object recognition
KW  - Radar equipment
KW  - Radar measurement
KW  - Semantics
KW  - Sensor data fusion
KW  - Tracking radar
KW  - Data association
KW  - Multi-modal data
KW  - Multi-modal data association
KW  - Objects detection
KW  - Probabilistics
KW  - Radar-vision fusion
KW  - Robust distance
KW  - Surrounding environment
KW  - Targets detection
KW  - Water surface
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, M.
AU  - Zhang, J.
AU  - Li, W.
AU  - Yin, T.
AU  - Chen, W.
AU  - Du, L.
AU  - Yan, X.
AU  - Liu, H.
TI  - Improved Taillight Detection Model for Intelligent Vehicle Lane-Change Decision-Making Based on YOLOv8
PY  - 2024
T2  - World Electric Vehicle Journal
DO  - 10.3390/wevj15080369
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202352249&doi=10.3390%2fwevj15080369&partnerID=40&md5=91281501c7a3a53fcb80686a00cfaa37
AB  - With the rapid advancement of autonomous driving technology, the recognition of vehicle lane-changing can provide effective environmental parameters for vehicle motion planning, decision-making and control, and has become a key task for intelligent vehicles. In this paper, an improved method for vehicle taillight detection and intent recognition based on YOLOv8 (You Only Look Once version 8) is proposed. Firstly, the CARAFE (Context-Aware Reassembly Operator) module is introduced to address fine perception issues of small targets, enhancing taillight detection accuracy. Secondly, the TriAtt (Triplet Attention Mechanism) module is employed to improve the model’s focus on key features, particularly in the identification of positive samples, thereby increasing model robustness. Finally, by optimizing the EfficientP2Head (a small object auxiliary head based on depth-wise separable convolutions) module, the detection capability for small targets is further strengthened while maintaining the model’s practicality and lightweight characteristics. Upon evaluation, the enhanced algorithm demonstrates impressive results, achieving a precision rate of 93.27%, a recall rate of 79.86%, and a mean average precision (mAP) of 85.48%, which shows that the proposed method could effectively achieve taillight detection. © 2024 by the authors.
KW  - deep learning
KW  - intelligent vehicle
KW  - lane-changing recognition
KW  - taillight detection
KW  - Deep learning
KW  - Autonomous driving
KW  - Decisions makings
KW  - Deep learning
KW  - Detection models
KW  - Environmental parameter
KW  - Lane change
KW  - Lane changing
KW  - Lane-changing recognition
KW  - Small targets
KW  - Taillight detection
KW  - Motion planning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Usman, M.
AU  - Zaka-Ud-Din, M.
AU  - Ling, Q.
TI  - Enhanced encoder–decoder architecture for visual perception multitasking of autonomous driving
PY  - 2024
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2024.123249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183467552&doi=10.1016%2fj.eswa.2024.123249&partnerID=40&md5=2e3cace897dfd2a47fb033f80cf4bfc7
AB  - Visual perception plays a vital role in autonomous driving systems, demanding high accuracy and real-time inference speed to ensure safety. In this paper, we propose a multi-task framework that simultaneously performs object detection, drivable area segmentation, and lane line identification, addressing the requirements of accurate and efficient visual perception. Our approach utilizes a shared-encoder architecture with three separate decoders, targeting each specific task. We investigate three configurations for the shared encoder: a Convolutional Neural Network (CNN), a Polyp Vision Transformer (PVT), and a hybrid CNN+PVT model. Through extensive experimentation and comparative analysis on the challenging BD100K dataset, we evaluate the performance of these shared-encoder models and provide valuable insights into their strengths and weaknesses. Our research contributes to the advancement of multi-task visual perception for autonomous driving systems by achieving competitive results in terms of accuracy and efficiency. The source code is publicly available on GitHub to facilitate further research in this domain. © 2024 Elsevier Ltd
KW  - Drivable area segmentation
KW  - Lane line detection
KW  - Multi-task learning
KW  - Traffic object detection
KW  - Visual perception
KW  - Autonomous vehicles
KW  - Convolutional neural networks
KW  - Decoding
KW  - Learning systems
KW  - Network architecture
KW  - Object recognition
KW  - Real time systems
KW  - Signal encoding
KW  - Vision
KW  - Autonomous driving
KW  - Drivable area segmentation
KW  - Driving systems
KW  - Lane line detection
KW  - Line detection
KW  - Multitask learning
KW  - Objects detection
KW  - Traffic object detection
KW  - Traffic objects
KW  - Visual perception
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ywet, N.L.
AU  - Maw, A.A.
AU  - Nguyen, T.A.
AU  - Lee, J.-W.
TI  - YOLOTransfer-DT: An Operational Digital Twin Framework with Deep and Transfer Learning for Collision Detection and Situation Awareness in Urban Aerial Mobility
PY  - 2024
T2  - Aerospace
DO  - 10.3390/aerospace11030179
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188712186&doi=10.3390%2faerospace11030179&partnerID=40&md5=218b9efc7fb723555c652df8caaf1bfc
AB  - Urban Air Mobility (UAM) emerges as a transformative approach to address urban congestion and pollution, offering efficient and sustainable transportation for people and goods. Central to UAM is the Operational Digital Twin (ODT), which plays a crucial role in real-time management of air traffic, enhancing safety and efficiency. This study introduces a YOLOTransfer-DT framework specifically designed for Artificial Intelligence (AI) training in simulated environments, focusing on its utility for experiential learning in realistic scenarios. The framework’s objective is to augment AI training, particularly in developing an object detection system that employs visual tasks for proactive conflict identification and mission support, leveraging deep and transfer learning techniques. The proposed methodology combines real-time detection, transfer learning, and a novel mix-up process for environmental data extraction, tested rigorously in realistic simulations. Findings validate the use of existing deep learning models for real-time object recognition in similar conditions. This research underscores the value of the ODT framework in bridging the gap between virtual and actual environments, highlighting the safety and cost-effectiveness of virtual testing. This adaptable framework facilitates extensive experimentation and training, demonstrating its potential as a foundation for advanced detection techniques in UAM. © 2024 by the authors.
KW  - collision detection
KW  - deep learning
KW  - operational digital twin
KW  - situation awareness
KW  - transfer learning
KW  - urbanair mobility
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Saesaria, S.S.
AU  - Trilaksono, B.R.
AU  - Hidayat, E.M.I.
TI  - YOLOv5-GT: A Balanced Improvement in Object Detection Speed and Accuracy for Autonomous Vehicles in Indonesian Mixed Traffic
PY  - 2024
T2  - 2024 14th International Conference on System Engineering and Technology, ICSET 2024 - Proceeding
DO  - 10.1109/ICSET63729.2024.10774910
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215109594&doi=10.1109%2fICSET63729.2024.10774910&partnerID=40&md5=3241bbcbc1659121a1ef5f2bc851bad2
AB  - Object detection speed and accuracy are critical aspects of the perception system in autonomous vehicles. Speed improvement helps the object detector model achieve performance close to real-time, while accuracy enhancement ensures robust detection across various scenes. Balancing these improvements enhances the safety of autonomous vehicles, particularly in mixed and dense traffic conditions, such as those in Indonesia. This study develops a detection model using local datasets to reflect real-world conditions. It achieved balanced improvements in frames per second (fps) and mean Average Precision (mAP@50-95) through a modified YOLOv5 deep learning model. The key enhancements of the model include the integration of GhostConv and Transformer layers into the YOLOv5 architecture, referred to as YOLOv5s-GT, as well as the application of image augmentations with various scenarios to the training data. Experimental results show that the modified YOLOv5 outperformed the baseline model in both fps and mAP metrics, achieving 82.6 fps and 80.1% mAP@50-95, compared to the baseline performance of 75.8 fps and 77.7% mAP@50-95.  © 2024 IEEE.
KW  - autonomous vehicle
KW  - deep learning
KW  - ghost convolution
KW  - Object detection
KW  - transformer
KW  - YOLOv5
KW  - Vehicle detection
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - Detection accuracy
KW  - Detection speed
KW  - Frames per seconds
KW  - Ghost convolution
KW  - Mixed traffic
KW  - Objects detection
KW  - Transformer
KW  - YOLOv5
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhu, Q.
AU  - Fan, L.
AU  - Weng, N.
TI  - Advancements in point cloud data augmentation for deep learning: A survey
PY  - 2024
T2  - Pattern Recognition
DO  - 10.1016/j.patcog.2024.110532
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192682416&doi=10.1016%2fj.patcog.2024.110532&partnerID=40&md5=13da906c107153e1b57ae5e42352b8b1
AB  - Deep learning (DL) has become one of the mainstream and effective methods for point cloud analysis tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys these methods, categorizing them into a taxonomy framework that comprises basic and specialized point cloud data augmentation methods. Through a comprehensive evaluation of these augmentation methods, this article identifies their potentials and limitations, serving as a useful reference for choosing appropriate augmentation methods. In addition, potential directions for future research are recommended. This survey contributes to providing a holistic overview of the current state of point cloud data augmentation, promoting its wider application and development. © 2024 Elsevier Ltd
KW  - Augmentation
KW  - Classification
KW  - Deep learning
KW  - Detection
KW  - Point cloud
KW  - Segmentation
KW  - Learning systems
KW  - Augmentation
KW  - Augmentation methods
KW  - Cloud analysis
KW  - Data augmentation
KW  - Deep learning
KW  - Detection
KW  - Overfitting
KW  - Point cloud data
KW  - Point-clouds
KW  - Segmentation
KW  - Deep learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Kumar, L.A.
AU  - Angalaeswari, S.
AU  - Mohana Sundaram, K.
AU  - Bansal, R.C.
AU  - Patil, A.
TI  - Intelligent solutions for sustainable power grids
PY  - 2024
T2  - Intelligent Solutions for Sustainable Power Grids
DO  - 10.4018/979-8-3693-3735-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195015694&doi=10.4018%2f979-8-3693-3735-6&partnerID=40&md5=2c2e160aa007f46e7633496ec4b0e98a
AB  - In the environment of energy systems, the effective utilization of both conventional and renewable sources poses a major challenge. The integration of microgrid systems, crucial for harnessing energy from distributed sources, demands intricate solutions due to the inherent intermittency of these sources. Academic scholars engaged in power system research find themselves at the forefront of addressing issues such as energy source estimation, coordination in dynamic environments, and the effective utilization of artificial intelligence (AI) techniques. Power systems grapple with the intermittent nature of renewable energy sources, necessitating advanced forecasting techniques and effective energy management. The coordination among distributed elements, smooth power transfer, and the optimization of power systems remain persistent challenges. Additionally, the competitive nature of distributed networks, coupled with the need for economic considerations, poses hurdles for young researchers entering the field. There is a pressing need for comprehensive insights into these challenges, coupled with practical solutions that leverage emerging technologies. Intelligent Solutions for Sustainable Power Grids focuses on emerging research areas, this book addresses the uncertainty of renewable energy sources, employs state-of-the-art forecasting techniques, and explores the application of AI techniques for enhanced power system operations. From economic aspects to the digitalization of power systems, the book provides a holistic approach. Tailored for undergraduate and postgraduate students as well as seasoned researchers, it offers a roadmap to navigate the intricate landscape of modern power systems. Dive into a wealth of knowledge encompassing smart energy systems, renewable energy integration, stability analysis of microgrids, power quality enhancement, and much more. This book is not just a guide; it is the solution to the pressing challenges in the dynamic field of energy systems. © 2024 by IGI Global. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhong, J.
AU  - Cheng, Q.
AU  - Hu, X.
AU  - Liu, Z.
TI  - YOLO Adaptive Developments in Complex Natural Environments for Tiny Object Detection
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13132525
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198381331&doi=10.3390%2felectronics13132525&partnerID=40&md5=9434ea902449b1bb646b22c7599198df
AB  - Detection of tiny object in complex environments is a matter of urgency, not only because of the high real-world demand, but also the high deployment and real-time requirements. Although many current single-stage algorithms have good detection performance under low computing power requirements, there are still significant challenges such as distinguishing the background from object features and extracting small-scale target features in complex natural environments. To address this, we first created real datasets based on natural environments and improved dataset diversity using a combination of copy–paste enhancement and multiple image enhancement techniques. As for the choice of network, we chose YOLOV5s due to its nature of fewer parameters and easier deployment in the same class of models. Most improvement strategies to boost detection performance claim to improve the performance of privilege extraction and recognition. However, we prefer to consider the combination of realistic deployment feasibility and detection performance. Therefore, based on the hottest improvement methods of YOLOV5s, we try to make adaptive improvements in three aspects, namely attention mechanism, head network, and backbone network. The experimental results proved that the decoupled head and Slimneck based improvements achieved, respectively, 0.872 and 0.849, 0.538 and 0.479, 87.5% and 89.8% on the mAP0.5, mAP0.5:0.95, and Precision metrics, surpassing the results of the baseline model on these three metrics: 0.705, 0.405 and 83.6%. This result suggests that the adaptively improved model can better meet routine testing needs without significantly increasing the number of parameters. These models perform well on our custom dataset and are also effective on images that are difficult to detect by naked eye. Meanwhile, we find that YOLOV8s, which also has the decoupled head improvement, has the results of 0.743, 0.461, and 87.17% on these three metrics. It proves that under our dataset, it is possible to achieve more advanced results with lower number of model parameters just by adding decoupled head. And according to the results, we also discuss and analyze some improvements that are not adapted to our dataset, which also provides ideas for researchers in similar scenarios: in the booming development of object detection, choosing the suitable model and adapting to combine with other technologies would help to provide solutions to real-world problems. © 2024 by the authors.
KW  - backbone network
KW  - complex natural environment
KW  - copy–paste
KW  - head network
KW  - object detection
KW  - self-attention mechanism
KW  - YOLOV5s
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xiong, X.
AU  - Meng, A.
AU  - Lu, J.
AU  - Tan, Y.
AU  - Chen, B.
AU  - Tang, J.
AU  - Zhang, C.
AU  - Xiao, S.
AU  - Hu, J.
TI  - Automatic detection and location of pavement internal distresses from ground penetrating radar images based on deep learning
PY  - 2024
T2  - Construction and Building Materials
DO  - 10.1016/j.conbuildmat.2023.134483
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180409059&doi=10.1016%2fj.conbuildmat.2023.134483&partnerID=40&md5=80cf341c7510d913e7274f80995d92a9
AB  - Ground penetrating radar (GPR) has been prevailingly applied in nondestructive testing of internal distresses within asphalt pavement. However, the interpretation of abnormal echo signals in massive pavement GPR images is time-consuming and labor-intensive, and prone to misjudgment. To address this issue, a three-step method was proposed for the automatic detection and location of the internal distresses (eg: crack and debonding) echo features from GPR images. The on-site and numerical simulated GPR images of the asphalt pavement together formed the dataset required for the subsequent deep learning models. The first step is that You Only Look Once version 3 (YOLOv3) model predicted rectangular boxes for enclosing the internal distress echo features from GPR images. The second step involves developing the U-net models to segment the internal distress echo feature pixels in the cropped rectangular boxes. The last step is that the median points of the segmentation of the internal distress echo feature were fitted with a theoretical curve equation, to estimate the location of the internal distress. The proposed method has shown that the comprehensive detection accuracy of the internal distress echo features can reach 96.99%, the semantic segmentation accuracies of the internal distress echo feature pixels are not less than 0.856, and the average deviation of the estimated depths of the internal distresses is 3.25 cm. The research method makes further advances in accurately and automatically detecting and locating the internal distresses of asphalt pavement. © 2023 Elsevier Ltd
KW  - Asphalt pavement
KW  - Debonding
KW  - Ground penetrating radar
KW  - Internal crack
KW  - U-net model
KW  - YOLOv3 model
KW  - Asphalt
KW  - Asphalt pavements
KW  - Feature extraction
KW  - Geological surveys
KW  - Ground penetrating radar systems
KW  - Location
KW  - Nondestructive examination
KW  - Pixels
KW  - Radar imaging
KW  - Seepage
KW  - Semantics
KW  - Automatic Detection
KW  - Automatic location
KW  - Echo features
KW  - Ground Penetrating Radar
KW  - Image-based
KW  - Internal crack
KW  - Net model
KW  - Rectangular box
KW  - U-net model
KW  - You only look once version 3 model
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Sun, H.
AU  - Zhang, Z.
TI  - A Multi-Scale-Enhanced YOLO-V5 Model for Detecting Small Objects in Remote Sensing Image Information
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24134347
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198347216&doi=10.3390%2fs24134347&partnerID=40&md5=08dcf2cd072fc06eff9f62a4f191cdc8
AB  - As a typical component of remote sensing signals, remote sensing image (RSI) information plays a strong role in showing macro, dynamic and accurate information on the earth’s surface and environment, which is critical to many application fields. One of the core technologies is the object detection (OD) of RSI signals (RSISs). The majority of existing OD algorithms only consider medium and large objects, regardless of small-object detection, resulting in an unsatisfactory performance in detection precision and the miss rate of small objects. To boost the overall OD performance of RSISs, an improved detection framework, I-YOLO-V5, was proposed for OD in high-altitude RSISs. Firstly, the idea of a residual network is employed to construct a new residual unit to achieve the purpose of improving the network feature extraction. Then, to avoid the gradient fading of the network, densely connected networks are integrated into the structure of the algorithm. Meanwhile, a fourth detection layer is employed in the algorithm structure in order to reduce the deficiency of small-object detection in RSISs in complex environments, and its effectiveness is verified. The experimental results confirm that, compared with existing advanced OD algorithms, the average accuracy of the proposed I-YOLO-V5 is improved by 15.4%, and the miss rate is reduced by 46.8% on the RSOD dataset. © 2024 by the authors.
KW  - densely connected network
KW  - residual network
KW  - RSI information
KW  - small-object detection
KW  - YOLO network
KW  - Image enhancement
KW  - Object recognition
KW  - Remote sensing
KW  - Densely connected networks
KW  - Image information
KW  - Object detection algorithms
KW  - Remote sensing image information
KW  - Remote sensing images
KW  - Residual network
KW  - Small object detection
KW  - Small objects
KW  - YOLO network
KW  - algorithm
KW  - altitude
KW  - article
KW  - diagnosis
KW  - feature extraction
KW  - human
KW  - remote sensing
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Narkhede, M.
AU  - Chopade, N.
TI  - CycleInSight: An enhanced YOLO approach for vulnerable cyclist detection in urban environments
PY  - 2024
T2  - International Journal of Electrical and Computer Engineering
DO  - 10.11591/ijece.v14i4.pp3986-3994
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195064937&doi=10.11591%2fijece.v14i4.pp3986-3994&partnerID=40&md5=7a6db02383feb111c080b55230e2e63e
AB  - As urbanization continues to reshape transportation, the safety of cyclists in complex traffic environments has become a pressing concern. In response to this challenge, our research introduces a CycleInSight framework, which harnesses advanced deep learning and computer vision techniques to enable precise and efficient cyclist detection in diverse urban settings. Utilizing you only look once version 8 (YOLOv8) object detection algorithm, the proposed model aims to detect and localize vulnerable cyclists near vehicles equipped with onboard cameras. Our research presents comprehensive experimental results demonstrating its effectiveness in identifying vulnerable cyclists amidst dynamic and challenging traffic conditions. With an impressive average precision of 90.91%, our approach outperforms existing models while maintaining efficient inference speeds. By effectively identifying and tracking cyclists, this framework holds significant potential to enhance urban traffic safety, inform data-driven infrastructure planning, and support the development of advanced driver assistance systems and autonomous vehicles. © 2024 Institute of Advanced Engineering and Science. All rights reserved.
KW  - Advanced driver assistance systems Autonomous vehicles Computer vision Deep learning Object detection Vulnerable cyclist YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hong, K.
AU  - Wu, M.
AU  - Gao, B.
AU  - Feng, Y.
TI  - A Grading Identification Method for Tea Buds Based on Improved YOLOv7-tiny
ST  - 基于改进 YOLOv7-tiny 的茶叶嫩芽分级识别方法
PY  - 2024
T2  - Journal of Tea Science
DO  - 10.13305/j.cnki.jts.2024.01.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192723299&doi=10.13305%2fj.cnki.jts.2024.01.006&partnerID=40&md5=548f22d3b890cbcc80e5adbd6cff96b3
AB  - The intelligent grading and recognition of tea buds in a natural environment are fundamental for the automation of premium tea harvesting. To address the problems of low recognition accuracy and limited robustness caused by complex environmental factors like lighting, obstruction, and dense foliage, we propose an enhanced model based on YOLOv7-tiny. Firstly, a CBAM module was added into the small object detection layer of the YOLOv7-tiny model to enhance the model's ability to focus on small object features and reduce the interference of complex environments on tea bud recognition. We adjusted the spatial pyramid pooling structure to lower computational costs and improve detection speed. Additionally, we utilized a loss function combining IoU and NWD to further enhance the model's robustness in small object detection by addressing the sensitivity of the IoU mechanism to position deviations. Experimental results demonstrate that the proposed model achieves a detection accuracy of 91.15%, a recall rate of 88.54%, and a mean average precision of 92.66%. The model's size is 12.4 MB. Compared to the original model, this represents an improvement of 2.83%, 2.00%, and 1.47% in accuracy, recall rate, and mean average precision, respectively, with a significant increase of 0.1 MB in model size. Comparative experiments with different models show that our model exhibits fewer false negatives and false positives in multiple scenarios, along with higher confidence scores. The improved model can be applied to the bud grading and recognition process of premium tea harvesting robots. © 2024 Editorial Office of Journal of Tea science. All rights reserved.
KW  - attention mechanisms
KW  - grading identification
KW  - NWD loss
KW  - tea bud
KW  - YOLOv7-tiny
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Casas, E.
AU  - Ramos, L.
AU  - Romero, C.
AU  - Rivas-Echeverría, F.
TI  - A comparative study of YOLOv5 and YOLOv8 for corrosion segmentation tasks in metal surfaces
PY  - 2024
T2  - Array
DO  - 10.1016/j.array.2024.100351
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195286396&doi=10.1016%2fj.array.2024.100351&partnerID=40&md5=6af8d8c6e9bf31482dadac6e53d8ac50
AB  - This study delves into the comparative efficacy of YOLOv5 and YOLOv8 in corrosion segmentation tasks. We employed three unique datasets, comprising 4942, 5501, and 6136 images, aiming to thoroughly evaluate the models’ adaptability and robustness in diverse scenarios. The assessment metrics included precision, recall, F1-score, and mean average precision. Furthermore, graphical tests offered a visual perspective on the segmentation capabilities of each architecture. Our results highlight YOLOv8’s superior speed and segmentation accuracy across datasets, further corroborated by graphical evaluations. These visual assessments were instrumental in emphasizing YOLOv8’s proficiency in handling complex corroded surfaces. However, in the largest dataset, both models encountered challenges, particularly with overlapping bounding boxes. YOLOv5 notably lagged, struggling to achieve the performance standards set by YOLOv8, especially with irregular corroded surfaces. In conclusion, our findings underscore YOLOv8’s enhanced capabilities, establishing it as a preferable choice for real-world corrosion detection tasks. This research thus offers invaluable insights, poised to redefine corrosion management strategies and guide future explorations in corrosion identification. © 2024 The Author(s)
KW  - Computer vision
KW  - Corrosion
KW  - Deep learning
KW  - Image segmentation
KW  - Instance segmentation
KW  - YOLO
KW  - Computer vision
KW  - Corrosion
KW  - Deep learning
KW  - Assessment metric
KW  - Comparatives studies
KW  - Corroded surface
KW  - Deep learning
KW  - F1 scores
KW  - Graphical test
KW  - Images segmentations
KW  - Instance segmentation
KW  - Metal surfaces
KW  - YOLO
KW  - Image segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Duan, Z.
AU  - Shao, J.
AU  - Zhang, M.
AU  - Zhang, J.
AU  - Zhai, Z.
TI  - A Small-Object-Detection Algorithm Based on LiDAR Point-Cloud Clustering for Autonomous Vehicles
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24165423
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202447200&doi=10.3390%2fs24165423&partnerID=40&md5=68704bf423d1953ab1dba42d29cc3f03
AB  - 3D object-detection based on LiDAR point clouds can help driverless vehicles detect obstacles. However, the existing point-cloud-based object-detection methods are generally ineffective in detecting small objects such as pedestrians and cyclists. Therefore, a small-object-detection algorithm based on clustering is proposed. Firstly, a new segmented ground-point clouds segmentation algorithm is proposed, which filters out the object point clouds according to the heuristic rules and realizes the ground segmentation by multi-region plane-fitting. Then, the small-object point cloud is clustered using an improved DBSCAN clustering algorithm. The K-means++ algorithm for pre-clustering is used, the neighborhood radius is adaptively adjusted according to the distance, and the core point search method of the original algorithm is improved. Finally, the detection of small objects is completed using the directional wraparound box model. After extensive experiments, it was shown that the precision and recall of our proposed ground-segmentation algorithm reached 91.86% and 92.70%, respectively, and the improved DBSCAN clustering algorithm improved the recall of pedestrians and cyclists by 15.89% and 9.50%, respectively. In addition, visualization experiments confirmed that our proposed small-object-detection algorithm based on the point-cloud clustering method can realize the accurate detection of small objects. © 2024 by the authors.
KW  - autonomous driving
KW  - ground segmentation
KW  - LiDAR
KW  - point cloud clustering
KW  - small object detection
KW  - Heuristic algorithms
KW  - Autonomous driving
KW  - Clusterings
KW  - Ground segmentation
KW  - LiDAR
KW  - Object detection algorithms
KW  - Point cloud clustering
KW  - Point-clouds
KW  - Segmentation algorithms
KW  - Small object detection
KW  - Small objects
KW  - algorithm
KW  - article
KW  - autonomous vehicle
KW  - cloud computing
KW  - clustering algorithm
KW  - controlled study
KW  - cyclist
KW  - detection algorithm
KW  - diagnosis
KW  - filter
KW  - k means clustering
KW  - neighborhood
KW  - pedestrian
KW  - segmentation algorithm
KW  - Bicycles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sha, X.
AU  - Guo, Z.
AU  - Guan, Z.
AU  - Li, W.
AU  - Wang, S.
AU  - Zhao, Y.
TI  - PBTA: Partial Break Triplet Attention Model for Small Pedestrian Detection Based on Vehicle Camera Sensors
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3398031
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193294357&doi=10.1109%2fJSEN.2024.3398031&partnerID=40&md5=1e918e99a16e6adf086af9599e3584e0
AB  - Successfully detecting small pedestrians through vehicle camera sensors would greatly facilitate the development of autonomous driving safety applications. However, the existing pedestrian detection models applied to vehicle camera applications were limited by the scale confusion problem and the weak feature problem of small pedestrian targets. To resolve these issues, this study proposed a partial break triplet attention (PBTA) network composed of two components: the partial break bidirectional feature pyramid network (PBFPN) and the TR-NCSPDarknet53. PBFPN was used to solve the scale confusion problem in shallow feature maps by employing a partial break operation and a branch fusion operation. In TR-NCSPDarknet53, the Ta-conv module was proposed to solve the weak feature problem. The PBTA network provided a new improvement idea for small pedestrian detection. It greatly improved the accuracy while keeping the parameters at a low level, which is vital for safety applications in autonomous driving. Extensive experiments on CityPersons, Crowdhuman, and WiderPerson datasets including various traffic images from camera sensors demonstrate the accuracy of the PBTA network in small pedestrian detection. Compared with the baseline (Yolov8S ) network, the accuracy of small objects (APS) is improved by 50%.  © 2001-2012 IEEE.
KW  - Attention mechanism
KW  - autonomous vehicle (AV)
KW  - camera sensors
KW  - feature fusion network
KW  - small pedestrian detection
KW  - Autonomous vehicles
KW  - Cameras
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Pedestrian safety
KW  - Attention mechanisms
KW  - Autonomous vehicle
KW  - Autonomous Vehicles
KW  - Camera sensor
KW  - Feature fusion network
KW  - Features extraction
KW  - Features fusions
KW  - Objects detection
KW  - Pedestrian
KW  - Pedestrian detection
KW  - Small pedestrian detection
KW  - Semantics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Han, J.
AU  - Cao, R.
AU  - Brighente, A.
AU  - Conti, M.
TI  - Light-YOLOv5: A Lightweight Drone Detector for Resource-Constrained Cameras
PY  - 2024
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2023.3329221
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181824089&doi=10.1109%2fJIOT.2023.3329221&partnerID=40&md5=142711fbe6a5a800a8b4560654de0673
AB  - Critical infrastructures (CIs), such as military bases and airports, are putting a lot of attention into defending against attacks delivered via drones by deploying drone detection systems. However, the CI area might be very large, with no-fly zones extending to regions where it might not be possible to deploy a power line for resourceful cameras. To this aim, the CI might deploy an Internet of Things (IoT)-based surveillance camera system to capture drone images. However, these IoT cameras are resource-constrained devices that cannot support the currently available detectors. In this article, we propose Light-YOLOv5, a lightweight image-based drone detector for resource-constrained cameras. We make targeted improvements to YOLOv5, including the replacement of the backbone network, the introduction of the transformer module, and the design of a parallel mixed efficient attention module (PEAM). We show that our modifications allow for reduced network size while achieving better classification than other state-of-the-art solutions. To prove these claims, we expanded an already available data set of blurred drone images by adding clear images of aircraft and birds. Since airplanes and birds are easily confused as drones by image classifiers, our addition proves the effectiveness of our solution. Experiments show that Light-YOLOv5 can achieve a very good tradeoff between performance (74.8% mAP) and efficiency (170 FPS). Compared to YOLOv5, Light-YOLOv5 improves mAP by 4.1%, reduces the number of network parameters by 15.7%, can perform detection at 170 frames per second (FPS), and achieves an average accuracy rate of 93.8%.  © 2014 IEEE.
KW  - Resource-constrained cameras
KW  - unmanned aerial vehicle (UAV) detection
KW  - YOLOv5
KW  - Aircraft detection
KW  - Birds
KW  - Cameras
KW  - Drones
KW  - Economic and social effects
KW  - Internet of things
KW  - Network security
KW  - Security systems
KW  - Tracking radar
KW  - Detection system
KW  - Features extraction
KW  - Frames per seconds
KW  - Military base
KW  - Power lines
KW  - Radar detection
KW  - Resource constrained camera
KW  - Surveillance cameras
KW  - UAV detection
KW  - YOLOv5
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sapkota, R.
AU  - Ahmed, D.
AU  - Karkee, M.
TI  - Comparing YOLOv8 and Mask R-CNN for instance segmentation in complex orchard environments
PY  - 2024
T2  - Artificial Intelligence in Agriculture
DO  - 10.1016/j.aiia.2024.07.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198959678&doi=10.1016%2fj.aiia.2024.07.001&partnerID=40&md5=c2701883942d79236e41403d5328f36a
AB  - Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively. These findings show YOLOv8's superior accuracy and efficiency in machine learning applications compared to two-stage models, specifically Mask-R-CNN, which suggests its suitability in developing smart and automated orchard operations, particularly when real-time applications are necessary in such cases as robotic harvesting and robotic immature green fruit thinning. © 2024 The Authors
KW  - Artificial intelligence
KW  - Automation
KW  - Deep learning
KW  - Machine learning
KW  - Machine vision
KW  - Mask R-CNN
KW  - Robotics
KW  - YOLOv8
KW  - Agricultural robots
KW  - Deep learning
KW  - Fruits
KW  - Image segmentation
KW  - Learning systems
KW  - Orchards
KW  - Automated tasks
KW  - Deep learning
KW  - Images processing
KW  - Individual objects
KW  - Machine-learning
KW  - Machine-vision
KW  - Mask R-CNN
KW  - Processing operations
KW  - Segmentation models
KW  - YOLOv8
KW  - Computer vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yao, C.
AU  - Liu, X.
AU  - Wang, J.
AU  - Cheng, Y.
TI  - Optimized Design of EdgeBoard Intelligent Vehicle Based on PP-YOLOE+
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24103180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194218234&doi=10.3390%2fs24103180&partnerID=40&md5=dcad72f2e38be7282f0200600cfdf6f3
AB  - Advances in deep learning and computer vision have overcome many challenges inherent in the field of autonomous intelligent vehicles. To improve the detection accuracy and efficiency of EdgeBoard intelligent vehicles, we proposed an optimized design of EdgeBoard based on our PP-YOLOE+ model. This model innovatively introduces a composite backbone network, incorporating deep residual networks, feature pyramid networks, and RepResBlock structures to enrich environmental perception capabilities through the advanced analysis of sensor data. The incorporation of an efficient task-aligned head (ET-head) in the PP-YOLOE+ framework marks a pivotal innovation for precise interpretation of sensor information, addressing the interplay between classification and localization tasks with high effectiveness. Subsequent refinement of target regions by detection head units significantly sharpens the system’s ability to navigate and adapt to diverse driving scenarios. Our innovative hardware design, featuring a custom-designed mainboard and drive board, is specifically tailored to enhance the computational speed and data processing capabilities of intelligent vehicles. Furthermore, the optimization of our Pos-PID control algorithm allows the system to dynamically adjust to complex driving scenarios, significantly enhancing vehicle safety and reliability. Besides, our methodology leverages the latest technologies in edge computing and dynamic label assignment, enhancing intelligent vehicles’ operations through seamless sensor integration. Our custom dataset, specifically designed for this study, includes 4777 images captured by intelligent vehicles under a variety of environmental and lighting conditions. The dataset features diverse scenarios and objects pertinent to autonomous driving, such as pedestrian crossings and traffic signs, ensuring a comprehensive evaluation of the model’s performance. We conducted extensive testing of our model on this dataset to thoroughly assess sensor performance. Evaluated against metrics including accuracy, error rate, precision, recall, mean average precision (mAP), and F1-score, our findings reveal that the model achieves a remarkable accuracy rate of 99.113%, an mAP of 54.9%, and a real-time detection frame rate of 192 FPS, all within a compact parameter footprint of just 81 MB. These results demonstrate the superior capability of our PP-YOLOE+ model to integrate sensor data, achieving an optimal balance between detection accuracy and computational speed compared with existing algorithms. © 2024 by the authors.
KW  - autonomous sensing
KW  - EdgeBoard intelligent vehicle
KW  - optimal route determination
KW  - Pos-PID
KW  - PP-YOLOE+
KW  - target detection
KW  - Autonomous vehicles
KW  - Classification (of information)
KW  - Data handling
KW  - Deep learning
KW  - Intelligent vehicle highway systems
KW  - Statistical tests
KW  - Traffic signs
KW  - Vehicle safety
KW  - Autonomous sensing
KW  - Detection accuracy
KW  - Edgeboard intelligent vehicle
KW  - Optimal route determination
KW  - Optimal routes
KW  - Optimized designs
KW  - Pos-PID
KW  - PP-YOLOE+
KW  - Sensors data
KW  - Targets detection
KW  - algorithm
KW  - article
KW  - benchmarking
KW  - computer vision
KW  - controlled study
KW  - data processing
KW  - deep learning
KW  - human
KW  - illumination
KW  - pedestrian
KW  - proportional integral derivative algorithm
KW  - reliability
KW  - sensor
KW  - vehicle safety
KW  - velocity
KW  - Digital storage
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Peng, C.
AU  - He, B.
AU  - Xi, W.
AU  - Lin, G.
TI  - Improved YOLOv7 Algorithm for Floating Waste Detection Based on GFPN and Long-Range Attention Mechanism
ST  - 基于 GFPN 和长程注意力机制的改进 YOLOv7 河面漂浮垃圾检测算法
PY  - 2024
T2  - Wuhan University Journal of Natural Sciences
DO  - 10.1051/wujns/2024294338
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005605759&doi=10.1051%2fwujns%2f2024294338&partnerID=40&md5=892deb3935a7be6d6f95f59681fdb97c
AB  - Floating wastes in rivers have specific characteristics such as small scale, low pixel density and complex backgrounds. These characteristics make it prone to false and missed detection during image analysis, thus resulting in a degradation of detection performance. In order to tackle these challenges, a floating waste detection algorithm based on YOLOv7 is proposed, which combines the improved GFPN (Generalized Feature Pyramid Network) and a long-range attention mechanism. Firstly, we import the improved GFPN to replace the Neck of YOLOv7, thus providing more effective information transmission that can scale into deeper networks. Secondly, the convolution-based and hardware-friendly long-range attention mechanism is introduced, allowing the algorithm to rapidly generate an attention map with a global receptive field. Finally, the algorithm adopts the WiseIoU optimization loss function to achieve adaptive gradient gain allocation and alleviate the negative impact of low-quality samples on the gradient. The simulation results reveal that the proposed algorithm has achieved a favorable average accuracy of 86.3% in real-time scene detection tasks. This marks a significant enhancement of approximately 6.3% compared with the baseline, indicating the algorithm’s good performance in floating waste detection. © Wuhan University 2024.
KW  - floating waste detection
KW  - GFPN (Generalized Feature Pyramid Network)
KW  - long-range attention
KW  - YOLOv7
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, C.
AU  - Cai, X.
AU  - Li, Y.
AU  - Zhai, R.
AU  - Wu, R.
AU  - Zhu, S.
AU  - Guan, L.
AU  - Luo, Z.
AU  - Zhang, S.
AU  - Zhang, J.
TI  - Research and Application of Panoramic Visual Perception-Assisted Navigation Technology for Ships
PY  - 2024
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse12071042
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199885394&doi=10.3390%2fjmse12071042&partnerID=40&md5=de6a7d35d3fd3fe8b7c012a3d638e29a
AB  - In response to challenges such as narrow visibility for ship navigators, limited field of view from a single camera, and complex maritime environments, this study proposes panoramic visual perception-assisted navigation technology. The approach includes introducing a region-of-interest search method based on SSIM and an elliptical weighted fusion method, culminating in the development of the ship panoramic visual stitching algorithm SSIM-EW. Additionally, the YOLOv8s model is improved by increasing the size of the detection head, introducing GhostNet, and replacing the regression loss function with the WIoU loss function, and a perception model yolov8-SGW for sea target detection is proposed. The experimental results demonstrate that the SSIM-EW algorithm achieves the highest PSNR indicator of 25.736, which can effectively reduce the stitching traces and significantly improve the stitching quality of panoramic images. Compared to the baseline model, the YOLOv8-SGW model shows improvements in the P, R, and mAP50 of 1.5%, 4.3%, and 2.3%, respectively, its mAP50 is significantly higher than that of other target detection models, and the detection ability of small targets at sea has been significantly improved. Implementing these algorithms in tugboat operations at ports enhances the fields of view of navigators, allowing for the identification of targets missed by AISs and radar systems, thus ensuring operational safety and advancing the level of vessel intelligence. © 2024 by the authors.
KW  - intelligent vessels
KW  - object detection
KW  - panoramic vision
KW  - visual perception
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lyu, Z.
AU  - An, W.
TI  - HDR-YOLO: Adaptive Object Detection in Haze, Dark, and Rain Scenes Based on YOLO
PY  - 2024
T2  - International Journal of Pattern Recognition and Artificial Intelligence
DO  - 10.1142/S021800142450006X
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194904978&doi=10.1142%2fS021800142450006X&partnerID=40&md5=761487c6deb5d696a9e0a1c1f0a4b66f
AB  - In the context of real-world environments, images acquired through surveillance cameras in such settings are frequently marred by issues including diminished contrast, suboptimal image quality, and color aberrations, rendering conventional object detection models ill-suited for the task. Taking inspiration from the foundational principles of image restoration, this study aims to extract environment-agnostic features across various weather conditions in order to enhance object detection performance in multiple scenarios while maintaining accuracy under typical meteorological conditions. In response to this question, we introduce a detection framework as HDR-YOLO that jointly trains feature extraction and object detection. Meantime, to solve the problem of visual impairments caused by adverse conditions, we propose a Dynamic Extraction of Environment-Agnostic Features (DEAF) module. Additionally, we joint mean squared error (MSE) loss and Log-Cosh loss as optimization techniques, carefully tailored to further elevate detection performance, especially under adverse meteorological conditions. Extensive empirical findings from the AGVS dataset validate the ability of HDR-YOLO to improve object detection performance in airport ground videos within real-world settings while maintaining precision under typical meteorological conditions, which underscores its innovative capabilities and adaptability in complex and diverse environments.  © 2024 World Scientific Publishing Company.
KW  - adaptive object detection
KW  - convolutional neural networks
KW  - Object detection
KW  - real-world object detection
KW  - Airport security
KW  - Convolutional neural networks
KW  - Extraction
KW  - Feature extraction
KW  - Image enhancement
KW  - Image reconstruction
KW  - Image segmentation
KW  - Mean square error
KW  - Object recognition
KW  - Security systems
KW  - Adaptive object detection
KW  - Condition
KW  - Convolutional neural network
KW  - Detection performance
KW  - Meteorological condition
KW  - Objects detection
KW  - Real-world object detection
KW  - Real-world objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Cao, X.
AU  - Zhang, H.
AU  - Shen, Y.
AU  - Yuan, X.
AU  - Cui, Z.
AU  - Lu, Z.
TI  - An Intelligent Obstacle Detection for Autonomous Mining Transportation With Electric Locomotive via Cellular Vehicle-to-Everything and Vehicular Edge Computing
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2023.3324145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176374982&doi=10.1109%2fTITS.2023.3324145&partnerID=40&md5=a2c4933ac6ce5da61b51baff0d180a65
AB  - The tremendous revolutionary progress of cellular vehicle-to-everything (C-V2X) and vehicular edge computing (VEC) technologies provide new opportunities to overcome the autonomous transportation issue of the mining electric locomotives (MELs), in which the accurate and fast detection of obstacles is crucial for the safe operation. With the VEC and C-V2X, we proposed a new high-precision obstacle detection strategy for MELs (MEL-YOLO). Firstly, we investigated the convolutional attention mechanism integrated into the path aggregation network of the Neck layer to strengthen the feature extraction capabilities. Secondly, we added a small-object oriented prediction layer in the Head to form the multi-scale feature prediction. Thirdly, we introduced a more efficient loss function to alleviate the gradient explosion problem in the feature transfer. Finally, we utilized the K-means++ optimization to derive the anchor boxes matchable with the dataset, which was collected and created by featuring different scenes to train validate the model. The MEL-YOLO was compressed by BN layer pruning and implemented on the edge device in a 6G/B5G based-V2X environment. Experimental results verify that the MEL-YOLO can effectively detect obstacles and significantly improve detection accuracy for small obstacles, computationally increasing mAP by 3.3% to original model, while maintaining detection speed and model size nearly unchanged. © 2000-2011 IEEE.
KW  - 6G-vehicle-to-everything
KW  - attention mechanism
KW  - edge computing
KW  - Mining electric locomotive
KW  - multi-scale feature prediction
KW  - obstacle detection
KW  - Edge computing
KW  - Engines
KW  - Feature extraction
KW  - Locomotives
KW  - Obstacle detectors
KW  - Vehicle to Everything
KW  - Vehicles
KW  - 6g-vehicle-to-everything
KW  - Attention mechanisms
KW  - Cellulars
KW  - Computing technology
KW  - Edge computing
KW  - Mining electric locomotive
KW  - Mining transportations
KW  - Multi-scale feature prediction
KW  - Multi-scale features
KW  - Obstacles detection
KW  - Forecasting
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gao, C.
AU  - Zhao, F.
AU  - Zhang, Y.
AU  - Wan, M.
TI  - Research on multitask model of object detection and road segmentation in unstructured road scenes
PY  - 2024
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/ad35dd
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189330824&doi=10.1088%2f1361-6501%2fad35dd&partnerID=40&md5=2c1d49b9c3b9f29a0b6ab0ed6c31eba3
AB  - With the rapid development of artificial intelligence and computer vision technology, autonomous driving technology has become a hot area of concern. The driving scenarios of autonomous vehicles can be divided into structured scenarios and unstructured scenarios. Compared with structured scenes, unstructured road scenes lack the constraints of lane lines and traffic rules, and the safety awareness of traffic participants is weaker. Therefore, there are new and higher requirements for the environment perception tasks of autonomous vehicles in unstructured road scenes. The current research rarely integrates the target detection and road segmentation to achieve the simultaneous processing of target detection and road segmentation of autonomous vehicle in unstructured road scenes. Aiming at the above issues, a multitask model for object detection and road segmentation in unstructured road scenes is proposed. Through the sharing and fusion of the object detection model and road segmentation model, multitask model can complete the tasks of multi-object detection and road segmentation in unstructured road scenes while inputting a picture. Firstly, MobileNetV2 is used to replace the backbone network of YOLOv5, and multi-scale feature fusion is used to realize the information exchange layer between different features. Subsequently, a road segmentation model was designed based on the DeepLabV3+ algorithm. Its main feature is that it uses MobileNetV2 as the backbone network and combines the binary classification focus loss function for network optimization. Then, we fused the object detection algorithm and road segmentation algorithm based on the shared MobileNetV2 network to obtain a multitask model and trained it on both the public dataset and the self-built dataset NJFU. The training results demonstrate that the multitask model significantly enhances the algorithm’s execution speed by approximately 10 frames per scond while maintaining the accuracy of object detection and road segmentation. Finally, we conducted validation of the multitask model on an actual vehicle. © 2024 IOP Publishing Ltd.
KW  - autonomous vehicles
KW  - multitask model
KW  - object detection
KW  - road segmentation
KW  - unstructured road scenes
KW  - Autonomous vehicles
KW  - Object recognition
KW  - Roads and streets
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Back-bone network
KW  - Computer vision technology
KW  - Multi-task model
KW  - Objects detection
KW  - Road segmentation
KW  - Segmentation models
KW  - Targets detection
KW  - Unstructured road scene
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Vellaidurai, A.
AU  - Rathinam, M.
TI  - A novel OYOLOV5 model for vehicle detection and classification in adverse weather conditions
PY  - 2024
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-023-16450-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168143115&doi=10.1007%2fs11042-023-16450-2&partnerID=40&md5=f552fdb04c680be2e0c0aceac7c29eb2
AB  - An autonomous vehicle must accurately detect its surrounding environment to operate reliably. Adverse weather conditions (ADWC) are snow, rain, sand, and haze, badly affect the quality of vehicle detection (VD) in an autonomous environment. Most existing techniques focused on VD under various weather effects such as signal control, travel pattern, traffic volume variations and collision risk. Only a limited number of works of literature were focused on VD under ADWC at different automation scales. In this paper, a novel deep learning (DL) model, Optimized You Look Only Once Version 5 (OYOLOV5), is proposed for autonomous VD (AVD) in ADWC. The proposed model consists of three phases: data collection, data preprocessing, feature extraction and classification. Initially, the data is collected from the DAWN and COCO dataset to perform VD, which is openly available. The augmentation of the data is carried out on the collected input data by including hue, saturation, blur, brightness, and noise, which helps to get a clear view of vehicles. After data augmentation, feature extraction and classification of the preprocessed images are done using the OYOLOV5 framework, which uses Resnet-50 as the backbone network and Feature Pyramid Network (FPN) for detecting the vehicles at multi-scales. Experiments are conducted, and the outcomes demonstrated the proposed OYOLOV5 model achieves better performance with the state-of-art methods in terms of precision (PRC), recall (RC), f-measure (FMS), accuracy (ACU), average IoU (AI), processing speed (PS), and training time (TTI). Also, the system attains good mean average precision (mAP) than the conventional methods. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
KW  - Autonomous vehicle
KW  - Detection in adverse weather nature
KW  - Feature pyramid network
KW  - Fuzzy C-means
KW  - Intersection over Union
KW  - Residual Network-50
KW  - Classification (of information)
KW  - Data acquisition
KW  - Deep learning
KW  - Extraction
KW  - Feature extraction
KW  - Meteorology
KW  - Adverse weather
KW  - Autonomous Vehicles
KW  - C-means
KW  - Detection in adverse weather nature
KW  - Feature pyramid
KW  - Feature pyramid network
KW  - Fuzzy C-mean
KW  - Intersection over union
KW  - Pyramid network
KW  - Residual network-50
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Lu, X.
AU  - Zhang, Z.
AU  - Yang, G.
AU  - He, Y.
AU  - Fang, H.
TI  - Simultaneous detection of reference lines in paddy fields using a machine vision-based framework
PY  - 2024
T2  - Computers and Electronics in Agriculture
DO  - 10.1016/j.compag.2024.108923
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190734091&doi=10.1016%2fj.compag.2024.108923&partnerID=40&md5=d944cb81f7bdf49f23463e5f6b89def1
AB  - Accurate and robust detection of reference lines in the field is essential for formulating linear tracking and steering strategies for agricultural machinery. The identification of planted and ridge areas poses challenges due to their similar appearance to unplanted areas, along with complex field conditions such as uneven illumination. In this study, we proposed a machine vision-based framework for the simultaneous detection of auxiliary navigation lines and ridge boundary lines. Firstly, we constructed a multi-area Paddy Area Segmentation dataset named PASeg, which contained ridge areas, planted areas, and unplanted areas. Additionally, a deep learning network called G-STDC that integrated the Ghost module into the STDC network was developed for efficient and robust area segmentation. Finally, a multi-line detection method was applied based on a central axis-based point clustering algorithm and random sample consensus (RANSAC) algorithm to extract reference lines. According to the results on PASeg, the proposed G-STDC model obtained a mean intersection over union (mIoU) of 95.23 %, outperforming the baseline model (with the mIoU of 93.18 %). The attitude error and distance error of line extraction on 640 × 512 resolution images were within 0.776° and 4.687 pixels, respectively. The overall detection speed reached 13.9 frames per second (FPS), while the faster G-STDC model (G-STDC-t) achieved 16.7 FPS. The proposed method could provide real-time reference lines for turning path planning and automatic navigation in agro-machinery. © 2024 Elsevier B.V.
KW  - Deep learning
KW  - Reference line detection
KW  - Semantic segmentation
KW  - Visual navigation
KW  - Agricultural robots
KW  - Clustering algorithms
KW  - Deep learning
KW  - Motion planning
KW  - Navigation
KW  - Semantic Segmentation
KW  - Semantics
KW  - Deep learning
KW  - Frames per seconds
KW  - Line detection
KW  - Machine-vision
KW  - Reference line detection
KW  - Reference lines
KW  - Semantic segmentation
KW  - Simultaneous detection
KW  - Vision based
KW  - Visual Navigation
KW  - computer vision
KW  - detection method
KW  - machine learning
KW  - machinery
KW  - navigation
KW  - paddy field
KW  - segmentation
KW  - tracking
KW  - Computer vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Borts, D.
AU  - Liang, E.
AU  - Broedermann, T.
AU  - Ramazzina, A.
AU  - Walz, S.
AU  - Palladin, E.
AU  - Sun, J.
AU  - Brueggemann, D.
AU  - Sakaridis, C.
AU  - Van Gool, L.
AU  - Bijelic, M.
AU  - Heide, F.
TI  - Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar
PY  - 2024
T2  - Proceedings - SIGGRAPH 2024 Conference Papers
DO  - 10.1145/3641519.3657510
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199888938&doi=10.1145%2f3641519.3657510&partnerID=40&md5=fdb9e900ffa141e80ed7de6006f732a3
AB  - Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle. While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored. Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques. Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors. We introduce Radar Fields - a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy. The proposed method does not rely on volume rendering. Instead, we learn fields in Fourier frequency space, supervised with raw radar data. We validate our method's effectiveness across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and harsh weather scenarios, where mm-wavelength sensing is favorable.  © 2024 Owner/Author.
KW  - neural rendering.
KW  - radar
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cai, L.
AU  - Zhang, B.
AU  - Li, Y.
AU  - Chai, H.
TI  - IFE-net: improved feature enhancement network for weak feature target recognition in autonomous underwater vehicles
PY  - 2024
T2  - Robotica
DO  - 10.1017/S0263574724000195
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184579632&doi=10.1017%2fS0263574724000195&partnerID=40&md5=007369f1affe8a3bd417d6b6e283e30d
AB  - The recognizing underwater targets is a crucial component of autonomous underwater vehicle patrols and detection efforts. In the process of visual image recognition in real underwater environment, the spatial and semantic features of the target often appear to different degrees of loss, and the scarcity of specific types of underwater samples leads to unbalanced data on categories. This kind of problem makes the target features appear weak and seriously affects the accuracy of underwater target recognition. Traditional deep learning methods based on data and feature enhancement cannot achieve ideal recognition effect. Based on the above difficulties, this paper proposes an improved feature enhancement network for weak feature target recognition. Firstly, a multi-scale spatial and semantic feature enhancement module is constructed to extract the feature information of the extraction target accurately. Secondly, this paper solves the influence of target feature distortion on classification through multi-scale feature comparison of positive and negative samples. Finally, the Rank & Sort Loss function was used to train the depth target detection to solve the problem of recognition accuracy under highly unbalanced sample data. Experimental results show that the recognition accuracy of the proposed method is 2.28% and 3.84% higher than that of the existing algorithms in the recognition of underwater fuzzy and distorted target images, which demonstrates the effectiveness and superiority of the proposed method. © The Author(s), 2024. Published by Cambridge University Press.
KW  - multi-scale feature comparison
KW  - ranking loss
KW  - spatial and semantic feature enhancement
KW  - unbalanced category data
KW  - underwater target recognition
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Image recognition
KW  - Learning systems
KW  - Semantics
KW  - Feature enhancement
KW  - Multi-scale feature comparison
KW  - Multi-scale features
KW  - Ranking loss
KW  - Semantic features
KW  - Spatial and semantic feature enhancement
KW  - Spatial features
KW  - Target recognition
KW  - Unbalanced category data
KW  - Underwater target recognition
KW  - Autonomous underwater vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, L.
AU  - Lan, J.
AU  - Li, M.
TI  - PAFNet: Pillar Attention Fusion Network for Vehicle–Infrastructure Cooperative Target Detection Using LiDAR
PY  - 2024
T2  - Symmetry
DO  - 10.3390/sym16040401
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191591674&doi=10.3390%2fsym16040401&partnerID=40&md5=a7491e9d798c438d4682d1a2ca08fc10
AB  - With the development of autonomous driving, consensus is gradually forming around vehicle–infrastructure cooperative (VIC) autonomous driving. The VIC environment-sensing system uses roadside sensors in collaboration with automotive sensors to capture traffic target information symmetrically from both the roadside and the vehicle, thus extending the perception capabilities of autonomous driving vehicles. However, the current target detection accuracy for feature fusion based on roadside LiDAR and automotive LiDAR is relatively low, making it difficult to satisfy the sensing requirements of autonomous vehicles. This paper proposes PAFNet, a VIC pillar attention fusion network for target detection, aimed at improving LiDAR target detection accuracy under feature fusion. The proposed spatial and temporal cooperative fusion preprocessing method ensures the accuracy of the fused features through frame matching and coordinate transformation of the point cloud. In addition, this paper introduces the first anchor-free method for 3D target detection for VIC feature fusion, using a centroid-based approach for target detection. In the feature fusion stage, we propose the grid attention feature fusion method. This method uses the spatial feature attention mechanism to fuse the roadside and vehicle-side features. The experiment on the DAIR-V2X-C dataset shows that PAFNet achieved a 6.92% higher detection accuracy in 3D target detection than FFNet in urban scenes. © 2024 by the authors.
KW  - feature fusion
KW  - LiDAR
KW  - target detection
KW  - vehicle–infrastructure cooperative
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wei, J.
AU  - Che, K.
AU  - Gong, J.
AU  - Zhou, Y.
AU  - Lv, J.
AU  - Que, L.
AU  - Liu, H.
AU  - Len, Y.
TI  - Fast and Accurate Detection of Dim and Small Targets for Smart Micro-Light Sight
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13163301
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202677821&doi=10.3390%2felectronics13163301&partnerID=40&md5=37e1c7d35b58efc791831bf6e91f0736
AB  - To deal with low recognition accuracy and large time-consumption for dim, small targets in a smart micro-light sight, we propose a lightweight model DS_YOLO (dim and small target detection). We introduce the adaptive channel convolution module (ACConv) to reduce computational redundancy while maximizing the utilization of channel features. To address the misalignment problem in multi-task learning, we also design a lightweight dynamic task alignment detection head (LTD_Head), which utilizes GroupNorm to improve the performance of detection head localization and classification, and shares convolutions to make the model lightweight. Additionally, to improve the network’s capacity to detect small-scale targets while maintaining its generalization to multi-scale target detection, we extract high-resolution feature map information to establish a new detection head. Ultimately, the incorporation of the attention pyramid pooling layer (SPPFLska) enhances the model’s regression accuracy. We conduct an evaluation of the proposed algorithm DS_YOLO on four distinct datasets: CityPersons, WiderPerson, DOTA, and TinyPerson, achieving a 66.6% mAP on the CityPersons dataset, a 4.3% improvement over the original model. Meanwhile, our model reduces the parameter count by 33.3% compared to the baseline model. © 2024 by the authors.
KW  - dim and small target detection
KW  - lightweight
KW  - micro-light sight
KW  - task alignment
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, M.
AU  - Liu, Y.
AU  - Zhang, Z.
AU  - Guo, W.
TI  - RCRFNet: Enhancing Object Detection with Self-Supervised Radar–Camera Fusion and Open-Set Recognition
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24154803
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200794826&doi=10.3390%2fs24154803&partnerID=40&md5=fb04ab0dae71b137c09d26ebd35e3287
AB  - Robust object detection in complex environments, poor visual conditions, and open scenarios presents significant technical challenges in autonomous driving. These challenges necessitate the development of advanced fusion methods for millimeter-wave (mmWave) radar point cloud data and visual images. To address these issues, this paper proposes a radar–camera robust fusion network (RCRFNet), which leverages self-supervised learning and open-set recognition to effectively utilise the complementary information from both sensors. Specifically, the network uses matched radar–camera data through a frustum association approach to generate self-supervised signals, enhancing network training. The integration of global and local depth consistencies between radar point clouds and visual images, along with image features, helps construct object class confidence levels for detecting unknown targets. Additionally, these techniques are combined with a multi-layer feature extraction backbone and a multimodal feature detection head to achieve robust object detection. Experiments on the nuScenes public dataset demonstrate that RCRFNet outperforms state-of-the-art (SOTA) methods, particularly in conditions of low visual visibility and when detecting unknown class objects. © 2024 by the authors.
KW  - autonomous driving
KW  - open-set recognition
KW  - radar–camera fusion
KW  - self-supervised learning
KW  - target detection
KW  - Cameras
KW  - Feature extraction
KW  - Millimeter waves
KW  - Object detection
KW  - Object recognition
KW  - Supervised learning
KW  - Target tracking
KW  - Autonomous driving
KW  - Complex environments
KW  - Objects detection
KW  - Open-set recognition
KW  - Radar–camera fusion
KW  - Robust fusion
KW  - Robust object detection
KW  - Self-supervised learning
KW  - Targets detection
KW  - Visual image
KW  - adult
KW  - article
KW  - camera
KW  - diagnosis
KW  - feature detection
KW  - feature extraction
KW  - human
KW  - learning
KW  - male
KW  - retina image
KW  - sensor
KW  - telecommunication
KW  - visibility
KW  - visual disorder
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Hussein, M.A.M.
AU  - Habib, M.K.
TI  - Navigating the Future: Advancing Autonomous Vehicles through Robust Target Recognition and Real-Time Avoidance
PY  - 2024
T2  - Proceedings - 2024 4th International Conference on Control Theory and Applications, ICoCTA 2024
DO  - 10.1109/ICoCTA64736.2024.00070
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002296789&doi=10.1109%2fICoCTA64736.2024.00070&partnerID=40&md5=b90dd5192c4bfbb02a62d2f3c8ff23a6
AB  - This research addresses the critical challenge of enhancing object recognition and real-Time response capabilities in autonomous vehicles (AVs) under varying simulated conditions, which is crucial for ensuring both navigational safety and operational efficiency. Utilizing the Carla 0.9.14 simulator and Unreal Engine 4.26 on Ubuntu 20.04, we focus on improving the detection and classification of key on-road obstacles-vehicles, pedestrians, and cyclists-using the YOLOv7 object detection algorithm. By integrating advanced sensory technologies, specifically stereo vision cameras and LIDAR, we create a dynamic testing environment that simulates diverse urban and rural scenarios. Our methodology enhances the YOLOv7 algorithm's accuracy and speed through extensive training on a meticulously curated dataset of 4,113 images, reflecting a broad spectrum of environmental conditions, including varying lighting and weather conditions. This rigorous approach yielded a significant increase in mean average precision (mAP) from 64.3% to 76.3%, and enhanced the algorithm's reliability, with notable improvements over previous models. The research delineates a clear advancement in AV technology by demonstrating substantial improvements in object detection metrics, contributing foundational insights for future implementations in real-world settings and supporting the further development of real-Time avoidance systems. This study not only progresses the field of AV but also sets a new benchmark for object detection performance, aligning with industry and academic goals to optimize AV systems for complex and unpredictable driving scenarios.  © 2024 IEEE.
KW  - autonomous vehicles (AVs)
KW  - CARLA simulator
KW  - dynamic driving scenarios
KW  - enhanced object detection
KW  - LIDAR
KW  - object detection
KW  - object recognition accuracy
KW  - real-Time processing
KW  - simulation environments
KW  - stereo cameras
KW  - unreal engine
KW  - YOLOv7
KW  - Automobile driver simulators
KW  - Automobile simulators
KW  - Pedestrian safety
KW  - Stereo vision
KW  - Autonomous vehicle
KW  - Autonomous Vehicles
KW  - CARLA simulator
KW  - Dynamic driving scenario
KW  - Enhanced object detection
KW  - Object recognition accuracy
KW  - Objects detection
KW  - Objects recognition
KW  - Realtime processing
KW  - Recognition accuracy
KW  - Simulation environment
KW  - Stereo cameras
KW  - Unreal engine
KW  - YOLOv7
KW  - Stereo image processing
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cao, J.
AU  - Zhang, T.
AU  - Hou, L.
AU  - Nan, N.
TI  - An improved YOLOv8 algorithm for small object detection in autonomous driving
PY  - 2024
T2  - Journal of Real-Time Image Processing
DO  - 10.1007/s11554-024-01517-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199417065&doi=10.1007%2fs11554-024-01517-6&partnerID=40&md5=0eccb513c817baef2a4524eb22843c50
AB  - In the task of visual object detection for autonomous driving, several challenges arise, such as detecting densely clustered targets, dealing with significant occlusion, and identifying small-sized targets. To address these challenges, an improved YOLOv8 algorithm for small object detection in autonomous driving (MSD-YOLO) is proposed. This algorithm incorporates several enhancements to improve the performance of detecting small and densely occluded targets. Firstly, the downsampling module is replaced with SPD-CBS (Space-to-Depth) to maintain the integrity of channel feature information. Subsequently, a multi-scale small object detection structure is designed to increase sensitivity for recognizing densely packed small objects. Additionally, DyHead (Dynamic Head) is introduced, equipped with simultaneous scale, spatial, and channel attention to ensure comprehensive perception of feature map information. In the post-processing stage, Soft-NMS (non-maximum suppression) is employed to effectively suppress redundant candidate boxes and reduce the missed detection rate of densely occluded targets. The effectiveness of these enhancements has been verified through various experiments conducted on the BDD100K autonomous driving public dataset. Experimental results indicate a significant improvement in the performance of the enhanced network. Compared to the YOLOv8n baseline model, MSD-YOLO shows a 13.7% increase in mAP50 and a 12.1% increase in mAP50:95, with only a slight increase in the number of parameters. Furthermore, the detection speed can reach 67.6 FPS, achieving a better balance between accuracy and speed. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.
KW  - DyHead
KW  - Small object detection
KW  - Soft-NMS
KW  - SPD-CBS
KW  - YOLOv8
KW  - Electric circuit breakers
KW  - Object detection
KW  - Object recognition
KW  - Autonomous driving
KW  - Down sampling
KW  - Dynamic head
KW  - Objects detection
KW  - Performance
KW  - Small object detection
KW  - Soft-NMS
KW  - SPD-CBS
KW  - Visual objects
KW  - YOLOv8
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ding, M.
AU  - Zhou, W.
AU  - Xu, Y.
AU  - Xu, Y.
TI  - Two-Stage Framework for Specialty Vehicles Detection and Classification: Toward Intelligent Visual Surveillance of Airport Surface
PY  - 2024
T2  - IEEE Transactions on Aerospace and Electronic Systems
DO  - 10.1109/TAES.2023.3342797
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180308678&doi=10.1109%2fTAES.2023.3342797&partnerID=40&md5=cf27d72440e5b5fcdad65f36d2f8760b
AB  - Intelligent visual surveillance (IVS) is being gradually introduced into the field of airport surface surveillance. The first task of IVS is to detect and recognize objects moving on the airport surface. Specialty vehicles play a considerable role in airport ground handling processes and are considerably monitored targets. Because specialty vehicles have diverse appearances and irregular shapes, pixel-level detection would enable them to be targeted more accurately. Specialty vehicles on the surface undertake different jobs in airport ground handling processes, and therefore subcategory classification would more precisely determine the function of these specialty vehicles. Moreover, pixel-level detection and subcategory classification are very useful for detecting key milestone nodes of airport ground handling processes. Thus, in this article, a two-stage framework for specialty vehicle pixel-level detection and subcategory classification for IVS of the airport surface is exploited, which seamlessly integrates state-of-the-art algorithms and techniques, and consists of two segmentation stages (coarse mask generation and refined mask generation). Furthermore, to evaluate related methods, a dataset of airport surface specialty vehicles is established, which contains four types of representative specialty vehicles and corresponding accurate mask labels. All samples in the dataset were captured from surveillance videos of civil airports. Experimental results on the dataset clearly demonstrate that the proposed framework performed favorably compared with the classic instance segmentation methods and achieved pixel-level detection and subcategory classification of specialty vehicles for airport surface surveillance.  © 1965-2011 IEEE.
KW  - Aircraft detection
KW  - Airport security
KW  - Airports
KW  - Classification (of information)
KW  - Object recognition
KW  - Pixels
KW  - Security systems
KW  - Surface treatment
KW  - Vehicles
KW  - Airport ground handling
KW  - Airport surfaces
KW  - Category Classification
KW  - Handling process
KW  - Instance segmentation
KW  - Intelligent visual surveillances
KW  - Objects detection
KW  - Pixel level detection
KW  - Surveillance
KW  - Task analysis
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sheng, X.
AU  - Li, S.
AU  - Qu, J.
AU  - Liu, L.
TI  - 3D Object Detection Algorithm Based on Improved YOLOv5
ST  - 基于改进 YOLOv5 的三维目标检测算法
PY  - 2024
T2  - Laser and Optoelectronics Progress
DO  - 10.3788/LOP240451
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217919454&doi=10.3788%2fLOP240451&partnerID=40&md5=e3f064e67941c87a6dc4361cfbb89d18
AB  - To address the challenge of handling large volumes of point cloud data for three-dimensional (3D) object detection and the limited effectiveness in detecting small objects, in this study, an enhanced 3D target detection method is proposed that improves the YOLOv5 network based on the idea of Complex-YOLO algorithm. The proposed approach first tackles the issue of lengthy processing times due to extensive point cloud data by adopting the Complex-YOLO strategy of converting point cloud data into an RGB-Map format, which is more manageable for the YOLOv5 network. Enhancements to YOLOv5 include an angle prediction branch and a rotation frame regression loss function to accurately position rotating targets within the RGB-Map. Additionally, the YOLOv5 architecture is modified to better detect small objects by incorporating a feature fusion layer and a dedicated prediction head, which heightens the network’s sensitivity to smaller targets. Furthermore, the convolutional block attention module (CBAM) attention mechanism is integrated into the network’s neck to further enhance detection sensitivity. Experimental evaluations on the KITTI dataset confirm the superiority of the modified YOLOv5 method over the original Complex-YOLO, with improvements in mean average precision (mAP): Car type mAP increased by 7. 48 percentage points, Pedestrian type by 12. 54 percentage points, Cyclist type by 1. 2 percentage points, and an overall increase of 7. 08 percentage points across all categories, demonstrating the effectiveness of this algorithm. © 2024 Universitat zu Koln. All rights reserved.
KW  - attention mechanism
KW  - Complex-YOLO
KW  - small target detection
KW  - three-dimensional object detection
KW  - YOLOv5
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yao, S.
AU  - Guan, R.
AU  - Huang, X.
AU  - Li, Z.
AU  - Sha, X.
AU  - Yue, Y.
AU  - Lim, E.G.
AU  - Seo, H.
AU  - Man, K.L.
AU  - Zhu, X.
AU  - Yue, Y.
TI  - Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review
PY  - 2024
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2023.3307157
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168687796&doi=10.1109%2fTIV.2023.3307157&partnerID=40&md5=e6bc7030e2fc180c92fd35a1fcd0bcb5
AB  - Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including 'why to fuse', 'what to fuse', 'where to fuse', 'when to fuse', and 'how to fuse', subsequently discussing various challenges and potential research directions within this domain. To ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: https://radar-camera-fusion.github.io.  © 2016 IEEE.
KW  - Autonomous driving
KW  - object detection
KW  - radar-camera fusion
KW  - semantic segmentation
KW  - Cameras
KW  - Cost effectiveness
KW  - Data handling
KW  - Deep learning
KW  - Object detection
KW  - Object recognition
KW  - Radar antennas
KW  - Radar cross section
KW  - Semantics
KW  - Tracking radar
KW  - Autonomous driving
KW  - Learning techniques
KW  - Object semantic
KW  - Objects detection
KW  - Perception capability
KW  - Radar cross-sections
KW  - Radar-camera fusion
KW  - Radars antennas
KW  - Semantic segmentation
KW  - Surrounding environment
KW  - Radar imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, X.
AU  - Wang, C.
AU  - Liu, C.
AU  - Zhu, X.
AU  - Zhang, Y.
AU  - Luo, T.
AU  - Zhang, J.
TI  - Autonomous Crack Detection for Mountainous Roads Using UAV Inspection System
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24144751
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199755540&doi=10.3390%2fs24144751&partnerID=40&md5=507d034d0a099b370a2e580547521b6c
AB  - Road cracks significantly affect the serviceability and safety of roadways, especially in mountainous terrain. Traditional inspection methods, such as manual detection, are excessively time-consuming, labor-intensive, and inefficient. Additionally, multi-function detection vehicles equipped with diverse sensors are costly and unsuitable for mountainous roads, primarily because of the challenging terrain conditions characterized by frequent bends in the road. To address these challenges, this study proposes a customized Unmanned Aerial Vehicle (UAV) inspection system designed for automatic crack detection. This system focuses on enhancing autonomous capabilities in mountainous terrains by incorporating embedded algorithms for route planning, autonomous navigation, and automatic crack detection. The slide window method (SWM) is proposed to enhance the autonomous navigation of UAV flights by generating path planning on mountainous roads. This method compensates for GPS/IMU positioning errors, particularly in GPS-denied or GPS-drift scenarios. Moreover, the improved MRC-YOLOv8 algorithm is presented to conduct autonomous crack detection from UAV imagery in an on/offboard module. To validate the performance of our UAV inspection system, we conducted multiple experiments to evaluate its accuracy, robustness, and efficiency. The results of the experiments on automatic navigation demonstrate that our fusion method, in conjunction with SWM, effectively enables real-time route planning in GPS-denied mountainous terrains. The proposed system displays an average localization drift of 2.75% and a per-point local scanning error of 0.33 m over a distance of 1.5 km. Moreover, the experimental results on the road crack detection reveal that the MRC-YOLOv8 algorithm achieves an F1-Score of 87.4% and a mAP of 92.3%, thus surpassing other state-of-the-art models like YOLOv5s, YOLOv8n, and YOLOv9 by 1.2%, 1.3%, and 3.0% in terms of mAP, respectively. Furthermore, the parameters of the MRC-YOLOv8 algorithm indicate a volume reduction of 0.19(×106) compared to the original YOLOv8 model, thus enhancing its lightweight nature. The UAV inspection system proposed in this study serves as a valuable tool and technological guidance for the routine inspection of mountainous roads. © 2024 by the authors.
KW  - mountainous road
KW  - MRC-YOLOv8
KW  - pavement crack detection
KW  - SWM
KW  - UAV inspection system
KW  - Aircraft detection
KW  - Antennas
KW  - Crack detection
KW  - Image enhancement
KW  - Motion planning
KW  - Navigation
KW  - Roads and streets
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Inspection system
KW  - Mountainoi road
KW  - MRC-YOLOv8
KW  - Pavement crack detection
KW  - Slide window method
KW  - Slide windows
KW  - Unmanned aerial vehicle inspection system
KW  - Vehicle inspections
KW  - Window methods
KW  - algorithm
KW  - article
KW  - diagnosis
KW  - global navigation satellite system
KW  - imagery
KW  - sensor
KW  - unmanned aerial vehicle
KW  - Global positioning system
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Burgos, D.C.
AU  - Iztueta, E.J.
AU  - Ormaechea, I.M.
AU  - Martínez-Otzeta, J.M.
AU  - Mugica, A.C.
TI  - Deep Learning-Based Traffic Light Detection in a Custom Embedded Hardware Platform for ADAS Applications
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3452608
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203406014&doi=10.1109%2fACCESS.2024.3452608&partnerID=40&md5=c85a32d6d10d8c07785bcdab50c2e4fa
AB  - Automotive Driver Assistance Systems (ADAS) applications are currently an intensive field of study and innovation. The development of an ADAS is a multidisciplinary task involving electronic hardware design, advanced software implementation, safety considerations and many more. Building an ADAS application implies some challenges that are addressed in this paper. Firstly, all ADAS applications run on highly specific hardware devices embedded in the car with limited computation resources. In this work a novel embedded platform, iADASys, is developed and tested. The platform integrates the elements required to implement an artificial vision based ADAS application such as high performance processor with Deep Learning (DL) computation co-processors or multi-channel high resolution video streaming hardware. Secondly, this work implements an artificial vision application for traffic light detection based on deep neural networks. The model selected in this work is SSD_Mobilenet_V1 and it was trained using Bosch Small Traffic Light (BSTL) dataset. To fulfill real time requirement, the model image input resolution was maintained low at 300 × 300 pixel. However, the small object size in the dataset together with low resolution lead to poor detection performance. This situation was addressed by fine tuning the model training hyperparameters related to detection scales and aspect ratios. Lastly, the model is deployed in the hardware platform and its performance is measured. Model inference is executed on a specialized mathematical co-processor obtaining the required real time response. The object detection performance is also measured, obtaining promising results. © 2013 IEEE.
KW  - ADAS
KW  - deep learning
KW  - neural network hardware
KW  - object detection
KW  - Advanced driver assistance systems
KW  - Deep neural networks
KW  - Digital storage
KW  - Embedded software
KW  - Laser beams
KW  - Automotive driver assistance systems
KW  - Co-processors
KW  - Deep learning
KW  - Detection performance
KW  - Hardware platform
KW  - Light detection
KW  - Neural network hardware
KW  - Objects detection
KW  - System applications
KW  - Traffic light
KW  - Video streaming
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Karim, A.
AU  - Raza, M.A.
AU  - Alharthi, Y.Z.
AU  - Abbas, G.
AU  - Othmen, S.
AU  - Hossain, M.S.
AU  - Nahar, A.
AU  - Mercorelli, P.
TI  - Visual Detection of Traffic Incident through Automatic Monitoring of Vehicle Activities
PY  - 2024
T2  - World Electric Vehicle Journal
DO  - 10.3390/wevj15090382
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205228215&doi=10.3390%2fwevj15090382&partnerID=40&md5=a3d34d8b472b3894c5c0a644bdd923de
AB  - Intelligent transportation systems (ITSs) derive significant advantages from advanced models like YOLOv8, which excel in predicting traffic incidents in dynamic urban environments. Roboflow plays a crucial role in organizing and preparing image data essential for computer vision models. Initially, a dataset of 1000 images is utilized for training, with an additional 500 images reserved for validation purposes. Subsequently, the Deep Simple Online and Real-time Tracking (Deep-SORT) algorithm enhances scene analyses over time, offering continuous monitoring of vehicle behavior. Following this, the YOLOv8 model is deployed to detect specific traffic incidents effectively. By combining YOLOv8 with Deep SORT, urban traffic patterns are accurately detected and analyzed with high precision. The findings demonstrate that YOLOv8 achieves an accuracy of 98.4%, significantly surpassing alternative methodologies. Moreover, the proposed approach exhibits outstanding performance in the recall (97.2%), precision (98.5%), and F1 score (95.7%), underscoring its superior capability in accurate prediction and analyses of traffic incidents with high precision and efficiency. © 2024 by the authors.
KW  - object detection
KW  - object tracking
KW  - sustainable transportation
KW  - traffic incident
KW  - Urban transportation
KW  - Vehicle detection
KW  - Advanced modeling
KW  - Automatic monitoring
KW  - High-precision
KW  - Intelligent transportation systems
KW  - Object Tracking
KW  - Objects detection
KW  - Sustainable transportation
KW  - Traffic incidents
KW  - Vehicle activity
KW  - Visual detection
KW  - Intelligent systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Paredes, J.A.
AU  - Hansard, M.
AU  - Rajab, K.Z.
AU  - Alvarez, F.J.
TI  - Spatial Calibration of Millimeter-Wave Radar for Close-Range Object Location
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3393030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192210881&doi=10.1109%2fJSEN.2024.3393030&partnerID=40&md5=3d920b1b0cf227a7f3bde4adcd1dcb3d
AB  - Accurate object detection and location systems are essential for many robotic applications, including autonomous grasping and manipulation systems. In some cases, the target object may be obscured from view, in clutter, packaging, or debris. Millimeter-wave radar (mmWave) is a potential alternative to visual sensing in such scenarios, owing to its ability to penetrate typical low-density non-metallic materials. However, this approach requires accurate spatial calibration of the radar signal, over the robot workspace. We propose to achieve this with reference to visual data, which provides ground-truth locations for initial training of the system. Specifically, we describe a commodity mmWave radar system for detecting and localizing static metallic objects, over a 2-D workspace. We compare similarity, affine, and thin-plate spline (TPS) models of the spatial transformation from radar estimates to actual locations. Experiments were performed with a frequency modulated continuous wave (FMCW) multiple-input multiple-output (MIMO) device, using a starting frequency of 60 GHz and a bandwidth of 3.4 GHz. It is shown that the spline model performs best, achieving an average spatial error of 7 mm, which is an order of magnitude lower than that of the uncalibrated system.  © 2001-2012 IEEE.
KW  - Mapping methods
KW  - millimeter-wave (mmWave) radar
KW  - RGB camera
KW  - spatial calibration
KW  - Frequency modulation
KW  - Location
KW  - Millimeter waves
KW  - MIMO radar
KW  - MIMO systems
KW  - Object detection
KW  - Object recognition
KW  - Radar imaging
KW  - Tracking radar
KW  - Close range
KW  - Mapping method
KW  - Millimeter-wave radar
KW  - Millimeterwave communications
KW  - Millimetre-wave radar
KW  - Mm waves
KW  - Mmwave radar
KW  - Object location
KW  - RGB cameras
KW  - Spatial calibration
KW  - Calibration
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Q.
AU  - Wang, J.
AU  - Wang, X.
AU  - Wu, L.
AU  - Feng, K.
AU  - Wang, G.
TI  - A YOLOv7-Based Method for Ship Detection in Videos of Drones
PY  - 2024
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse12071180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199609922&doi=10.3390%2fjmse12071180&partnerID=40&md5=2652778bfd0280ade6550aad845be8e2
AB  - With the rapid development of the shipping industry, the number of ships is continuously increasing, and maritime accidents happen frequently. In recent years, computer vision and drone flight control technology have continuously developed, making drones widely used in related fields such as maritime target detection. Compared to the cameras fixed on ships, a greater flexibility and a wider field of view is provided by cameras equipped on drones. However, there are still some challenges in high-altitude detection with drones. Firstly, from a top-down view, the shapes of ships are very different from ordinary views. Secondly, it is difficult to achieve faster detection speeds because of limited computing resources. To solve these problems, we propose YOLOv7-DyGSConv, a deep learning-based model for detecting ships in real-time videos captured by drones. The model is built on YOLOv7 with an attention mechanism, which enhances the ability to capture targets. Furthermore, the Conv in the Neck of the YOLOv7 model is replaced with the GSConv, which reduces the complexity of the model and improves the detection speed and detection accuracy. In addition, to compensate for the scarcity of ship datasets in top-down views, a ship detection dataset containing 2842 images taken by drones or with a top-down view is constructed in the research. We conducted experiments on our dataset, and the results showed that the proposed model reduced the parameters by 16.2%, the detection accuracy increased by 3.4%, and the detection speed increased by 13.3% compared with YOLOv7. © 2024 by the authors.
KW  - deep learning
KW  - drones
KW  - object detection
KW  - ship detection
KW  - YOLOv7
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, R.
AU  - Wang, K.
AU  - Xiao, Y.
AU  - Gao, F.
AU  - Gao, Z.
TI  - Leveraging Monte Carlo Dropout for Uncertainty Quantification in Real-Time Object Detection of Autonomous Vehicles
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3355199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182926594&doi=10.1109%2fACCESS.2024.3355199&partnerID=40&md5=800e400e02689672f57a77d5198414f2
AB  - With the recent advancements in machine learning technology, the accuracy of autonomous driving object detection models has significantly improved. However, due to the complexity and variability of real-world traffic scenarios, such as extreme weather conditions, unconventional lighting, and unknown traffic participants, there is inherent uncertainty in autonomous driving object detection models, which may affect the planning and control in autonomous driving. Thus, the rapid and accurate quantification of this uncertainty is crucial. It contributes to a better understanding of the intentions of autonomous vehicles and strengthens trust in autonomous driving technology. This research pioneers in quantifying uncertainty in the YOLOv5 object detection model, thereby improving the accuracy and speed of probabilistic object detection, and addressing the real-time operational constraints of current models in autonomous driving contexts. Specifically, a novel probabilistic object detection model named M-YOLOv5 is proposed, which employs the MC-drop method to capture discrepancies between detection results and the real world. These discrepancies are then converted into Gaussian parameters for class scores and predicted bounding box coordinates to quantify uncertainty. Moreover, due to the limitations of the Mean Average Precision (MAP) evaluation metric, we introduce a new measure, Probability-based Detection Quality (PDQ), which is incorporated as a component of the loss function. This metric simultaneously assesses the quality of label uncertainty and positional uncertainty. Experiments demonstrate that compared to the original YOLOv5 algorithm, the M-YOLOv5 algorithm shows a 74.7% improvement in PDQ. When compared with the most advanced probabilistic object detection models targeting the MS COCO dataset, M-YOLOv5 achieves a 14% increase in MAP, a 17% increase in PDQ, and a 65% improvement in FPS. Furthermore, against the state-of-the-art probabilistic object detection models for the BDD100K dataset, M-YOLOv5 exhibits a 31.67% enhancement in MAP and a 125.6% increase in FPS.  © 2013 IEEE.
KW  - autonomous vehicles
KW  - Monte Carlo dropout
KW  - object detection
KW  - Uncertainty quantification
KW  - YOLOv5
KW  - Interactive computer systems
KW  - Learning systems
KW  - Monte Carlo methods
KW  - Object detection
KW  - Object recognition
KW  - Probabilistic logics
KW  - Quality control
KW  - Uncertainty analysis
KW  - Vehicles
KW  - Autonomous Vehicles
KW  - Computational modelling
KW  - Monte carlo dropout
KW  - Objects detection
KW  - Real - Time system
KW  - Transformer
KW  - Uncertainty
KW  - Uncertainty quantifications
KW  - YOLO
KW  - YOLOv5
KW  - Real time systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hua, C.
AU  - Luo, K.
AU  - Wu, Y.
AU  - Shi, R.
TI  - YOLO-ABD: A Multi-Scale Detection Model for Pedestrian Anomaly Behavior Detection
PY  - 2024
T2  - Symmetry
DO  - 10.3390/sym16081003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202483006&doi=10.3390%2fsym16081003&partnerID=40&md5=1fe254c0c529e6f29874e12650cc2b0c
AB  - Public safety and intelligent surveillance systems rely on anomaly detection for effective monitoring. In real-world pedestrian detection scenarios, Pedestrians often exhibit various symmetrical features such as body contours, facial features, posture, and clothing. However, the accuracy of pedestrian anomaly detection is affected by factors such as complex backgrounds, pedestrian obstruction, and small target sizes. To address these issues, this study introduced YOLO-ABD, a lightweight method for anomaly behavior detection that integrated small object detection and channel shuffling. This approach enhanced the YOLOv8n baseline model by integrating a small-object detection mechanism at the head and employing the symmetric GSConv convolutional module in the backbone network to improve perceptual capabilities. Furthermore, it incorporated the SimAM attention mechanism to mitigate complex background interference and thus enhance target detection performance. Evaluation on the IITB-Corridor dataset showed mAP50 and mAP50-95 scores of 89.3% and 60.6%, respectively. Generalization testing on the street-view-gdogo dataset further underscored the superiority of YOLO-ABD over advanced detection algorithms, demonstrating its effectiveness and generalization capabilities. With relatively fewer parameters, YOLO-ABD provided an excellent lightweight solution for pedestrian anomaly detection. © 2024 by the authors.
KW  - lightweight surveillance systems
KW  - pedestrian anomaly detection
KW  - SimAM attention mechanism
KW  - small object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Du, Y.
AU  - Liu, X.
AU  - Yi, Y.
AU  - Wei, K.
TI  - Incorporating bidirectional feature pyramid network and lightweight network: a YOLOv5-GBC distracted driving behavior detection model
PY  - 2024
T2  - Neural Computing and Applications
DO  - 10.1007/s00521-023-09043-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173937507&doi=10.1007%2fs00521-023-09043-5&partnerID=40&md5=161029716b3aeb080f815540322d75d6
AB  - Distracted driving is one of the leading causes of traffic accidents and has become a bottleneck for improving driver assistance technologies. It is still a challenge to detect distracted driving behavior in real-life scenarios, which have the features of complex backgrounds, different target scales, and resolutions. In this context, a lightweight YOLOv5-GBC model is proposed for real-time distracted driving detection in this work. Firstly, the lightweight network GhostConv is used to perform lightweight operations on the convolutional layers, aiming to reduce a large number of parameters and computations. Secondly, the path aggregation network structure is improved to enhance the model fusion ability for different scale features, and coordinated attention is introduced to enhance the model extraction ability for effective information. The proposed YOLOv5-GBC model can predict different types of distracted driving. Finally, this work conducts extensive experiments; the results show that the proposed model has a mean accuracy (mAP) of 91.8%, which is 3.9% better than the baseline model, with a reduction of 6.5% and 9.1% in the weight file and Floating-point Operations Per Second, respectively. It outperforms the models of Faster-RCNN, SSD, YOLOv3-tiny, and YOLOv4-tiny, which indicates that the proposed model can identify distracted driving behaviors efficiently and rapidly. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023.
KW  - Advanced driving assistance system
KW  - BiFPN
KW  - Distracted driving
KW  - GhostConv
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Behavioral research
KW  - Digital arithmetic
KW  - Advanced driving assistance system
KW  - Behavior detection
KW  - Detection models
KW  - Distracted driving
KW  - Driver assistance
KW  - Driving assistance systems
KW  - Driving behaviour
KW  - Feature pyramid
KW  - Ghostconv
KW  - Pyramid network
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Z.
AU  - Yang, Y.
AU  - Xu, X.
AU  - Liu, L.
AU  - Yue, J.
AU  - Ding, R.
AU  - Lu, Y.
AU  - Liu, J.
AU  - Qiao, H.
TI  - GVC-YOLO: A Lightweight Real-Time Detection Method for Cotton Aphid-Damaged Leaves Based on Edge Computing
PY  - 2024
T2  - Remote Sensing
DO  - 10.3390/rs16163046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202429577&doi=10.3390%2frs16163046&partnerID=40&md5=69d89df8edbc47066d0fa56b01a6d3c3
AB  - Cotton aphids (Aphis gossypii Glover) pose a significant threat to cotton growth, exerting detrimental effects on both yield and quality. Conventional methods for pest and disease surveillance in agricultural settings suffer from a lack of real-time capability. The use of edge computing devices for real-time processing of cotton aphid-damaged leaves captured by field cameras holds significant practical research value for large-scale disease and pest control measures. The mainstream detection models are generally large in size, making it challenging to achieve real-time detection on edge computing devices with limited resources. In response to these challenges, we propose GVC-YOLO, a real-time detection method for cotton aphid-damaged leaves based on edge computing. Building upon YOLOv8n, lightweight GSConv and VoVGSCSP modules are employed to reconstruct the neck and backbone networks, thereby reducing model complexity while enhancing multiscale feature fusion. In the backbone network, we integrate the coordinate attention (CA) mechanism and the SimSPPF network to increase the model’s ability to extract features of cotton aphid-damaged leaves, balancing the accuracy loss of the model after becoming lightweight. The experimental results demonstrate that the size of the GVC-YOLO model is only 5.4 MB, a decrease of 14.3% compared with the baseline network, with a reduction of 16.7% in the number of parameters and 17.1% in floating-point operations (FLOPs). The mAP@0.5 and mAP@0.5:0.95 reach 97.9% and 90.3%, respectively. The GVC-YOLO model is optimized and accelerated by TensorRT and then deployed onto the embedded edge computing device Jetson Xavier NX for detecting cotton aphid damage video captured from the camera. Under FP16 quantization, the detection speed reaches 48 frames per second (FPS). In summary, the proposed GVC-YOLO model demonstrates good detection accuracy and speed, and its performance in detecting cotton aphid damage in edge computing scenarios meets practical application needs. This research provides a convenient and effective intelligent method for the large-scale detection and precise control of pests in cotton fields. © 2024 by the authors.
KW  - cotton aphid
KW  - edge computing
KW  - real-time video detection
KW  - YOLOv8
KW  - Edge computing
KW  - Insect control
KW  - Photomapping
KW  - Computing devices
KW  - Cotton aphid
KW  - Detection methods
KW  - Edge computing
KW  - Large-scales
KW  - Real time videos
KW  - Real-time detection
KW  - Real-time video detection
KW  - Video detection
KW  - YOLOv8
KW  - Cotton
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Baek, H.
AU  - Yu, S.
AU  - Son, S.
AU  - Seo, J.
AU  - Chung, Y.
TI  - Automated Region of Interest-Based Data Augmentation for Fallen Person Detection in Off-Road Autonomous Agricultural Vehicles
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24072371
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190233568&doi=10.3390%2fs24072371&partnerID=40&md5=c980107bcfc6ffbacfab8cd2a41724d7
AB  - Due to the global population increase and the recovery of agricultural demand after the COVID-19 pandemic, the importance of agricultural automation and autonomous agricultural vehicles is growing. Fallen person detection is critical to preventing fatal accidents during autonomous agricultural vehicle operations. However, there is a challenge due to the relatively limited dataset for fallen persons in off-road environments compared to on-road pedestrian datasets. To enhance the generalization performance of fallen person detection off-road using object detection technology, data augmentation is necessary. This paper proposes a data augmentation technique called Automated Region of Interest Copy-Paste (ARCP) to address the issue of data scarcity. The technique involves copying real fallen person objects obtained from public source datasets and then pasting the objects onto a background off-road dataset. Segmentation annotations for these objects are generated using YOLOv8x-seg and Grounded-Segment-Anything, respectively. The proposed algorithm is then applied to automatically produce augmented data based on the generated segmentation annotations. The technique encompasses segmentation annotation generation, Intersection over Union-based segment setting, and Region of Interest configuration. When the ARCP technique is applied, significant improvements in detection accuracy are observed for two state-of-the-art object detectors: anchor-based YOLOv7x and anchor-free YOLOv8x, showing an increase of 17.8% (from 77.8% to 95.6%) and 12.4% (from 83.8% to 96.2%), respectively. This suggests high applicability for addressing the challenges of limited datasets in off-road environments and is expected to have a significant impact on the advancement of object detection technology in the agricultural industry. © 2024 by the authors.
KW  - automated region of interest
KW  - autonomous agricultural vehicles
KW  - data augmentation
KW  - fallen person detection
KW  - Agriculture
KW  - Algorithms
KW  - Automation
KW  - Humans
KW  - Pandemics
KW  - Technology
KW  - Accidents
KW  - Agriculture
KW  - Autonomous vehicles
KW  - COVID-19
KW  - Image segmentation
KW  - Object recognition
KW  - Roads and streets
KW  - Automated region of interest
KW  - Autonomous agricultural vehicles
KW  - Data augmentation
KW  - Detection technology
KW  - Fallen person detection
KW  - Objects detection
KW  - Person detection
KW  - Region-of-interest
KW  - Regions of interest
KW  - Road environment
KW  - agriculture
KW  - algorithm
KW  - automation
KW  - human
KW  - pandemic
KW  - technology
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Çelik, M.T.
AU  - Arslankaya, S.
AU  - Yildiz, A.
TI  - Real-time detection of plastic part surface defects using deep learning- based object detection model
PY  - 2024
T2  - Measurement: Journal of the International Measurement Confederation
DO  - 10.1016/j.measurement.2024.114975
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194278180&doi=10.1016%2fj.measurement.2024.114975&partnerID=40&md5=e103bb4b14945ec4775fba7f0e2fe98e
AB  - In this study, it was aimed to detect defects in plastic parts produced in a company operating in the automotive sub-industry using the YOLOv8 object detection model. The defect types seen in plastic parts were evaluated with the help of Pareto analysis, and scratches, stains and shine were selected as the most common defect types, and data on the three defect types were collected. YOLOv8 models were trained using faulty part images. As a result of the training, the highest mean average precision value of 0.990 was obtained in the YOLOv8s model, and the shortest training time was obtained in the YOLOv8n model. In the YOLOv8s model, which gave the highest mAP value, hyperparameter adjustment was made according to the batch size and learning rate values. The testing phase was carried out with the hyperparameter values that gave the best results and the mAP value was obtained as 0.902. © 2024 Elsevier Ltd
KW  - Artificial intelligence
KW  - Deep learning
KW  - Defect detection
KW  - Quality control
KW  - You-Only-Look-Once version 8
KW  - Deep learning
KW  - Object detection
KW  - Object recognition
KW  - Quality control
KW  - Signal detection
KW  - Surface defects
KW  - Deep learning
KW  - Defect detection
KW  - Defect type
KW  - Detection models
KW  - Hyper-parameter
KW  - Objects detection
KW  - Part surface
KW  - Plastics parts
KW  - Real-time detection
KW  - You-only-look-once version 8
KW  - Plastic parts
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, Z.
AU  - Wang, C.
AU  - Huang, K.
TI  - BiF-DETR:Remote sensing object detection based on Bidirectional information fusion
PY  - 2024
T2  - Displays
DO  - 10.1016/j.displa.2024.102802
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199145525&doi=10.1016%2fj.displa.2024.102802&partnerID=40&md5=e9802c6f6776fb381e06158250c121ef
AB  - Remote Sensing Object Detection(RSOD) is a fundamental task in the field of remote sensing image processing. The complexity of the background, the diversity of object scales and the locality limitation of Convolutional Neural Network (CNN) present specific challenges for RSOD. In this paper, an innovative hybrid detector, Bidirectional Information Fusion DEtection TRansformer (BiF-DETR), is proposed to mitigate the above issues. Specifically, BiF-DETR takes anchor-free detection network, CenterNet, as the baseline, designs the feature extraction backbone in parallel, extracts the local feature details using CNNs, and obtains the global information and long-term dependencies using Transformer branch. A Bidirectional Information Fusion (BIF) module is elaborately designed to reduce the semantic differences between different styles of feature maps through multi-level iterative information interactions, fully utilizing the complementary advantages of different detectors. Additionally, Coordination Attention(CA), is introduced to enables the detection network to focus on the saliency information of small objects. To address diversity insufficiency of remote sensing images in the training stage, Cascade Mixture Data Augmentation (CMDA), is designed to improve the robustness and generalization ability of the model. Comparative experiments with other cutting-edge methods are conducted on the publicly available DOTA and NWPU VHR-10 datasets. The experimental results reveal that the performance of proposed method is state-of-the-art, with mAP reaching 77.43% and 94.75%, respectively, far exceeding the other 25 competitive methods. © 2024 The Author(s)
KW  - Anchor-free detector
KW  - Feature fusion
KW  - Remote sensing object detection(RSOD)
KW  - Visual transformer
KW  - Convolutional neural networks
KW  - Feature extraction
KW  - Image enhancement
KW  - Information fusion
KW  - Iterative methods
KW  - Object recognition
KW  - Remote sensing
KW  - Semantics
KW  - Anchor-free
KW  - Anchor-free detector
KW  - Convolutional neural network
KW  - Detection networks
KW  - Features fusions
KW  - Objects detection
KW  - Remote sensing image processing
KW  - Remote sensing object detection
KW  - Remote-sensing
KW  - Visual transformer
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rakhmonov, A.A.U.
AU  - Subramanian, B.
AU  - Varnousefaderani, B.A.
AU  - Kim, J.
TI  - Aed-net: Attention-based detection model for disabled signage detection
PY  - 2024
T2  - Journal of Korean Institute of Communications and Information Sciences
DO  - 10.7840/kics.2024.49.7.976
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203705215&doi=10.7840%2fkics.2024.49.7.976&partnerID=40&md5=e0bf7601cd8de6d34b580e114e92d697
AB  - The aim of having designated parking spaces for individuals with disabilities is to ensure that only vehicles with proper handicapped signage use them, while preventing unauthorized vehicles from occupying those spaces. To achieve this, real-time monitoring is essential. Existing two-stage object detection models suffer from slow image processing and enhanced backbones with feature pyramid networks are also burdened with expanded parameters. While YOLOv5 model is a compelling choice due to its superior speed and performance compared to existing models. Therefore, this study proposes to make certain modifications to a baseline YOLOv5 model. Instead of the original 9 blocks in the backbone and 4 C3 blocks, we propose to replace them with 6 and 4 EfficientNet blocks, accordingly. These EfficientNet blocks have fewer parameters but still offer higher accuracy in detecting disabled signs, among other types of signs on car windshields. To make up for the reduced number of blocks, we have incorporated an attention mechanism into the proposed architecture before the detection phase. This mechanism enables the model to focus on the crucial regions required for the task. Furthermore, we propose utilizing a more advanced optimizer called AdamW to prevent overfitting. With these enhancements, a novel object detector, attention-based efficient detection model (AED-Net) is proposed. To assess the effectiveness of the proposed approach, we will gather and label a dataset comprising images of cars displaying disabled signage on their windshields. Experiments conducted using this dataset demonstrate that the proposed model achieves a superior F1 score of 0.73 compared to that of baseline model, 0.57. The proposed model utilizes 10 percent fewer parameters compared to the baseline model. © 2024, Korean Institute of Communications and Information Sciences. All rights reserved.
KW  - Depthwise Separable Convolution
KW  - Disabled Signage
KW  - Small Object Detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Soumya, A.
AU  - Mohan, C.K.
AU  - Cenkeramaddi, L.R.
TI  - High Precision Single Shot Object Detection in Automotive Scenarios
PY  - 2024
T2  - Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
DO  - 10.5220/0012383100003660
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192136235&doi=10.5220%2f0012383100003660&partnerID=40&md5=85b9eb22de71e38b1adc623633c654ca
AB  - Object detection in low-light scenarios is a challenging task with numerous real-world applications, ranging from surveillance and autonomous vehicles to augmented reality. However, due to reduced visibility and limited information in the image data, carrying out object detection in low-lighting settings brings distinct challenges. This paper introduces a novel object detection model designed to excel in low-light imaging conditions, prioritizing inference speed and accuracy. The model leverages advanced deep-learning techniques and is optimized for efficient inference on resource-constrained devices. The inclusion of cross-stage partial (CSP) connections is key to its effectiveness, which maintains low computational complexity, resulting in minimal training time. This model adapts seamlessly to low-light conditions through specialized feature extraction modules, making it a valuable resource in challenging visual environments. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.
KW  - Computer Vision
KW  - Convolutional Neural Network
KW  - Deep Learning
KW  - Multi-Class Classification
KW  - Object Detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Farooq, J.
AU  - Muaz, M.
AU  - Khan Jadoon, K.
AU  - Aafaq, N.
AU  - Khan, M.K.A.
TI  - An improved YOLOv8 for foreign object debris detection with optimized architecture for small objects
PY  - 2024
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-023-17838-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180648562&doi=10.1007%2fs11042-023-17838-w&partnerID=40&md5=2270328d3529ddfca144b15060c31e40
AB  - Automated Foreign Object Debris (FOD) detection offers significant benefit to the aviation industry by reducing human error and enabling continuous surveillance. This paper focuses on addressing the intricacies of FOD detection, with a specific emphasis on treating FODs as “small” objects, a facet which has received limited attention in prior research. This study provides a pioneering evaluation of state-of-the-art object detectors, including both anchor-based models including SSD, YOLOv5m, Scaled YOLOv4 and anchorless models CenterNet and YOLOv8m, applied to a multiclass FOD dataset, “FOD in Airports (FOD-A)”, as well as meticulously curated subset of FOD-A featuring small FODs. The findings reveal that the anchorless object detector YOLOv8m gives the best time accuracy trade off outperforming all compared anchor-based and anchorless approaches. To address the challenge of detecting small FODs, this study optimizes YOLOv8m model by making architectural modifications and incorporating a dedicated, shallow detection head that is purpose-built for the precise identification of small objects. The proposed model, termed as “Improved YOLOv8”, outperforms YOLOv8m by a margin of 1.02 in Average Precision for small objects (APs), achieving a mean average precision (mAP) of 93.8%. Notably, Improved YOLOv8 also has better mAP than all the considered anchor-based and anchorless object detectors examined, as well as those featured in prior FOD-A dataset research. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
KW  - Deep learning
KW  - FOD detection
KW  - Foreign object debris
KW  - Deep learning
KW  - Economic and social effects
KW  - Object detection
KW  - Petroleum reservoir evaluation
KW  - Anchorless
KW  - Aviation industry
KW  - Deep learning
KW  - Foreign object debris
KW  - Foreign object debris detection
KW  - Human errors
KW  - Object detectors
KW  - Optimized architectures
KW  - Small objects
KW  - Debris
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, R.
AU  - Li, Y.
AU  - Wang, J.
AU  - Chen, Y.
AU  - Wang, Z.
AU  - Li, Y.
TI  - Multiscale Feature Fusion Approach for Dual-Modal Object Detection
ST  - 多尺度特征融合的双模态目标检测方法
PY  - 2024
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2305-0412
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007326797&doi=10.3778%2fj.issn.1002-8331.2305-0412&partnerID=40&md5=fdfdb1d24f7511ade69730e550da8af6
AB  - Object detection based on visible images is difficult to adapt to complex lighting conditions such as low light, no light, strong light, etc., while object detection based on infrared images is greatly affected by background noise. Infrared objects lack color information and have weak texture features, which pose a greater challenge. To address these problems, a dual-modal object detection approach that can effectively fuse the features of visible and infrared dual-modal images is proposed. A multiscale feature attention module is proposed, which can extract the multiscale features of the input IR and RGB images separately. Meanwhile, channel attention and spatial pixel attention is introduced to focus the multiscale feature information of dual-modal images from both channel and pixel dimensions. Finally, a dual-modal feature fusion module is proposed to adaptively fuse the feature information of dual-modal images. On the large-scale dual-modal image dataset DroneVehicle, compared with the benchmark algorithm YOLOv5s using visible or infrared single-modal image detection, the proposed algorithm improves the detection accuracy by 13.42 and 2.27 percentage points, and the detection speed reaches 164 frame/s, with ultra-real-time end-to-end detection capability. The proposed algorithm effectively improves the robustness and accuracy of object detection in complex scenes, which has good application prospects. © 2024 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
KW  - attention mechanism
KW  - dual-modal image
KW  - multiscale features fusion
KW  - object detection
KW  - Benchmarking
KW  - Feature extraction
KW  - Image enhancement
KW  - Image fusion
KW  - Image texture
KW  - Large datasets
KW  - Modal analysis
KW  - Object detection
KW  - Object recognition
KW  - Attention mechanisms
KW  - Dual-modal image
KW  - Feature information
KW  - Features fusions
KW  - Lighting conditions
KW  - Low light
KW  - Multi-scale features
KW  - Multiscale feature fusion
KW  - Objects detection
KW  - Visible image
KW  - Infrared imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - He, B.
AU  - Ji, X.
AU  - Li, G.
AU  - Cheng, B.
TI  - Key Technologies and Applications of UAVs in Underground Space: A Review
PY  - 2024
T2  - IEEE Transactions on Cognitive Communications and Networking
DO  - 10.1109/TCCN.2024.3358545
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184319807&doi=10.1109%2fTCCN.2024.3358545&partnerID=40&md5=134c63d77b78e18f1e71a706d2a9d413
AB  - Robots, particularly unmanned aerial vehicles (UAVs), offer significant advantages in challenging environments. Their application in searching and exploring underground areas has garnered considerable attention. However, subsurface environments present various challenges, such as a lack of localization signals, weak illumination, and severe electromagnetic interference, which make positioning, detecting, and communicating difficult for UAVs. The ability to address these challenges is crucial for successful underground UAV applications. In this article, we highlight high-performance perception and communication as key technologies for underground UAVs. We first summarize the current state of UAV perception and communication, and then analyse the supporting role of multi-source fusion perception technology and joint communication optimization methods for underground UAVs. We also evaluate existing subsurface UAV applications and discuss potential research challenges and future development directions to overcome these challenges in underground UAV application scenarios.  © 2015 IEEE.
KW  - joint communication optimization methods
KW  - multi-source fusion perception
KW  - Subsurface environments
KW  - subsurface UAV applications
KW  - Aircraft detection
KW  - Antennas
KW  - Electromagnetic pulse
KW  - Electromagnetic wave interference
KW  - Signal interference
KW  - Aerial vehicle
KW  - Autonomous system
KW  - Communication optimization
KW  - Joint communication optimization method
KW  - Multi-source fusion
KW  - Multi-source fusion perception
KW  - Optimization method
KW  - Simultaneous localization and mapping
KW  - Subsurface environment
KW  - Subsurface unmanned aerial vehicle application
KW  - Vehicle applications
KW  - Drones
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Alaa, R.
AU  - Al-Libawy, H.
AU  - Hussein, E.A.
TI  - Low-Cost Blind Spot Detection System Based on Lite Object Detection Algorithm and Limited Resources Hardware
PY  - 2024
T2  - ICSINTESA 2024 - 2024 4th International Conference of Science and Information Technology in Smart Administration: The Collaboration of Smart Technology and Good Governance for Sustainable Development Goals
DO  - 10.1109/ICSINTESA62455.2024.10747868
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211568440&doi=10.1109%2fICSINTESA62455.2024.10747868&partnerID=40&md5=de1ed53c9f58610f065a25cd3719c28f
AB  - Blind spot detection systems have become integral components of modern vehicle safety systems, aiming to mitigate the risks associated with lane-changing maneuvers. However, existing blind spot detection solutions often rely on expensive external sensors or complex infrastructure such as Radar and LIDAR, limiting their widespread adoption, particularly in budget-conscious markets. In this work, the combination of computer vision techniques and edge computing to detect vehicles is used in the blind spot region in real time. A lightweight convolutional neural network (CNN) model is deployed on an embedded device, such as a microcontroller, to process video streams from onboard cameras. The CNN model analyzes the captured frames, identifying vehicles near the host vehicle. This study compares the performance of several algorithms, including SSD FPN-Lite, YOLO, and FOMO, using a dataset of automated rickshaws, with images captured at various angles and dimensions. It has been shown that the FOMO algorithm is the best option for working on microcontrollers. An Arduino was used to connect to a camera that captures images in real time, recognizes the object, and identifies it with a center point. The best possible testing accuracy for this work using the collected dataset was 88.24%. © 2024 IEEE.
KW  - Depthwise Separable Convolution
KW  - Embedded system
KW  - MobileNetV2
KW  - TensorFlowLite
KW  - Tiny machine learning
KW  - Analog storage
KW  - Convolutional neural networks
KW  - Digital storage
KW  - Microcontrollers
KW  - Blind spot detection system
KW  - Convolutional neural network
KW  - Depthwise separable convolution
KW  - Embedded-system
KW  - Machine-learning
KW  - Mobilenetv2
KW  - Neural network model
KW  - Real- time
KW  - Tensorflowlite
KW  - Tiny machine learning
KW  - Budget control
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Qiu, Q.
AU  - Lau, D.
TI  - Assessment of Trees’ Structural Defects via Hybrid Deep Learning Methods Used in Unmanned Aerial Vehicle (UAV) Observations
PY  - 2024
T2  - Forests
DO  - 10.3390/f15081374
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202688898&doi=10.3390%2ff15081374&partnerID=40&md5=7421917a44291bd2786b1d31f48002e6
AB  - Trees’ structural defects are responsible for the reduction in forest product quality and the accident of tree collapse under extreme environmental conditions. Although the manual view inspection for assessing tree health condition is reliable, it is inefficient in discriminating, locating, and quantifying the defects with various features (i.e., crack and hole). There is a general need for investigation of efficient ways to assess these defects to enhance the sustainability of trees. In this study, the deep learning algorithms of lightweight You Only Look Once (YOLO) and encoder-decoder network named DeepLabv3+ are combined in unmanned aerial vehicle (UAV) observations to evaluate trees’ structural defects. Experimentally, we found that the state-of-the-art detector YOLOv7-tiny offers real-time (i.e., 50–60 fps) and long-range sensing (i.e., 5 m) of tree defects but has limited capacity to acquire the patterns of defects at the millimeter scale. To address this limitation, we further utilized DeepLabv3+ cascaded with different network architectures of ResNet18, ResNet50, Xception, and MobileNetv2 to obtain the actual morphology of defects through close-range and pixel-wise image semantic segmentation. Moreover, the proposed hybrid scheme YOLOv7-tiny_DeepLabv3+_UAV assesses tree’s defect size with an averaged accuracy of 92.62% (±6%). © 2024 by the authors.
KW  - assessment
KW  - deep learning
KW  - DeepLabv3+
KW  - defect detection
KW  - forest
KW  - hybrid methods
KW  - remote sensing
KW  - tree structural defect
KW  - unmanned aerial vehicle
KW  - YOLO-tiny
KW  - Algorithms
KW  - Capacity
KW  - Defects
KW  - Patterns
KW  - Reduction
KW  - Segmentation
KW  - Trees
KW  - Aircraft accidents
KW  - Deep learning
KW  - Hybrid vehicles
KW  - Trees (mathematics)
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Assessment
KW  - Deep learning
KW  - Deeplabv3+
KW  - Defect detection
KW  - Forest
KW  - Hybrid method
KW  - Remote-sensing
KW  - Structural defect
KW  - Tree structural defect
KW  - Unmanned aerial vehicle
KW  - You only look once-tiny
KW  - environmental conditions
KW  - image processing
KW  - machine learning
KW  - pixel
KW  - remote sensing
KW  - sustainability
KW  - tree
KW  - unmanned vehicle
KW  - Semantic Segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Manakitsa, N.
AU  - Maraslidis, G.S.
AU  - Moysis, L.
AU  - Fragulis, G.F.
TI  - A Review of Machine Learning and Deep Learning for Object Detection, Semantic Segmentation, and Human Action Recognition in Machine and Robotic Vision
PY  - 2024
T2  - Technologies
DO  - 10.3390/technologies12020015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185941349&doi=10.3390%2ftechnologies12020015&partnerID=40&md5=c70fc90160e6a4127b015fb6130609ac
AB  - Machine vision, an interdisciplinary field that aims to replicate human visual perception in computers, has experienced rapid progress and significant contributions. This paper traces the origins of machine vision, from early image processing algorithms to its convergence with computer science, mathematics, and robotics, resulting in a distinct branch of artificial intelligence. The integration of machine learning techniques, particularly deep learning, has driven its growth and adoption in everyday devices. This study focuses on the objectives of computer vision systems: replicating human visual capabilities including recognition, comprehension, and interpretation. Notably, image classification, object detection, and image segmentation are crucial tasks requiring robust mathematical foundations. Despite the advancements, challenges persist, such as clarifying terminology related to artificial intelligence, machine learning, and deep learning. Precise definitions and interpretations are vital for establishing a solid research foundation. The evolution of machine vision reflects an ambitious journey to emulate human visual perception. Interdisciplinary collaboration and the integration of deep learning techniques have propelled remarkable advancements in emulating human behavior and perception. Through this research, the field of machine vision continues to shape the future of computer systems and artificial intelligence applications. © 2024 by the authors.
KW  - artificial intelligence
KW  - computer vision
KW  - deep learning
KW  - image processing
KW  - machine learning
KW  - machine vision
KW  - mechatronics
KW  - object classification
KW  - object detection
KW  - object segmentation
KW  - pattern recognition
KW  - robotics
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Rahman, M.
AU  - Islam, F.
AU  - Ball, J.E.
AU  - Goodin, C.
TI  - Traffic Light Recognition and V2I Communications of an Autonomous Vehicle with the Traffic Light for Effective Intersection Navigation using YOLOv8 and MAVS Simulation
PY  - 2024
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.3013514
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197523236&doi=10.1117%2f12.3013514&partnerID=40&md5=416fd8ed3b6aab42cec214772d845a8f
AB  - We integrate advanced computer vision and Vehicle-to-Infrastructure (V2I) communication systems for effective intersection navigation. In the first phase, the YOLOv8 deep learning model is employed to accurately detect traffic lights, with specialized training on the S2TLD Dataset for precision. Then we establish seamless V2I communication in MAVS Simulation, allowing vehicles to receive Signal Phase and Timing (SPaT) messages from traffic lights, enabling autonomous adjustment of speed and behavior. Simulating the scenarios in a high-fidelity automotive simulator demonstrates accurate traffic light detection and timely phase information, promising safer and more efficient intersection navigation for autonomous vehicles. © 2024 SPIE.
KW  - autonomous driving
KW  - intersection navigation
KW  - object detection
KW  - traffic light recognition
KW  - V2X communication
KW  - Deep learning
KW  - Navigation
KW  - Object detection
KW  - Vehicle to Everything
KW  - Vehicle to vehicle communications
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Communications systems
KW  - Intersection navigation
KW  - Objects detection
KW  - Traffic light
KW  - Traffic lights recognition
KW  - V2I communications
KW  - V2X communication
KW  - Vehicle-to-infrastructure
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Hassan, J.
AU  - Alsamhi, S.
TI  - Applications of machine learning in UAV networks
PY  - 2024
T2  - Applications of Machine Learning in UAV Networks
DO  - 10.4018/979-8-3693-0578-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193675656&doi=10.4018%2f979-8-3693-0578-2&partnerID=40&md5=2759b200074b385cb7c8e5d1dd27fac6
AB  - Unmanned aerial vehicles (UAVs) continue to become more advanced and complex as researchers push the boundaries of other supporting technologies. Applications of Machine Learning in UAV Networks presents a pioneering exploration into the symbiotic relationship between machine learning techniques and UAVs. In an age where UAVs are revolutionizing sectors as diverse as agriculture, environmental preservation, security, and disaster response, this meticulously crafted volume offers an analysis of the manifold ways machine learning drives advancements in UAV network efficiency and efficacy. This book navigates through an expansive array of domains, each demarcating a pivotal application of machine learning in UAV networks. From the precision realm of agriculture and its dynamic role in yield prediction to the ecological sensitivity of biodiversity monitoring and habitat restoration, the contours of each domain are vividly etched. These explorations are not limited to the terrestrial sphere; rather, they extend to the pivotal aerial missions of wildlife conservation, forest fire monitoring, and security enhancement, where UAVs adorned with machine learning algorithms wield an instrumental role. Scholars and practitioners from fields as diverse as machine learning, UAV technology, robotics, and IoT networks will find themselves immersed in a confluence of interdisciplinary expertise. The book's pages cater equally to professionals entrenched in agriculture, environmental studies, disaster management, and beyond. Furthermore, the students and researchers finds knowledge that illuminates the convergence of UAVs and machine learning, arguably one of the most riveting frontiers in contemporary research. © 2024 by IGI Global. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Arsenos, A.
AU  - Karampinis, V.
AU  - Petrongonas, E.
AU  - Skliros, C.
AU  - Kollias, D.
AU  - Kollias, S.
AU  - Voulodimos, A.
TI  - Common Corruptions for Evaluating and Enhancing Robustness in Air-to-Air Visual Object Detection
PY  - 2024
T2  - IEEE Robotics and Automation Letters
DO  - 10.1109/LRA.2024.3408485
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195405957&doi=10.1109%2fLRA.2024.3408485&partnerID=40&md5=a2472f162564ef9ac1999d84da3468f0
AB  - —The main barrier to achieving fully autonomous flights lies in autonomous aircraft navigation. Managing non-cooperative traffic presents the most important challenge in this problem. The most efficient strategy for handling non-cooperative traffic is based on monocular video processing through deep learning models. This letter contributes to the vision-based deep learning aircraft detection and tracking literature by investigating the impact of data corruption arising from environmental and hardware conditions on the effectiveness of these methods. More specifically, we designed 7 types of common corruptions for camera inputs taking into account real-world flight conditions. By applying these corruptions to the Airborne Object Tracking (AOT) dataset we constructed the first robustness benchmark dataset named AOT-C for air-to-air aerial object detection. The corruptions included in this dataset cover a wide range of challenging conditions such as adverse weather and sensor noise. The second main contribution of this letter is to present an extensive experimental evaluation involving 8 diverse object detectors to explore the degradation in the performance under escalating levels of corruptions (domain shifts). Based on the evaluation results, the key observations that emerge are the following: 1) One-stage detectors of the YOLO family demonstrate better robustness, 2) Transformer-based and multi-stage detectors like Faster R-CNN are extremely vulnerable to corruptions, 3) Robustness against corruptions is related to the generalization ability of models. The third main contribution is to present that finetuning on our augmented synthetic data results in improvements in the generalisation ability of the object detector in real-world flight experiments. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
KW  - aerial detection
KW  - aerial systems
KW  - autonomous vehicle navigation
KW  - collision avoidance
KW  - Common Corruptions
KW  - out-of-distribution robustness
KW  - perception and autonomy
KW  - robot safety
KW  - Air navigation
KW  - Aircraft accidents
KW  - Aircraft detection
KW  - Antennas
KW  - C (programming language)
KW  - Cameras
KW  - Deep learning
KW  - Object recognition
KW  - Robots
KW  - Unmanned aerial vehicles (UAV)
KW  - Video signal processing
KW  - Aerial detection
KW  - Aerial systems
KW  - Autonomous vehicle navigation
KW  - Benchmark testing
KW  - Collisions avoidance
KW  - Common corruption
KW  - Objects detection
KW  - Out-of-distribution robustness
KW  - Perception and autonomy
KW  - Robot safety
KW  - Robustness
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gao, W.
AU  - Fan, B.
AU  - Fang, Y.
AU  - Song, N.
TI  - Lightweight and multi-lesion segmentation model for diabetic retinopathy based on the fusion of mixed attention and ghost feature mapping
PY  - 2024
T2  - Computers in Biology and Medicine
DO  - 10.1016/j.compbiomed.2023.107854
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180365112&doi=10.1016%2fj.compbiomed.2023.107854&partnerID=40&md5=cb2314673f16dc1f3442bc58dacdcfae
AB  - Diabetic retinopathy is the main cause of blindness, and lesion segmentation is an important basic work for the diagnosis of this disease. The main lesions include soft and hard exudates, microaneurysms, and hemorrhages. However, the segmentation of these four types of lesions is difficult because of their variability in size and contrast, and high intertype similarity. Currently, many network models have problems, such as a large number of parameters and complex calculations, and most segmentation models for diabetic retinopathy focus only on one type of lesion. In this study, a lightweight algorithm based on BiSeNet V2 was proposed for the segmentation of multiple lesions in diabetic retinopathy fundus. First, a hybrid attention module was embedded in the semantic branch of BiSeNet V2 for 8- and 16-fold downsampling, which helped reassign deep feature-map weights and enhanced the ability to extract local key features. Second, a ghost feature-mapping unit was used to optimize the traditional convolution layers and further reduce the computational cost. Third, a new loss function based on the dynamic threshold loss function was applied to supervise the training by adjusting the training weights of the high-loss difficult samples, which enhanced the model's attention to small goals. In experiments on the IDRiD dataset, we conducted an ablation study to verify the effectiveness of each component and compared the proposed model, BiSeNet V2-Pro, with several state-of-the-art models. In comparison with the baseline BiSeNet V2, the segmentation performance of BiSeNet V2-Pro improved by 12.17 %, 11.44 %, and 8.49 % in terms of Sensitivity (SEN), Intersection over Union (IoU), and Dice coefficient (DICE), respectively. Specifically, IoU of MA reaches 0.5716. Compared with other methods, the segmentation speed was significantly improved while ensuring segmentation accuracy, and the number of model parameters was lower. These results demonstrate the superiority of BiSeNet V2-Pro in the multi-lesion segmentation of diabetic retinopathy. © 2023
KW  - Diabetic retinopathy
KW  - Fundus image
KW  - Lightweight network
KW  - Multi-lesion segmentation
KW  - Algorithms
KW  - Diabetes Mellitus
KW  - Diabetic Retinopathy
KW  - Fundus Oculi
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Semantics
KW  - Diagnosis
KW  - Eye protection
KW  - Mapping
KW  - Semantics
KW  - Diabetic retinopathy
KW  - Feature mapping
KW  - Fundus image
KW  - Hard exudates
KW  - Lesion segmentations
KW  - Lightweight network
KW  - Loss functions
KW  - Microaneurysms
KW  - Multi-lesion segmentation
KW  - Segmentation models
KW  - Article
KW  - BiSeNet V2 method
KW  - BiSeNet V2 Pro method
KW  - convolutional neural network
KW  - deep learning
KW  - DeepLabv3+ method
KW  - diabetic retinopathy
KW  - eye fundus
KW  - FC-DenseNet method
KW  - feature extraction
KW  - FFU-Net method
KW  - ghost feature mapping
KW  - image artifact
KW  - image segmentation
KW  - intermethod comparison
KW  - L-Seg method
KW  - M-Net method
KW  - measurement accuracy
KW  - mixed attention module
KW  - processing speed
KW  - segmentation algorithm
KW  - algorithm
KW  - diabetes mellitus
KW  - human
KW  - image processing
KW  - semantics
KW  - Image segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, W.
AU  - Ren, C.
AU  - Tan, A.
TI  - Study on Nighttime Pedestrian Trajectory-Tracking from the Perspective of Driving Blind Spots
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13173460
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203619800&doi=10.3390%2felectronics13173460&partnerID=40&md5=97b0583304b82a2a7d220fcd820fb64e
AB  - With the acceleration of urbanization and the growing demand for traffic safety, developing intelligent systems capable of accurately recognizing and tracking pedestrian trajectories at night or under low-light conditions has become a research focus in the field of transportation. This study aims to improve the accuracy and real-time performance of nighttime pedestrian-detection and -tracking. A method that integrates the multi-object detection algorithm YOLOP with the multi-object tracking algorithm DeepSORT is proposed. The improved YOLOP algorithm incorporates the C2f-faster structure in the Backbone and Neck sections, enhancing feature extraction capabilities. Additionally, a BiFormer attention mechanism is introduced to focus on the recognition of small-area features, the CARAFE module is added to improve shallow feature fusion, and the DyHead dynamic target-detection head is employed for comprehensive fusion. In terms of tracking, the ShuffleNetV2 lightweight module is integrated to reduce model parameters and network complexity. Experimental results demonstrate that the proposed FBCD-YOLOP model improves lane detection accuracy by 5.1%, increases the IoU metric by 0.8%, and enhances detection speed by 25 FPS compared to the baseline model. The accuracy of nighttime pedestrian-detection reached 89.6%, representing improvements of 1.3%, 0.9%, and 3.8% over the single-task YOLO v5, multi-task TDL-YOLO, and the original YOLOP models, respectively. These enhancements significantly improve the model’s detection performance in complex nighttime environments. The enhanced DeepSORT algorithm achieved an MOTA of 86.3% and an MOTP of 84.9%, with ID switch occurrences reduced to 5. Compared to the ByteTrack and StrongSORT algorithms, MOTA improved by 2.9% and 0.4%, respectively. Additionally, network parameters were reduced by 63.6%, significantly enhancing the real-time performance of nighttime pedestrian-detection and -tracking, making it highly suitable for deployment on intelligent edge computing surveillance platforms. © 2024 by the authors.
KW  - deep SORT
KW  - lane lines
KW  - multi-object tracking
KW  - nighttime pedestrian-detection
KW  - YOLOP
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shuai, Y.
AU  - Lin, Z.
AU  - Chen, W.
AU  - Shenghuai, W.
AU  - Yu, T.
TI  - SF-YOLO: An Evolutionary Deep Neural Network for Gear End Surface Defect Detection
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3403870
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194818606&doi=10.1109%2fJSEN.2024.3403870&partnerID=40&md5=2a85a925ba419c8739eb6d7d90c89590
AB  - Metal gears are an essential component of various important mechanical parts, and their quality directly impacts the overall performance and longevity of the automated system. Potential problems can be identified and solved promptly by detecting defects on the gear end face, improving overall product quality. In metal gear end-face defect detection, the inhomogeneity of the gear end-face structure and the multiscale, small-sized defects are the common issues, resulting in current detection methods performing poorly in terms of accuracy. To address the issues mentioned above, we propose a significant region extraction (SF)-YOLO metal gear end-face defect detection method based on evolutionary algorithm optimization to complete automatic detection of gear end-face defects. First, we offer a visual saliency region-based image extraction method that eliminates the interference of invalid features in the nonprocessing regions and reduces image complexity. Then, the neck network feature extraction pyramid is replaced by a weighted bidirectional feature pyramid network to enhance the model's multiscale adaptability and improve its fusion speed and efficiency. Afterward, the convolutional block attention module (CBAM) is combined with the C3 module to constitute the CBAM-C3 attention module and improve the model's attention to small sizes. Finally, an improved sparrow algorithm is proposed to optimize the hyperparameters of the model's neural network and avoid the inadmissible determinism of manual parameter tuning. The experiments showed that the SF-YOLO model achieved an average accuracy of 98.01% on a test set of metal gear end-face defects, with an F1 value of 0.99 and an average detection computation time per image of 0.025 s. Compared to other deep learning models, the proposed SF-YOLO model improves the accuracy and efficiency of gear end-face defect detection, and it can efficiently detect small-size and multiscale metal gear end-face defects to meet enterprises' real-time online inspection needs.  © 2001-2012 IEEE.
KW  - BiFPN
KW  - convolutional block attention module (CBAM)
KW  - gear face defect detection
KW  - improved sparrow search algorithm (ISSA)
KW  - multiscale
KW  - small size
KW  - Automation
KW  - Data mining
KW  - Deep neural networks
KW  - Efficiency
KW  - Face recognition
KW  - Feature extraction
KW  - Surface defects
KW  - CBAM
KW  - Computational modelling
KW  - Convolutional neural network
KW  - Defect detection
KW  - Features extraction
KW  - Gear face defect detection
KW  - ISSA
KW  - Multi-scales
KW  - Optimisations
KW  - Small size
KW  - Extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, X.
AU  - Lu, J.
AU  - Lin, M.
AU  - Zhou, L.
AU  - Ou, L.
TI  - Shape-adaptive ellipse label assignment for remote sensing image based on FCOS
ST  - 基于单阶段全卷积检测器的遥感图像形状自适应椭圆标签分配方法①
PY  - 2024
T2  - Gaojishu Tongxin/Chinese High Technology Letters
DO  - 10.3772/j.issn.1002-0470.2024.08.009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205924160&doi=10.3772%2fj.issn.1002-0470.2024.08.009&partnerID=40&md5=c0e199bf1c29d2e08bb2aa091c102428
AB  - Anchor-free object detection algorithms have experienced rapid development in object detection in recent years. However,in remote sensing images, the objects with arbitrary angles, dense distribution, and large shape differences make the detection still a challenge. Therefore, an anchor-free method based on improved fully convolu-tional one-stage (FCOS) is proposed. Firstly, to mine more potential high-quality anchor points, a shape-adaptive feature point sampling method based on the ellipse equation is proposed. To further reduce the negative influence of low-quality anchor points, the ellipse centerness is proposed. It can provide more accurate and reasonable weights than the traditional centerness. In addition, to address the inconsistency between classification and regression, a joint intersection over union ( IoU ) guidance strategy is proposed. The proposed ellipse centerness and IoU score are combined as quality scores to guide the training of the classification branch and to make the results of regression more accurate. The mean average precision on the DOTA 1. 0 dataset reaches 79. 17% , which is better than most existing anchor-free detection methods. © 2024 Inst. of Scientific and Technical Information of China. All rights reserved.
KW  - deep learning
KW  - label assignment
KW  - object detection
KW  - remote sensing image
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, W.
AU  - Liu, C.
AU  - Zheng, H.
TI  - A panoramic driving perception fusion algorithm based on multi-task learning
PY  - 2024
T2  - PLoS ONE
DO  - 10.1371/journal.pone.0304691
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195101332&doi=10.1371%2fjournal.pone.0304691&partnerID=40&md5=43b925fdbf1b7eedaa3c6a8150509b35
AB  - With the rapid development of intelligent connected vehicles, there is an increasing demand for hardware facilities and onboard systems of driver assistance systems. Currently, most vehicles are constrained by the hardware resources of onboard systems, which mainly process single-task and single-sensor data. This poses a significant challenge in achieving complex panoramic driving perception technology. While the panoramic driving perception algorithm YOLOP has achieved outstanding performance in multi-task processing, it suffers from poor adaptability of feature map pooling operations and loss of details during downsampling. To address these issues, this paper proposes a panoramic driving perception fusion algorithm based on multi-task learning. The model training involves the introduction of different loss functions and a series of processing steps for lidar point cloud data. Subsequently, the perception information from lidar and vision sensors is fused to achieve synchronized processing of multi-task and multi-sensor data, thereby effectively improving the performance and reliability of the panoramic driving perception system. To evaluate the performance of the proposed algorithm in multi-task processing, the BDD100K dataset is used. The results demonstrate that, compared to the YOLOP model, the multi-task learning network performs better in lane detection, drivable area detection, and vehicle detection tasks. Specifically, the lane detection accuracy improves by 11.6%, the mean Intersection over Union (mIoU) for drivable area detection increases by 2.1%, and the mean Average Precision at 50% IoU (mAP50) for vehicle detection improves by 3.7%.  © 2024 Wu et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
KW  - Algorithms
KW  - Automobile Driving
KW  - Humans
KW  - Task Performance and Analysis
KW  - Article
KW  - classification algorithm
KW  - computer aided design
KW  - image reconstruction
KW  - image segmentation
KW  - learning algorithm
KW  - machine learning
KW  - mathematical analysis
KW  - motor vehicle
KW  - multi task learning
KW  - nonhuman
KW  - normal distribution
KW  - pedestrian
KW  - perception
KW  - signal noise ratio
KW  - spatiotemporal analysis
KW  - telecommunication
KW  - visual field
KW  - visual information
KW  - algorithm
KW  - car driving
KW  - human
KW  - psychology
KW  - task performance
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Zhou, Z.
AU  - Qi, G.
AU  - Hu, G.
AU  - Zhu, Z.
AU  - Huang, X.
TI  - Remote Sensing Micro-Object Detection under Global and Local Attention Mechanism
PY  - 2024
T2  - Remote Sensing
DO  - 10.3390/rs16040644
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185711614&doi=10.3390%2frs16040644&partnerID=40&md5=310519fd3919d878a0e18fea02a8af32
AB  - With the rapid advancement of technology, satellite and drone technologies have had significant impacts on various fields, creating both opportunities and challenges. In areas like the military, urban planning, and environmental monitoring, the application of remote sensing technology is paramount. However, due to the unique characteristics of remote sensing images, such as high resolution, large-scale scenes, and small, densely packed targets, remote sensing object detection faces numerous technical challenges. Traditional detection methods are inadequate for effectively detecting small targets, rendering the accurate and efficient detection of objects in complex remote sensing images a pressing issue. Current detection techniques fall short in accurately detecting small targets compared to medium and large ones, primarily due to limited feature information, insufficient contextual data, and poor localization capabilities for small targets. In response, we propose an innovative detection method. Unlike previous approaches that often focused solely on either local or contextual information, we introduce a novel Global and Local Attention Mechanism (GAL), providing an in-depth modeling method for input images. Our method integrates fine-grained local feature analysis with global contextual information processing. The local attention concentrates on details and spatial relationships within local windows, enabling the model to recognize intricate details in complex images. Meanwhile, the global attention addresses the entire image’s global information, capturing overarching patterns and structures, thus enhancing the model’s high-level semantic understanding. Ultimately, a specific mechanism fuses local details with global context, allowing the model to consider both aspects for a more precise and comprehensive interpretation of images. Furthermore, we have developed a multi-head prediction module that leverages semantic information at various scales to capture the multi-scale characteristics of remote sensing targets. Adding decoupled prediction heads aims to improve the accuracy and robustness of target detection. Additionally, we have innovatively designed the Ziou loss function, an advanced loss calculation, to enhance the model’s precision in small target localization, thereby boosting its overall performance in small target detection. Experimental results on the Visdrone2019 and DOTA datasets demonstrate that our method significantly surpasses traditional methods in detecting small targets in remote sensing imagery. © 2024 by the authors.
KW  - attention mechanism
KW  - loss function
KW  - multi scale feature fusion
KW  - remote-sensing detection
KW  - Environmental technology
KW  - Feature extraction
KW  - Image enhancement
KW  - Military applications
KW  - Object detection
KW  - Object recognition
KW  - Semantics
KW  - Attention mechanisms
KW  - Features fusions
KW  - Loss functions
KW  - Multi scale feature fusion
KW  - Multi-scale features
KW  - Objects detection
KW  - Remote sensing images
KW  - Remote-sensing
KW  - Remote-sensing detection
KW  - Small targets
KW  - Remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, G.
AU  - Chao, P.
AU  - Yang, J.
AU  - Ding, H.
TI  - SGST-YOLOv8: An Improved Lightweight YOLOv8 for Real-Time Target Detection for Campus Surveillance
PY  - 2024
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app14125341
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197251266&doi=10.3390%2fapp14125341&partnerID=40&md5=0c581eb21566f61491dd0e45b207a850
AB  - Real-time target detection plays an important role in campus intelligent surveillance systems. This paper introduces Soft-NMS, GSConv, Triplet Attention, and other advanced technologies to propose a lightweight pedestrian and vehicle detection model named SGST-YOLOv8. In this paper, the improved YOLOv8 model is trained on the self-made dataset, and the tracking algorithm is combined to achieve an accurate and efficient real-time pedestrian and vehicle tracking detection system. The improved model achieved an accuracy of 88.6%, which is 1.2% higher than the baseline model YOLOv8. Additionally, the mAP0.5:0.95 increased by 3.2%. The model parameters and GFLOPS reduced by 5.6% and 7.9%, respectively. In addition, this study also employed the improved YOLOv8 model combined with the bot sort tracking algorithm on the website for actual detection. The results showed that the improved model achieves higher FPS than the baseline YOLOv8 model when detecting the same scenes, with an average increase of 3–5 frames per second. The above results verify the effectiveness of the improved model for real-time target detection in complex environments. © 2024 by the authors.
KW  - GSConv
KW  - Soft-NMS
KW  - target tracking
KW  - triplet attention
KW  - YOLOv8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tang, H.
AU  - Xiong, W.
AU  - Dong, K.
AU  - Cui, Y.
TI  - Radar-optical fusion detection of UAV based on improved YOLOv7-tiny
PY  - 2024
T2  - Measurement Science and Technology
DO  - 10.1088/1361-6501/ad440b
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193000451&doi=10.1088%2f1361-6501%2fad440b&partnerID=40&md5=fdf3ca149356b54aeffcb58dcdf3480f
AB  - This study presents a radar-optical fusion detection method for unmanned aerial vehicles (UAVs) in maritime environments. Radar and camera technologies are integrated to improve the detection capabilities of the platforms. The proposed method involves generating regions of interest (ROI) by projecting radar traces onto optical images through matrix transformation and geometric centroid registration. The generated ROI are matched with YOLO detection boxes using the intersection-over-union (IoU) algorithm, enabling radar-optical fusion detection. A modified algorithm, called SPN-YOLOv7-tiny, is developed to address the challenge of detecting small UAV targets that are easily missed in images. In this algorithm, the convolutional layers in the backbone network are replaced with a space-to-depth convolution, and a small object detection layer is added. In addition, the loss function was replaced with a normalized weighted distance loss function. Experimental results demonstrate that compared to the original YOLOv7-tiny method, SPN-YOLOv7-tiny achieves an improved mAP@0.5 (mean average precision at an IoU threshold of 0.5) from 0.852 to 0.93, while maintaining a high frame rate of 135.1 frames per second. Moreover, the proposed radar-optical fusion detection method achieves an accuracy of 96.98%, surpassing the individual detection results of the radar and camera. The proposed method effectively addresses the detection challenges posed by closely spaced overlapping targets on a radar chart. © 2024 IOP Publishing Ltd.
KW  - centroid registration
KW  - improved YOLOv7-tiny
KW  - radar-optical fusion detection
KW  - small target
KW  - UAV
KW  - Aircraft detection
KW  - Antennas
KW  - Cameras
KW  - Convolution
KW  - Drones
KW  - Mathematical transformations
KW  - Object detection
KW  - Radar imaging
KW  - Tracking radar
KW  - Aerial vehicle
KW  - Centroid registration
KW  - Detection methods
KW  - Improved YOLOv7-tiny
KW  - Loss functions
KW  - Radar-optical fusion detection
KW  - Region-of-interest
KW  - Regions of interest
KW  - Small targets
KW  - Unmanned aerial vehicle
KW  - Geometrical optics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kim, J.
AU  - Lee, C.
AU  - Chung, D.
AU  - Cho, Y.
AU  - Kim, J.
AU  - Jang, W.
AU  - Park, S.
TI  - Field experiment of autonomous ship navigation in canal and surrounding nearshore environments
PY  - 2024
T2  - Journal of Field Robotics
DO  - 10.1002/rob.22262
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176284813&doi=10.1002%2frob.22262&partnerID=40&md5=1b16550798ebdeac8599c3419d072960
AB  - In this paper, we present the development of autonomous navigation capabilities for small cruise boats, and their verification by field experiments in a canal and its surrounding waters. A cruise boat was converted to an autonomous surface vehicle (ASV) by installing various sensors and actuators to enable autonomous navigation. Navigation and perception sensors, such as global positioning system, attitude and heading reference system, radar, light detection and ranging (LiDAR), and cameras, were mounted on the ASV to estimate its motion and perceive the surrounding environment. Motors and potentiometers were installed for active control of the ASV. Software system components including navigation filters, object-detection, path-planning, and control algorithms were designed and implemented. In the narrow canal region, LiDARs were used to detect the side walls and boundaries of the canal. In open areas outside the canal, obstacles and object features were detected using various combinations of onboard sensors. A model-based path-planning algorithm was designed to avoid the detected obstacles, and the line-of-sight guidance was employed to control the vehicle. The performance of the developed system was verified through a field experiment in a real-world maritime environment. © 2023 Wiley Periodicals LLC.
KW  - autonomous navigation
KW  - autonomous surface vehicle
KW  - field experiment
KW  - model-based path planning
KW  - multisensor object detection
KW  - ship control
KW  - system identification
KW  - Autonomous vehicles
KW  - Boats
KW  - Navigation
KW  - Object detection
KW  - Object recognition
KW  - Ships
KW  - Tracking radar
KW  - Unmanned surface vehicles
KW  - Voltage dividers
KW  - Autonomous navigation
KW  - Autonomous surface vehicles
KW  - Field experiment
KW  - Model-based OPC
KW  - Model-based path planning
KW  - Multi sensor
KW  - Multisensor object detection
KW  - Objects detection
KW  - Ship control
KW  - System-identification
KW  - Motion planning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhu, Z.
AU  - Li, X.
AU  - Zhai, J.
AU  - Hu, H.
TI  - PODB: A learning-based polarimetric object detection benchmark for road scenes in adverse weather conditions
PY  - 2024
T2  - Information Fusion
DO  - 10.1016/j.inffus.2024.102385
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189001927&doi=10.1016%2fj.inffus.2024.102385&partnerID=40&md5=809f48be58f9fe3fa65029110a1f1b24
AB  - Due to its insensitivity to light intensity and the capability to capture multidimensional information, polarimetric imaging technology has been proven to have advantages over traditional intensity-based imaging techniques for object detection tasks in adverse environmental conditions, particularly in road traffic scenarios. Recently, with the rapid development of artificial intelligence technology, deep learning (DL)-powered object detection techniques can further enhance recognition accuracy and algorithm robustness. This improvement is made possible by the ability of DL technology to leverage large datasets and extract deeper levels of target-specific features. However, constructing large-scale polarimetric datasets poses challenges as obtaining polarimetric information requires multiple captures of intensity images. In other words, the workload is several times higher compared to traditional imaging techniques. To address the current scarcity of polarimetric datasets and evaluate the practical performance of various algorithms on polarimetric datasets, this paper proposes a Polarimetric Object Detection Benchmark (PODB) dataset. The PODB provides an integrated quality evaluation framework for DL-based object detection algorithms in complex road scenes by incorporating polarimetric imaging. Besides, we conducted extensive object detection experiments using the PODB, which allowed for a comprehensive validation and performance evaluation of effective benchmark algorithms. Furthermore, a physics-based multi-scale image fusion cascaded object detection neural network model is proposed. By combining the multidimensional information provided by polarized images with an adaptive learning multi-decision object detection neural network model, the recognition accuracy of complex road scenes in adverse weather conditions has been improved by approximately 10%. Additionally, we anticipate that PODB will serve as an effective platform for evaluating and comparing the performance of object detection algorithms, as well as providing researchers with a baseline for future studies in developing new DL-based methods. © 2024 Elsevier B.V.
KW  - Adverse weather conditions
KW  - Benchmark
KW  - Deep learning
KW  - Image fusion
KW  - Object detection
KW  - Polarimetric imaging
KW  - Quality assessment
KW  - Benchmarking
KW  - Complex networks
KW  - Deep learning
KW  - Image enhancement
KW  - Image fusion
KW  - Large datasets
KW  - Object recognition
KW  - Roads and streets
KW  - Adverse weather
KW  - Adverse weather condition
KW  - Benchmark
KW  - Condition
KW  - Deep learning
KW  - Multidimensional information
KW  - Objects detection
KW  - Polarimetric imaging
KW  - Quality assessment
KW  - Recognition accuracy
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yin, H.
AU  - Tian, D.
AU  - Lin, C.
AU  - Duan, X.
AU  - Zhou, J.
AU  - Zhao, D.
AU  - Cao, D.
TI  - V2VFormer++: Multi-Modal Vehicle-to-Vehicle Cooperative Perception via Global-Local Transformer
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2023.3314919
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173289868&doi=10.1109%2fTITS.2023.3314919&partnerID=40&md5=e720a1f86422a2bdde25b27be3fa4fae
AB  - Multi-vehicle cooperative perception has recently emerged for facilitating long-range and large-scale perception ability of connected automated vehicles (CAVs). Nonetheless, enormous efforts formulate collaborative perception as LiDAR-only 3D detection paradigm, neglecting the significance and complementary of dense image. In this work, we construct the first multi-modal vehicle-to-vehicle cooperative perception framework dubbed as V2VFormer++, where individual camera-LiDAR representation is incorporated with dynamic channel fusion (DCF) at bird's-eye-view (BEV) space and ego-centric BEV maps from adjacent vehicles are aggregated by global-local transformer module. Specifically, channel-token mixer (CTM) with MLP design is developed to capture global response among neighboring CAVs, and position-aware fusion (PAF) further investigate the spatial correlation between each ego-networked map in a local perspective. In this manner, we could strategically determine which CAVs are desirable for collaboration and how to aggregate the foremost information from them. Quantitative and qualitative experiments are conducted on both publicly-available OPV2V and V2X-Sim 2.0 benchmarks, and our proposed V2VFormer++ reports the state-of-the-art cooperative perception performance, demonstrating its effectiveness and advancement. Moreover, ablation study and visualization analysis further suggest the strong robustness against diverse disturbances from real-world scenarios.  © 2000-2011 IEEE.
KW  - 3D object detection
KW  - autonomous driving
KW  - intelligent transportation systems
KW  - multi-modal fused perception
KW  - transformer
KW  - Vehicle-to-vehicle (V2V) cooperative perception
KW  - Benchmarking
KW  - Feature extraction
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Object detection
KW  - Object recognition
KW  - Three dimensional computer graphics
KW  - Three dimensional displays
KW  - Vehicles
KW  - Vehicular ad hoc networks
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous driving
KW  - Collaboration
KW  - Cooperative perception
KW  - Features extraction
KW  - Intelligent transportation systems
KW  - Multi-modal
KW  - Multi-modal fused perception
KW  - Objects detection
KW  - Three-dimensional display
KW  - Transformer
KW  - Vehicle to vehicles
KW  - Vehicle-to-vehicle (V2V) cooperative perception
KW  - Vehicular Adhoc Networks (VANETs)
KW  - Optical radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hekmatnejad, M.
AU  - Hoxha, B.
AU  - Deshmukh, J.V.
AU  - Yang, Y.
AU  - Fainekos, G.
TI  - Formalizing and evaluating requirements of perception systems for automated vehicles using spatio-temporal perception logic
PY  - 2024
T2  - International Journal of Robotics Research
DO  - 10.1177/02783649231223546
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183133814&doi=10.1177%2f02783649231223546&partnerID=40&md5=c7e7f692b18c3a6629c8b874690b687f
AB  - Automated vehicles (AV) heavily depend on robust perception systems. Current methods for evaluating vision systems focus mainly on frame-by-frame performance. Such evaluation methods appear to be inadequate in assessing the performance of a perception subsystem when used within an AV. In this paper, we present a logic—referred to as Spatio-Temporal Perception Logic (STPL)—which utilizes both spatial and temporal modalities. STPL enables reasoning over perception data using spatial and temporal operators. One major advantage of STPL is that it facilitates basic sanity checks on the functional performance of the perception system, even without ground truth data in some cases. We identify a fragment of STPL which is efficiently monitorable offline in polynomial time. Finally, we present a range of specifications for AV perception systems to highlight the types of requirements that can be expressed and analyzed through offline monitoring with STPL. © The Author(s) 2024.
KW  - Programming environment
KW  - recognition
KW  - sensing and perception computer vision
KW  - simulation, interfaces and virtual reality
KW  - visual tracking
KW  - Computer circuits
KW  - Computer programming
KW  - Computer vision
KW  - Polynomial approximation
KW  - Automated vehicles
KW  - Perception systems
KW  - Performance
KW  - Programming environment
KW  - Recognition
KW  - Sensing and perception computer visions
KW  - Simulation, interface and virtual reality
KW  - Spatio-temporal
KW  - Temporal perception
KW  - Visual Tracking
KW  - Virtual reality
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - He, Z.
AU  - Li, L.
AU  - Xu, H.
AU  - Zong, L.
AU  - Dai, Y.
TI  - Collaborative Obstacle Detection for Dual USVs Using MGNN-DANet with Movable Virtual Nodes and Double Attention
PY  - 2024
T2  - Drones
DO  - 10.3390/drones8090418
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205099822&doi=10.3390%2fdrones8090418&partnerID=40&md5=829380749a38cee94ddb1eb269be65e9
AB  - To reduce missed detections in LiDAR-based obstacle detection, this paper proposes a dual unmanned surface vessels (USVs) obstacle detection method using the MGNN-DANet template matching framework. Firstly, point cloud templates for each USV are created, and a clustering algorithm extracts suspected targets from the point clouds captured by a single USV. Secondly, a graph neural network model based on the movable virtual nodes is designed, introducing a neighborhood distribution uniformity metric. This model enhances the local point cloud distribution features of the templates and suspected targets through a local sampling strategy. Furthermore, a feature matching model based on double attention is developed, employing self-attention to aggregate the features of the templates and cross-attention to evaluate the similarity between suspected targets and aggregated templates, thereby identifying and locating another USV within the targets detected by each USV. Finally, the deviation between the measured and true positions of one USV is used to correct the point clouds obtained by the other USV, and obstacle positions are annotated through dual-view point cloud clustering. Experimental results show that, compared to single USV detection methods, the proposed method reduces the missed detection rate of maritime obstacles by 7.88% to 14.69%. © 2024 by the authors.
KW  - double attention
KW  - dual unmanned surface vessels
KW  - movable virtual nodes
KW  - obstacle detection
KW  - Obstacle detectors
KW  - Template matching
KW  - Unmanned surface vehicles
KW  - Virtual environments
KW  - Virtual reality
KW  - Clusterings
KW  - Detection methods
KW  - Double attention
KW  - Dual unmanned surface vessel
KW  - Missed detections
KW  - Movable virtual node
KW  - Obstacles detection
KW  - Point-clouds
KW  - Unmanned surface vessels
KW  - Virtual node
KW  - Graph neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sun, Y.
AU  - Zhang, Y.
AU  - Wang, H.
AU  - Guo, J.
AU  - Zheng, J.
AU  - Ning, H.
TI  - SES-YOLOv8n: automatic driving object detection algorithm based on improved YOLOv8
PY  - 2024
T2  - Signal, Image and Video Processing
DO  - 10.1007/s11760-024-03003-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188325749&doi=10.1007%2fs11760-024-03003-9&partnerID=40&md5=cc37cec82f8ad97276580999b2cb88bf
AB  - The perception system in autonomous driving mainly uses object detection algorithms to obtain the distribution of obstacles for recognition and analysis. Current object detection algorithms have rapidly developed, but it is challenging to balance the requirements of real-time detection and high detection accuracy in actual application scenarios. To solve the above problems, this paper uses YOLOv8n as the baseline model and proposes an object detection network named SES-YOLOv8n. Firstly, the SPPF module in the network was replaced by the SPPCSPC module to enhance further the model’s fusion ability under feature maps of different scales. The efficient multi-scale attention module EMA is introduced into the C2F module of the backbone network, which improves the perception ability in critical areas and the efficiency of feature extraction. Finally, the SPD-Conv module is used to replace part of the convolution modules in the backbone network to replace the downsampling operation, which can more effectively retain the feature information and improve the network’s accuracy and learning ability. Experimental results on the KITTI dataset and BDD100K dataset show that the average accuracy of the improved network model reaches 92.7% and 41.9%, which is 3.4% and 5.0% higher than that of the baseline model and is significantly better than the baseline model. This model can realize real-time image processing in general scenes based on ensuring high detection accuracy. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.
KW  - Autonomous driving
KW  - EMA
KW  - Object detection
KW  - SPD-Conv
KW  - SPPCSPC
KW  - YOLOv8n
KW  - Automobile drivers
KW  - Autonomous vehicles
KW  - Object recognition
KW  - Signal detection
KW  - Autonomous driving
KW  - Back-bone network
KW  - Baseline models
KW  - Detection accuracy
KW  - EMA
KW  - Object detection algorithms
KW  - Objects detection
KW  - SPD-conv
KW  - SPPCSPC
KW  - YOLOv8n
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ocholla, I.A.
AU  - Pellikka, P.
AU  - Karanja, F.
AU  - Vuorinne, I.
AU  - Väisänen, T.
AU  - Boitt, M.
AU  - Heiskanen, J.
TI  - Livestock Detection and Counting in Kenyan Rangelands Using Aerial Imagery and Deep Learning Techniques
PY  - 2024
T2  - Remote Sensing
DO  - 10.3390/rs16162929
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202453125&doi=10.3390%2frs16162929&partnerID=40&md5=dd9f2500b6c26b67a98ee6985feeed6b
AB  - Accurate livestock counts are essential for effective pastureland management. High spatial resolution remote sensing, coupled with deep learning, has shown promising results in livestock detection. However, challenges persist, particularly when the targets are small and in a heterogeneous environment, such as those in African rangelands. This study evaluated nine state-of-the-art object detection models, four variants each from YOLOv5 and YOLOv8, and Faster R-CNN, for detecting cattle in 10 cm resolution aerial RGB imagery in Kenya. The experiment involved 1039 images with 9641 labels for training from sites with varying land cover characteristics. The trained models were evaluated on 277 images and 2642 labels in the test dataset, and their performance was compared using Precision, Recall, and Average Precision (AP0.5–0.95). The results indicated that reduced spatial resolution, dense shrub cover, and shadows diminish the model’s ability to distinguish cattle from the background. The YOLOv8m architecture achieved the best AP0.5–0.95 accuracy of 39.6% with Precision and Recall of 91.0% and 83.4%, respectively. Despite its superior performance, YOLOv8m had the highest counting error of −8%. By contrast, YOLOv5m with AP0.5–0.95 of 39.3% attained the most accurate cattle count with RMSE of 1.3 and R2 of 0.98 for variable cattle herd densities. These results highlight that a model with high AP0.5–0.95 detection accuracy may struggle with counting cattle accurately. Nevertheless, these findings suggest the potential to upscale aerial-imagery-trained object detection models to satellite imagery for conducting cattle censuses over large areas. In addition, accurate cattle counts will support sustainable pastureland management by ensuring stock numbers do not exceed the forage available for grazing, thereby mitigating overgrazing. © 2024 by the authors.
KW  - aerial survey
KW  - deep learning
KW  - livestock census
KW  - object detection
KW  - remote sensing
KW  - Remote sensing
KW  - Aerial imagery
KW  - Aerial surveys
KW  - Cattles
KW  - Deep learning
KW  - Detection models
KW  - Learning techniques
KW  - Livestock census
KW  - Objects detection
KW  - Performance
KW  - Remote-sensing
KW  - Aerial photography
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shi, T.
AU  - Guo, P.
AU  - Wang, R.
AU  - Ma, Z.
AU  - Zhang, W.
AU  - Li, W.
AU  - Fu, H.
AU  - Hu, H.
TI  - A Survey on Multi-Sensor Fusion Perimeter Intrusion Detection in High-Speed Railways
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24175463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203873290&doi=10.3390%2fs24175463&partnerID=40&md5=7d9d02affc5c7c0591d25703bc1096aa
AB  - In recent years, the safety issues of high-speed railways have remained severe. The intrusion of personnel or obstacles into the perimeter has often occurred in the past, causing derailment or parking, especially in the case of bad weather such as fog, haze, rain, etc. According to previous research, it is difficult for a single sensor to meet the application needs of all scenario, all weather, and all time domains. Due to the complementary advantages of multi-sensor data such as images and point clouds, multi-sensor fusion detection technology for high-speed railway perimeter intrusion is becoming a research hotspot. To the best of our knowledge, there has been no review of research on multi-sensor fusion detection technology for high-speed railway perimeter intrusion. To make up for this deficiency and stimulate future research, this article first analyzes the situation of high-speed railway technical defense measures and summarizes the research status of single sensor detection. Secondly, based on the analysis of typical intrusion scenarios in high-speed railways, we introduce the research status of multi-sensor data fusion detection algorithms and data. Then, we discuss risk assessment of railway safety. Finally, the trends and challenges of multi-sensor fusion detection algorithms in the railway field are discussed. This provides effective theoretical support and technical guidance for high-speed rail perimeter intrusion monitoring. © 2024 by the authors.
KW  - high-speed railways
KW  - multi-sensor fusion
KW  - object detection
KW  - perimeter intrusion
KW  - Derailments
KW  - Information fusion
KW  - Intrusion detection
KW  - Network intrusion
KW  - Railroad cars
KW  - Railroad transportation
KW  - Railroad yards and terminals
KW  - Rails
KW  - Risk analysis
KW  - Risk assessment
KW  - Risk perception
KW  - Sensor data fusion
KW  - Detection algorithm
KW  - Detection technology
KW  - High-speed railways
KW  - Intrusion-Detection
KW  - Multi-sensor fusion
KW  - Objects detection
KW  - Perimeter intrusion
KW  - Research status
KW  - Safety issues
KW  - Single sensor
KW  - detection algorithm
KW  - fog
KW  - haze
KW  - human
KW  - railway
KW  - rain
KW  - review
KW  - risk assessment
KW  - sensor
KW  - velocity
KW  - weather
KW  - Railroads
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Osmani, K.
AU  - Schulz, D.
TI  - Comprehensive Investigation of Unmanned Aerial Vehicles (UAVs): An In-Depth Analysis of Avionics Systems
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24103064
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194218675&doi=10.3390%2fs24103064&partnerID=40&md5=545f6226c631849e0f0bca4fe00df3fa
AB  - The evolving technologies regarding Unmanned Aerial Vehicles (UAVs) have led to their extended applicability in diverse domains, including surveillance, commerce, military, and smart electric grid monitoring. Modern UAV avionics enable precise aircraft operations through autonomous navigation, obstacle identification, and collision prevention. The structures of avionics are generally complex, and thorough hierarchies and intricate connections exist in between. For a comprehensive understanding of a UAV design, this paper aims to assess and critically review the purpose-classified electronics hardware inside UAVs, each with the corresponding performance metrics thoroughly analyzed. This review includes an exploration of different algorithms used for data processing, flight control, surveillance, navigation, protection, and communication. Consequently, this paper enriches the knowledge base of UAVs, offering an informative background on various UAV design processes, particularly those related to electric smart grid applications. As a future work recommendation, an actual relevant project is openly discussed. © 2024 by the authors.
KW  - communication modules
KW  - control algorithms
KW  - embedded sensors
KW  - obstacles avoidance
KW  - thermal imaging
KW  - unmanned aerial vehicles
KW  - Air navigation
KW  - Aircraft accidents
KW  - Antennas
KW  - Cyber Physical System
KW  - Data handling
KW  - Embedded systems
KW  - Infrared imaging
KW  - Knowledge based systems
KW  - Military vehicles
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Avionic systems
KW  - Communication modules
KW  - Diverse domains
KW  - Embedded sensors
KW  - In-depth analysis
KW  - Obstacles avoidance
KW  - Thermal-imaging
KW  - Unmanned aerial vehicle
KW  - Vehicle design
KW  - aircraft
KW  - algorithm
KW  - commercial phenomena
KW  - data processing
KW  - electronics
KW  - human
KW  - knowledge base
KW  - prevention
KW  - review
KW  - sensor
KW  - thermography
KW  - unmanned aerial vehicle
KW  - Avionics
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zarei, N.
AU  - Moallem, P.
AU  - Shams, M.
TI  - Real-time vehicle detection using segmentation-based detection network and trajectory prediction
PY  - 2024
T2  - IET Computer Vision
DO  - 10.1049/cvi2.12236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172790636&doi=10.1049%2fcvi2.12236&partnerID=40&md5=cb92613cf8a1f5fb1d5748edd2fb8f10
AB  - The position of vehicles is determined using an algorithm that includes two stages of detection and prediction. The more the number of frames in which the detection network is used, the more accurate the detector is, and the more the prediction network is used, the algorithm is faster. Therefore, the algorithm is very flexible to achieve the required accuracy and speed. YOLO's base detection network is designed to be robust against vehicle scale changes. Also, feature maps are produced in the detector network, which contribute greatly to increasing the accuracy of the detector. In these maps, using differential images and a u-net-based module, image segmentation has been done into two classes: vehicle and background. To increase the accuracy of the recursive predictive network, vehicle manoeuvres are classified. For this purpose, the spatial and temporal information of the vehicles are considered simultaneously. This classifier is much more effective than classifiers that consider spatial and temporal information separately. The Highway and UA-DETRAC datasets demonstrate the performance of the proposed algorithm in urban traffic monitoring systems. © 2023 The Authors. IET Computer Vision published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
KW  - convolutional neural nets
KW  - object detection
KW  - recurrent neural nets
KW  - vehicles
KW  - Classification (of information)
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Forecasting
KW  - Image segmentation
KW  - Recurrent neural networks
KW  - Vehicles
KW  - Convolutional neural net
KW  - Detection networks
KW  - Network prediction
KW  - Objects detection
KW  - Real- time
KW  - Recurrent neural net
KW  - Spatial informations
KW  - Temporal information
KW  - Trajectory prediction
KW  - Vehicles detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Narkhede, M.
AU  - Chopade, N.
TI  - Real-Time Detection of Vulnerable Road Users Using a Lightweight Object Detection Model
PY  - 2024
T2  - International Journal of Intelligent Systems and Applications in Engineering
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185962890&partnerID=40&md5=ad1b569473d29f49c75a56de2f69e715
AB  - Vulnerable Road Users (VRUs), including pedestrians, cyclists, and motorcyclists, face a heightened risk in traffic scenarios. The safety enhancement for VRUs relies heavily on evolving driver assistance systems and autonomous vehicles, anchored by swift VRU detection and localisation. Object detection models in computer vision are pivotal for this. Deploying such models on edge devices, especially the NVIDIA Jetson Nano presents challenges due to computational and power constraints. Our study compares the SSD MobileNetV2 FPN-Lite 320x320 model on the Jetson Nano for VRU detection with models like YOLOv3 and Faster RCNN. Key findings indicate that the SSD MobileNetV2 FPN-Lite 320x320 model achieves a mean average precision (mAP) of 0.45, precision of 0.80, and recall of 0.65 at 25 FPS in baseline evaluations. With optimisation, the FPS improved to 32 with slight changes in other metrics. The insights also touch upon inherent challenges in VRU detection, suggesting future research directions to refine the SSD MobileNetV2 FPN-Lite model's efficiency, ultimately striving for a safer transportation ecosystem. © 2024, Ismail Saritas. All rights reserved.
KW  - Deep Learning
KW  - Lightweight Model
KW  - Object Detection
KW  - Real-Time Processing
KW  - Road Safety
KW  - Vulnerable Road Users
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Farhat, W.
AU  - Ben Rhaiem, O.
AU  - Faiedh, H.
AU  - Souani, C.
TI  - A novel cooperative collision avoidance system for vehicular communication based on deep learning
PY  - 2024
T2  - International Journal of Information Technology (Singapore)
DO  - 10.1007/s41870-023-01574-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175379965&doi=10.1007%2fs41870-023-01574-3&partnerID=40&md5=3a83a1c4416e7ac73c4a486758865e1f
AB  - Global road traffic injuries represent a significant safety challenge, with the highest fatality rates worldwide stemming from a combination of drivers' reckless behavior and the ever-expanding number of vehicles on the roads. In light of these factors, Autonomous Vehicles (AVs) have received a lot of attention as a technology that has the potential to revolutionize various industries and solve a lot of problems. The rise of the Internet of Vehicles (IoV) and Connected Vehicles (CVs) enables AVs to perceive and react swiftly in complex road scenarios, prioritizing safety. These vehicles prioritize individual safety in all situations, significantly enhancing road safety. Mobile Edge Computing (MEC) plays a crucial role thanks to its capacity for minimal latency and high bandwidth capabilities, enabling critical vehicular applications. In this paper, we introduce a cooperative collision avoidance system based on MEC, named 2CAS-MEC, that proactively identifies and locates road hazards. Our system is based on MEC servers positioned alongside roads, it consistently analyzes traffic and hazard data, issuing targeted alerts to nearby vehicles regarding potential risks. Particularly, our work introduces an advanced deep learning system that utilizes 5G, mobile edge computing, and cloud intelligence to prevent vehicle collisions. The experimental results show that our system outperforms the existing related works and exhibits the anticipated performance, effectively helping drivers in accident avoidance. © The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management 2023.
KW  - Cloud
KW  - Connected and autonomous vehicles
KW  - Deep learning
KW  - IOV
KW  - MEC
KW  - Vehicle to-everything (V2X)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xue, J.
AU  - Chen, H.
AU  - Hu, Y.
AU  - Chen, M.
AU  - Wu, L.-I.
AU  - Chang, X.
TI  - Reduce Detection Latency of YOLOv5 to Prevent Real-Time Tracking Failures for Lightweight Robots
PY  - 2024
T2  - ACM International Conference Proceeding Series
DO  - 10.1145/3671016.3671392
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200919229&doi=10.1145%2f3671016.3671392&partnerID=40&md5=f231388dcb0f7c65543477d762a3bd47
AB  - Lightweight robots are frequently engaged in real-time tracking tasks to provide human companionship services. For effective target tracking, the YOLO series is often employed as a lightweight object detection framework in robot systems. However, YOLO still demands substantial resources to train larger-scale models, striking a balance between accuracy and resource efficiency. Deploying YOLO directly on robots with limited computing resources can lead to significant delays in detection, compromising the effectiveness of tracking tasks. A deeper concern arises from the prevalent use of CPUs as the primary computing units in robots, rendering many existing model optimization techniques, which primarily target GPU computing, unsuitable for this context. To tackle this challenge, we propose a novel detection framework called SLCNet-YOLOv5, specifically designed for deployment in CPU-centric computing environments on robots. The core concept of SLCNet-YOLOv5 entails substituting the native YOLOv5 backbone network with SLCNet, which is a simplified version derived from the existing CPU convolutional neural network, PP-LCNet. It is important to note that our aim is not to enhance PP-LCNet to improve inference accuracy but rather to simplify it to enhance inference speed, while tolerating a certain degree of accuracy loss. This is because excessive inference latency may lead to real-time tracking failures. By employing a backbone network optimized for CPU-centric computation and reducing the computational complexity of the detection model, SLCNet markedly reduces latency, expediting the detection process, with only a minor trade-off in accuracy. In comparison to the performance of the state-of-the-art detector YOLOv5, experimental results on publicly available coco-foot-and-leg and PASCAL VOC datasets demonstrate significant enhancements in detection speed per image on CPU-centric terminals, with respective increases of 62.8% and 81.3%, alongside marginal declines in mean Average Precision (mAP) at 0.5 Intersection over Union (IoU) threshold, with losses of 0.077 and 0.165. © 2024 ACM.
KW  - Inference Latency
KW  - Lightweight Robots
KW  - Object Detection
KW  - Real-Time Tracking
KW  - Chemical detection
KW  - Economic and social effects
KW  - Image enhancement
KW  - Object recognition
KW  - Program processors
KW  - Robots
KW  - Target tracking
KW  - Back-bone network
KW  - Detection framework
KW  - Detection latency
KW  - Inference latency
KW  - Lightweight robot
KW  - Objects detection
KW  - Real time tracking
KW  - Robots system
KW  - Targets tracking
KW  - Tracking failure
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Dai, Y.
AU  - Kim, D.
AU  - Lee, K.
TI  - An Advanced Approach to Object Detection and Tracking in Robotics and Autonomous Vehicles Using YOLOv8 and LiDAR Data Fusion
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13122250
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197290154&doi=10.3390%2felectronics13122250&partnerID=40&md5=ab3a6d55dce91bf2a0cbac942c2a39c3
AB  - Accurately and reliably perceiving the environment is a major challenge in autonomous driving and robotics research. Traditional vision-based methods often suffer from varying lighting conditions, occlusions, and complex environments. This paper addresses these challenges by combining a deep learning-based object detection algorithm, YOLOv8, with LiDAR data fusion technology. The principle of this combination is to merge the advantages of these technologies: YOLOv8 excels in real-time object detection and classification through RGB images, while LiDAR provides accurate distance measurement and 3D spatial information, regardless of lighting conditions. The integration aims to apply the high accuracy and robustness of YOLOv8 in identifying and classifying objects, as well as the depth data provided by LiDAR. This combination enhances the overall environmental perception, which is critical for the reliability and safety of autonomous systems. However, this fusion brings some research challenges, including data calibration between different sensors, filtering ground points from LiDAR point clouds, and managing the computational complexity of processing large datasets. This paper presents a comprehensive approach to address these challenges. Firstly, a simple algorithm is introduced to filter out ground points from LiDAR point clouds, which are essential for accurate object detection, by setting different threshold heights based on the terrain. Secondly, YOLOv8, trained on a customized dataset, is utilized for object detection in images, generating 2D bounding boxes around detected objects. Thirdly, a calibration algorithm is developed to transform 3D LiDAR coordinates to image pixel coordinates, which are vital for correlating LiDAR data with image-based object detection results. Fourthly, a method for clustering different objects based on the fused data is proposed, followed by an object tracking algorithm to compute the 3D poses of objects and their relative distances from a robot. The Agilex Scout Mini robot, equipped with Velodyne 16-channel LiDAR and an Intel D435 camera, is employed for data collection and experimentation. Finally, the experimental results validate the effectiveness of the proposed algorithms and methods. © 2024 by the authors.
KW  - calibrations
KW  - ground threshold
KW  - object detection and tracking
KW  - onboard sensors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Shyam, P.
AU  - Yoo, H.
TI  - Lightweight Thermal Super-Resolution and Object Detection for Robust Perception in Adverse Weather Conditions
PY  - 2024
T2  - Proceedings - 2024 IEEE Winter Conference on Applications of Computer Vision, WACV 2024
DO  - 10.1109/WACV57701.2024.00730
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192022911&doi=10.1109%2fWACV57701.2024.00730&partnerID=40&md5=3539ae78be6015cadb397a16f7c5dacc
AB  - In this work, we examine the potential application of thermal cameras in improving perception capabilities in adverse weather conditions like snow, night-time driving, and haze, focusing on retaining the performance of Advanced Driver Assistance Systems (ADAS), thus enhancing its functionality and safety characteristics. While thermal sensors offer the advantage of robust information capture in adverse weather conditions, their integration is plagued with issues surrounding poor feature capture in normal conditions, low imaging resolution, and high sensor costs. We address the former by formulating the problem definition as information switching wherein thermal images are selected when visible images are degraded. Furthermore, we consider a single object detector for RGB and thermal images to ensure low latency. We propose utilizing a learnable projection function that translates the thermal image into RGB color space, thus providing minimal modifications to the underlying object detector. We address the issues of low imaging resolution and cost by proposing a novel procedure that combines super-resolution and object detection, enabling the utilization of low-resolution and low-cost uncooled thermal imaging sensors. To ensure the complete pipeline meets the actual deployment requirements of real-time inference on resource-constrained devices, we introduce a lightweight super-resolution algorithm, implementing optimizations within the network structure followed by global pruning. In addition, to improve the feature representations extracted by lightweight encoders, we propose a bidirectional feature pyramid network to enhance the feature representation. We demonstrate the efficacy of the proposed mechanism through extensive simulated evaluations on automotive datasets such as FLIR, KAIST, DENSE, and Freiburg Thermal. © 2024 IEEE.
KW  - Applications
KW  - Autonomous Driving
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Computer vision
KW  - Constrained optimization
KW  - Inference engines
KW  - Infrared imaging
KW  - Meteorology
KW  - Object detection
KW  - Optical resolving power
KW  - Adverse weather
KW  - Autonomous driving
KW  - Condition
KW  - Imaging resolutions
KW  - Object detectors
KW  - Objects detection
KW  - Resolution detection
KW  - Superresolution
KW  - Thermal
KW  - Thermal images
KW  - Object recognition
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hütten, N.
AU  - Alves Gomes, M.
AU  - Hölken, F.
AU  - Andricevic, K.
AU  - Meyes, R.
AU  - Meisen, T.
TI  - Deep Learning for Automated Visual Inspection in Manufacturing and Maintenance: A Survey of Open- Access Papers
PY  - 2024
T2  - Applied System Innovation
DO  - 10.3390/asi7010011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185930289&doi=10.3390%2fasi7010011&partnerID=40&md5=10048ad46c43f10e45d6e019bb0c55b4
AB  - Quality assessment in industrial applications is often carried out through visual inspection, usually performed or supported by human domain experts. However, the manual visual inspection of processes and products is error-prone and expensive. It is therefore not surprising that the automation of visual inspection in manufacturing and maintenance is heavily researched and discussed. The use of artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a possible paradigm shift and have the potential to facilitate automated visual inspection, even under complex environmental conditions. For this reason, we explore the question of to what extent deep learning is already being used in the field of automated visual inspection and which potential improvements to the state of the art could be realized utilizing concepts from academic research. By conducting an extensive review of the openly accessible literature, we provide an overview of proposed and in-use deep-learning models presented in recent years. Our survey consists of 196 open-access publications, of which 31.7% are manufacturing use cases and 68.3% are maintenance use cases. Furthermore, the survey also shows that the majority of the models currently in use are based on convolutional neural networks, the current de facto standard for image classification, object recognition, or object segmentation tasks. Nevertheless, we see the emergence of vision transformer models that seem to outperform convolutional neural networks but require more resources, which also opens up new research opportunities for the future. Another finding is that in 97% of the publications, the authors use supervised learning techniques to train their models. However, with the median dataset size consisting of 2500 samples, deep-learning models cannot be trained from scratch, so it would be beneficial to use other training paradigms, such as self-supervised learning. In addition, we identified a gap of approximately three years between approaches from deep-learning-based computer vision being published and their introduction in industrial visual inspection applications. Based on our findings, we additionally discuss potential future developments in the area of automated visual inspection. © 2024 by the authors.
KW  - automated visual inspection
KW  - computer vision
KW  - convolutional neural network
KW  - deep learning
KW  - industrial applications
KW  - vision transformer
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tong, K.
AU  - Wu, Y.-Q.
TI  - Research Advances on Deep Learning Based Small Object Detection Benchmarks
ST  - 基于深度学习的小目标检测基准研究进展
PY  - 2024
T2  - Tien Tzu Hsueh Pao/Acta Electronica Sinica
DO  - 10.12263/DZXB.20230624
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193003601&doi=10.12263%2fDZXB.20230624&partnerID=40&md5=93efe83679212f23b93e4d5eb5666d02
AB  - Small object detection is an extremely challenging task in computer vision. It is widely used in remote sensing, intelligent transportation, national defense and military, daily life and other fields. Compared to other visual tasks such as image segmentation, action recognition, object tracking, generic object detection, image classification, video caption and human pose estimation, the research progress of small object detection is relatively slow. We believe that the constraints mainly include two aspects: the intrinsic difficulty of learning small object features and the scarcity of small object detection benchmarks. In particular, the scarcity of small object detection benchmarks can be considered from two aspects: the scarcity of small object detection datasets and the difficulty of establishing evaluation metrics for small object detection. To gain a deeper understanding of small object detection, this article conducts a brand-new and thorough investigation on small object detection benchmarks based on deep learning for the first time. The existing 35 small object detection datasets are introduced from 7 different application scenarios, such as remote sensing images, traffic sign and traffic light detection, pedestrian detection, face detection, synthetic aperture radar images and infrared images, daily life and others. Meanwhile, comprehensively summarize the definition of small objects from both relative scale and absolute scale. For the absolute scale, it mainly includes 3 categories: the width or height of the object bounding box, the product of the width and height of the object bounding box, and the square root of the area of the object bounding box. The focus is on exploring the evaluation metrics of small object detection in detail from 3 aspects: based on IoU (Intersection over Union) and its variants, based on average precision and its variants, and other evaluation metrics. In addition, in-depth analysis and comparison of the performance of some representative small object detection algorithms under typical evaluation metrics are conducted on 6 datasets. These categories of typical evaluation metrics can be further subdivided, including the evaluation metric plus the definition of objects, the evaluation metric plus single object category. More concretely, the evaluation metrics plus the definition of objects can be divided into 4 categories: average precision plus the definition of objects, miss rate plus the definition of objects, DoR-AP-SM (Degree of Reduction in Average Precision between Small objects and Medium objects) and DoR-APSL (Degree of Reduction in Average Precision between Small objects and Large objects). For the evaluation metrics plus single object category, it mainly includes 2 types: average precision plus single object category, OLRP (Optimal Localization Recall Precision) plus single object category. These representative small object detection methods mainly include anchor mechanism, scale-aware and fusion, context information, super-resolution technique and other improvement ideas. Finally, we point out the possible trends in the future from 6 aspects: a new benchmark for small object detection, a unified definition of small objects, a new framework for small object detection, multi-modal small object detection algorithms, rotating small object detection, and high precision and real time small object detection. We hope that this paper could provide a timely and comprehensive review of the research progress of small object detection benchmarks based on deep learning, and inspire relevant researchers to further promote the development of this field. © 2024 Chinese Institute of Electronics. All rights reserved.
KW  - deep learning
KW  - evaluation metric of small objects
KW  - small object dataset
KW  - small object detection
KW  - small object detection benchmark
KW  - the definition of small objects
KW  - Deep learning
KW  - Face recognition
KW  - Image classification
KW  - Image segmentation
KW  - Infrared imaging
KW  - Object recognition
KW  - Radar imaging
KW  - Remote sensing
KW  - Synthetic aperture radar
KW  - Tracking radar
KW  - Traffic signs
KW  - Vision
KW  - Deep learning
KW  - Evaluation metric of small object
KW  - Evaluation metrics
KW  - Small object dataset
KW  - Small object detection
KW  - Small object detection benchmark
KW  - Small objects
KW  - The definition of small object
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Y.
AU  - Gong, Y.
AU  - Chen, X.
TI  - Research on YOLOv5 Vehicle Detection and Positioning System Based on Binocular Vision
PY  - 2024
T2  - World Electric Vehicle Journal
DO  - 10.3390/wevj15020062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185551708&doi=10.3390%2fwevj15020062&partnerID=40&md5=78a93cd6a5bf8752459d408f78934419
AB  - Vehicle detection and location is one of the key sensing tasks of automatic driving systems. Traditional detection methods are easily affected by illumination, occlusion and scale changes in complex scenes, which limits the accuracy and robustness of detection. In order to solve these problems, this paper proposes a vehicle detection and location method for YOLOv5(You Only Look Once version 5) based on binocular vision. Binocular vision uses two cameras to obtain images from different angles at the same time. By calculating the difference between the two images, more accurate depth information can be obtained. The YOLOv5 algorithm is improved by adding the CBAM attention mechanism and replacing the loss function to improve target detection. Combining these two techniques can achieve accurate detection and localization of vehicles in 3D space. The method utilizes the depth information of binocular images and the improved YOLOv5 target detection algorithm to achieve accurate detection and localization of vehicles in front. Experimental results show that the method has high accuracy and robustness for vehicle detection and localization tasks. © 2024 by the authors.
KW  - binocular vision
KW  - positioning
KW  - ranging
KW  - stereo matching
KW  - vehicle detection
KW  - YOLOv5 algorithm
KW  - Automobile drivers
KW  - Image enhancement
KW  - Stereo image processing
KW  - Stereo vision
KW  - Vehicles
KW  - Depth information
KW  - Detection and localization
KW  - Detection methods
KW  - Positioning
KW  - Stereo-matching
KW  - Vehicle detection systems
KW  - Vehicle location
KW  - Vehicle positioning
KW  - Vehicles detection
KW  - You only look once version 5 algorithm
KW  - Binocular vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cui, S.
AU  - Liu, F.
AU  - Wang, Z.
AU  - Zhou, X.
AU  - Yang, B.
AU  - Li, H.
AU  - Yang, J.
TI  - DAN-YOLO: A Lightweight and Accurate Object Detector Using Dilated Aggregation Network for Autonomous Driving
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13173410
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204153465&doi=10.3390%2felectronics13173410&partnerID=40&md5=96ed02400f4b76edda34ed4b4991837b
AB  - Object detection is becoming increasingly critical in autonomous driving. However, the accuracy and effectiveness of object detectors are often constrained by the obscuration of object features and details in adverse weather conditions. Therefore, this paper presented the DAN-YOLO vehicle object detector specifically designed for driving conditions in adverse weather. Building on the YOLOv7-Tiny network, SPP was replaced with SPPF, resulting in the SPPFCSPC structure, which enhances processing speed. The concept of Hybrid Dilated Convolution (HDC) was also introduced to improve the SPPFCSPC and ELAN-T structures, expanding the network’s receptive field (RF) while maintaining a lightweight design. Furthermore, an efficient multi-scale attention (EMA) mechanism was introduced to enhance the effectiveness of feature fusion. Finally, the Wise-IoUv1 loss function was employed as a replacement for CIoU to enhance the localization accuracy of the bounding box (bbox) and the convergence speed of the model. With an input size of 640 × 640, the DAN-YOLO algorithm proposed in this study achieved an increase in mAP0.5 values of 3.4% and 6.3% compared to the YOLOv7-Tiny algorithm in the BDD100K and DAWN benchmark tests, respectively, while achieving real-time detection (142.86 FPS). When compared with other state-of-the-art detectors, it reports better trade-off in terms of detection accuracy and speed under adverse driving conditions, indicating the suitability for autonomous driving applications. © 2024 by the authors.
KW  - adverse weather conditions
KW  - attention mechanism
KW  - autonomous driving
KW  - object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lu, A.
AU  - Liu, J.
AU  - Cui, H.
AU  - Ma, L.
AU  - Ma, Q.
TI  - MLP-YOLOv5: A Lightweight Multi-Scale Identification Model for Lotus Pods with Scale Variation
PY  - 2024
T2  - Agriculture (Switzerland)
DO  - 10.3390/agriculture14010030
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183397240&doi=10.3390%2fagriculture14010030&partnerID=40&md5=1e21eeb234d62eda2bea8161570fa1fa
AB  - Lotus pods in unstructured environments often present multi-scale characteristics in the captured images. As a result, it makes their automatic identification difficult and prone to missed and false detections. This study proposed a lightweight multi-scale lotus pod identification model, MLP-YOLOv5, to deal with this difficulty. The model adjusted the multi-scale detection layer and optimized the anchor box parameters to enhance the small object detection accuracy. The C3 module with transformer encoder (C3-TR) and the shuffle attention (SA) mechanism were introduced to improve the feature extraction ability and detection quality of the model. GSConv and VoVGSCSP modules were adopted to build a lightweight neck, thereby reducing model parameters and size. In addition, SIoU was utilized as the loss function of bounding box regression to achieve better accuracy and faster convergence. The experimental results on the multi-scale lotus pod test set showed that MLP-YOLOv5 achieved a mAP of 94.9%, 3% higher than the baseline. In particular, the model’s precision and recall for small-scale objects were improved by 5.5% and 7.4%, respectively. Compared with other mainstream algorithms, MLP-YOLOv5 showed more significant advantages in detection accuracy, parameters, speed, and model size. The test results verified that MLP-YOLOv5 can quickly and accurately identify multi-scale lotus pod objects in complex environments. It could effectively support the harvesting robot by accurately and automatically picking lotus pods. © 2023 by the authors.
KW  - deep learning
KW  - lightweight
KW  - lotus pod
KW  - MLP-YOLOv5
KW  - multi-scale object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mori, K.T.
AU  - Peters, S.
TI  - SHARD: Safety and Human Performance Analysis for Requirements in Detection
PY  - 2024
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2023.3320395
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173304894&doi=10.1109%2fTIV.2023.3320395&partnerID=40&md5=0a6201b9626493b6a556bc343742a3d2
AB  - Automated driving requires reliable perception of the environment to ensure the safety of the driving task. One common perception task is 3D object detection, which aims at perceiving location and attributes of dynamic objects. This task is typically evaluated on different benchmark datasets, which each propose different metrics. However, these different metrics generally lack consistency and bear no relation to safety. Most notably, there is a lack of consistent definitions of pass/fail criteria for any given detection metric. In this work, the issue is addressed by systematically considering safety and human performance across different aspects of the object detection task. This approach yields interpretable detection metrics as well as thresholds for pass/fail criteria. Furthermore, a validation approach leveraging a prediction network is introduced and successfully applied to the requirements. A comparison of existing detectors shows that current perception algorithms exhibit failures for a majority of objects on the nuScenes dataset. Therefore, the results indicate the necessity of explicit safety consideration in the development of perception algorithms for the automated driving task.  © 2016 IEEE.
KW  - Environment perception
KW  - object detection
KW  - requirements
KW  - testing
KW  - Benchmarking
KW  - Job analysis
KW  - Object recognition
KW  - Reliability analysis
KW  - Safety engineering
KW  - Safety testing
KW  - Automated driving
KW  - Benchmark testing
KW  - Environment perceptions
KW  - Human performance
KW  - Location awareness
KW  - Objects detection
KW  - Requirement
KW  - Safety performance
KW  - Task analysis
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, S.
AU  - Wu, J.
AU  - Lv, B.
AU  - Pan, X.
AU  - Wang, X.
TI  - Data Fusion of Roadside Camera, LiDAR, and Millimeter-Wave Radar
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3448428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001209750&doi=10.1109%2fJSEN.2024.3448428&partnerID=40&md5=4e9bb5a1b5c1c372cf065d8ea765a6d3
AB  - Roadside sensor data fusion is an essential component of the vehicle-road cooperation system, thus effectively enhancing the interactive perception level among road targets. However, due to the complex road environment, occlusion, and other problems, the single sensor has low accuracy in the process of target tracking. How to realize the fusion of multisensor trajectory tracking data is the main problem to be solved at present. Therefore, a new multisensor data fusion method for roadside camera, LiDAR, and millimeter wave (mm-Wave) radar is proposed in this study. According to the change in reflection intensity caused by the shift of the LiDAR point cloud with the change in distance and the detection accuracy of mm-Wave radar used in this article, the weight parameters of LiDAR and mm-Wave radar in the fusion process are determined. Finally, the target missed detection rate and trajectory disconnected repair rate were customized, and experimental tests were conducted in five natural environments to verify the robustness of the proposed method.  © 2001-2012 IEEE.
KW  - Camera
KW  - dynamic weight
KW  - LiDAR
KW  - millimeter-wave (mm-Wave) radar
KW  - object detection
KW  - roadside sensor fusion
KW  - trajectory tracking
KW  - Image compression
KW  - Image segmentation
KW  - Information fusion
KW  - Intelligent systems
KW  - Motor transportation
KW  - Network security
KW  - Radar reflection
KW  - Radar target recognition
KW  - Radar tracking
KW  - Remote sensing
KW  - Sensor data fusion
KW  - Dynamic weight
KW  - LiDAR
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Objects detection
KW  - Roadside cameras
KW  - Roadside sensor fusion
KW  - Sensor fusion
KW  - Sensors data fusion
KW  - Trajectory-tracking
KW  - Cameras
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tanzib Hosain, M.
AU  - Zaman, A.
AU  - Abir, M.R.
AU  - Akter, S.
AU  - Mursalin, S.
AU  - Khan, S.S.
TI  - Synchronizing Object Detection: Applications, Advancements and Existing Challenges
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3388889
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190733293&doi=10.1109%2fACCESS.2024.3388889&partnerID=40&md5=485748a70e12d9688561673375c683ab
AB  - From pivotal roles in autonomous vehicles, healthcare diagnostics, and surveillance systems to seamlessly integrating with augmented reality, object detection algorithms stand as the cornerstone in unraveling the complexities of the visual world. Tracing the trajectory from conventional region-based methods to the latest neural network architectures reveals a technological renaissance where algorithms metamorphose into digital artisans. However, this journey is not without hurdles, prompting researchers to grapple with real-time detection, robustness in varied environments, and interpretability amidst the intricacies of deep learning. The allure of addressing issues such as occlusions, scale variations, and fine-grained categorization propels exploration into uncharted territories, beckoning the scholarly community to contribute to an ongoing saga of innovation and discovery. This research offers a comprehensive panorama, encapsulating the applications reshaping our digital reality, the advancements pushing the boundaries of perception, and the open issues extending an invitation to the next generation of visionaries to explore uncharted frontiers within object detection.  © 2013 IEEE.
KW  - image classification
KW  - image recognition
KW  - Object detection
KW  - object segmentation
KW  - object tracking
KW  - semantic detection
KW  - Augmented reality
KW  - Computer architecture
KW  - Deep learning
KW  - Image classification
KW  - Image recognition
KW  - Image segmentation
KW  - Interactive computer systems
KW  - Network architecture
KW  - Neural networks
KW  - Object detection
KW  - Object recognition
KW  - Real time systems
KW  - Signal detection
KW  - Tracking (position)
KW  - Computational modelling
KW  - Deep learning
KW  - Images classification
KW  - Object Tracking
KW  - Objects detection
KW  - Objects segmentation
KW  - Real - Time system
KW  - Semantic detection
KW  - YOLO
KW  - Semantics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Liu, L.
AU  - Lin, R.
AU  - Zhang, F.
TI  - Vision and Laser-Based Mobile Robot Following and Mapping
PY  - 2024
T2  - 2024 2nd International Conference on Mechatronics, Control and Robotics, ICMCR 2024
DO  - 10.1109/ICMCR60777.2024.10481877
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190510154&doi=10.1109%2fICMCR60777.2024.10481877&partnerID=40&md5=4bcf42050c509993d4e480cb7652eb2a
AB  - This paper presents an algorithm called Real-time Vision-based Following Map (RVFM), which enables a mobile robot to perceive and track human body positions in space based on visual perception. Simultaneously, it creates a map of the environment, addressing the autonomous mapping issue for mobile robots in unpredictable environments. The algorithm utilizes the YOLOv5 algorithm for target recognition and optimizes detection results by clustering real bounding box sizes using the DBSCAN algorithm. By optimizing the loss function, detection performance is further improved. For target tracking, a combination of siamese neural networks and RPN network regression is employed, enhancing tracking accuracy and stability. In terms of path planning, the DWA algorithm is introduced, combined with a cost function representing the field of view, to achieve more accurate path planning and keep the target human within the camera's field of view center. For mapping, the algorithm is based on Gmapping and incorporates Kalman filtering and outlier removal techniques to ensure mapping accuracy and reliability. Experimental results demonstrate that the RVFM algorithm provides more accurate human body position information, achieving precise following. It also performs well in indoor path planning and obstacle avoidance, ensuring safety and efficiency during the following process.  © 2024 IEEE.
KW  - DWA
KW  - human following
KW  - mapping
KW  - siamese neural networks
KW  - YOLOv5
KW  - Clustering algorithms
KW  - Cost functions
KW  - Mobile robots
KW  - Motion planning
KW  - Robot vision
KW  - Target tracking
KW  - Body positions
KW  - DWA
KW  - Field of views
KW  - Human bodies
KW  - Human following
KW  - Neural-networks
KW  - Real time vision
KW  - Siamese neural network
KW  - Vision based
KW  - YOLOv5
KW  - Mapping
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Bouassida, S.
AU  - Nouveliere, L.
AU  - Neji, N.
AU  - Neji, J.
TI  - Enhancing Road Safety: A Comparative Study Between UAV-Assisted and Autonomous Vehicles
PY  - 2024
T2  - 2024 18th International Conference on Control, Automation, Robotics and Vision, ICARCV 2024
DO  - 10.1109/ICARCV63323.2024.10821584
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217365115&doi=10.1109%2fICARCV63323.2024.10821584&partnerID=40&md5=9aa7410164ee6335afe50440b6074379
AB  - The integration of connected autonomous vehicles (CAV) on open roads has gained significant progress in addressing road safety. These vehicles use advanced sensor technology to perceive and react to the road environment, reducing accident risks. Despite these advancements, limitations persist in their perception capabilities. To overcome these limitations, interest is growing in using Unmanned Aerial Vehicles (UAVs) for traffic surveillance, offering extensive coverage and enhanced responsiveness over fixed sensors. In this article, by tackling an optimization problem in road safety using Particle Swarm Optimization (PSO), we particularly focus on a situation where a random flow of vehicles aims to navigate an intersection safely. We compare two scenarios with and without the assistance of an UAV: one where vehicles autonomously manage their speed, and another one where an UAV improve traffic management. Simulation results underscore the pivotal role of drone-assisted vehicles in enhancing road safety, compared to sensors embedded within CAVs. Towards the end of the article, we explore the efficiency of the drone strategy by addressing the issue of delay in the acceptance and implementation of optimal speed instructions, comparing scenarios with and without this delay.  © 2024 IEEE.
KW  - Acceptability
KW  - CAV
KW  - drone to vehicle communication (U2V)
KW  - Multi-Agent system
KW  - optimization
KW  - road safety
KW  - UAV
KW  - Advanced traffic management systems
KW  - Air navigation
KW  - Air traffic control
KW  - Aircraft accidents
KW  - Aircraft communication
KW  - Autonomous vehicles
KW  - Highway accidents
KW  - Highway administration
KW  - Highway traffic control
KW  - Magnetic levitation vehicles
KW  - Motor transportation
KW  - Road vehicles
KW  - Street traffic control
KW  - Vehicle safety
KW  - Acceptability
KW  - Aerial vehicle
KW  - Autonomous Vehicles
KW  - Connected autonomous vehicle
KW  - Drone to vehicle communication (U2V)
KW  - Multiagent systems (MASs)
KW  - Optimisations
KW  - Road safety
KW  - Unmanned aerial vehicle
KW  - Vehicle communications
KW  - Drones
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chang, C.-Y.
AU  - Khanum, A.
AU  - Su, S.-G.
AU  - Lebaku Moses, M.
AU  - Liu, K.-X.
TI  - Vertical-Line Mura Defect Detection for TFT-LCDs
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3486567
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209184562&doi=10.1109%2fACCESS.2024.3486567&partnerID=40&md5=4dd3cc1e308afdfa844270fdb3376b54
AB  - Mura defects, which manifest as irregularities in brightness and color on display screens, present persistent challenges for Thin Film Transistor Liquid Crystal Display (TFT-LCD) manufacturers. While traditional methods effectively detect significant abnormalities in vertical-line inspections, identifying minor defects, particularly Level-1, remains formidable. In this study, we leverage artificial intelligence to address this challenge, focusing specifically on detecting vertical-line mura defects. Given the difficulty in discerning Level-1 defects directly from single images, where higher-level abnormalities are more conspicuous, our approach introduces You Only Look Once-Ghost Attention (YOLO-GA). This advanced YOLOv8-based algorithm is meticulously designed to swiftly and accurately identify vertical lines mura in liquid crystal displays (LCD) images, even amidst complex backgrounds and minor irregularities. To enhance the model's efficacy, we adopt two pivotal strategies. Firstly, we incorporate the Ghost layer as the backbone and neck network to ensure lightweight deployment while improving feature extraction capabilities, particularly in images with intricate backgrounds. Additionally, we integrate the CBAM (Convolutional Block Attention Module) into the network's architecture, explicitly targeting vertical line mura detection. This augmentation aims to bolster feature extraction and refine detection, especially for defects within liquid crystal displays (LCDs). The dataset utilized in our study is sourced from AUO Corporation and captured using a real single-direction camera. The dataset contains 107,080 images, divided into an 80:20 ratio for training and validation. Each image has a high resolution of 1624× 1240$. Our experimental results show that our approach is highly effective, achieving mAP and F1-Scores of 99.5% and 99.7%, respectively, compared to the baseline model and the comparative attention module. Moreover, the proposed model can significantly reduce the time required to recognize mura defects, fully meeting the manufacturer's production requirements within a mere 1-second timeframe.  © 2013 IEEE.
KW  - Computer vision
KW  - mura defect
KW  - smart manufacturing
KW  - TFT-LCD
KW  - vertical-line mura
KW  - Inspection
KW  - Liquid crystal displays
KW  - Liquid crystals
KW  - Process control
KW  - Smart manufacturing
KW  - Defect detection
KW  - Display screen
KW  - Features extraction
KW  - Level-1
KW  - Liquid-crystal display
KW  - Mura defect
KW  - Smart manufacturing
KW  - Thin film transistors liquid crystal display
KW  - Vertical lines
KW  - Vertical-line mura
KW  - Thin film transistors
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.-Y.
AU  - Qu, Z.
AU  - Gao, L.-Y.
TI  - Multi-Spatial Pyramid Feature and Optimizing Focal Loss Function for Object Detection
PY  - 2024
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2023.3282996
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161600294&doi=10.1109%2fTIV.2023.3282996&partnerID=40&md5=edfe3bf5e9dc3abdd8802480ba9d73d5
AB  - Previous deep convolutional neural network research has made significant progress toward improving the speed and accuracy of object detection. However, despite these advancements, the inaccurate detection of multi-object (small objects) remains challenging in the traffic environments. In this article, we propose a new architecture called YOLOM, which is specifically designed to achieve enhanced multi-object (small objects) detection precision. YOLOM incorporates several innovative features: a multi-spatial pyramid (MSP), an optimized focal loss (OFLoss) function, and an objectness loss that incorporates effective intersection over union (EIoU) calculations. These features collectively yield enhanced accuracy and reduce the miss rate of small objects, particularly in the multi-object cases. According to the sizes of receptive field features with different spatial scales with pooling layers, we propose the MSP module. We optimize the focal loss as a classification function instead of the cross-entropy loss, which solves some class imbalance problems caused by anchor-free detection when encountering disparate datasets. Due to the superior performance of EIoU in confidence scoring, we use EIoU to participate in the objectness loss calculation of our work. Therefore, our method substitutes EIoU for YOLOX's objectness loss. The experimental results demonstrate that our strategies significantly outperform some end-to-end object detection methods.  © 2016 IEEE.
KW  - Convolutional neural networks
KW  - multi-spatial pyramid feature
KW  - object detection
KW  - objectness loss with EIoU
KW  - optimizing focal loss function
KW  - Classification (of information)
KW  - Convolution
KW  - Deep neural networks
KW  - Intelligent vehicle highway systems
KW  - Object detection
KW  - Object recognition
KW  - Convolutional neural network
KW  - Features extraction
KW  - Loss functions
KW  - Multi-spatial pyramid feature
KW  - Objectness loss with effective intersection over union
KW  - Objects detection
KW  - Optimizing focal loss function
KW  - Spatial pyramids
KW  - Transformer
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, H.
AU  - Zhang, X.
AU  - He, J.
AU  - Geng, Z.
AU  - Yu, Y.
AU  - Cheng, Y.
TI  - Panoptic Water Surface Visual Perception for USVs Using Monocular Camera Sensor
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3413088
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196734650&doi=10.1109%2fJSEN.2024.3413088&partnerID=40&md5=d11a2924840dc5cc3e5553d60079d38a
AB  - In recent years, the significance of unmanned surface vehicles (USVs) has grown substantially across a wide range of applications. Monocular cameras, as the most common perception sensors deployed in USVs, offer the inherent advantages of rich semantic information and low-cost deployment. However, visual perception methods for USVs face challenges from water surface application and lose efficiency in harsh weather cases. To achieve robust perception capabilities through monocular cameras for USVs, we propose a panoptic water surface visual perception framework that can accomplish various perception tasks, including drivable area segmentation, object detection, and raindrop segmentation in water surface scenes. In addition, our framework provides a segmentation anything model (SAM)-driven training method to improve the robustness of the proposed model through low-cost model iteration. Moreover, the proposed perception method demonstrates excellent accuracy, robustness, and high inference speed compared to other lightweight baseline methods and can be deployed to low-power embedded platforms in USVs. 1558-1748  © 2024 IEEE.
KW  - Lightweight network
KW  - multitask learning
KW  - segmentation anything model (SAM) segmentation
KW  - unmanned surface vehicles (USVs) perception
KW  - visual perception
KW  - Cameras
KW  - Iterative methods
KW  - Job analysis
KW  - Object detection
KW  - Object recognition
KW  - Robotics
KW  - Semantic Segmentation
KW  - Unmanned surface vehicles
KW  - Vision
KW  - Head
KW  - Images segmentations
KW  - Lightweight network
KW  - Multitask learning
KW  - SAM segmentation
KW  - Task analysis
KW  - Unmanned surface vehicle  perception
KW  - Visual perception
KW  - Water surface
KW  - Semantics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Wei, C.
AU  - Wu, G.
AU  - Barth, M.J.
TI  - Feature Corrective Transfer Learning: End-to-End Solutions to Object Detection in Non-Ideal Visual Conditions
PY  - 2024
T2  - IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops
DO  - 10.1109/CVPRW63382.2024.00007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206494782&doi=10.1109%2fCVPRW63382.2024.00007&partnerID=40&md5=9ea2de00aa4ab4a279b12b37b2848e5a
AB  - A significant challenge in the field of object detection lies in the system's performance under non-ideal imaging conditions, such as rain, fog, low illumination, or raw Bayer images that lack ISP processing. Our study introduces 'Feature Corrective Transfer Learning', a novel approach that leverages transfer learning and a bespoke loss function to facilitate the end-to-end detection of objects in these challenging scenarios without the need to convert non-ideal images into their RGB counterparts. In our methodology, we initially train a comprehensive model on a pristine RGB image dataset. Subsequently, non-ideal images are processed by comparing their feature maps against those from the initial ideal RGB model. This comparison employs the Extended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function designed to quantify similarities and integrate them into the detection loss. This approach refines the model's ability to perform object detection across varying conditions through direct feature map correction, encapsulating the essence of Feature Corrective Transfer Learning. Experimental validation on variants of the KITTI dataset demonstrates a significant improvement in mean Average Precision (mAP), resulting in a 3.8-8.1% relative enhancement in detection under non-ideal conditions compared to the baseline model, and a less marginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved under ideal conditions by the standard Faster RCNN algorithm. © 2024 IEEE.
KW  - Feature Correction
KW  - Non-Ideal Visual Conditions
KW  - Object Detection
KW  - Transfer Learning
KW  - Federated learning
KW  - Image enhancement
KW  - Transfer learning
KW  - Condition
KW  - Feature correction
KW  - Feature map
KW  - Ideal images
KW  - Loss functions
KW  - Non-ideal visual condition
KW  - Nonideal
KW  - Objects detection
KW  - Transfer learning
KW  - Visual condition
KW  - Contrastive Learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Deng, L.
AU  - Fu, R.
AU  - Li, Z.
AU  - Liu, B.
AU  - Xue, M.
AU  - Cui, Y.
TI  - Lightweight cross-modal multispectral pedestrian detection based on spatial reweighted attentionmechanism
PY  - 2024
T2  - Computers, Materials and Continua
DO  - 10.32604/cmc.2024.048200
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189247095&doi=10.32604%2fcmc.2024.048200&partnerID=40&md5=da963075dce58c9906cd9a1d62e73063
AB  - Multispectral pedestrian detection technology leverages infrared images to provide reliable information for visible light images, demonstrating significant advantages in low-light conditions and background occlusion scenarios. However, while continuously improving cross-modal feature extraction and fusion, ensuring themodel s detection speed is also a challenging issue. We have devised a deep learning network model for cross-modal pedestrian detectionbasedonResnet50, aiming tofocus onmore reliable features andenhance themodel s detectionefficiency. This model employs a spatial attention mechanism to reweight the input visible light and infrared image data, enhancing the model s focus on different spatial positions and sharing the weighted feature data across different modalities, thereby reducing the interference of multi-modal features. Subsequently, lightweight modules with depthwise separable convolution are incorporated to reduce the model s parameter count and computational load through channel-wise and point-wise convolutions. The network model algorithm proposed in this paper was experimentally validated on the publicly available KAIST dataset and compared with other existing methods. The experimental results demonstrate that our approach achieves favorable performance in various complex environments, affirming the effectiveness of the multispectral pedestrian detection technology proposed in this paper. © 2024 Tech Science Press. All rights reserved.
KW  - Convolutional neural networks
KW  - Depth separable convolution
KW  - Multispectral pedestrian detection
KW  - Spatially reweighted attention mechanism
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Feature extraction
KW  - Image enhancement
KW  - Learning systems
KW  - Scattering parameters
KW  - Attention mechanisms
KW  - Convolutional neural network
KW  - Cross-modal
KW  - Depth separable convolution
KW  - Detection technology
KW  - Multi-spectral
KW  - Multispectral pedestrian detection
KW  - Network models
KW  - Pedestrian detection
KW  - Spatially reweighted attention mechanism
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, H.
AU  - Yu, K.
AU  - Qiu, J.
AU  - Wang, Z.
AU  - Yang, Y.
TI  - IA-Det: Iterative Attention-Based Robust Object Detection in Adverse Traffic Scenes
PY  - 2024
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2024.3438845
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201759313&doi=10.1109%2fTIM.2024.3438845&partnerID=40&md5=2486d2f0c72e4cae9bb9cf8064365edd
AB  - Robust traffic object detection has significant meanings for applications like intelligent transportation and surveillance. However, detection in the open world faces various adverse factors like occlusion, bad weather, and low light. Current endeavors focus on proposing better learnable modules for better feature representation, but their generalization ability is limited by the quality and diversity of training data. In this article, we propose an iterative attention (IA)-Det to achieve robust detection at various challenging transportation scenes by fully exploring semantics and suppressing the noises. First, an IA based on the t-mixture probability model is proposed to collect rich semantics at the localization branch. Unlike earlier attention methods, the IA module inherits the robust characteristic of t-mixture and asymmetrically redistributes the attention to features based on their similarity to semantic centers. Therefore, out-of-distribution features can receive greater opportunities for clustering into their semantic neighbors rather than being suppressed as noises. For the classification branch, an effective and efficient classification responding block (CRB) is designed to activate classification-aware semantics from the collected attention map. Comprehensive experiments on normal, rainy, foggy, and low light fusion datasets show that the proposed IA-Det achieves state-of-the-art performance with an impressive generalization ability.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
KW  - Attention mechanism
KW  - domain generalization
KW  - probability model
KW  - robust detection
KW  - transportation application
KW  - Image segmentation
KW  - Attention mechanisms
KW  - Domain generalization
KW  - Features extraction
KW  - Generalisation
KW  - Location awareness
KW  - Noise
KW  - Objects detection
KW  - Probability modelling
KW  - Robust detection
KW  - Transportation application
KW  - Normal distribution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Guan, R.
AU  - Yao, S.
AU  - Liu, L.
AU  - Zhu, X.
AU  - Man, K.L.
AU  - Yue, Y.
AU  - Smith, J.
AU  - Lim, E.G.
AU  - Yue, Y.
TI  - Mask-VRDet: A robust riverway panoptic perception model based on dual graph fusion of vision and 4D mmWave radar
PY  - 2024
T2  - Robotics and Autonomous Systems
DO  - 10.1016/j.robot.2023.104572
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176149010&doi=10.1016%2fj.robot.2023.104572&partnerID=40&md5=40c8ea5a69d8cc4636a641bc6abba89d
AB  - With the development of Unmanned Surface Vehicles (USVs), the perception of inland waterways has become significant to autonomous navigation. RGB cameras can capture images with rich semantic features, but they would fail in adverse weather and at night. As a perception sensor that has initially emerged in recent years, 4D millimeter-wave radar (4D mmWave radar) can work in all weather and has more abundant point-cloud features than ordinary radar, but it also suffers from water-surface clutter seriously. Furthermore, the shape and outline of dense point cloud captured by 4D mmWave radar are irregular. CNN-based neural networks treat features as 2D rectangle grids, which excessively favor image modality and are unfriendly to radar modality. Therefore, we transform both features of image and radar into non-Euclidean space as graph structures. In this paper, we focus on robust panoptic perception in inland waterways. Firstly, we propose the first Clutter-Point-Removal (CPR) algorithm for 4D mmWave radar, removing water-surface clutter and improving the recall of radar targets. Secondly, we propose a high-performance panoptic perception model based on the graph neural network called Mask-VRDet, fusing features of vision and radar to simultaneously perform object detection and semantic segmentation. To the best of our knowledge, Mask-VRDet is the first riverway panoptic perception model based on vision-radar graphical fusion. It outperforms other single-modal and fusion models, and achieves state-of-the-art performance on our collected dataset. We release our code at https://github.com/GuanRunwei/Mask-VRDet-Official. © 2023 Elsevier B.V.
KW  - Fusion of vision and radar
KW  - Graph convolution network
KW  - Radar clutter removal
KW  - Riverway panoptic perception
KW  - Clutter (information theory)
KW  - Graphic methods
KW  - Image segmentation
KW  - Millimeter waves
KW  - Object detection
KW  - Object recognition
KW  - Radar imaging
KW  - Semantics
KW  - Surface waters
KW  - Unmanned surface vehicles
KW  - Fusion of vision and radar
KW  - Graph convolution network
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Model-based OPC
KW  - Perception model
KW  - Point-clouds
KW  - Radar clutter removal
KW  - Riverway panoptic perception
KW  - Water surface
KW  - Radar clutter
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kaur, J.
AU  - Kaur, N.
TI  - Enhancing Object Detection in Autonomous Driving with Improved-YOLOv8
PY  - 2024
T2  - IET Conference Proceedings
DO  - 10.1049/icp.2025.0845
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003572005&doi=10.1049%2ficp.2025.0845&partnerID=40&md5=6a96cb19c2dd8da6637a720fe4de4612
AB  - Autonomous driving systems rely on object recognition to perform localization and classification, among other essential functions. Even though deep learning has greatly improved object identification, training periods are frequently rather lengthy due to the large number of parameters. The Improved-YOLOv8 model, which uses a pruning technique to decrease the number of parameters and speed up training, is presented to solve this problem. The effectiveness of the suggested method is shown by experiments run on the KITTI dataset, which is widely used in autonomous driving research. © The Institution of Engineering & Technology 2024.
KW  - autonomous driving
KW  - deep learning
KW  - object detection
KW  - yolo
KW  - Object recognition
KW  - Autonomous driving
KW  - Deep learning
KW  - Driving systems
KW  - Localisation
KW  - Object identification
KW  - Objects detection
KW  - Objects recognition
KW  - Pruning techniques
KW  - Speed up
KW  - Yolo
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Yang, B.
AU  - Yang, J.
TI  - SITAR: Evaluating the Adversarial Robustness of Traffic Light Recognition in Level-4 Autonomous Driving
PY  - 2024
T2  - IEEE Intelligent Vehicles Symposium, Proceedings
DO  - 10.1109/IV55156.2024.10588456
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199752181&doi=10.1109%2fIV55156.2024.10588456&partnerID=40&md5=e46c79189a7e605edcb22a76cc36ad9b
AB  - Traffic Light Recognition (TLR) is vital for Autonomous Driving Systems as it supplies critical information at intersections. Modern TLRs leverage camera and geolocation data, incorporating complex pre-(post)-processing steps and multiple deep learning (DL) models for detecting, recognizing, and tracking traffic lights. While the adversarial robustness of standalone DL models has been extensively studied, the robustness of a modern TLR system, i.e., a complex software component with code and DL models, is rarely studied and hence requires research efforts.In this work, we propose a novel testing framework (namely SITAR) targeting TLR modules from a representative Level-4 ADS, such as Baidu Apollo and Autoware. We design a novel adversarial attack loss function to evaluate and improve the adversarial robustness of modern TLR systems. We applied SITAR on Apollo TLR and compared our novel loss function with the state-of-the-art approaches that can effectively attack object detection and image recognition models. SITAR is shown to be effective and our novel loss function performs better than previous SOTAs with a 93% to 100% success rate with a maximum of five-step iteration and eight pixels per perturbation.  © 2024 IEEE.
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Function evaluation
KW  - Image recognition
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Autonomous driving
KW  - Driving systems
KW  - Geolocations
KW  - Learning models
KW  - Level 4
KW  - Loss functions
KW  - Post-processing
KW  - Processing steps
KW  - Recognition systems
KW  - Traffic lights recognition
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Pang, J.
AU  - Zhou, Y.
TI  - SODet: A LiDAR-Based Object Detector in Bird’s-Eye View
PY  - 2024
T2  - Communications in Computer and Information Science
DO  - 10.1007/978-981-99-8148-9_7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178637306&doi=10.1007%2f978-981-99-8148-9_7&partnerID=40&md5=bfca90cc9ab34d6c95e36f56eac3b4cf
AB  - LiDAR-based object detection is of paramount significance in the realm of autonomous driving applications. Nevertheless, the detection of small objects from a bird’s-eye view perspective remains challenging. To address this issue, the paper presents SODet, an efficient single-stage 3D object detector designed to enhance the perception of small objects like pedestrians and cyclists. SODet incorporates several key components and techniques. To capture broader context information and augment the capability of feature representation, the model constructs residual blocks comprising large-kernel depthwise convolutions and inverted bottleneck structures, forming the foundation of the CSP-based NeXtDark backbone network. Furthermore, the NeXtFPN feature extraction network is designed with the introduced SPPF module and the proposed special residual blocks, enabling the extraction and fusion of multi-scale information. Additionally, training strategies such as mosaic data augmentation and cosine annealing learning rate are employed to further improve small object detection accuracy. The effectiveness of SODet is demonstrated through experimental results on the KITTI dataset, showcasing a remarkable enhancement in detecting small objects from a bird’s-eye perspective while maintaining a detection speed of 20.6 FPS. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
KW  - Autonomous Driving
KW  - Bird’s-Eye View
KW  - LiDAR Point Cloud
KW  - Object Detection
KW  - Autonomous vehicles
KW  - Birds
KW  - Extraction
KW  - Object recognition
KW  - Optical radar
KW  - 3D object
KW  - Autonomous driving
KW  - Bird’s-eye view
KW  - Context information
KW  - LiDAR point cloud
KW  - Object detectors
KW  - Objects detection
KW  - Point-clouds
KW  - Single stage
KW  - Small objects
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Han, Y.
AU  - Wang, F.
AU  - Wang, W.
AU  - Li, X.
AU  - Zhang, J.
TI  - YOLO-SG: Small traffic signs detection method in complex scene
PY  - 2024
T2  - Journal of Supercomputing
DO  - 10.1007/s11227-023-05547-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165662470&doi=10.1007%2fs11227-023-05547-y&partnerID=40&md5=fa102339688667e9bcb8be26e6c93dd6
AB  - Fast and accurate detection of traffic signs is crucial for the development of intelligent transportation systems. To address the issue of false detection and missing detection of small traffic signs in complex scenes, this paper proposes a YOLO-SG model based on YOLOv5. The YOLO-SG approach employs SPD-Conv as a down-sampling structure to mitigate the loss of feature information during the down-sampling process. This enhances the detection performance of small objects in complex scenes and improves the generalization and robustness of the model. The feature extraction architecture uses GhostNet, which effectively reduces the number of model parameters and weight, enhancing the feasibility of practical model deployment. Furthermore, this study optimizes the output feature structure by introducing a small object detection layer and removing the large object detection layer, enabling the detection of small objects. Extensive experiments conducted on the GTSDB and TT100K datasets demonstrate that YOLO-SG exhibits excellent detection performance. On the GTSDB dataset, YOLO-SG achieved a 2.3% increase in mAP compared to the baseline network, while reducing the number of parameters by 42%. Similarly, on the TT100K dataset, YOLO-SG increased mAP by 6.3% and reduced the number of parameters by 43%. These experimental results showcase the effectiveness and accuracy of YOLO-SG, particularly in detecting small traffic signs. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Deep learning
KW  - Small object detection
KW  - Traffic sign detection
KW  - YOLOv5
KW  - Complex networks
KW  - Deep learning
KW  - Feature extraction
KW  - Intelligent systems
KW  - Object recognition
KW  - Signal sampling
KW  - Traffic signs
KW  - Complex scenes
KW  - Deep learning
KW  - Detection methods
KW  - Detection performance
KW  - Down sampling
KW  - Intelligent transportation systems
KW  - Small object detection
KW  - Small objects
KW  - Traffic sign detection
KW  - YOLOv5
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Khow, Z.J.
AU  - Tan, Y.-F.
AU  - Karim, H.A.
AU  - Rashid, H.A.A.
TI  - Improved YOLOv8 Model for a Comprehensive Approach to Object Detection and Distance Estimation
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3396224
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192151007&doi=10.1109%2fACCESS.2024.3396224&partnerID=40&md5=a5ca25aadb53811222c0a43949b7dc6d
AB  - The rapid advancements in deep learning have revolutionized the field of computer vision. However, despite the significant progress in computer vision, there remains a scarcity of research focused on utilizing this technology for distance estimation. Exploring such studies can bring immense convenience to people, especially in applications like anomaly object detection. On that account, this research proposes an improved detection model based on You Only Look Once version 8 (YOLOv8) namely YOLOv8-CAW, which is capable of both detecting target objects and accurately calculating their distances. The proposed method involves incorporating the Coordinate Attention and Wise-IoU into the YOLOv8 network, enhancing the detection accuracy. Combined with the distance estimation algorithm, results in a comprehensive output that includes both detection results and calculated distances. At the end of the experiment, a substantial improvement in performance metrics was observed, the model achieved increases in recall (0.4%), precision (2.2%), and Mean Average Precision (mAP) (1.5%) within the 0.5 to 0.95 threshold range, while maintaining inference speeds similar to the baseline model in PASCAL VOC dataset. Besides that, distance estimation achieved an approximate average accuracy of 90% which shows the results are highly encouraging and promising. The successful integration of computer vision and distance estimation opens new possibilities for practical applications, showcasing the potential of this approach in real-world scenarios.  © 2013 IEEE.
KW  - attention module
KW  - deep learning
KW  - distance estimation
KW  - loss function
KW  - object detection
KW  - You only look once (YOLO)
KW  - Computer vision
KW  - Deep learning
KW  - Feature extraction
KW  - Object recognition
KW  - Semantics
KW  - Attention module
KW  - Computational modelling
KW  - Context models
KW  - Deep learning
KW  - Distance estimation
KW  - Features extraction
KW  - Loss functions
KW  - Objects detection
KW  - You only look once
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ge, Q.
AU  - Da, W.
AU  - Wang, M.
TI  - MARFPNet: Multiattention and Adaptive Reparameterized Feature Pyramid Network for Small Target Detection on Water Surfaces
PY  - 2024
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2024.3485463
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208384132&doi=10.1109%2fTIM.2024.3485463&partnerID=40&md5=23a0d04895f9f2cad9c5869cd8643b99
AB  - The images captured by unmanned aerial vehicles (UAVs) are often limited in scale and feature information, making it challenging for current detection algorithms to learn the features of objects effectively. This limitation hampers accurate identification of small objects on water surfaces. We introduce a multiattention and adaptive reparameterized feature pyramid network for small target detection on water surfaces (MARFPNet) to tackle this issue. First, to address the loss of small object features during extraction, we improved the attention mechanism based on the characteristics of small objects and proposed a multiattention module, integrating it into the feature extraction process. Second, to address the semantic information of small objects being retained mostly in shallow feature maps and not fully utilized, we introduced an adaptive reparameterized generalized feature pyramid network (Adaptive_RepGFPN). This module reorganizes features, expands the fusion scale, and incorporates adaptive weighting in the concat operation. Third, to overcome the challenge of ineffective restoration of feature map information by upsampling, we introduce the Dysample. Finally, to address the problem of the loss function being sensitive to scale changes, we propose the normalized Wasserstein distance (NWD) loss function to reduce the sudden drop in loss due to scale changes. We conducted experiments on VisDrone, SeaDronsSee, and the self-build dataset. MARFPNet showed higher accuracy compared to other detection algorithms. Notably, on the self-build dataset, mAP50 and mAP50:95 improved by 9.1% and 3.5% over the baseline network. This demonstrates MARFPNet's effectiveness and suitability for detecting small targets in drone aerial photography on water surfaces.  © 1963-2012 IEEE.
KW  - Deep learning
KW  - feature fusion
KW  - object detection
KW  - small target
KW  - Aerial photography
KW  - Aircraft detection
KW  - Image reconstruction
KW  - Medical imaging
KW  - Photomapping
KW  - Scales (weighing instruments)
KW  - Deep learning
KW  - Feature map
KW  - Feature pyramid
KW  - Features fusions
KW  - Objects detection
KW  - Pyramid network
KW  - Small objects
KW  - Small target detection
KW  - Small targets
KW  - Water surface
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Q.
AU  - Wang, C.
AU  - Li, H.
AU  - Shen, S.
AU  - Cao, W.
AU  - Li, X.
AU  - Wang, D.
TI  - Improved YOLOv8-CR Network for Detecting Defects of the Automotive MEMS Pressure Sensors
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3419806
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197549896&doi=10.1109%2fJSEN.2024.3419806&partnerID=40&md5=1237a3939998eb3bb42c9e535a25747b
AB  - Micro-electro-mechanical system (MEMS) sensors have been widely used in the automotive field owing to their small size, low cost, and high reliability. Various defects are caused by complicated environments and multistep processes in the manufacturing process of automotive MEMS pressure sensors. These defects are often small in size but can significantly impact the performance of products. The traditional detection method is inefficient and inaccurate. In this study, an improved you only look once v8 (YOLOv8) C2f-RFCBAMConv (CR) network is proposed to detect the defects of the automotive MEMS pressure sensors. The network is an improved version of YOLOv8. The faster version of the CSP bottleneck with two convolutions (C2f) and receptive-field convolutional block attention (RFCBAMConv) modules are applied. The ablation experiments confirm the impact of C2f and RFCBAMConv modules on network performance. The improved YOLOv8-CR network can effectively enhance the accuracy and speed of defect detection of the automotive MEMS pressure sensor. Furthermore, comparisons are made between the improved YOLOv8-CR network and other networks such as YOLOv8-large, YOLOv5, and faster region-based convolutional neural network (Fast-RCNN), demonstrating its effectiveness in identifying five types of defects in the automotive MEMS pressure sensors. The improved YOLOv8-CR exhibits considerably superior defect detection capabilities with a mean average precision (mAP) of 94.98% and a single picture detection time of 42.68 ms on the test set. The improved YOLOv8-CR network is helpful in realizing real-time and accurate online monitoring of product quality in the key process of the automotive MEMS pressure sensors. © 2001-2012 IEEE.
KW  - Defect detection
KW  - faster version of CSP bottleneck with two convolutions (C2f)
KW  - receptive-field convolutional block attention (RFCBAMConv)
KW  - you only look once v8 (YOLOv8) network
KW  - Convolution
KW  - MEMS
KW  - Pressure sensors
KW  - Automotives
KW  - Defect detection
KW  - Detecting defects
KW  - Fast version of CSP bottleneck with two convolution (c2f)
KW  - Low-costs
KW  - MEMS (microelectromechanical system)
KW  - Receptive fields
KW  - Receptive-field convolutional block attention (RFCBAMConv)
KW  - System Pressure sensors
KW  - You only look once v8  network
KW  - Defects
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sun, H.
AU  - Chen, W.
AU  - Zhang, C.
AU  - Zhang, Z.
TI  - Deep Hierarchical Rear-Lamp Tracking at Nighttime
PY  - 2024
T2  - IEEJ Transactions on Electrical and Electronic Engineering
DO  - 10.1002/tee.23951
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177825068&doi=10.1002%2ftee.23951&partnerID=40&md5=21f495c4bc2577f1bac399b46e5f7566
AB  - Rear-lamp tracking at night is a research topic in night vision that is essential for Advanced Driver Assistance Systems (ADAS). Most current computer vision-based methods address this problem using color features because rear lamps are lit during night driving. However, such methods are sufficiently robust in complex environments owing to the lack of feature diversity. On the other hand, as there is no off-the-shelf dataset, the application of deep learning to nighttime rear-lamp tracking has been sparsely explored. To tackle the above issues, in this paper, we propose hierarchical rear-lamp tracking at nighttime (H-RTN) and create a novel dataset for rear-lamp tracking. Specifically, the H-RTN consists of a rough detection hierarchy (R-hierarchy), an accurate detection hierarchy (A-hierarchy), and an optimization hierarchy (O-hierarchy). The R-hierarchy determines the region of interest (ROI) containing the target rear lamps, the A-hierarchy samples the rear-lamp candidates in the ROI, and the O-hierarchy selects the best pair of candidates as the final location of the target rear lamps. The experimental results show that H-RTN outperforms the alternative existing methods. © 2023 Institute of Electrical Engineer of Japan and Wiley Periodicals LLC. © 2023 Institute of Electrical Engineer of Japan and Wiley Periodicals LLC.
KW  - deep learning
KW  - night vision
KW  - rear-lamp tracking
KW  - vehicle detection
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Image segmentation
KW  - Lighting
KW  - Vision
KW  - 'current
KW  - Deep learning
KW  - Night vision
KW  - Optimisations
KW  - Rear-lamp tracking
KW  - Region-of-interest
KW  - Regions of interest
KW  - Research topics
KW  - Vehicles detection
KW  - Vision-based methods
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pei, C.
AU  - Zhang, S.
AU  - Cao, L.
AU  - Zhao, L.
TI  - ContextNet: Leveraging Comprehensive Contextual Information for Enhanced 3D Object Detection
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3437642
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200207765&doi=10.1109%2fACCESS.2024.3437642&partnerID=40&md5=a1a753d878670d0b3fdcf79a25ffa126
AB  - The progress in object detection for autonomous driving using LiDAR point cloud data has been remarkable. However, current voxel-based two-stage detectors have not fully capitalized on the wealth of contextual information present in the point cloud data. Typically, Voxel Feature Encoding (VFE) layers tend to focus exclusively on internal voxel information, neglecting the broader context. Additionally, the process of extracting 3D proposal features through Region of Interest (ROI) spatial quantization and pooling downsampling results in a loss of spatial detail within the proposed regions. This limitation in capturing contextual details presents challenges for accurate object detection and positioning, particularly over long distances. In this paper, we propose ContextNet, which leverages comprehensive contextual information for enhanced 3D object detection. Specifically, it comprises two modules: the Voxel Self-Attention Encoding module (VSAE) and the Joint Channel Self-Attention Re-weight module (JCSR). VSAE establishes dependencies between voxels through self-attention, expanding the receptive field and introducing substantial contextual information. JCSR employs joint attention to extract both local channel information and global context information from the raw point cloud within the RoI region. By integrating these two sets of information and re-weighting the point features, the 3D proposal is refined, enabling a more accurate estimation of the object's position and confidence. Extensive experiments conducted on the KITTI dataset demonstrate that our approach outperforms voxel-based two-stage methods, particularly with a 9.5% improvement in the mAP compared to the baseline on the nuScenes test dataset, and an improved 1.61% hard AP compared to the baseline on the KITTI benchmark. © 2013 IEEE.
KW  - 3D object detection
KW  - Autonomous driving
KW  - LiDAR sensor
KW  - Computer vision
KW  - Encoding (symbols)
KW  - Image segmentation
KW  - Object recognition
KW  - Optical radar
KW  - Signal encoding
KW  - Statistical tests
KW  - 'current
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous driving
KW  - Contextual information
KW  - Encoding modules
KW  - Encodings
KW  - LiDAR sensor
KW  - Objects detection
KW  - Point cloud data
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Mukherjee, S.
AU  - Beard, C.
AU  - Li, Z.
TI  - MODIPHY: Multimodal Obscured Detection for IoT using PHantom Convolution-Enabled Faster YOLO
PY  - 2024
T2  - Proceedings - International Conference on Image Processing, ICIP
DO  - 10.1109/ICIP51287.2024.10648081
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216857933&doi=10.1109%2fICIP51287.2024.10648081&partnerID=40&md5=6cad6cdda4216af7781dd352411436b4
AB  - Low-light conditions and occluded scenarios impede object detection in real-world Internet of Things (IoT) applications like autonomous vehicles and security systems. While advanced machine learning models strive for accuracy, their computational demands clash with the limitations of resource-constrained devices, hampering real-time performance. In our current research, we tackle this challenge, by introducing “YOLO Phantom", one of the smallest YOLO models ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block, achieving comparable accuracy to the latest YOLOv8n model while simultaneously reducing both parameters and model size by 43%, resulting in a significant 19% reduction in Giga Floating-Point Operations (GFLOPs). YOLO Phantom leverages transfer learning on our multimodal RGB-infrared dataset to address low-light and occlusion issues, equipping it with robust vision under adverse conditions. Its real-world efficacy is demonstrated on an IoT platform with advanced low-light and RGB cameras, seamlessly connecting to an AWS-based notification endpoint for efficient real-time object detection. Benchmarks reveal a substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB detection, respectively, compared to the baseline YOLOv8n model. For community contribution, both the code and the multimodal dataset are available on GitHub. © 2024 IEEE.
KW  - IoT
KW  - Low light object detection
KW  - Multimodal fusion
KW  - Phantom Convolution
KW  - YOLO
KW  - Benchmarking
KW  - Image coding
KW  - Image segmentation
KW  - Network security
KW  - Transfer learning
KW  - Low light
KW  - Low light conditions
KW  - Low light object detection
KW  - Multi-modal
KW  - Multi-modal fusion
KW  - Objects detection
KW  - Phantom convolution
KW  - Phantoms
KW  - Real-world
KW  - YOLO
KW  - Phantoms
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xiong, Z.
AU  - Zou, L.
AU  - Wei, Y.
AU  - Yu, H.
TI  - Optimizing Automotive Component Assembly Inspection via an Advanced Detection Model Based on Faster R-CNN
PY  - 2024
T2  - 2024 4th International Conference on Artificial Intelligence, Robotics, and Communication, ICAIRC 2024
DO  - 10.1109/ICAIRC64177.2024.10900029
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000631149&doi=10.1109%2fICAIRC64177.2024.10900029&partnerID=40&md5=18efcc80c46e4c803bfbef593da9c4f7
AB  - In the field of automotive manufacturing, the precision of assembly processes is critical for ensuring product quality and efficient cost management.To address the inefficiency and unreliability of traditional manual inspection techniques,this study proposes an advanced detection model based on the Faster R-CNN framework for evaluating the assembly status of eight critical components within a vehicle's engine bay.Firstly, deformable convolutions(DCN) are incorporated into the backbone network to enhance adaptability to objects of varying scales.Secondly, a Balanced Feature Pyramid Network (BFPN) is adopted to improve feature integration and minimize information loss across different feature levels.Lastly, a Dynamic Label Assignment (DLA)mechanism is introduced, which adjusts the Intersection over Union (IoU) threshold to improve the quality of positive sample classification.Experimental results demonstrate that the improved model achieves a mean Average Precision (mAP) of 87.1%, representing a 2.3% improvement over the baseline Faster R-CNN model.  © 2024 IEEE.
KW  - Balanced Feature Pyramid Network
KW  - component
KW  - Deformable Convolutions
KW  - Faster R-CNN
KW  - Object Detection
KW  - Automobile engine manufacture
KW  - Automobiles
KW  - Failure analysis
KW  - Advanced detections
KW  - Balanced feature pyramid network
KW  - Component
KW  - Deformable convolution
KW  - Detection models
KW  - Fast R-CNN
KW  - Feature pyramid
KW  - Model-based OPC
KW  - Objects detection
KW  - Pyramid network
KW  - Inspection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Wang, H.
AU  - Liu, Y.
AU  - Li, G.
AU  - Diao, X.
AU  - Zheng, Q.
TI  - Multi-Level Feature Fusion Based UAV Object Detection Method Under Foggy Weather
PY  - 2024
T2  - 2024 6th International Conference on Frontier Technologies of Information and Computer, ICFTIC 2024
DO  - 10.1109/ICFTIC64248.2024.10913360
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001670144&doi=10.1109%2fICFTIC64248.2024.10913360&partnerID=40&md5=8a6daf7cbf1564251bb40a87bfefcd30
AB  - Adverse weather conditions can blur aerial images captured by optical cameras on UAVs, complicating the already challenging task of detecting small-sized targets from a UAV's perspective. In this paper, a novel model called DU-YOLO is proposed. Firstly, an FFA-Net-based defog module is introduced, ensuring effective image preprocessing while meeting real-time requirement. Then, a multi-level feature fusion module based on the squeeze-excitation attention mechanism is proposed to enhance information extraction and detail preservation. Additionally, the focaler-MPDIoU loss function is designed to enhance the learning ability for small target samples and accelerate convergence. Finally, experimental results are tested on the synthetic fog dataset VisDrone-Fog, demonstrating that DU-YOLO achieves a 4.9% increase on mAP compared to the baseline model, YOLOvlOs.  © 2024 IEEE.
KW  - feature fusion
KW  - foggy weather condition
KW  - object detection
KW  - synthetic fog dataset
KW  - UAV
KW  - Aircraft detection
KW  - Fog computing
KW  - Image enhancement
KW  - Adverse weather
KW  - Aerial images
KW  - Condition
KW  - Features fusions
KW  - Foggy weather condition
KW  - Multilevels
KW  - Object detection method
KW  - Objects detection
KW  - Optical camera
KW  - Synthetic fog dataset
KW  - Aerial photography
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Tang, Z.
AU  - Zhu, L.
AU  - Wang, W.
AU  - Gong, Y.
TI  - PE-SSD:Improved SSD For Small Object Detection
PY  - 2024
T2  - Proceedings - 2024 China Automation Congress, CAC 2024
DO  - 10.1109/CAC63892.2024.10865724
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000793969&doi=10.1109%2fCAC63892.2024.10865724&partnerID=40&md5=00536ba6e2d46bd521290c018a4a6203
AB  - With the flourishing development of deep learning, object detection technology has achieved encouraging results. However, small objects remain a major challenge in the field of object detection due to their lack of texture and contextual information caused by their small size. Despite such fact, we observed that their edges typically exhibit high contrast in relation to the background. Specifically, we innovatively employ a Laplacian image pyramid incorporated with feature enhancement modules rather than the traditional image pyramid, with the aim of more effectively capturing image details and edge information, given that the Laplacian pyramid has the ability to retain such information within its lower levels. Furthermore, we introduce attention mechanisms to optimize the feature fusion process, thus improving the utilization efficiency of multi-scale features. Taking SSD as the baseline, we propose a novel method called PE-SSD, which integrates the advantages of featureized Laplacian pyramid. Experimental results show that our method achieves higher accuracy in detecting small objects compared to traditional SSD. For inputs of size 512 × 512, the extensive experiments on the VisDrone2019 dataset demonstrates that our method achieves an average precision of 15.4%, surpassing traditional SSD networks by 1.2%. Specifically, The mAP_s score reaches 6.6%, which is 0.4% higher than the baseline network, indicating the effectiveness of our method in the field of small object detection. © 2024 IEEE.
KW  - Attention Module
KW  - Feature Fusion
KW  - Feature Pyramid
KW  - Laplacian Pyramid
KW  - Small Object Detection
KW  - Contrastive Learning
KW  - Deep learning
KW  - Feature extraction
KW  - Image enhancement
KW  - Object detection
KW  - Object recognition
KW  - Attention module
KW  - Detection technology
KW  - Feature pyramid
KW  - Features fusions
KW  - Image pyramids
KW  - Laplacian Pyramid
KW  - Learning objects
KW  - Objects detection
KW  - Small object detection
KW  - Small objects
KW  - Laplace transforms
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fu, H.
AU  - Liu, H.
AU  - Yuan, J.
AU  - He, X.
AU  - Lin, J.
AU  - Li, Z.
TI  - YOLO-Adaptor: A Fast Adaptive One-Stage Detector for Non-Aligned Visible-Infrared Object Detection
PY  - 2024
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2024.3393015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191685914&doi=10.1109%2fTIV.2024.3393015&partnerID=40&md5=cc0131af2815adcbddc1d211d9272faf
AB  - Visible-infrared object detection has attracted increasing attention recently due to its superior performance and cost-efficiency. Most existing methods focus on the detection of strictly-aligned data, significantly limiting its practical applications. Although several researchers have attempted to explore weakly-aligned visible-infrared object detection, they are limited to small translational deviations and suffer from a low detection speed. This paper first explores non-aligned visible-infrared object detection with complex deviations in translation, scaling, and rotation, and proposes a fast one-stage detector YOLO-Adaptor, which introduces a lightweight multi-modal adaptor to simultaneously predict alignment parameters and confidence weights between modalities. The adaptor adopts a feature-level alignment during the feature extraction process, ensuring high alignment efficiency. Moreover, we introduce a feature contrastive learning loss to guide the alignment learning of the adaptor, aiming to reduce the representation gap between the two modalities in hyperbolic space to implement feature spatial and distributional consistency. Extensive experiments are conducted on three datasets, including one weakly-aligned and two non-aligned datasets, and the experimental results demonstrate that YOLO-Adaptor could achieve significant performance improvements in terms of speed and accuracy. © 2016 IEEE.
KW  - adaptor
KW  - cross-modal feature alignment
KW  - multi-modal fusion
KW  - Visible-infrared object detection
KW  - Alignment
KW  - Autonomous vehicles
KW  - Efficiency
KW  - Extraction
KW  - Feature extraction
KW  - Object recognition
KW  - Adaptor
KW  - Autonomous Vehicles
KW  - Cross-modal
KW  - Cross-modal feature alignment
KW  - Feature alignment
KW  - Features extraction
KW  - Infrared object
KW  - Multi-modal fusion
KW  - Objects detection
KW  - Self-supervised learning
KW  - Visible-infrared object detection
KW  - YOLO
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Rao Muvva, V.V.R.M.K.
AU  - Joseph, K.T.
AU  - Samal, K.
AU  - Wolf, M.
AU  - Pitla, S.
TI  - Adaptive Perception Control for Aerial Robots with Twin Delayed DDPG
PY  - 2024
T2  - Proceedings -Design, Automation and Test in Europe, DATE
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196550044&partnerID=40&md5=51f55098008c31d9e79a21ce3eef5a6b
AB  - Robotic perception is commonly assisted by convo-lutional neural networks. However, these networks are static in nature and do not adjust to changes in the environment. Additionally, these are computationally complex and impose latency in inference. We propose an adaptive perception system that changes in response to the robot's requirements. The perception controller has been designed using a recently proposed reinforcement learning technique called Twin Delayed DDPG (TD3). Our proposed method outperformed the baseline approaches. © 2024 EDAA.
KW  - Closed loop systems
KW  - Control Systems
KW  - Deep Learning
KW  - Drone
KW  - Neural Networks
KW  - UAS
KW  - UAV
KW  - Adaptive control systems
KW  - Aircraft control
KW  - Antennas
KW  - Closed loop control systems
KW  - Drones
KW  - Learning systems
KW  - Reinforcement learning
KW  - Aerial robots
KW  - Closed-loop system
KW  - Deep learning
KW  - Neural-networks
KW  - Perception systems
KW  - Reinforcement learning techniques
KW  - UAS
KW  - Deep learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Deshmukh, P.
AU  - Chaitanya Rayasam, K.
AU  - Kumar Sahoo, U.
AU  - Kumar Das, S.
AU  - Majhi, S.
TI  - Multi-Class Vehicle Detection Using VDnet in Heterogeneous Traffic
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2024.3476122
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207765663&doi=10.1109%2fTITS.2024.3476122&partnerID=40&md5=44e5fcbbe35b82b22ac6f461e286e320
AB  - Intelligent vehicles detection (IVD) provides information to manage traffic efficiently, drive autonomous vehicles and feed data to intelligent traffic management systems (ITMS). IVD is a challenging task for the close proximity vehicles in lane-less traffic and heterogeneous systems. Most vehicle detection models are complex and limited to multi-scale feature extraction due to the involvement of existing feature extraction backbones. Also, they do not include heterogeneous traffic vehicles, usually present in developing countries. Therefore, this paper proposes a multi-class vehicle detection (MCVD) model to detect vehicles in heterogeneous traffic using a realistic traffic dataset from a developing country. MCVD is a deep learning (DL) model that consists of a convolutional neural network backbone called VDnet, a light fusion bi-directional feature pyramid network (LFBFPN) and a modified vehicle detection head (MVDH). VDnet extracts multi-scale features from the traffic input images using feature reuse methods. LFBFPN combines these features bi-directionally and provides robust feature maps. Finally, MVDH is applied to detect multi-class vehicles and classify them into respective categories. The proposed model achieves 91.45% mean average precision (mAP) on the heterogeneous traffic labeled dataset (HTLD). The proposed MCVD is tested over Nvidia Jetson TX2 edge computing boards to verify the real-time performance. It achieves 17 frames per second (FPS) on TX2. The performance evaluation results indicate that the proposed MCVD model is fast, accurate and better than the existing works.  © 2024 IEEE.
KW  - Convolutional neural network
KW  - feature reuse method
KW  - heterogeneous traffic
KW  - intelligent vehicle detection
KW  - Advanced traffic management systems
KW  - Deep neural networks
KW  - Highway administration
KW  - Reusability
KW  - Street traffic control
KW  - Bi-directional
KW  - Convolutional neural network
KW  - Detection models
KW  - Feature reuse
KW  - Feature reuse method
KW  - Features extraction
KW  - Heterogeneous traffic
KW  - Intelligent vehicle detection
KW  - Multi-scale features
KW  - Vehicles detection
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hanzla, M.
AU  - Yusuf, M.O.
AU  - Al Mudawi, N.
AU  - Sadiq, T.
AU  - Almujally, N.A.
AU  - Rahman, H.
AU  - Alazeb, A.
AU  - Algarni, A.
TI  - Vehicle recognition pipeline via DeepSort on aerial image datasets
PY  - 2024
T2  - Frontiers in Neurorobotics
DO  - 10.3389/fnbot.2024.1430155
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202653909&doi=10.3389%2ffnbot.2024.1430155&partnerID=40&md5=35b92cd5fb4c22fcda5ddae1ec100991
AB  - Introduction: Unmanned aerial vehicles (UAVs) are widely used in various computer vision applications, especially in intelligent traffic monitoring, as they are agile and simplify operations while boosting efficiency. However, automating these procedures is still a significant challenge due to the difficulty of extracting foreground (vehicle) information from complex traffic scenes. Methods: This paper presents a unique method for autonomous vehicle surveillance that uses FCM to segment aerial images. YOLOv8, which is known for its ability to detect tiny objects, is then used to detect vehicles. Additionally, a system that utilizes ORB features is employed to support vehicle recognition, assignment, and recovery across picture frames. Vehicle tracking is accomplished using DeepSORT, which elegantly combines Kalman filtering with deep learning to achieve precise results. Results: Our proposed model demonstrates remarkable performance in vehicle identification and tracking with precision of 0.86 and 0.84 on the VEDAI and SRTID datasets, respectively, for vehicle detection. Discussion: For vehicle tracking, the model achieves accuracies of 0.89 and 0.85 on the VEDAI and SRTID datasets, respectively. Copyright © 2024 Hanzla, Yusuf, Al Mudawi, Sadiq, Almujally, Rahman, Alazeb and Algarni.
KW  - deep learning
KW  - DeepSort
KW  - dynamic environments
KW  - object recognition
KW  - path planning
KW  - remote sensing
KW  - unmanned aerial vehicles
KW  - Adaptive boosting
KW  - Aerial photography
KW  - Aircraft detection
KW  - Image segmentation
KW  - Kalman filters
KW  - Motion planning
KW  - Aerial images
KW  - Aerial vehicle
KW  - Deep learning
KW  - Deepsort
KW  - Dynamic environments
KW  - Image datasets
KW  - Objects recognition
KW  - Remote-sensing
KW  - Unmanned aerial vehicle
KW  - Vehicle recognition
KW  - accuracy
KW  - aerial image dataset
KW  - Article
KW  - autonomous vehicle
KW  - computer vision
KW  - deep learning
KW  - DeepSORT
KW  - image analysis
KW  - image enhancement
KW  - image processing
KW  - image quality
KW  - image reconstruction
KW  - image segmentation
KW  - information processing
KW  - learning algorithm
KW  - machine learning
KW  - pipeline
KW  - recognition
KW  - traffic
KW  - unmanned aerial vehicle
KW  - vehicle tracking
KW  - virtual reality
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Malligere Shivanna, V.
AU  - Guo, J.-I.
TI  - Object Detection, Recognition, and Tracking Algorithms for ADASs—A Study on Recent Trends
PY  - 2024
T2  - Sensors
DO  - 10.3390/s24010249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181895213&doi=10.3390%2fs24010249&partnerID=40&md5=edf0aabc75d5a029ca16049ae6a6e102
AB  - Advanced driver assistance systems (ADASs) are becoming increasingly common in modern-day vehicles, as they not only improve safety and reduce accidents but also aid in smoother and easier driving. ADASs rely on a variety of sensors such as cameras, radars, lidars, and a combination of sensors, to perceive their surroundings and identify and track objects on the road. The key components of ADASs are object detection, recognition, and tracking algorithms that allow vehicles to identify and track other objects on the road, such as other vehicles, pedestrians, cyclists, obstacles, traffic signs, traffic lights, etc. This information is then used to warn the driver of potential hazards or used by the ADAS itself to take corrective actions to avoid an accident. This paper provides a review of prominent state-of-the-art object detection, recognition, and tracking algorithms used in different functionalities of ADASs. The paper begins by introducing the history and fundamentals of ADASs followed by reviewing recent trends in various ADAS algorithms and their functionalities, along with the datasets employed. The paper concludes by discussing the future of object detection, recognition, and tracking algorithms for ADASs. The paper also discusses the need for more research on object detection, recognition, and tracking in challenging environments, such as those with low visibility or high traffic density. © 2023 by the authors.
KW  - advanced driver assistance system (ADAS)
KW  - deep learning
KW  - object detection
KW  - object tracking
KW  - Accidents
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Deep learning
KW  - Object recognition
KW  - Roads and streets
KW  - Tracking (position)
KW  - Traffic signs
KW  - Vehicles
KW  - Advanced driver assistance system
KW  - Deep learning
KW  - Object detection algorithms
KW  - Object recognition algorithm
KW  - Object Tracking
KW  - Object tracking algorithm
KW  - Objects detection
KW  - Potential hazards
KW  - Recent trends
KW  - Traffic light
KW  - algorithm
KW  - camera
KW  - cyclist
KW  - deep learning
KW  - electric potential
KW  - pedestrian
KW  - review
KW  - sensor
KW  - telecommunication
KW  - visibility
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, L.J.
AU  - Fang, J.J.
AU  - Liu, Y.X.
AU  - Feng Le, H.
AU  - Rao, Z.Q.
AU  - Zhao, J.X.
TI  - CR-YOLOv8: Multiscale Object Detection in Traffic Sign Images
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3347352
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181564845&doi=10.1109%2fACCESS.2023.3347352&partnerID=40&md5=9184a1356ad19b595e2c36262ff427b6
AB  - Due to the large-scale changes of different forms of traffic signs and the rapid speed of vehicles, the detection accuracy and real-time performance of general object detectors are greatly challenged, especially the detection accuracy of small objects. In order to solve this problem, a multi-scale traffic sign detection model CR-YOLOv8 is proposed based on the latest YOLOv8. In the feature extraction stage, the attention module is introduced to enhance the channel and spatial features, so that the network can learn the key information of the small objects more easily. The RFB module is introduced in the feature fusion stage, which improves the feature diversity with less computational overhead and improves the network's ability to detect multi-scale objects. By improving the loss function to enable the model to effectively balance multi-scale objectives during training, the model generalization ability is improved.The experimental results on TT100k dataset show that compared with the baseline network, the average detection accuracy of the improved method is increased by 2.3 %, and the detection accuracy of small objects is increased by 1.6 %, which effectively reduces the detection accuracy gap among different scales.  © 2023 The Authors.
KW  - Traffic sign recognition
KW  - traffic sign recognition
KW  - YOLOv8
KW  - Extraction
KW  - Object detection
KW  - Object recognition
KW  - Traffic signs
KW  - Detection accuracy
KW  - Features extraction
KW  - Image color analysis
KW  - Sensitivity
KW  - Small objects
KW  - Solid modelling
KW  - Traffic sign recognition
KW  - YOLO
KW  - YOLOv8
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Eggert, M.
AU  - Schade, M.
AU  - Bröhl, F.
AU  - Moriz, A.
TI  - Generating Synthetic LiDAR Point Cloud Data for Object Detection Using the Unreal Game Engine
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
DO  - 10.1007/978-3-031-61175-9_20
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195277231&doi=10.1007%2f978-3-031-61175-9_20&partnerID=40&md5=d035b891b80bb2f96afa74320efe9b2a
AB  - Object detection based on artificial intelligence is ubiquitous in today’s computer vision research and application. The training of the neural networks for object detection requires large and high-quality datasets. Besides datasets based on image data, datasets derived from point clouds offer several advantages. However, training datasets are sparse and their generation requires a lot of effort, especially in industrial domains. A solution to this issue offers the generation of synthetic point cloud data. Based on the design science research method, the work at hand proposes an approach and its instantiation for generating synthetic point cloud data based on the Unreal Engine. The point cloud quality is evaluated by comparing the synthetic cloud to a real-world point cloud. Within a practical example the applicability of the Unreal Game engine for synthetic point cloud generation could be successfully demonstrated. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - neural network
KW  - object detection
KW  - point cloud
KW  - synthetic data generation
KW  - Unreal
KW  - Large datasets
KW  - Object recognition
KW  - Game Engine
KW  - Neural-networks
KW  - Objects detection
KW  - Point cloud data
KW  - Point-clouds
KW  - Research and application
KW  - Synthetic data generations
KW  - Unreal
KW  - Vision applications
KW  - Vision research
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, Z.
AU  - Zheng, X.
AU  - Yang, J.
AU  - Su, J.
TI  - Improved YOLOX-DeepSORT for Multitarget Detection and Tracking of Automated Port RTG
PY  - 2024
T2  - IEEE Open Journal of the Industrial Electronics Society
DO  - 10.1109/OJIES.2024.3388632
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190725960&doi=10.1109%2fOJIES.2024.3388632&partnerID=40&md5=61e964472e6a36bc7db3cbe46f671ee0
AB  - Rubber tire gantry (RTG) plays a pivotal role in facilitating efficient container handling within port operations. Conventional RTG, highly depending on human operations, is inefficient, labor-intensive, and also poses safety issues in adverse environments. This article introduces a multitarget detection and tracking (MTDT) algorithm specifically tailored for automated port RTG operations. The approach seamlessly integrates enhanced YOLOX for object detection and improved DeepSORT for object tracking to enhance the MTDT performance in the complex port settings. In particular, Light-YOLOX, an upgraded version of YOLOX incorporating separable convolution and attention mechanism, is introduced to improve real-time capability and small target detection. Subsequently, OSNet-DeepSORT, an enhanced version of DeepSORT, is proposed to mitigate ID switching challenges arising from unreliable data communication or occlusion in real port scenarios. The effectiveness of the proposed method is validated in various real-life port operations. Ablation studies and comparative experiments against typical MTDT algorithms demonstrate noteworthy enhancements in key performance metrics, encompassing small target detection, tracking accuracy, ID switching frequency, and real-time performance.  © 2020 IEEE.
KW  - DeepSORT
KW  - multitarget tracking
KW  - rubber tire gantry (RTG)
KW  - target detection
KW  - YOLOX
KW  - Clutter (information theory)
KW  - Convolution
KW  - Object detection
KW  - Object recognition
KW  - Ports and harbors
KW  - Rubber
KW  - Target tracking
KW  - DeepSORT
KW  - Features extraction
KW  - Multi-target detection and tracking
KW  - Multi-target-tracking
KW  - Objects detection
KW  - Rubber tire gantries
KW  - Seaport
KW  - Targets detection
KW  - Targets tracking
KW  - Yolox
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, X.
AU  - Xuan, Y.
AU  - Huang, X.
AU  - Yan, Q.
TI  - YOLO-AFK: Advanced Fine-Grained Object Detection for Complex Solder Joints Defect
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3495540
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209630354&doi=10.1109%2fACCESS.2024.3495540&partnerID=40&md5=b5f9def5225577c9f7b80c5e5d6fb309
AB  - Welding processes significantly impact product quality as a crucial part of industrial production. Due to the reflection, diversity, complexity, and minuteness of solder defects, traditional detection methods struggle to detect surface defects in solder points effectively. Although object detection based on deep learning has made significant advances, detecting smaller objects remains challenging. To address these issues, we propose an improved defect detection network based on YOLOv9, named attention flexible kernel YOLO (YOLO-AFK). In particular, we propose a fusion attention network (FANet) that can enhance the model's ability to detect small defects by adaptively adjusting the receptive field of targets during feature extraction. Meanwhile, we use the alterable kernel convolution (AKConv), a variable kernel convolution, that breaks away from traditional convolutions limited to fixed local windows and sampling shapes. It can flexibly adjust the size and shape of the convolution kernels according to the solder targets, leading to more efficient feature extraction, thus achieving a lighter network. To gather more contextual and high-resolution information and enhance the detection accuracy and generalization ability for small objects and low-contrast targets, the cross-stage partial network fusion (C2f) module is designed to fuse feature maps from different levels. We evaluated the model using the publicly available NEU dataset and our proprietary solder point dataset, the fine-grain solder defect dataset (FG-SDD). Compared to previous studies, YOLO-AFK outperforms other state-of-the-art networks in terms of mean Average Precision (mAP) and Precision, with the parameter count increasing by only 12.4M, Precision improving by 10.1%, mAP increasing by 5.6%, and FPS improving by 23%. These results demonstrate the superior performance of the proposed network in detecting defects with complex structures. In particular, for industrial solder joint defect detection, YOLO-AFK not only improves detection accuracy but also significantly enhances the recognition of small targets and complex solder joint defects, showcasing the network's substantial potential and practical value in real-world production environments. The code is available at: https://github.com/Lwsk-wxy/yolo_afk.git.  ©2024 The Authors.
KW  - complex feature extraction
KW  - Deep learning
KW  - object detection
KW  - solder defect detection
KW  - Image coding
KW  - Image segmentation
KW  - Point defects
KW  - Soldering
KW  - Complex feature extraction
KW  - Deep learning
KW  - Defect detection
KW  - Features extraction
KW  - Kernel convolution
KW  - Objects detection
KW  - Small objects
KW  - Solder defect detection
KW  - Solder defects
KW  - Solder-joint defects
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Nafaa, S.
AU  - Ashour, K.
AU  - Mohamed, R.
AU  - Essam, H.
AU  - Emad, D.
AU  - Elhenawy, M.
AU  - Ashqar, H.I.
AU  - Hassan, A.A.
AU  - Alhadidi, T.I.
TI  - Advancing Roadway Sign Detection with YOLO Models and Transfer Learning
PY  - 2024
T2  - 2024 IEEE 3rd International Conference on Computing and Machine Intelligence, ICMI 2024 - Proceedings
DO  - 10.1109/ICMI60790.2024.10586105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199427434&doi=10.1109%2fICMI60790.2024.10586105&partnerID=40&md5=73405918738bbfbab16af9d19c8e1e45
AB  - Roadway signs detection and recognition is an essential element in the Advanced Driving Assistant Systems (ADAS). Several artificial intelligence methods have been used widely among of them YOLOv5 and YOLOv8. In this paper, we used a modified YOLOv5 and YOLOv8 to detect and classify different roadway signs under different illumination conditions. Experimental results indicated that for the YOLOv8 model, varying the number of epochs and batch size yields consistent MAP50 scores, ranging from 94.6% to 97.1% on the testing set. The YOLOv5 model demonstrates competitive performance, with MAP50 scores ranging from 92.4% to 96.9%. These results suggest that both models perform well across different training setups, with YOLOv8 generally achieving slightly higher MAP50 scores. These findings suggest that both models can perform well under different training setups, offering valuable insights for practitioners seeking reliable and adaptable solutions in object detection applications.  © 2024 IEEE.
KW  - Assets Management
KW  - Deep Learning
KW  - Signs Detection
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Deep learning
KW  - Intelligent systems
KW  - Traffic signs
KW  - Artificial intelligence methods
KW  - Assets management
KW  - Deep learning
KW  - Driving assistant systems
KW  - Essential elements
KW  - Illumination conditions
KW  - Model learning
KW  - Sign detection
KW  - Sign recognition
KW  - Transfer learning
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, Y.
AU  - Hu, H.
AU  - Zhu, X.
AU  - Nan, Y.
AU  - Wang, K.
AU  - Liu, Z.
AU  - Lian, S.
TI  - RAOD: A Benchmark for Road Abandoned Object Detection From Video Surveillance
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3407955
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194849112&doi=10.1109%2fACCESS.2024.3407955&partnerID=40&md5=b932ac7453566499fdf0305bf75ea7b2
AB  - Road abandoned objects are potential safety hazards in modern traffic transport, especially in highway scenes. Promptly detecting such obstacles on the road is of great significance for driving safety and Intelligent Transportation Systems (ITS). Current research primarily focuses on developing diverse road anomaly detection approaches to discriminate the unknown objects regarded as abandoned ones. However, previous efforts have been largely inadequate due to the absence of abundant datasets. In addition, prevailing benchmarks mainly provide data pertinent to autonomous driving, which might not effectively generalize to highway scenarios owing to camera perspective and scope limitations. To address these challenges, we introduce a large-scale Road Abandoned Object Detection (RAOD) benchmark derived from video surveillance. First, we collect abundant real-world video clips containing various potential abandoned object categories on the road from our commercial ITS, then assemble a road abandoned object dataset comprising 557 video sequences and 18,953 images with pixel-level manual annotations. Second, we conduct exhaustive evaluation experiments employing a range of baseline models from mainstream algorithms on our dataset to illustrate the performance of different approaches. Third, we propose a novel image segmentation framework based on an area-aware attention mechanism. Experimental results reveal that our method outperforms the UNet-based model by nearly 9% in terms of dice score. Our dataset represents the most extensive open-source resource dedicated to road abandoned object detection, accessible publicly at https://github.com/yajunbaby/A-Benchmark-for-Road-Abandoned-Object-Detection-from-Video-Surveillance.  © 2013 IEEE.
KW  - area-aware attention mechanism
KW  - intelligent transportation system
KW  - Road abandoned object detection
KW  - video surveillance
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Object detection
KW  - Object recognition
KW  - Roads and streets
KW  - Security systems
KW  - Traffic control
KW  - Abandoned object detections
KW  - Area-aware attention mechanism
KW  - Attention mechanisms
KW  - Benchmark testing
KW  - Images segmentations
KW  - Intelligent transportation systems
KW  - Objects recognition
KW  - Road
KW  - Road abandoned object detection
KW  - Uncertainty
KW  - Video surveillance
KW  - Image segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, Y.
AU  - Weng, D.
AU  - Tian, Z.
AU  - Hou, J.
AU  - Li, Z.
TI  - YOLO-Ti: an efficient object detection approach for tiny facial markers
PY  - 2024
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.3048940
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210236113&doi=10.1117%2f12.3048940&partnerID=40&md5=5eaa50427b761c9260b0a26ce16d104d
AB  - In this paper, an efficient object detection method YOLO-Ti is proposed to detect tiny facial markers. Our study is driven by the practical requirements of 3D face modeling, requiring the incorporation of as many facial features as possible for reference. This research can even provide information for facial expression recognition and joint deformation. To achieve this, we first present a feature fusion module called Cross-BiFPN, which incorporates additional cross-connecting branches between different network layers to utilize low-level features more effectively. Secondly, we add a high-resolution detection head and attention module to the YOLOv8 model to improve the ability of detecting tiny objects, while at the same time ensuring the lightweight detection model by reducing redundant network layers. Thirdly, we collect a dataset of facial markers with an average size much smaller than publicly available small object datasets. Ablation studies and comparison experiments are conducted to evaluate the performance of our approach. Compared with the baseline YOLOv8 model, YOLO-Ti shows a 30.4% improvement in mAP50 while reducing model parameters by 65.1%. The automatic feature extraction provided by our model facilitates the construction of digital humans, providing significant savings in manpower and time for modelers. © 2024 SPIE.
KW  - 3D face reconstruction
KW  - Facial markers
KW  - improved YOLOv8 algorithm
KW  - tiny object detection
KW  - 3D modeling
KW  - Face recognition
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Photons
KW  - 3D face modeling
KW  - 3D face reconstruction
KW  - Detection approach
KW  - Efficient object detections
KW  - Facial marker
KW  - Improved YOLOv8 algorithm
KW  - Object detection method
KW  - Objects detection
KW  - Practical requirements
KW  - Tiny object detection
KW  - 3D reconstruction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Wang, J.
AU  - Wang, Z.
AU  - Weng, Y.
AU  - Li, Y.
TI  - DRPDDet: Dynamic Rotated Proposals Decoder for Oriented Object Detection
PY  - 2024
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
DO  - 10.1007/978-981-99-8076-5_8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190388429&doi=10.1007%2f978-981-99-8076-5_8&partnerID=40&md5=ba92952365833d81188218df0d219fef
AB  - Oriented object detection has gained popularity in diverse fields. However, in the domain of two-stage detection algorithms, the generation of high-quality proposals with a high recall rate remains a formidable challenge, especially in the context of remote sensing images where sparse and dense scenes coexist. To address this, we propose the DRPDDet method, which aims to improve the accuracy and recall of proposals for Oriented target detection. Our approach involves generating high-quality horizontal proposals and dynamically decoding them into rotated proposals to predict the final rotated bounding boxes. To achieve high-quality horizontal proposals, we introduce the innovative HarmonyRPN module. This module integrates foreground information from the RPN classification branch into the original feature map, creating a fused feature map that incorporates multi-scale foreground information. By doing so, the RPN generates horizontal proposals that focus more on foreground objects, which leads to improved regression performance. Additionally, we design a dynamic rotated proposals decoder that adaptively generates rotated proposals based on the constraints of the horizontal proposals, enabling accurate detection in complex scenes. We evaluate our proposed method on the DOTA and HRSC2016 remote sensing datasets, and the experimental results demonstrate its effectiveness in complex scenes. Our method improves the accuracy of proposals in various scenarios while maintaining a high recall rate. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.
KW  - Dynamic rotated proposals decoder
KW  - Foreground information
KW  - Harmony RPN
KW  - Oriented object detection
KW  - Classification (of information)
KW  - Decoding
KW  - Object recognition
KW  - Remote sensing
KW  - Rotation
KW  - Complex scenes
KW  - Diverse fields
KW  - Dynamic rotated proposal decoder
KW  - Feature map
KW  - Foreground information
KW  - Harmony RPN
KW  - High quality
KW  - Objects detection
KW  - Oriented object detection
KW  - Recall rate
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wu, G.
AU  - Zhou, F.
AU  - Meng, C.
AU  - Li, X.-Y.
TI  - Precise UAV MMW-Vision Positioning: A Modal-Oriented Self-Tuning Fusion Framework
PY  - 2024
T2  - IEEE Journal on Selected Areas in Communications
DO  - 10.1109/JSAC.2023.3322851
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174826831&doi=10.1109%2fJSAC.2023.3322851&partnerID=40&md5=613c59641d44c3621ae2ac7ec431e44b
AB  - Precise real-time unmanned aerial vehicle (UAV) positioning is crucial for preventing unauthorized UAVs from damaging cooperative intelligent transportation systems (C-ITSs). However, UAV positioning remains extremely challenging due to the small target size and high flexibility. Therefore, we develop a modal-oriented self-tuning fusion framework for precise UAV millimeter-wave(MMW)-vision positioning. The framework selects and extracts cross-modal features based on modality characters, and migrates the Doppler features of MMW radar data to the image features for precise pixel-level positioning. Based on the framework, a modal-oriented self-tuning fusion network is proposed to adaptively enhance UAV feature without direct supervision by exploiting the cross-modal correlations. A novel characteristic-based 3DMMW feature extraction method is presented to extract UAV Doppler motion characteristics while a self-tuning cross-modal affine transfer is proposed for UAV visual feature enhancement. Due to lack of dataset for our task, we establish a practical positioning platform and two novel datasets containing synchronized visual images and MMW radio frequency (RF) sequences in various scenarios. Experimental results confirm that our framework outperforms the benchmark methods in terms of positioning accuracy while maintaining real-time performance. Moreover, ablation studies also confirm the effectiveness of each module in the framework.  © 1983-2012 IEEE.
KW  - cooperative intelligent transportation systems
KW  - MMW-vision fusion
KW  - modal-oriented self-tuning framework
KW  - Precise UAV positioning
KW  - Antennas
KW  - Benchmarking
KW  - Extraction
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Interactive computer systems
KW  - Millimeter waves
KW  - Radar imaging
KW  - Real time systems
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Cooperative intelligent transportation system
KW  - Features extraction
KW  - Intelligent transportation systems
KW  - Millimeter-wave-vision fusion
KW  - Modal-oriented self-tuning framework
KW  - Precise unmanned aerial vehicle positioning
KW  - Real - Time system
KW  - Selftuning
KW  - Vehicle positioning
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhu, J.
AU  - Wu, Y.
AU  - Ma, T.
TI  - Multi-Object Detection for Daily Road Maintenance Inspection With UAV Based on Improved YOLOv8
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2024.3437770
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204710166&doi=10.1109%2fTITS.2024.3437770&partnerID=40&md5=79f795d607f17393c783eb5306842105
AB  - Daily road maintenance is essential to road safety and serviceability, particularly to highways. In daily road maintenance inspection tasks, the objectives include a variety of targets, such as pavement cracks, foreign objects, guardrail damages etc. There is a lack of rapid detection methods that allow for the uniform identification of multiple targets for road maintenance. This paper proposes an automatic multi-object detection method for road daily maintenance inspection assisted by unmanned aerial vehicles (UAV). A dataset of multiple roadway anomalies (UAVROAD) for daily road maintenance needs was constructed. UM-YOLO, an improved algorithm based on the YOLOv8n algorithm was created to better extract the features of multiple targets, as well as fuse features and reduce computation while maintaining accuracy. The improvements include adding EMA (Efficient multiscale Attention Module) in the C2f module in the backbone, employing Bi-FPN fusion mechanism in the neck and using GSConv, a lightweight convolutional network, for the convolution operation. Compared with the YOLOv8n, the proposed UM-YOLO improved mean average precision(mAP) by 4.6% and reduced the model computation by 14%.  © 2024 IEEE.
KW  - computer vision
KW  - Daily road maintenance
KW  - deep learning
KW  - multi-object detection
KW  - UAV
KW  - Aircraft detection
KW  - Automatic vehicle identification
KW  - Bismuth alloys
KW  - Deep learning
KW  - Highway accidents
KW  - Image annotation
KW  - Intelligent systems
KW  - Motor transportation
KW  - Risk analysis
KW  - Risk management
KW  - Aerial vehicle
KW  - Daily road maintenance
KW  - Deep learning
KW  - Maintenance inspections
KW  - Multi-object detection
KW  - Multiobject
KW  - Multiple targets
KW  - Objects detection
KW  - Road maintenance
KW  - Unmanned aerial vehicle
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, W.
AU  - Lu, P.
AU  - Zhao, Y.
TI  - E-RODNet: Lightweight Approach to Object Detection by Vehicular Millimeter-Wave Radar
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3437474
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001210004&doi=10.1109%2fJSEN.2024.3437474&partnerID=40&md5=e014ebeac75def315221953cdec3b1ff
AB  - The main sensors employed for achieving autonomous driving perception tasks include cameras, LiDAR, and millimeter-wave radar. Millimeter-wave radar offers the advantages of all-weather and all-day operation, helping to compensate for the shortcomings of cameras and LiDAR in adverse environmental conditions and ensuring the safety of autonomous driving technology. Currently, most object perception models based on millimeter-wave radar are overly complex, requiring substantial computational and storage resources during inference, which cannot meet the real-time requirements of autonomous driving. In light of this, this article proposes a more efficient new model, referred to as E-RODNet. The model employs the ConvFormer block as the primary feature extraction unit and adjusts the SepConv operator for spatiotemporal data modeling. To enhance the model’s global modeling capability, a global feature fusion module is introduced, adding global context information to each pixel after encoding features. In addition, the short-term sequence feature fusion module is improved based on the characteristics of the CRUW dataset. The application of these two techniques allows the model to achieve state-of-the-art performance with minimal parameters and computational resources. Compared to T-RODNet, E-RODNet achieves competitive performance with only 3.8% of the model parameters, improving average precision (AP) by 1.63%, and requiring only 18.2% of the GFLOPs of T-RODNet. Our model and training code can be available at https://github.com/lupeng-xm/E-RODNet.(Figure presented). © 2001-2012 IEEE.
KW  - Autonomous driving
KW  - ConvFormer block
KW  - global contextual information
KW  - millimeter-wave radar object detection
KW  - short-term sequence feature fusion
KW  - Digital elevation model
KW  - Image coding
KW  - Image compression
KW  - Image segmentation
KW  - Remote sensing
KW  - Autonomous driving
KW  - Contextual information
KW  - Convformer block
KW  - Features fusions
KW  - Global contextual information
KW  - Millimeter-wave radar
KW  - Millimeter-wave radar object detection
KW  - Millimetre-wave radar
KW  - Objects detection
KW  - Radar objects
KW  - Sequence features
KW  - Short-term sequence feature fusion
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kramar, V.A.
AU  - Kramar, O.A.
AU  - Alchakov, V.V.
TI  - The Computer Vision System for a Crewless Boat
PY  - 2024
T2  - Proceedings - 2024 International Conference on Industrial Engineering, Applications and Manufacturing, ICIEAM 2024
DO  - 10.1109/ICIEAM60818.2024.10553783
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197250001&doi=10.1109%2fICIEAM60818.2024.10553783&partnerID=40&md5=cecc27b10f9ada76bed3fafddb658ece
AB  - The article considers a computer vision system for a crewless ship. The existing approaches to the construction of such systems are described, and the strengths and weaknesses of different methods are analyzed. The hardware part of the system is considered. The functional scheme of the system and technical characteristics of the equipment are given. An approach to solving the problem of detecting a given class of objects based on the YOLOv5 algorithm is described. The accuracy of the obtained model is confirmed with the help of precision-recall curve. The paper also presents an algorithm for calculating the heading angle for the dynamic positioning of an uncrewed vessel based on data from the vision system. The advantage of the proposed approach is its fast performance, which allows the use of onboard computers in real-time. The highly effective strategy allows the onboard system's image to be broadcast over a local network without losing image quality. The performance of the computer vision system and the algorithm for detecting a given class of objects is verified using long-term hard-loaded testing. The test results showed good accuracy and stability of the developed system.  © 2024 IEEE.
KW  - computer vision
KW  - control system
KW  - crewless boat
KW  - image processing
KW  - object detection
KW  - onboard systems
KW  - YOLOv5
KW  - Boats
KW  - Computer control systems
KW  - Object detection
KW  - Computer vision system
KW  - Crewless boat
KW  - Functional scheme
KW  - Images processing
KW  - Object based
KW  - Objects detection
KW  - Objects-based
KW  - On-board systems
KW  - Performance
KW  - YOLOv5
KW  - Computer vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gong, H.
AU  - Wang, X.
AU  - Zhuang, W.
TI  - Research on Real-Time Detection of Maize Seedling Navigation Line Based on Improved YOLOv5s Lightweighting Technology
PY  - 2024
T2  - Agriculture (Switzerland)
DO  - 10.3390/agriculture14010124
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183188927&doi=10.3390%2fagriculture14010124&partnerID=40&md5=db0e7dfc68d3f02bfda118f0cf79075e
AB  - This study focuses on real-time detection of maize crop rows using deep learning technology to meet the needs of autonomous navigation for weed removal during the maize seedling stage. Crop row recognition is affected by natural factors such as soil exposure, soil straw residue, mutual shading of plant leaves, and light conditions. To address this issue, the YOLOv5s network model is improved by replacing the backbone network with the improved MobileNetv3, establishing a combination network model YOLOv5-M3 and using the convolutional block attention module (CBAM) to enhance detection accuracy. Distance-IoU Non-Maximum Suppression (DIoU-NMS) is used to improve the identification degree of the occluded targets, and knowledge distillation is used to increase the recall rate and accuracy of the model. The improved YOLOv5s target detection model is applied to the recognition and positioning of maize seedlings, and the optimal target position for weeding is obtained by max-min optimization. Experimental results show that the YOLOv5-M3 network model achieves 92.2% mean average precision (mAP) for crop targets and the recognition speed is 39 frames per second (FPS). This method has the advantages of high detection accuracy, fast speed, and is light weight and has strong adaptability and anti-interference ability. It determines the relative position of maize seedlings and the weeding machine in real time, avoiding squeezing or damaging the seedlings. © 2024 by the authors.
KW  - autonomous navigation
KW  - crop row detection
KW  - deep learning
KW  - inter-row weeding
KW  - maize seedlings
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Lambertenghi, S.C.
AU  - Stocco, A.
TI  - Assessing Quality Metrics for Neural Reality Gap Input Mitigation in Autonomous Driving Testing
PY  - 2024
T2  - Proceedings - 2024 IEEE Conference on Software Testing, Verification and Validation, ICST 2024
DO  - 10.1109/ICST60714.2024.00024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193831439&doi=10.1109%2fICST60714.2024.00024&partnerID=40&md5=25840efaa189d15957228b81fcfc3bec
AB  - Simulation-based testing of automated driving systems (ADS) is the industry standard, being a controlled, safe, and cost-effective alternative to real-world testing. Despite these advantages, virtual simulations often fail to accurately replicate real-world conditions like image fidelity, texture representation, and environmental accuracy. This can lead to significant differences in ADS behavior between simulated and real-world domains, a phenomenon known as the sim2real gap. Researchers have used Image-to-Image (I2I) neural translation to mitigate the sim2real gap, enhancing the realism of simulated environments by transforming synthetic data into more authentic representations of real-world conditions. However, while promising, these techniques may potentially introduce artifacts, distortions, or inconsistencies in the generated data that can affect the effectiveness of ADS testing. In our empirical study, we investigated how the quality of image-to-image (I2I) techniques influences the mitigation of the sim2real gap, using a set of established metrics from the literature. We evaluated two popular generative I2I architectures, pix2pix and CycleGAN, across two ADS perception tasks at a model level, namely vehicle detection and end-to-end lane keeping, using paired simulated and real-world datasets. Our findings reveal that the effectiveness of I2I architectures varies across different ADS tasks, and existing evaluation metrics do not consistently align with the ADS behavior. Thus, we conducted task-specific fine-tuning of perception metrics, which yielded a stronger correlation. Our findings indicate that a perception metric that incorporates semantic elements, tailored to each task, can facilitate selecting the most appropriate I2I technique for a reliable assessment of the sim2real gap mitigation.  © 2024 IEEE.
KW  - autonomous vehicles testing
KW  - generative adversarial networks
KW  - reality gap
KW  - sim2real
KW  - Virtual environments
KW  - Adversarial networks
KW  - Automated driving systems
KW  - Autonomous vehicle testing
KW  - Autonomous Vehicles
KW  - Condition
KW  - Real-world
KW  - Reality gaps
KW  - Sim2real
KW  - System behaviors
KW  - Vehicle testing
KW  - Automobile testing
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kaur, J.
AU  - Singh, W.
TI  - A systematic review of object detection from images using deep learning
PY  - 2024
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-023-15981-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169104184&doi=10.1007%2fs11042-023-15981-y&partnerID=40&md5=469fe7785dc708591b2a90cdedaeb7a9
AB  - The development of object detection has led to huge improvements in human interaction systems. Object detection is a challenging task because it involves many parameters including variations in poses, resolution, occlusion, and daytime versus nighttime detection. This study surveys on various aspects of object detection that includes (1) basics of object detection, (2) object detection techniques, (3) datasets, (4) metrics and deep learning libraries. This study presents a systematic analysis of recent publications on object detection covering around 400 research articles and synthesised the findings to provide empirical answers to research questions. The review is based on relevant articles published from 2015 through 2022, as well as discussions of challenges and future directions in this field. Furthermore, the survey examined the contributions of various researchers concerning their respective application domains, while emphasizing the advantages and disadvantages of the research work. Despite the success of various methods proposed in literature for predicting results, there remains room for improvement in the accuracy of object detection. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Backbone architecture
KW  - Computer vision
KW  - deep learning
KW  - object detection
KW  - object detection application
KW  - Computer vision
KW  - Deep learning
KW  - Object recognition
KW  - Backbone architecture
KW  - Deep learning
KW  - Humaninteraction
KW  - Interaction systems
KW  - Object detection application
KW  - Objects detection
KW  - Synthesised
KW  - Systematic analysis
KW  - Systematic Review
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Jiang, H.
AU  - Yang, J.
AU  - Ma, X.
AU  - Chen, J.
AU  - Li, Z.
AU  - Tang, X.
TI  - Lightweight tomato ripeness detection algorithm based on the improved RT-DETR
PY  - 2024
T2  - Frontiers in Plant Science
DO  - 10.3389/fpls.2024.1415297
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198833519&doi=10.3389%2ffpls.2024.1415297&partnerID=40&md5=cc8922bcecb466fc9ffc87fe78f82dd8
AB  - Tomatoes, widely cherished for their high nutritional value, necessitate precise ripeness identification and selective harvesting of mature fruits to significantly enhance the efficiency and economic benefits of tomato harvesting management. Previous studies on intelligent harvesting often focused solely on identifying tomatoes as the target, lacking fine-grained detection of tomato ripeness. This deficiency leads to the inadvertent harvesting of immature and rotten fruits, resulting in economic losses. Moreover, in natural settings, uneven illumination, occlusion by leaves, and fruit overlap hinder the precise assessment of tomato ripeness by robotic systems. Simultaneously, the demand for high accuracy and rapid response in tomato ripeness detection is compounded by the need for making the model lightweight to mitigate hardware costs. This study proposes a lightweight model named PDSI-RTDETR to address these challenges. Initially, the PConv_Block module, integrating partial convolution with residual blocks, replaces the Basic_Block structure in the legacy backbone to alleviate computing load and enhance feature extraction efficiency. Subsequently, a deformable attention module is amalgamated with intra-scale feature interaction structure, bolstering the capability to extract detailed features for fine-grained classification. Additionally, the proposed slimneck-SSFF feature fusion structure, merging the Scale Sequence Feature Fusion framework with a slim-neck design utilizing GSConv and VoVGSCSP modules, aims to reduce volume of computation and inference latency. Lastly, by amalgamating Inner-IoU with EIoU to formulate Inner-EIoU, replacing the original GIoU to expedite convergence while utilizing auxiliary frames enhances small object detection capabilities. Comprehensive assessments validate that the PDSI-RTDETR model achieves an average precision mAP50 of 86.8%, marking a 3.9% enhancement over the original RT-DETR model, and a 38.7% increase in FPS. Furthermore, the GFLOPs of PDSI-RTDETR have been diminished by 17.6%. Surpassing the baseline RT-DETR and other prevalent methods regarding precision and speed, it unveils its considerable potential for detecting tomato ripeness. When applied to intelligent harvesting robots in the future, this approach can improve the quality of tomato harvesting by reducing the collection of immature and spoiled fruits. Copyright © 2024 Wang, Jiang, Yang, Ma, Chen, Li and Tang.
KW  - deep learning
KW  - deformable attention
KW  - Inner-EIoU
KW  - PConv
KW  - ripeness recognition
KW  - RT-DETR
KW  - slimneck
KW  - tomato
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kumar, A.
AU  - Meel, P.
TI  - Object Detection using SSD and Efficient net B7 as base Network
PY  - 2024
T2  - Proceedings - IEEE 2024 1st International Conference on Advances in Computing, Communication and Networking, ICAC2N 2024
DO  - 10.1109/ICAC2N63387.2024.10894816
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000773130&doi=10.1109%2fICAC2N63387.2024.10894816&partnerID=40&md5=25d2e9b17ce8370bb53b55902f26306d
AB  - Object detection is the method of recognizing and finding objects in digital images and video frames. Over the years, object detection techniques have made significant advances driven by breakthroughs in large-scale datasets, powerful computing resources, and deep learning architectures. This article provides an overview of target detection, its uses, applications and recent advances in the field.Object detection is an essential component of various real-world applications, like autonomous vehicle driving, military surveillance and radar systems, augmented reality, and robotics. By correctly identifying and finding objects of interest, machines can learn about their surroundings and make intelligent decisions. This article discusses the use of an efficient network as the central network for feature extraction and a single-shot multi-box detector for target detection in order to implement targeted detection. © 2024 IEEE.
KW  - Military radar
KW  - Object detection
KW  - Object recognition
KW  - Radar target recognition
KW  - Surveillance radar
KW  - Computing resource
KW  - Digital image
KW  - Digital videos
KW  - Image frames
KW  - Large-scale datasets
KW  - Learning architectures
KW  - Objects detection
KW  - Real-world
KW  - Targets detection
KW  - Video frame
KW  - Military vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, K.
AU  - Xue, X.
AU  - Wen, H.
AU  - Ji, Y.
AU  - Yan, Y.
AU  - Meng, Q.
TI  - Misaligned Visible-Thermal Object Detection: A Drone-Based Benchmark and Baseline
PY  - 2024
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2024.3398429
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192971017&doi=10.1109%2fTIV.2024.3398429&partnerID=40&md5=82f64eb655e5978b97ae76a2045f2f36
AB  - Multispectral object detection has achieved remarkable results due to its ability to fuse information from visible and thermal modalities in recent years. However, the existing visible-thermal datasets are constructed based on manually aligned image pairs, which cannot fully represent the challenges of real-world scenarios where image pairs are often misaligned. Existing methods for visible-thermal object detection are based on aligned data and are limited by the accuracy of registration. To address the above issues, we propose a dataset, namely DVTOD, which is a misaligned visible-thermal object detection dataset captured by drones. DVTOD includes 16 challenging attributes and 54 capture scenes. Furthermore, we introduce a cross-modal alignment detector (CMA-Det) for misaligned visible-thermal object detection. Firstly, we design an alignment network to estimate the visible-to-thermal deformation field, which is used to correct for misalignment of the corresponding visible and thermal features. Secondly, we propose a strategy called Object Search Rectification (OSR) to improve the robustness of feature alignment. To better remove the interference of complex backgrounds, a bi-directional feature correction fusion module (BFCFM) is designed to calibrate bimodal features by exploiting the correlation of channel and spatial information between two modalities. CMA-Det outperforms existing methods on the DVTOD dataset and two other visible-thermal object detection datasets. © 2016 IEEE.
KW  - cross-modal alignment
KW  - feature alignment
KW  - Multispectral object detection
KW  - visible-thermal dataset
KW  - Aircraft detection
KW  - Alignment
KW  - Drones
KW  - Feature extraction
KW  - Interactive computer systems
KW  - Object detection
KW  - Object recognition
KW  - Annotation
KW  - Cross-modal
KW  - Cross-modal alignment
KW  - Feature alignment
KW  - Features extraction
KW  - Multi-spectral
KW  - Multispectral object detection
KW  - Objects detection
KW  - Real - Time system
KW  - Thermal
KW  - Vibration
KW  - Visible-thermal dataset
KW  - Real time systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Babu, M.A.A.
AU  - Pandey, S.K.
AU  - Durisic, D.
AU  - Koppisetty, A.C.
AU  - Staron, M.
TI  - Impact of Image Data Splitting on the Performance of Automotive Perception Systems
PY  - 2024
T2  - Lecture Notes in Business Information Processing
DO  - 10.1007/978-3-031-56281-5_6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192177434&doi=10.1007%2f978-3-031-56281-5_6&partnerID=40&md5=15628e168be69da764bda23aa7ca5927
AB  - Context: Training image recognition systems is one of the crucial elements of the AI Engineering process in general and for automotive systems in particular. The quality of data and the training process can have a profound impact on the quality, performance, and safety of automotive software. Objective: Splitting data between train and test sets is one of the crucial elements in this process as it can determine both how well the system learns and generalizes to new data. Typical data splits take into consideration either randomness or timeliness of data points. However, in image recognition systems, the similarity of images is of equal importance. Methods: In this computational experiment, we study the impact of six data-splitting techniques. We use an industrial dataset with high-definition color images of driving sequences to train a YOLOv7 network. Results: The mean average precision (mAP) was 0.943 and 0.841 when the similarity-based and the frame-based splitting techniques were applied, respectively. However, the object-based splitting technique produces the worst mAP score (0.118). Conclusion: There are significant differences in the performance of object detection methods when applying different data-splitting techniques. The most positive results are the random selections, whereas the most objective ones are splits based on sequences that represent different geographical locations. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Autonomous driving
KW  - Data splitting technique
KW  - Image perception system
KW  - Object detection
KW  - YOLOv7
KW  - Autonomous vehicles
KW  - Image recognition
KW  - Object recognition
KW  - Autonomous driving
KW  - Data splitting
KW  - Data splitting technique
KW  - Image perception
KW  - Image perception system
KW  - Objects detection
KW  - Perception systems
KW  - Performance
KW  - Splitting techniques
KW  - YOLOv7
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Adiuku, N.
AU  - Avdelidis, N.P.
AU  - Tang, G.
AU  - Plastropoulos, A.
AU  - Diallo, Y.
TI  - Mobile Robot Obstacle Detection and Avoidance with NAV-YOLO
PY  - 2024
T2  - International Journal of Mechanical Engineering and Robotics Research
DO  - 10.18178/ijmerr.13.2.219-226
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189462962&doi=10.18178%2fijmerr.13.2.219-226&partnerID=40&md5=244beff11293d476a3917b75fd597b6e
AB  - Intelligent robotics is gaining significance in Maintenance, Repair, and Overhaul (MRO) hangar operations, where mobile robots navigate complex and dynamic environments for Aircraft visual inspection. Aircraft hangars are usually busy and changing, with objects of varying shapes and sizes presenting harsh obstacles and conditions that can lead to potential collisions and safety hazards. This makes Obstacle detection and avoidance critical for safe and efficient robot navigation tasks. Conventional methods have been applied with computational issues, while learning-based approaches are limited in detection accuracy. This paper proposes a vision-based navigation model that integrates a pre-trained Yolov5 object detection model into a Robot Operating System (ROS) navigation stack to optimise obstacle detection and avoidance in a complex environment. The experiment is validated and evaluated in ROS-Gazebo simulation and turtlebot3 waffle-pi robot platform. The results showed that the robot can increasingly detect and avoid obstacles without colliding while navigating through different checkpoints to the target location. © 2024 by the authors. This is an open access article distributed under the Creative Commons Attribution License (CC BY-NC-ND 4.0), which permits use, distribution and reproduction in any medium, provided that the article is properly cited, the use is noncommercial and no modifications or adaptations are made. All Rights Reserved.
KW  - autonomous navigation
KW  - deep learning
KW  - mobile robot
KW  - object detection
KW  - obstacle avoidance
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lu, Z.
AU  - Liu, F.
AU  - Wang, L.
AU  - Xu, L.
AU  - Liu, X.
TI  - A Novel Lung Nodule Detection and Recognition Model Based on Deep Learning
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3478358
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207630037&doi=10.1109%2fACCESS.2024.3478358&partnerID=40&md5=06c5397f8e7417ddf93a4176f2fc23ee
AB  - To solve the problems of missing and false detection of pulmonary nodules in complex lung environments, as well as trivial and inefficient detection procedures, an end-to-end pulmonary nodules detection and recognition model based on deep learning was proposed. Innovation and improvement are made on the basis of YOLOv5. In the feature extraction stage of the model, a convolutional structure integrating self-attention mechanism is proposed to capture the global feature and the dependence relationship of long-distance information, and screen the key pathological information. Then, a convolution structure integrating internal convolution operators is proposed to reduce the computational redundancy in the feature channel and improve the inference speed of the model. In the feature fusion stage of the model, the structure of cross-scale coordinate attention feature fusion is proposed, and the different features enhanced with attention are weighted by jumping links to promote the fusion of multi-scale feature information. The proposed model obtained 97.8% mAP@0.5 indexes in the self-built diagnosis and treatment data set of pulmonary nodules in Huaihai area. The pulmonary nodule detection model proposed in this paper can significantly reduce the false positive rate and obtain the location and classification results of diseased nodules with higher detection accuracy and faster detection speed, which has important practical value in clinical application.  © 2013 IEEE.
KW  - cross-scale feature fusion
KW  - involution
KW  - multi-head self-attention mechanism
KW  - Pulmonary nodule detection
KW  - Deep learning
KW  - Diagnosis
KW  - Attention mechanisms
KW  - Cross-scale feature fusion
KW  - Detection models
KW  - Features fusions
KW  - Involution
KW  - Model-based OPC
KW  - Multi-head self-attention mechanism
KW  - Pulmonary nodule detection
KW  - Pulmonary nodules
KW  - Recognition models
KW  - Lung cancer
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Jhala, J.S.
AU  - Joshi, C.
AU  - Anand, D.
TI  - Deep Learning Driven Object Detection and Classification for Autonomous Vehicles in Diverse Traffic and Weather Conditions
PY  - 2024
T2  - 1st International Conference on Emerging Technologies for Dependable Internet of Things, ICETI 2024
DO  - 10.1109/ICETI63946.2024.10777214
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214977783&doi=10.1109%2fICETI63946.2024.10777214&partnerID=40&md5=f6774ab61018499df1bdbd35510142e4
AB  - The rapid development of self-driving vehicles necessitates integrating a sophisticated sensing system to address various obstacles posed by road traffic efficiently. While several datasets support object detection in autonomous vehicles, evaluating their suitability for different weather conditions globally is crucial. In this study, we present deep learning models trained on a novel dataset derived from YouTube videos recorded from Indian car’s dashcams. These videos capture a wide range of conditions, including rain, fog, daytime, hazy and night-time driving scenarios prevalent in India. The dataset comprises a total of 1450 annotated images depicting vehicles and other road assets across six different classes. In this work, performance analysis of the YOLOv8 models trained using an existing dataset was compared with the model trained on an expanded version using the proposed weather-specific dataset. The results demonstrate improved accuracy metrics of 91.3%, 84.5%, and 91.2% for Precision, Recall, and mean Average Precision (mAP) upon integrating the proposed dataset. The model trained on this diverse dataset exhibits heightened robustness, proving highly beneficial for autonomous and conventional vehicle operations in India’s dynamic traffic environments. This research contributes to advancing object detection capabilities crucial for autonomous driving technologies in real-world settings. © 2024 IEEE.
KW  - Autonomous-driving Cars
KW  - Deep Learning Neural Networks
KW  - Object Identification Model
KW  - Smart Transportation Systems
KW  - YOLOv8 Algorithm
KW  - Autonomous vehicles
KW  - Deep neural networks
KW  - Vehicle detection
KW  - Autonomous driving
KW  - Autonomous-driving car
KW  - Deep learning neural network
KW  - Identification modeling
KW  - Learning neural networks
KW  - Object identification
KW  - Object identification model
KW  - Smart transportation system
KW  - Transportation system
KW  - YOLOv8 algorithm
KW  - Image annotation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, X.
AU  - Jiang, H.
AU  - Zheng, H.
AU  - Yang, J.
AU  - Liang, R.
AU  - Xiang, D.
AU  - Cheng, H.
AU  - Jiang, Z.
TI  - DET-YOLO: An Innovative High-Performance Model for Detecting Military Aircraft in Remote Sensing Images
PY  - 2024
T2  - IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
DO  - 10.1109/JSTARS.2024.3462745
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204493009&doi=10.1109%2fJSTARS.2024.3462745&partnerID=40&md5=8ad401bc1f998456a3f535590a68d55a
AB  - To address the challenges of low detection rate and high missed detection rate of military aircraft in current complex remote sensing data, and to meet the requirements of real-Time detection and easy deployment of models, this article introduces DET-you only look once (YOLO), an innovative detection model. First, to tackle the issue of reduced accuracy in identifying small targets amidst intricate backgrounds, a novel feature extraction component, C2fDEF, was devised. This module replaced all existing C2f components within YOLOv8n, thereby significantly enhancing the model's ability to cope with complicated environmental contexts. Second, to achieve the functionality of easy deployment of the model, some deep structures were simplified to make the model more lightweight. Afterward, to further improve the model's ability to handle complex backgrounds and dense environments in remote sensing images and to improve the model's detection accuracy for military aircraft, the DAT module was embedded in the model. Finally, this article also optimized the loss function and regmax to further reduce computational costs while improving the detection accuracy of the model. To verify the effectiveness and strong universality of DET-YOLO, extensive experimental verification was conducted on three publicly available datasets, namely MAR20, NWPU VHR-10, and NEU-DET. On the MAR20 dataset, compared with other advanced models, DET-YOLO achieved the highest mAP0.5 (namely 94.7%) with only 80 training epochs while meeting lightweight and real-Time requirements. While on the other two datasets, DET-YOLO also achieved the best detection performance.  © 2008-2012 IEEE.
KW  - Attention mechanism
KW  - deep learning
KW  - deformable convolution (DCN)
KW  - loss function
KW  - remote sensing images (RSIs)
KW  - you only look once (YOLO)
KW  - Fighter aircraft
KW  - Military helicopters
KW  - Military photography
KW  - Remote sensing
KW  - Attention mechanisms
KW  - DCN
KW  - Deep learning
KW  - Detection accuracy
KW  - Detection rates
KW  - High performance modeling
KW  - Loss functions
KW  - Modeling abilities
KW  - Remote sensing images
KW  - YOLO
KW  - accuracy assessment
KW  - detection method
KW  - image analysis
KW  - image classification
KW  - machine learning
KW  - remote sensing
KW  - Aircraft detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, X.
AU  - Yang, K.
AU  - Huang, R.
AU  - Zhou, B.
AU  - Xiao, J.
AU  - Gao, Z.
TI  - Detecting Small Objects using Multi-Scale Feature Fusion Mechanism with Convolutional Block Attention
PY  - 2024
T2  - Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics
DO  - 10.1109/SMC54092.2024.10831911
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217843435&doi=10.1109%2fSMC54092.2024.10831911&partnerID=40&md5=3eae145891fd0ff46c009b37c9c16c52
AB  - Small object detection is difficult because of their low resolution and the inclusion of unimportant background information. Aiming at the problem of small objects information loss in multiscale feature fusion and the impact of background information, this paper proposes a multi-scale feature fusion mechanism (MFFM) with the convolutional block attention modules (CBAM). The proposed approach efficiently utilizes the low three-layer feature information (P1-P3) output from the backbone network and the improved feature fusion technique to enhance the characterization ability of the single-layer feature information through the attention mechanism. This mechanism enables the single-layer features to carry the three-layer feature information, thereby improving the fusion ability among the feature layers. The experimental results demonstrate that MFFM enhances the overall accuracy mAP on the VisDrone2019-DET validation set by 1.4% and the accuracy APs on small objects by 1.5% in comparison to the baseline model YOLOX-X. This approach effectively improves the performance of small object detection. © 2024 IEEE.
KW  - Attention mechanism
KW  - Multi-scale feature fusion
KW  - Small object detection
KW  - Unmanned aerial vehicle imagery
KW  - Aerial photography
KW  - Aircraft detection
KW  - Image fusion
KW  - Information fusion
KW  - Object detection
KW  - Object recognition
KW  - Object tracking
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Attention mechanisms
KW  - Feature information
KW  - Features fusions
KW  - Fusion mechanism
KW  - Multi-scale feature fusion
KW  - Multi-scale features
KW  - Small object detection
KW  - Small objects
KW  - Unmanned aerial vehicle imagery
KW  - Antennas
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, Y.
AU  - Kong, B.
AU  - Yu, W.
AU  - Jiang, J.
TI  - Small Object Vehicle Detection Based on Improved YOLOv5
PY  - 2024
T2  - 2024 9th International Conference on Intelligent Computing and Signal Processing, ICSP 2024
DO  - 10.1109/ICSP62122.2024.10743827
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211453348&doi=10.1109%2fICSP62122.2024.10743827&partnerID=40&md5=1882746209acfc5b0e37a2464bc95dcb
AB  - With the advent of deep learning, object detection has been extensively utilized in intelligent transportation systems. However, the performance of baseline object detection models in traffic scenes is suboptimal, particularly for small object detection. To address this issue, this paper introduces an improved method for small object detection in traffic scenes based on YOLOv5, designated as YOLOv5-CBS. Initially, the convolutional modules of the model network are replaced to enhance the capability of feature extraction. Subsequently, an attention mechanism is incorporated into the Neck section of the model network, which bolsters the network's ability to capture contextual information through bidirectional encoding and decoding. Additionally, the imbalance between simple and challenging samples is mitigated by introducing a weighting function, Slide, into the model's loss function. Experimental results on the SODA10M dataset demonstrate that YOLOv5-CBS achieves average detection accuracies of 64.3% for mAP@0.5, 46.3% for mAP@0.5:0.95, and 16.2% for small object detection, representing improvements of 4.3%, 7.8%, and 2.1% over YOLOv5, respectively. © 2024 IEEE.
KW  - Attention mechanism
KW  - Object detection
KW  - Small target
KW  - Deep learning
KW  - Object detection
KW  - Object recognition
KW  - Vehicle detection
KW  - Attention mechanisms
KW  - Intelligent transportation systems
KW  - Learning objects
KW  - Model networks
KW  - Objects detection
KW  - Small object detection
KW  - Small objects
KW  - Small targets
KW  - Traffic scene
KW  - Vehicles detection
KW  - Intelligent systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, Y.
AU  - Chen, Q.
TI  - Radar Can See and Hear as Well: A New Multimodal Benchmark Based on Radar Sensing
PY  - 2024
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2024.3396285
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192205171&doi=10.1109%2fJIOT.2024.3396285&partnerID=40&md5=8557fa1028c88ac6a9611c1c4a59a0e7
AB  - Radar technology has emerged as a pivotal component for various applications within the Internet of Things (IoT). To promote the understanding and integration of radar sensing in developing multimodal applications, we introduce the Radar Can See and Hear (RACER) data set. This data set encompasses synchronized radar sensing, audio, and visual data. Radar, with its capability to detect vocal cord vibrations and lip movements, addresses scenarios where conventional microphone and camera setups may falter, such as through-wall or non-line-of-sight sensing. Specifically, the radar discerns and characterizes human lip and vocal cord movements in the range-Doppler domain. We employ deep neural networks to capture the inherent relationships among radar signatures, audio vocal sound, and visual lip movements during human pronunciations. We evaluate the performances of radar sensing using experiments on speech classification, cross-modality retrieval among audio, video, and radar, and cross-modality distillation from video or audio to radar. We summarize the findings and the limitations of using radar sensing in speech-related multimodal analysis applications. Our codes are available at: https://github.com/SPIresearch/RACER. © 2014 IEEE.
KW  - Deep neural networks
KW  - Internet of Things (IoT)
KW  - radar sensing
KW  - speech classification
KW  - Advanced driver assistance systems
KW  - Audio acoustics
KW  - Deep neural networks
KW  - Distillation
KW  - Modal analysis
KW  - Tracking radar
KW  - Cross modality
KW  - Lip movements
KW  - Multi-modal
KW  - Radar applications
KW  - Radar detection
KW  - Radar sensing
KW  - Speech classification
KW  - Task analysis
KW  - Vibration
KW  - Vocal cords
KW  - Internet of things
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Jiao, H.
AU  - Su, X.
AU  - Yuan, Q.
TI  - An Ensemble Learning Approach With Attention Mechanism for Detecting Pavement Distress and Disaster-Induced Road Damage
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2024.3391751
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192213357&doi=10.1109%2fTITS.2024.3391751&partnerID=40&md5=f950b3336787b2077615d6f043525da8
AB  - Road damage presents a significant risk to traffic safety, including pavement distress and disaster-induced damage. Thanks to their high efficiency, computer vision-based methods for pavement distress detection have been widely developed. In disaster scenarios, the automatic extraction of road damage information from extensive social media images plays a critical role in rescue efforts. However, few existing studies have focused on detecting object-level disaster-induced road damage. To fill the gap, this paper presents a Social media image dataset of Object detection for Disaster-induced Road damage (SODR), including 1,552 images and two categories (i.e., collapses and blockages). Additionally, this paper proposes an ensemble learning approach with attention mechanisms based on YOLOv5 (You Only Look Once) network. Initially, attention modules are employed to create two distinct detectors for ensemble learning. Subsequently, one standard YOLOv5 and two variant networks are trained with consistent settings, and test time augmentation is applied during the inference phase. The proposed method has been implemented across five scales of YOLOv5, offering alternatives for balancing accuracy and computational cost. To demonstrate the validity, comprehensive experiments were conducted on two datasets. Compared with some mainstream detectors and ensemble learning methods, our approach achieved competitive results with a fewer number of parameters and a simpler training and testing process. The SODR dataset and source code are available at (https://github.com/nonondayo/yolov5_SODRv1).  © 2024 IEEE.
KW  - attention mechanism
KW  - disaster response
KW  - ensemble learning
KW  - object detection
KW  - Road damage detection
KW  - Damage detection
KW  - Disasters
KW  - E-learning
KW  - Learning systems
KW  - Object detection
KW  - Object recognition
KW  - Pavements
KW  - Attention mechanisms
KW  - Disaster-response
KW  - Ensemble learning
KW  - Objects detection
KW  - Road
KW  - Road damage
KW  - Road damage detection
KW  - Social networking (online)
KW  - YOLO
KW  - Social networking (online)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Chandrasekaran, N.
AU  - Alamsyah, N.
AU  - Haq, Q.M.U.
AU  - Fan, H.-W.
AU  - Chou, C.-Y.
AU  - Lu, K.-H.
TI  - Advancing Aerial Imagery Analysis: A Comparative Study of YOLOv3 and YOLOv8 on the VisDrone Dataset
PY  - 2024
T2  - Proceedings - ICE3IS 2024: 4th International Conference on Electronic and Electrical Engineering and Intelligent System: Leading-Edge Technologies for Sustainable Societies
DO  - 10.1109/ICE3IS62977.2024.10775447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215108958&doi=10.1109%2fICE3IS62977.2024.10775447&partnerID=40&md5=9a2efab8d40baf458605a8bf860c0cfe
AB  - Object detection in aerial imagery is crucial for applications such as security monitoring, traffic oversight, and emergency response. This study investigates the effectiveness of the YOLOv3 model in identifying objects within the diverse VisDrone dataset, focusing on its ability to manage varying object scales, occlusions, and intricate backgrounds in real-time scenarios. YOLOv3°s architecture is leveraged for its proficiency in handling these challenges, making it highly suitable for practical applications such as identifying vehicles and pedestrians in traffic management systems, monitoring large areas for security purposes, and detecting critical situations in disaster response operations. Comprehensive experimentation demonstrates YOLOv3s capability to address the unique challenges of aerial imagery. Data augmentation and transfer learning techniques are employed to enhance detection accuracy. A comparative analysis with the YOLOv8 model reveals that YOLOv8 consistently outperforms YOLOv3 in terms of mean Average Precision (mAP), underscoring advancements in the latest model. These findings highlight the potential of both models to significantly improve aerial surveillance, traffic management, and disaster response, providing robust tools for precise and efficient object detection in drone-captured images. © 2024 IEEE.
KW  - Aerial Imagery
KW  - Deep Learning
KW  - Object Detection
KW  - VisDrone Dataset
KW  - YOLOv3
KW  - Aircraft accidents
KW  - Emergency traffic control
KW  - Highway administration
KW  - Image analysis
KW  - Image enhancement
KW  - Intelligent systems
KW  - Network security
KW  - Aerial imagery
KW  - Comparatives studies
KW  - Deep learning
KW  - Emergency response
KW  - Imagery analysis
KW  - Objects detection
KW  - Real- time
KW  - Security monitoring
KW  - Visdrone dataset
KW  - YOLOv3
KW  - Aerial photography
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Dixit, I.A.
AU  - Bhoite, S.
TI  - Analysis of Performance of YOLOv8 Algorithm for Pedestrian Detection
PY  - 2024
T2  - Proceedings of the 9th International Conference on Communication and Electronics Systems, ICCES 2024
DO  - 10.1109/ICCES63552.2024.10859981
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218420349&doi=10.1109%2fICCES63552.2024.10859981&partnerID=40&md5=aeb632c3747a291a4ce56383cb9db316
AB  - Pedestrian detection is widely used nowadays in several applications like Robotics, Security systems and autonomous driving systems. Pedestrian detection is a rapidly advancing field within computer vision. Many algorithms have been developed for pedestrian detection using various deep learning approaches. Despite this development, the task of pedestrian detection from a mixed object dataset containing pedestrian and non-pedestrian images still poses a big challenge. After going through latest papers, it has been found that there is a need for enhancement in the correctness and speed of pedestrian detection for use in real world scenarios like autonomous driving systems. This paper proposes to analyze the execution of the Yolo V8algorithm with an aim of pedestrian detection. To achieve this objective, we have used machine learning pipeline from data collection to evaluation with the help of Yolo V8. The objectives of this paper are to observe and analyse the performance of the Yolo V8 model for pedestrian detection, to compare the time required by the model for two different datasets and to fine tune the model as per requirement. The Yolo v8 algorithm has demonstrated great success in the domain of computer vision for real time object detection. Initially, this algorithm was applied on the Roboflow Pedestrian Yolo V5 dataset and has shown to effectively improve pedestrian detection accuracy as the number of epochs increases. Later the algorithm was applied on the Yolo dataset Sindhi Label containing pedestrian as well as non-pedestrian images. This was done to avoid the chances of overfitting the model. © 2024 IEEE.
KW  - Computer Vision
KW  - Deep learning
KW  - Mixed Object Dataset
KW  - Pedestrian detection
KW  - Security System
KW  - Yolo V8
KW  - Adversarial machine learning
KW  - Deep learning
KW  - Object detection
KW  - Autonomous driving
KW  - Deep learning
KW  - Driving systems
KW  - Learning approach
KW  - Machine-learning
KW  - Mixed object dataset
KW  - Pedestrian detection
KW  - Performance
KW  - Real-world scenario
KW  - Yolo v8
KW  - Machine vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, M.
AU  - Lan, J.
AU  - Wang, L.
AU  - Zhang, Y.
AU  - Huang, K.
TI  - Infrared Multiobject Contrast Enhancement and Detection Based on Layered Visual Transformer Network for Autonomous Driving
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3466397
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205826122&doi=10.1109%2fJSEN.2024.3466397&partnerID=40&md5=df181e27121ad5ff0531eccf335c918f
AB  - For autonomous driving in urban environments, achieving reliable object detection under various lighting conditions is crucial. Thermal infrared cameras, capable of capturing images passively without relying on external light sources, face challenges due to the low contrast of infrared images, which complicates the detection and recognition of multiple objects. To address this challenge, we propose the layered visual transformer network (LVTN), which is divided into three components: backbone, encoder, and decoder and detection head: 1) the backbone network, LVTN, transforms image pixels into vectors and employs layering techniques combined with feature aggregation from each layer to maintain the integrity of features and information in infrared images; 2) the encoder introduces adaptive-feedback attention (AFA) to replace traditional attention mechanisms, focusing on subtle object features and enhancing the contrast between the object and the background; and 3) the decoder and detection head introduces the fine-grained matching loss function (FMLF), which dynamically adjusts training weights, gives higher attention to object-dense regions, and addresses the issues of multiscale and dense object detection. We trained and validated our model on the FLIR-ADAS and KAIST datasets, achieving mAP scores of 62.6% and 75.6%, respectively, surpassing other state-of-the-art infrared detection algorithms. © 2001-2012 IEEE.
KW  - Attention mechanism
KW  - automatic driving
KW  - infrared (IR) object detection
KW  - thermal infrared camera
KW  - transformer network
KW  - Image coding
KW  - Image enhancement
KW  - Infrared detectors
KW  - Infrared radiation
KW  - Laser beams
KW  - Network coding
KW  - Remote sensing
KW  - Temperature indicating cameras
KW  - Attention mechanisms
KW  - Automatic driving
KW  - Infra-red cameras
KW  - Infrared  object detection
KW  - Infrared cameras
KW  - Infrared object
KW  - Objects detection
KW  - Thermal infrared camera
KW  - Thermal-infrared
KW  - Transformer network
KW  - Infrared imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kumar, A.K.
AU  - Palanisamy, V.
TI  - Detection of lanes, obstacles and drivable areas for self-driving cars using multifusion perception metrics
PY  - 2024
T2  - Journal of Autonomous Intelligence
DO  - 10.32629/jai.v7i3.1059
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186233295&doi=10.32629%2fjai.v7i3.1059&partnerID=40&md5=ec31450ae10d440aee0381249ae766f5
AB  - Autonomous vehicles have been a recent trend and active research area from the onset of machine learning and deep learning algorithms. Computer vision and deep learning techniques have simplified the operations of continuous monitoring and decision-making capabilities of autonomous vehicles. A navigation system is facilitated by a visual system, where sensors and collectors process input in form of images or videos, and the navigation system will be making certain decisions to adhere to the safety of drivers and passers-by. This research article contemplates the model of obstacle detection, lane detection, and how the vehicle is supposed to act in terms of autonomous driving situation. This situation should resemble human driving conditions and should ensure maximum safety to both the stakeholders. A unified neural network for detecting lanes, objects, obstacles and to advise the driving speed is defined in this architecture. As far as autonomous driving is considered, these target elements are considered to be the predominant areas of focus for autonomous driving vehicles. Since capturing the images or videos have to be performed in real-time scenarios and processing them for relevant decision making have to be completed at a swift pace, a concept of context tensors is introduced in the decoders for discriminating the tasks based on priority. Every task is associated with the other tasks and also the decision-making process and hence this architecture will continue to learn every day. From the obtained results, it is evident that multitask networks can be improved using the proposed method in terms of accuracy, decision-making capability and reduced computational time. This model investigates the performance using Berkeley deep drive datasets which are considered to be a challenging dataset. © 2024 by author(s).
KW  - autonomous vehicle
KW  - deep learning
KW  - image processing
KW  - self-driving car, multi-task network
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, H.
AU  - Cheng, W.
AU  - Li, C.
AU  - Xu, Y.
AU  - Fan, S.
TI  - Lightweight Detection Model RM-LFPN-YOLO for Rebar Counting
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3349978
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182349392&doi=10.1109%2fACCESS.2024.3349978&partnerID=40&md5=13592489c1f79fce8dd07c5bd73e9c3a
AB  - In this study, we propose a novel lightweight detection model for rebar counting, which is rectified mobilenet lightweight feature pyramid network based on YOLO (RM-LFPN-YOLO). The model incorporates a lightweight backbone network that integrates the coordinate attention (CA) mechanism, a lightweight feature pyramid network (LFPN), and a loss function that combines focal loss and efficient intersection over union (EIOU) loss, all meticulously designed to enhance the model's performance. Experimental results demonstrate that our improved algorithm, with a mere 25.08M parameters, computes efficiently at 7.60G with an input size of 416 pixels. Additionally, it achieves an impressive average precision (AP) of 99.03% at an IOU of 0.5. The proposed lightweight model can be deployed on embedded devices and achieve efficient rebar detection and counting performance.  © 2013 IEEE.
KW  - attention
KW  - focal loss
KW  - LFPN
KW  - YOLO
KW  - Job analysis
KW  - Attention
KW  - Computational modelling
KW  - Feature pyramid
KW  - Features extraction
KW  - Focal loss
KW  - Lightweight feature pyramid network
KW  - Pyramid network
KW  - Task analysis
KW  - YOLO
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - He, X.
AU  - Wu, D.
AU  - Wu, D.
AU  - You, Z.
AU  - Zhong, S.
AU  - Liu, Q.
TI  - Millimeter-Wave Radar and Camera Fusion for Multiscenario Object Detection on USVs
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3444826
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201757832&doi=10.1109%2fJSEN.2024.3444826&partnerID=40&md5=e751946d2fb59069624d3be042b83e9f
AB  - Accurate object detection is fundamental for unmanned surface vehicles (USVs) to achieve intelligent perception. This article proposes an object detection network that integrates millimeter-wave radar and a camera. The method utilizes the complementary advantages of millimeter-wave radar and camera data modalities to realize multiscenario object detection for USVs applications. To address the drawback of sparse point clouds in millimeter-wave radar and improve the suboptimal performance of the camera in adverse weather conditions and small object detection, as well as to effectively utilize the features of both millimeter-wave radar and camera, a multisensor deep learning fusion object detection network [fusion mixture with AFPN (FMA)-fully convolutional one-stage (FCOS)] is proposed. To validate the effectiveness of FMA-FCOS, training, and testing are conducted on the multiscenario vessel dataset collected specifically for this study and the nuScenes dataset. In comparison with methods solely relying on a camera, such as the original FCOS object detection framework and YOLOv9, as well as other fusion methodologies combining camera and radar, the results demonstrate that FMA-FCOS delivers notable advantages, achieving a superior or comparable detection accuracy in the datasets.  © 2001-2012 IEEE.
KW  - Deep learning
KW  - fusion mixture with AFPN (FMA)-fully convolutional one-stage (FCOS)
KW  - multiscenario
KW  - object detection
KW  - sensor fusion
KW  - unmanned surface vehicle (USV)
KW  - Deep learning
KW  - Image coding
KW  - Image segmentation
KW  - Remote sensing
KW  - Sensor data fusion
KW  - Ship testing
KW  - Unmanned surface vehicles
KW  - Deep learning
KW  - Features extraction
KW  - FMA-FCOS
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Multi scenarios
KW  - Objects detection
KW  - Radar detection
KW  - Sensor fusion
KW  - Surface vehicles
KW  - Radar imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, Y.
AU  - Kong, Z.
AU  - Xu, Q.
TI  - A Novel Collaborative Heterogeneous Supervision Network for Small Object Detection Method Based on Panchromatic and Hyperspectral Images
PY  - 2024
T2  - 2024 IEEE International Conference on Control Science and Systems Engineering, ICCSSE 2024
DO  - 10.1109/ICCSSE63803.2024.10823779
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217238054&doi=10.1109%2fICCSSE63803.2024.10823779&partnerID=40&md5=f9ca0ffc55fa6f375a3fe556aeeafb53
AB  - With the increasing availability of simultaneous panchromatic and hyperspectral images, object detection methods based on them have demonstrated significant application advantages. However, they still face several challenges that limit detection performance: 1) the sizes of small objects remain very small even in the fused images, insufficient texture information and spectral information that is easily confused, leading to lower accuracy in object detection; 2) etection-by-Preprocess (DBP) methods often suffer from spectral and spatial detail distortions, compromising target features; 3) Preprocess-free detection (PFD) methods extract panchromatic and hyperspectral features directly through networks, but the black-box nature of deep network makes it difficult to ensure precise alignment of these two types of features, thereby hindering further improvements in detection accuracy. Therefore, this paper proposed a novel Collaborative Heterogeneous Supervision Network (CHS-Net) for small object detection on panchromatic and hyperspectral images. First, integrating fusion and detection components into a heterogeneous supervision network enhances learning capabilities by incorporating more empirical knowledge. Second, a unified joint regulation strategy was introduced to enhance integrated learning capabilities using optimized feedback loss functions. This approach enhanced the attention of different components to target features, effectively improving weak small target detection performance. Finally, comparative experiments based on EO-1 dataset demonstrate that the proposed method outperforms many start-of-the-art approaches. © 2024 IEEE.
KW  - Collaborative supervision
KW  - Deep learning
KW  - Object Detection
KW  - Contrastive Learning
KW  - Deep learning
KW  - Image fusion
KW  - Image texture
KW  - Supervised learning
KW  - Collaborative supervision
KW  - Deep learning
KW  - Detection performance
KW  - HyperSpectral
KW  - Learning capabilities
KW  - Object detection method
KW  - Objects detection
KW  - Preprocess
KW  - Small object detection
KW  - Target feature
KW  - Self-supervised learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, H.
AU  - Li, Y.
AU  - Leng, L.
AU  - Che, K.
AU  - Liu, Q.
AU  - Guo, Q.
AU  - Liao, J.
AU  - Cheng, R.
TI  - Automotive Object Detection via Learning Sparse Events by Spiking Neurons
PY  - 2024
T2  - IEEE Transactions on Cognitive and Developmental Systems
DO  - 10.1109/TCDS.2024.3410371
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195394926&doi=10.1109%2fTCDS.2024.3410371&partnerID=40&md5=60c834f76394e69420a3c55573c6f1be
AB  - Event-based sensors, distinguished by their high temporal resolution of 1 μ s and a dynamic range of 120 dB, stand out as ideal tools for deployment in fast-paced settings such as vehicles and drones. Traditional object detection techniques that utilize artificial neural networks (ANNs) face challenges due to the sparse and asynchronous nature of the events these sensors capture. In contrast, spiking neural networks (SNNs) offer a promising alternative, providing a temporal representation that is inherently aligned with event-based data. This article explores the unique membrane potential dynamics of SNNs and their ability to modulate sparse events. We introduce an innovative spike-triggered adaptive threshold mechanism designed for stable training. Building on these insights, we present a specialized spiking feature pyramid network (SpikeFPN) optimized for automotive event-based object detection. Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional SNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPN achieves a mean average precision (mAP) of 0.477 on the GEN1 automotive detection (GAD) benchmark dataset, marking significant increases over the selected SNN baselines. Moreover, the efficient design of SpikeFPN ensures robust performance while optimizing computational resources, attributed to its innate sparse computation capabilities.  © 2016 IEEE.
KW  - Deep learning
KW  - dynamical vision sensor (DVS)
KW  - object detection
KW  - spiking neural networks (SNNs)
KW  - Deep learning
KW  - Job analysis
KW  - Neural networks
KW  - Object detection
KW  - Object recognition
KW  - Adaptation models
KW  - Deep learning
KW  - Dynamical vision sensor
KW  - Features extraction
KW  - Neural-networks
KW  - Objects detection
KW  - Spiking neural network
KW  - Task analysis
KW  - Vehicle's dynamics
KW  - Vision sensors
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Wang, H.
TI  - Accurate and Robust Roadside 3-D Object Detection Based on Height-Aware Scene Reconstruction
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3444816
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201784278&doi=10.1109%2fJSEN.2024.3444816&partnerID=40&md5=7155aa34cbe393ae2ffb08b7bf944766
AB  - Roadside 3-D object detection allows for a drastic expansion of the visibility range and a reduction in occlusions for autonomous vehicles. Recent approaches are based on bird's-eye view (BEV) fusion, which unifies multimodal features in the shared BEV representation space. However, the camera-to-BEV projection throws away the geometric information of camera features, hindering the effectiveness of such methods. Besides, depth-based camera lifting results in inefficiency and instability in disrupted roadside scenarios. To address these challenges, this article introduces a novel 3-D object detection framework based on height-aware scene reconstruction, dubbed HSRDet. Specifically, we leverage height-aware 3-D reconstruction to ensure geometric consistency in BEV feature mapping and employ a fast camera-to-BEV transformation based on feature distillation to boost efficiency without compromising performance. In addition, we integrate a novel data augmentation method, namely, View Shake (VS), to further improve the performance of our model. Extensive experiments on the DAIR-V2X dataset demonstrate that HSRDet not only achieves state-of-the-art detection accuracy but also exhibits strong robustness in disturbance scenarios. Further experiments on the intelligent roadside units (RSUs) have revealed that our method runs stably at 11.8 frames/s on a RTX 3090 Ti GPU, thus promising vast engineering application prospects.  © 2001-2012 IEEE.
KW  - 3-D object detection
KW  - bird's-eye view (BEV)
KW  - intelligent transport system
KW  - multimodality fusion
KW  - 3D object
KW  - 3d ojbect detection
KW  - Accuracy
KW  - BEV
KW  - Intelligent transport
KW  - Intelligent transport system
KW  - Multi-modality fusion
KW  - Objects detection
KW  - Three-dimensional display
KW  - Transport systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ardic, O.
AU  - Cetinel, G.
TI  - Deep Learning-Based Real-Time Engine Part Inspection With Collaborative Robot Application
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3489714
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208389742&doi=10.1109%2fACCESS.2024.3489714&partnerID=40&md5=e55ce0c668feae444927074b4f5986ca
AB  - Vehicle manufacturing requires error-free processes, as modern vehicles are made up of thousands of parts, including around 280 critical components for safe driving. According to the National Highway Traffic Safety Administration (NHTSA), 2% of vehicle accidents will be caused by defective parts. Current inspection systems in manufacturing plants have limitations, with a high risk of defective parts reaching consumers and leading to recalls. This study aims to develop a real-time, deep learning-based engine part inspection system to improve accuracy and efficiency in mass production. The system, implemented in a large automotive manufacturing plant, uses a Fanuc CR-15ia collaborative robot to inspect engine parts. Combining the single-shot detector (SSD) and faster region-based convolutional neural network (R-CNN) algorithms, the system achieves 99.9% accuracy measured after four months of use, with an Average Precision (AP) of 0.994 for Faster R-CNN and 0.955 for SSD. The inspection system addresses cycle time concerns and is integrated with factory systems for real-time data sharing. Ongoing enhancements aim to improve system performance further.  © 2013 IEEE.
KW  - Collaborative robot
KW  - deep learning
KW  - part inspection
KW  - real-time applications
KW  - Automobile engine manufacture
KW  - Automobile plants
KW  - Energy security
KW  - Highway accidents
KW  - Industrial robots
KW  - Inspection
KW  - Intelligent robots
KW  - Robot learning
KW  - Collaborative robots
KW  - Deep learning
KW  - Defective parts
KW  - Engine parts
KW  - Inspection system
KW  - Manufacturing plant
KW  - Part inspection
KW  - Real- time
KW  - Real-time application
KW  - Single-shot
KW  - Collaborative robots
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, K.
AU  - Dai, Z.
AU  - Zuo, C.
AU  - Wang, X.
AU  - Cui, H.
AU  - Song, H.
AU  - Cui, M.
TI  - Scene adaptation in adverse conditions: a multi-sensor fusion framework for roadside traffic perception
PY  - 2024
T2  - Journal of Intelligent Transportation Systems: Technology, Planning, and Operations
DO  - 10.1080/15472450.2024.2390844
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201549876&doi=10.1080%2f15472450.2024.2390844&partnerID=40&md5=9f0145f245b6fe0b16a89a66bc341fd1
AB  - Robust roadside traffic perception requires integrating the strengths of multi-source sensors under various adverse conditions, which is challenging but indispensable for formulating effective traffic management strategies. One limitation of existing radar-camera perception systems is that they focus on integrating multi-source information without directly considering scene information, leading to difficulties in achieving scene adaptive fusion. How to establish the connection between scene information and multi-source information is the key challenge to solving this problem. In this article, we propose a Scene adaptive Sensor Fusion (SSF) framework that characterizes scene information and integrates it into radar-camera fusion schemes, aiming to achieve high-quality roadside traffic perception. Specifically, we introduce a multi-source object association method that accurately associates multi-source sensor information on the roadside. We then utilize coding techniques to characterize the scene information, including visibility characterization regarding lighting and weather conditions, and road characterization regarding sensor viewpoint. By incorporating sensor and scene information into the fusion model, the SSF framework effectively establishes the connection between them. We evaluate the SSF framework on the Roadside Radar and Video Dataset (RRVD) and the Traffic flow Parameter Estimation Dataset (TPED), both collected from real-world traffic scenarios. Experiments demonstrate that SSF significantly improves vehicle detection accuracy under various adverse conditions compared to traditional single-source sensing methods and other state-of-the-art fusion techniques. Furthermore, vehicle trajectories based on SSF detection results enable accurate traffic parameter estimation, such as volume, speed, and density, in complex and dynamic environments. © 2024 Taylor & Francis Group, LLC.
KW  - multi-sensor fusion
KW  - scene adaptation
KW  - traffic parameter estimation
KW  - traffic perception
KW  - Air traffic control
KW  - Highway administration
KW  - Highway traffic control
KW  - Motor transportation
KW  - Street traffic control
KW  - Time difference of arrival
KW  - Vehicle detection
KW  - Condition
KW  - Multi-sensor fusion
KW  - Multi-source informations
KW  - Multi-Sources
KW  - Parameters estimation
KW  - Scene adaptation
KW  - Sensor fusion
KW  - Traffic parameter estimation
KW  - Traffic parameters
KW  - Traffic perception
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Devi, S.
AU  - Dayana, R.
AU  - Vadivukkarasi, K.
AU  - Malarvezhi, P.
TI  - Navigating the Domain Shift: Object Detection in Indian Road Datasets
PY  - 2024
T2  - Communications in Computer and Information Science
DO  - 10.1007/978-3-031-64067-4_21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202190456&doi=10.1007%2f978-3-031-64067-4_21&partnerID=40&md5=0b8c2e06c6c53bfc2fcb479f3e3d9bd8
AB  - Object detection plays a vital role in several applications, including autonomous driving systems and intelligent transportation systems. However, accurate Object detection in unstructured road environments, particularly on Indian roads, poses a significant challenge due to the diverse and complex nature of the surroundings and perpetually changing environment. Moreover, the limited availability of annotated data exacerbates the problem. Real-world deployment of object detection models often introduces a significant challenge: domain shift. To address this issue, we introduced a new augmentation technique called InterAug. The suggested method may use region of an object using a bounding box to define a “semantic context” for each object in contrast to existing policies. InterAug approach illustrates the feasibility of choosing the most suitable context for each object within a specific scene through annotation, as opposed to altering the entire scene or solely specifying bounding boxes. To prove effectiveness of proposed work makes use of an extensive Indian road dataset JUVDsi V1, that includes a variety of terrains, weather patterns, and road kinds. Through extensive experiments and evaluations using benchmark models Faster R-CNN and YOLOV5, we’ve demonstrated our approach’s remarkable effectiveness, achieving a substantial 4.3% accuracy boost compared to the baseline. We address the critical issue of domain shift, emphasizing the importance of day-to-night adaptation in challenging Indian road scenarios. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - data augmentation
KW  - Data scarcity
KW  - Day to Night adaptation
KW  - Domain Shift
KW  - Indian Road conditions
KW  - Object Detection
KW  - Motor transportation
KW  - Autonomous driving
KW  - Bounding-box
KW  - Data augmentation
KW  - Data scarcity
KW  - Day to night adaptation
KW  - Domain shift
KW  - Driving systems
KW  - Indian road condition
KW  - Objects detection
KW  - Road condition
KW  - Intelligent systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, F.
AU  - Zhao, Y.
AU  - Wei, J.
AU  - Li, S.
AU  - Shan, Y.
TI  - SNCE-YOLO: An Improved Target Detection Algorithm in Complex Road Scenes
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3481642
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207423589&doi=10.1109%2fACCESS.2024.3481642&partnerID=40&md5=fc4b6362e0585c523ec50eac49c165c3
AB  - Autonomous driving is a prominent research area, with the primary challenge being the precise perception and interpretation of the surrounding environment. To address issues such as low resolution, dense occlusion, and small targets in complex road scenes, we propose an improved road target detection algorithm, SNCE-YOLO, based on YOLOv5. Firstly, to avoid the loss of feature information during training for low-resolution and small targets, the Space-to-Depth Convolution(SPD-Conv) is introduced to replace the original convolutional layer for retaining global feature information. Secondly, to address the issue of confusing complex backgrounds with detection targets when features are extracted too early and deeply, the Normalization-based Attention Module (NAM) is incorporated into the backbone network without increasing the computational effort, focusing on the channel and spatial information of interest. Thirdly, to enhance the capture and fusion of multi-level feature information, we optimized the neck network's feature fusion module by implementing the C2f structure. Finally, to reduce the bias in the regression of the occluded target positions, we introduced the EIOU loss function to better define the relationship between the predicted and real frames. The SNCE-YOLO algorithm achieves a mean Average Precision (mAP@0.5) of 91.9% on the KITTI dataset, which represents a 2.6% improvement over the baseline model. Additionally, with a detection speed of 95.24 frames per second (FPS), it meets the requirements for real-time target detection in complex road scenes. Comparisons are also made with other mainstream target detection algorithms to verify the effectiveness of SNCE-YOLO. © 2013 IEEE.
KW  - Autonomous driving
KW  - C2f
KW  - EIOU
KW  - NAM
KW  - SPD-Conv
KW  - YOLOv5
KW  - Autonomous driving
KW  - C2f
KW  - EIOU
KW  - Feature information
KW  - Lower resolution
KW  - Normalisation
KW  - Normalization-based attention module
KW  - Space-to-depth convolution
KW  - Target detection algorithm
KW  - YOLOv5
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, K.
AU  - Cui, H.
AU  - Dai, Z.
AU  - Song, H.
TI  - RVIFNet: Radar-Visual Information Fusion Network for All-Weather Vehicle Perception in Roadside Monitoring Scenarios
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3481492
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207376332&doi=10.1109%2fJSEN.2024.3481492&partnerID=40&md5=98e90ee663f199d89becf2ff75654343
AB  - Achieving all-weather vehicle perception in roadside monitoring systems composed of cameras and millimeter-wave radars is challenging, primarily due to the ineffective integration of information from these two sources. Particularly in adverse weather conditions, this lack of integration can lead to the monitoring system's inability to promptly identify and address hazardous situations. However, current fusion methods often have the problem of being dominated by visual information, and they do not sufficiently utilize the complementary aspects of the two-source information. In this article, we present the radar and visual information fusion network (RVIFNet), a novel method that tackles these challenges through enhanced radar data representation and multilevel fusion strategies. First, we develop a pseudoimage representation method for sparse radar data and its feature extraction technique, which enhances its expressiveness and lays the groundwork for feature-level fusion with visual features. Second, we propose a multilevel fusion approach that leverages the complementary attributes of the dual-modal data in terms of spatial localization, resolution, and semantic understanding to achieve fusion at levels of low-level semantics, high-level semantics, and anchor box level. In additiona, we introduce the monitoring perspective for radar and camera (MPRC) dataset, collected and annotated specifically for roadside monitoring scenarios, and elaborate on the spatial-temporal synchronization method for the dual-source data. We evaluate RVIFNet on MPRC and the widely used in-vehicle dataset NuScenes, confirming its effectiveness for all-weather vehicle detection. To best of the authors' knowledge, this work is among the early attempts to fuse radar and camera data for all-weather vehicle perception in the roadside monitoring scenarios. © 2024 IEEE.
KW  - All-weather vehicle perception
KW  - deep learning
KW  - radar-visual fusion
KW  - roadside monitoring
KW  - spatial-temporal synchronization
KW  - Network security
KW  - All-weather vehicle perception
KW  - Deep learning
KW  - Monitoring system
KW  - Radar information
KW  - Radar-visual fusion
KW  - Roadside monitoring
KW  - Spatial temporals
KW  - Spatial-temporal synchronization
KW  - Temporal synchronization
KW  - Visual information fusion
KW  - Data fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pu, X.
AU  - Xu, X.
AU  - Yu, Y.
TI  - LinkedFormer: Radar Communication and Multiscale Imaging for Object Detection under Complex Sea Background
PY  - 2024
T2  - Sensors and Materials
DO  - 10.18494/SAM5062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201163478&doi=10.18494%2fSAM5062&partnerID=40&md5=a2bebf4c01fbcb577fbed25ebf01b653
AB  - The advent of deep learning has propelled significant advancements in object detection, thereby enhancing the intelligence of underwater autonomous driving systems. In this paper, we explore the cutting-edge applications of autonomous driving technology in the field of underwater exploration, addressing the pivotal role of target detection in navigating and executing tasks within challenging marine environments. In this study, the object detection capability of such systems is enhanced by integrating deep learning and multisensor fusion technology, especially by combining high-precision sensor data with multitask learning models to achieve efficient and robust detection. Our study has three principal contributions. First, we introduce a novel light perception detection system that combines monocular camera technology with 4D radar. It enriches environmental perception by weaving in radar signals and significantly enhances the accuracy and stability of target detection. Second, we have developed a dual-modal detection framework, named Radar-Picture Detection, which utilizes a parallel sequence prediction method. This approach prioritizes radar signal processing, aiding in the improvement of target detection accuracy in intricate underwater environments. Third, we conducted a comprehensive evaluation of our model’s performance using the FloW Dataset, which is specifically curated for identifying floating waste in inland waters through unmanned vessel footage. We not only propel forward the field of target detection for underwater autonomous systems but also establish new avenues and a solid foundation for deploying deep learning and multisensor fusion technology in marine environmental perception. Insights and methodologies from this study are poised to spearhead further developments in autonomous marine exploration, enhancing safety, efficiency, and our understanding of underwater environments. © MYU K.K.
KW  - 4D radar
KW  - multisensor fusion techniques
KW  - object detection
KW  - underwater autonomous communication
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Marine applications
KW  - Marine radar
KW  - Military applications
KW  - Object recognition
KW  - Radar imaging
KW  - Tracking radar
KW  - Underwater acoustics
KW  - Underwater imaging
KW  - 4d radar
KW  - Autonomous communications
KW  - Autonomous driving
KW  - Environmental perceptions
KW  - Fusion technology
KW  - Multi-sensor fusion techniques
KW  - Objects detection
KW  - Targets detection
KW  - Underwater autonomous communication
KW  - Underwater environments
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Widodo, H.
AU  - Taufiqurrohman, H.
AU  - Muis, A.
AU  - Wijayanto, Y.N.
AU  - Prihantoro, G.
AU  - Dwiyanti, H.
AU  - Cahya, Z.
AU  - Widaryanto, A.
AU  - Nugroho, T.H.
TI  - Experimental Evaluation of Pothole Detection and Its Dimension Estimation Using YOLOv8 and Depth Camera for Road Surface Analysis
PY  - 2024
T2  - Proceeding - 2024 International Conference on Radar, Antenna, Microwave, Electronics, and Telecommunications, ICRAMET 2024
DO  - 10.1109/ICRAMET62801.2024.10809331
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215943526&doi=10.1109%2fICRAMET62801.2024.10809331&partnerID=40&md5=89fe54267b767c91e80d8a360a0b9793
AB  - Pothole detection and dimension estimation are essential to improve the safety and comfort of autonomous vehicles. This paper uses a depth camera to present a new approach for position estimation and pothole detection using the You Only Look Once (YOLO) method. Experimental results on detection applications using real cars show good pothole detection accuracy in both bright and dimly lit road conditions covered by trees with precision. This object detection uses the YOLO version 8 nano model accompanied by coco as pre-trained training. In the training dataset, only one class, namely the pothole dataset, is used. A depth camera from Intel Realsense type D455 will be employed, and the Jetson Orin Nano will subsequently apply the training results. During the field test, data about the position of the pothole coordinate inside the pixel frame, the pothole's width, and its distance from the camera will be displayed, in addition to pothole-detecting objects. Every pothole object found will have its data shown in real time. Validating the width and length of the pothole involves taking actual measurements with a meter. The estimated distance and width of potholes showed good agreement with direct manual measurements with R -squared values above 0.97 and gradients approaching unity. © 2024 IEEE.
KW  - autonomous vehicle
KW  - pothole detection
KW  - pothole distance
KW  - pothole width
KW  - road surface
KW  - unstructured path
KW  - Autonomous Vehicles
KW  - Depth camera
KW  - Detection estimation
KW  - Dimension estimation
KW  - Experimental evaluation
KW  - Pothole detection
KW  - Pothole distance
KW  - Pothole width
KW  - Road surfaces
KW  - Unstructured path
KW  - Magnetic levitation vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wen, L.
AU  - Peng, Y.
AU  - Lin, M.
AU  - Gan, N.
AU  - Tan, R.
TI  - Multi-Modal Contrastive Learning for LiDAR Point Cloud Rail-Obstacle Detection in Complex Weather
PY  - 2024
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics13010220
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181920159&doi=10.3390%2felectronics13010220&partnerID=40&md5=4549e001b848e517543856cd2d27da05
AB  - Obstacle intrusion is a serious threat to the safety of railway traffic. LiDAR point cloud 3D semantic segmentation (3DSS) provides a new method for unmanned rail-obstacle detection. However, the inevitable degradation of model performance occurs in complex weather and hinders its practical application. In this paper, a multi-modal contrastive learning (CL) strategy, named DHT-CL, is proposed to improve point cloud 3DSS in complex weather for rail-obstacle detection. DHT-CL is a camera and LiDAR sensor fusion strategy specifically designed for complex weather and obstacle detection tasks, without the need for image input during the inference stage. We first demonstrate how the sensor fusion method is more robust under rainy and snowy conditions, and then we design a Dual-Helix Transformer (DHT) to extract deeper cross-modal information through a neighborhood attention mechanism. Then, an obstacle anomaly-aware cross-modal discrimination loss is constructed for collaborative optimization that adapts to the anomaly identification task. Experimental results on a complex weather railway dataset show that with an mIoU of 87.38%, the proposed DHT-CL strategy achieves better performance compared to other high-performance models from the autonomous driving dataset, SemanticKITTI. The qualitative results show that DHT-CL achieves higher accuracy in clear weather and reduces false alarms in rainy and snowy weather. © 2024 by the authors.
KW  - complex weather
KW  - contrastive learning
KW  - multi-modal
KW  - point clouds
KW  - rail-obstacle detection
KW  - semantic segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, H.
AU  - Zhang, X.
AU  - He, J.
AU  - Geng, Z.
AU  - Pang, C.
AU  - Yu, Y.
TI  - Surround-View Water Surface BEV Segmentation for Autonomous Surface Vehicles: Dataset, Baseline and Hybrid-BEV Network
PY  - 2024
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2024.3395653
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192186582&doi=10.1109%2fTIV.2024.3395653&partnerID=40&md5=892f88dd7e772ada9e33ec3c9609e373
AB  - Autonomous surface vessels (ASVs) are growing rapidly due to their ability to execute hazardous and time-consuming missions over water surfaces. Recently, camera-based surround-view bird's eye view (BEV) segmentation has attracted increasing attention because of its popular application in autonomous driving. However, most existing surround-view BEV segmentation studies have focused on road scenes for autonomous cars, and the development of water surface surround-view BEV segmentation has been relatively slow due to the absence of water surface BEV datasets. Different from road scenes with complex dynamic scenes, water surface BEV segmentation task also face new interference like vessel swaying and reflection interference. To address these problems and stimulate relevant research, we first introduce the WSBEV dataset, a surround-view visual segmentation dataset on water surface scenes. Notably, our compiled WSBEV utilizes a hybrid camera configuration and contains pinhole cameras and fisheye cameras with different focal lengths, which also brings new challenges for the surround-view BEV segmentation method. To address the issues above, we propose a novel water surface BEV segmentation network named Hybrid-BEV that supports hybrid camera inputs. The geometry-based view transformer module is designed to directly transfer the extracted feature from the perspective view to BEV space. The pose query adjustment module introduces vessel pose data to suppress vessel swaying interference. Compared to other baseline methods, our Hybrid-BEV achieves excellent accuracy in our WSBEV dataset. Extensive experiments validate the effectiveness of our approach and the exceptional generalizability of the WSBEV dataset. © 2016 IEEE.
KW  - Autonomous surface vessels
KW  - deep learning
KW  - surround-view BEV segmentation
KW  - visual perception dataset
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Pinhole cameras
KW  - Roads and streets
KW  - Surface waters
KW  - Unmanned surface vehicles
KW  - Vision
KW  - Autonomous surface vessels
KW  - Deep learning
KW  - Images segmentations
KW  - Interference
KW  - Road
KW  - Sea surfaces
KW  - Surround-view BEV segmentation
KW  - Visual perception
KW  - Visual perception dataset
KW  - Image segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Gheorghe, C.
AU  - Duguleana, M.
AU  - Boboc, R.G.
AU  - Postelnicu, C.C.
TI  - Analyzing Real-Time Object Detection with YOLO Algorithm in Automotive Applications: A Review
PY  - 2024
T2  - CMES - Computer Modeling in Engineering and Sciences
DO  - 10.32604/cmes.2024.054735
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208374812&doi=10.32604%2fcmes.2024.054735&partnerID=40&md5=1dbb6200abcce710e766a7958f9bc358
AB  - Identifying objects in real-time is a technology that is developing rapidly and has a huge potential for expansion in many technical fields. Currently, systems that use image processing to detect objects are based on the information from a single frame. A video camera positioned in the analyzed area captures the image, monitoring in detail the changes that occur between frames. The You Only Look Once (YOLO) algorithm is a model for detecting objects in images, that is currently known for the accuracy of the data obtained and the fast-working speed. This study proposes a comprehensive literature review of YOLO research, as well as a bibliometric analysis to map the trends in the automotive field from 2020 to 2024. Object detection applications using YOLO were categorized into three primary domains: road traffic, autonomous vehicle development, and industrial settings. A detailed analysis was conducted for each domain, providing quantitative insights into existing implementations. Among the various YOLO architectures evaluated (v2-v8, H, X, R, C), YOLO v8 demonstrated superior performance with a mean Average Precision (mAP) of 0.99.  © 2024 The Authors.
KW  - automotive
KW  - autonomous vehicles
KW  - industry
KW  - traffic
KW  - YOLO
KW  - Automotive industry
KW  - Object detection
KW  - Automotive applications
KW  - Automotives
KW  - Autonomous Vehicles
KW  - Images processing
KW  - Objects detection
KW  - Real- time
KW  - Single frames
KW  - Technical fields
KW  - Traffic
KW  - You only look once
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Gong, D.
AU  - Li, J.
AU  - Wang, C.
AU  - Wang, Z.
TI  - PVSA : A General and Elegant Sampling Algorithm for Voxel-Based 3D Object Detection
PY  - 2024
T2  - 2024 10th International Conference on Control, Automation and Robotics, ICCAR 2024
DO  - 10.1109/ICCAR61844.2024.10569566
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198229865&doi=10.1109%2fICCAR61844.2024.10569566&partnerID=40&md5=73c389ea7cc7bedae1c34f3c0941ce10
AB  - Perceiving the environment is vital for autonomous vehicles as it serves as the foundation for decision making and path planning. LiDAR is a widely employed sensor, which produces a voluminous and sparsely populated point cloud. For voxel-based 3D object detection methods, the initial step involves the division of the raw point cloud into voxels, the process known as voxelization. Nevertheless, once the number of point clouds contained within a voxel reaches the certain threshold, the allocation of additional point clouds to that voxel ceases. This leads to a greater degree of information loss. Scholars primarily focus on the subsequent stages following voxelization, such as feature extraction and utilization. We first focus on the sampling issue during the voxelization. In the paper, we propose a general and elegant Points in Voxel Sampling Algorithm module named PVSA. During the voxelization, the assignment of all points into their respective voxels continues even after the maximum number of points in a voxel has been reached. For voxels in which the number of internal point clouds exceeds the certain threshold, the farthest distance sampling method is utilized as it ensures a genuine and uniform distribution of the point cloud within the voxel. We conducted an evaluation of the proposed module using the Kitti dataset. Experimental findings suggest that the incorporation of the PVSA module enhances the object detection capabilities of the voxel-based model, particularly in the identification of samll targets like pedestrians. The incorporation of PVSA modules significantly enhances Pillarnet's capacity to recognize pedestrians, resulting in a 46.2% pt improvement in performance at a distance of 20 meters. On average, there is an enhancement of 1.43% pt.  © 2024 IEEE.
KW  - Autonomous Vehicle
KW  - Sampling Algorithm
KW  - Small Object Detection
KW  - Voxelization
KW  - Autonomous vehicles
KW  - Decision making
KW  - Learning algorithms
KW  - Motion planning
KW  - Object recognition
KW  - 3D object
KW  - Autonomous Vehicles
KW  - Decision paths
KW  - Decisions makings
KW  - Object detection method
KW  - Objects detection
KW  - Point-clouds
KW  - Sampling algorithm
KW  - Small object detection
KW  - Voxelization
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kim, J.
AU  - Kim, J.
TI  - Perception and Sensing Technologies for Maritime Autonomous Ships
PY  - 2024
T2  - Journal of Institute of Control, Robotics and Systems
DO  - 10.5302/J.ICROS.2024.24.0024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191201499&doi=10.5302%2fJ.ICROS.2024.24.0024&partnerID=40&md5=1b00abdabfc4ec465c424116924e0d7e
AB  - Recent advancements in artificial intelligence and sensor technology have promoted increasing interest in maritime autonomous ships. In this paper, we present an overview of the perception and sensing technologies applied in autonomous ship research. We introduce various datasets collected using perception sensors, such as a camera, LiDAR, and marine radar. Object detection and semantic segmentation methods for detecting floating objects and navigable regions using individual sensors are presented. Additionally, sensor fusion methods that integrate data from multiple sensors to enhance detection and navigation capabilities are addressed. © ICROS 2024.
KW  - autonomous ship
KW  - perception
KW  - sensing
KW  - sensor fusion
KW  - Autonomous vehicles
KW  - Object detection
KW  - Object recognition
KW  - Optical radar
KW  - Semantics
KW  - Artificial intelligence technologies
KW  - Autonomous ship
KW  - Object semantic
KW  - Objects detection
KW  - Segmentation methods
KW  - Semantic segmentation
KW  - Sensing
KW  - Sensing technology
KW  - Sensor fusion
KW  - Sensor technologies
KW  - Ships
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, W.
AU  - Zhou, J.
AU  - Li, X.
AU  - Cao, Y.
AU  - Jin, G.
AU  - Zhang, X.
TI  - InfRS: Incremental Few-Shot Object Detection in Remote Sensing Images
PY  - 2024
T2  - IEEE Transactions on Geoscience and Remote Sensing
DO  - 10.1109/TGRS.2024.3475482
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207150382&doi=10.1109%2fTGRS.2024.3475482&partnerID=40&md5=d20eb379c0b95a3d799cade1271673b4
AB  - Few-shot detection in remote sensing images has witnessed significant advancements recently. Despite these progresses, the capacity for continuous conceptual learning still poses a significant challenge to existing methodologies. In this article, we explore the intricate task of incremental few-shot object detection (iFSOD) in remote sensing images. We present a pioneering transfer-learning-based technique, termed InfRS, designed to enable the incremental learning of novel classes using a restricted set of examples, while simultaneously preserving the knowledge learned from previously seen classes without the need to revisit old data. Specifically, we pretrain the detector using sufficient data from base datasets and then generate a set of classwise prototypes that represent the intrinsic characteristics of the data. In the incremental learning session, we design a hybrid prototypical contrastive (HPC) encoding module for learning discriminative representations. Furthermore, we develop a prototypical calibration strategy based on the Wasserstein distance to overcome the catastrophic forgetting problem. Comprehensive evaluations conducted with two aerial imagery datasets show that our InfRS effectively addresses the iFSOD issue in remote sensing imagery. Code is available at https://github.com/lyanna4869/InfRS.git.  © 1980-2012 IEEE.
KW  - Incremental few-shot object detection (iFSOD)
KW  - prototypical contrastive learning
KW  - remote sensing images
KW  - Object detection
KW  - Object recognition
KW  - Proximity sensors
KW  - Conceptual learning
KW  - Incremental few-shot object detection
KW  - Incremental learning
KW  - Intrinsic characteristics
KW  - Learning sessions
KW  - Objects detection
KW  - Prototypical contrastive learning
KW  - Remote sensing images
KW  - Shot detection
KW  - Transfer learning
KW  - calibration
KW  - catastrophic event
KW  - detection method
KW  - image analysis
KW  - remote sensing
KW  - satellite imagery
KW  - Aerial photography
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Meng, Z.
AU  - Xia, X.
AU  - Ma, J.
TI  - Toward Foundation Models for Inclusive Object Detection: Geometry- and Category-Aware Feature Extraction Across Road User Categories
PY  - 2024
T2  - IEEE Transactions on Systems, Man, and Cybernetics: Systems
DO  - 10.1109/TSMC.2024.3385711
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191298049&doi=10.1109%2fTSMC.2024.3385711&partnerID=40&md5=240ed02a4d75904b2fbc484f830d1ae3
AB  - The safety of different categories of road users comprising motorized vehicles and vulnerable road users (VRUs) such as pedestrians and cyclists is one of the priorities of automated driving and smart infrastructure services. Three-dimensional (3-D) LiDAR-based object detection has been a promising approach to perceiving road users. Despite accurate 3-D geometry information, the point cloud from LiDAR is usually nonuniform, and learning the effective point cloud abstract representations for diverse road users remains challenging for 3-D object detection, particularly for small objects such as VRUs. For inclusive object detection (IDetect), we propose a general foundation convolution component, called geometry-aware convolution (GA Conv) toward a foundation feature extraction model, to serve as basic convolution operations of the neutral network for inclusive 3-D object detection. Further, the GA Conv operations are then utilized as the elementary feature extraction layers to build a novel elegant and pyramid network for IDetect. It learns the effective geometric-related features from the unstructured point cloud data by implicitly learning the distribution property and geometry-related features from different categories of road users in particular for VRUs. The proposed IDetect is comprehensively evaluated on the large-scale benchmark Waymo open datasets with all categories of road users. The qualitative and quantitative experiment results demonstrate that IDetect can effectively consider the nonuniform distributed point clouds and learn the geometric features to assist the different categories of road user detection. In addition, the GA Conv has been integrated with other state-of-the-art neural networks and a performance boost for VRU detection has been demonstrated, showing the foundation functionality of the GA Conv and making it a general component in the future inclusive 3-D object detection foundation model.  © 2013 IEEE.
KW  - 3-dimensional (3-D) object detection
KW  - automated driving
KW  - geometry-aware convolution (GA Conv)
KW  - vulnerable road user (VRU) perception
KW  - Convolution
KW  - Extraction
KW  - Feature extraction
KW  - Foundations
KW  - Geometry
KW  - Large datasets
KW  - Object recognition
KW  - Optical radar
KW  - Pedestrian safety
KW  - Roads and streets
KW  - 3-dimensional
KW  - 3-dimensional (3-D) object detection
KW  - Automated driving
KW  - Features extraction
KW  - Geometry-aware convolution
KW  - Kernel
KW  - Objects detection
KW  - Pedestrian
KW  - Point cloud compression
KW  - Point-clouds
KW  - Road
KW  - Road users
KW  - User perceptions
KW  - Vulnerable road user  perception
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Gautam, V.
AU  - Prasad, S.
AU  - Sinha, S.
TI  - Joint-YODNet: A Light-Weight Object Detector for UAVs to Achieve Above 100fps
PY  - 2024
T2  - Communications in Computer and Information Science
DO  - 10.1007/978-3-031-58174-8_47
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200667210&doi=10.1007%2f978-3-031-58174-8_47&partnerID=40&md5=0866f26eefb89dc4b6c5ee14c94eb4d1
AB  - Small object detection via UAV (Unmanned Aerial Vehicle) images captured from drones and radar is a complex task. This domain encompasses numerous complexities, including size and scale variations, image resolution constraints, and occlusion issues, all of which impede the accurate detection and localization of small objects. To address these challenges, we propose a novel method called Joint-YODNet for UAVs to detect small objects, leveraging a joint loss function specifically designed for this task. Our method revolves around the development of a joint loss function tailored to enhance the detection performance of small objects. Through extensive experimentation on a diverse dataset of UAV images captured under varying environmental conditions, we evaluated different variations of the loss function and determined the most effective formulation. The results demonstrate that our proposed joint loss function outperforms existing methods in accurately localizing small objects. Specifically, Joint-YODNet achieves a recall of 0.971 and a F1Score of 0.975, surpassing state-of-the-art (SOTA) techniques. Additionally, our method achieves a mAP@.5(%) of 98.6, indicating it’s robustness in detecting small objects across varying scales. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Deep Neural Networks
KW  - Object Detection
KW  - SAR
KW  - SAR Ship Detection
KW  - Small Object Detection
KW  - Unmanned Aerial Vehicle
KW  - Aircraft detection
KW  - Antennas
KW  - Drones
KW  - Image resolution
KW  - Object detection
KW  - Object recognition
KW  - Radar imaging
KW  - Synthetic aperture radar
KW  - Tracking radar
KW  - Aerial vehicle
KW  - Light weight
KW  - Loss functions
KW  - Objects detection
KW  - SAR ship detection
KW  - Ship detection
KW  - Small object detection
KW  - Small objects
KW  - Unmanned aerial vehicle
KW  - Vehicle images
KW  - Deep neural networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Bakirci, M.
AU  - Bayraktar, I.
TI  - Transforming Aircraft Detection Through LEO Satellite Imagery and YOLOv9 for Improved Aviation Safety
PY  - 2024
T2  - 2024 26th International Conference on Digital Signal Processing and its Applications, DSPA 2024
DO  - 10.1109/DSPA60853.2024.10510106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193021274&doi=10.1109%2fDSPA60853.2024.10510106&partnerID=40&md5=733e5414ebd97e79072abe601b5176c9
AB  - The utilization of object detection algorithms in conjunction with satellite imagery to detect aircraft has become a crucial focus of investigation, bearing significant implications for the management of airports, as well as for the enhancement of aviation safety and security. Traditional monitoring methods, reliant on manual observation or radar systems, are laborintensive, error-prone, and may struggle to provide comprehensive coverage, particularly in vast or remote areas. In contrast, satellite imagery offers wide-area coverage and high-resolution imagery, ideal for capturing detailed views of airport facilities and surrounding areas. By coupling satellite imagery with advanced object detection algorithms like YOLO, significant improvements in aircraft detection capabilities are achievable. These algorithms streamline monitoring processes, enhance situational awareness, and enable prompt responses to potential safety or security threats, marking a paradigm shift in aircraft monitoring practices. Object detection algorithms, especially those based on deep learning techniques like convolutional neural networks (CNNs), have revolutionized object identification in images or video streams. Their adaptability to diverse environmental conditions and high accuracy make them invaluable across numerous applications, from surveillance to autonomous vehicles. Satellite imagery, particularly from Low Earth Orbit (LEO) satellites, offers exceptional detail and clarity, with shorter revisit times enabling near-real-time monitoring of dynamic events on the ground. Leveraging these advantages, this study focuses on enhancing airport security and aircraft safety through the detection of aircraft on the ground utilizing YOLOv9, chosen for its promising capabilities. While our findings demonstrate notable advancements, it is crucial to address certain limitations in the algorithm's detection capabilities to ensure its effectiveness in real-world applications. © 2024 IEEE.
KW  - aviation security
KW  - convolutional neural network
KW  - deep learning
KW  - satellite imagery
KW  - YOLOv9
KW  - Aircraft
KW  - Aircraft detection
KW  - Airport security
KW  - Convolution
KW  - Deep neural networks
KW  - Image enhancement
KW  - Learning systems
KW  - Object detection
KW  - Object recognition
KW  - Orbits
KW  - Security systems
KW  - Aviation safety
KW  - Aviation Security
KW  - Convolutional neural network
KW  - Deep learning
KW  - Detection capability
KW  - Low earth orbit satellites
KW  - Monitoring methods
KW  - Object detection algorithms
KW  - Safety and securities
KW  - YOLOv9
KW  - Convolutional neural networks
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Becker, S.
AU  - Bayer, J.
AU  - Hug, R.
AU  - Huebner, W.
AU  - Arens, M.
TI  - Utilizing Dataset Affinity Prediction in Object Detection to Assess Training Data
PY  - 2024
T2  - Communications in Computer and Information Science
DO  - 10.1007/978-3-031-59057-3_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194075509&doi=10.1007%2f978-3-031-59057-3_17&partnerID=40&md5=b9d33878e3c013cac8f412d58b90fc16
AB  - Data pooling offers various advantages, such as increasing the sample size, improving generalization, reducing sampling bias, and addressing data sparsity and quality, but it is not straightforward and may even be counterproductive. Assessing the effectiveness of pooling datasets in a principled manner is challenging due to the difficulty in estimating the overall information content of individual datasets. Towards this end, we propose incorporating a data source prediction module into standard object detection pipelines. The module runs with minimal overhead during inference time, providing additional information about the data source assigned to individual detections. We show the benefits of the so-called dataset affinity score by automatically selecting samples from a heterogeneous pool of vehicle datasets. The results show that object detectors can be trained on a significantly sparser set of training samples without losing detection accuracy. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.
KW  - Ante-hoc explanation
KW  - Dataset label prediction
KW  - Dataset origin prediction
KW  - Object detection
KW  - Sample selection
KW  - Selection bias
KW  - Training data analysis
KW  - Object detection
KW  - Object recognition
KW  - Ante-hoc explanation
KW  - Data-source
KW  - Dataset label prediction
KW  - Dataset origin prediction
KW  - Label predictions
KW  - Objects detection
KW  - Samples selection
KW  - Selection bias
KW  - Training data
KW  - Training data analyse
KW  - Forecasting
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hou, X.
AU  - Guan, Y.
AU  - Choi, N.
AU  - Han, T.
TI  - BPS: Batching, Pipelining, Surgeon of Continuous Deep Inference on Collaborative Edge Intelligence
PY  - 2024
T2  - IEEE Transactions on Cloud Computing
DO  - 10.1109/TCC.2024.3399616
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192765162&doi=10.1109%2fTCC.2024.3399616&partnerID=40&md5=dfb7624592fd7e117974a0b91404ec3e
AB  - Users on edge generate deep inference requests continuously over time. Mobile/edge devices located near users can undertake the computation of inference locally for users, e.g., the embedded edge device on an autonomous vehicle. Due to limited computing resources on one mobile/edge device, it may be challenging to process the inference requests from users with high throughput. An attractive solution is to (partially) offload the computation to a remote device in the network. In this paper, we examine the existing inference execution solutions across local and remote devices and propose an adaptive scheduler, a BPS scheduler, for continuous deep inference on collaborative edge intelligence. By leveraging data parallel, neurosurgeon, reinforcement learning techniques, BPS can boost the overall inference performance by up to 8.2× over the baseline schedulers. A lightweight compressor, FF, specialized in compressing intermediate output data for neurosurgeon, is proposed and integrated into the BPS scheduler. FF exploits the operating character of convolutional layers and utilizes efficient approximation algorithms. Compared to existing compression methods, FF achieves up to 86.9% lower accuracy loss and up to 83.6% lower latency overhead.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
KW  - convolutional neural networks
KW  - Edge computing
KW  - efficient AI
KW  - reinforcement learning
KW  - Approximation algorithms
KW  - Convolution
KW  - Deep learning
KW  - Edge computing
KW  - Neural networks
KW  - Neurosurgery
KW  - Collaboration
KW  - Computational modelling
KW  - Convolutional neural network
KW  - Deep inference
KW  - Edge computing
KW  - Edge intelligence
KW  - Efficient AI
KW  - Performances evaluation
KW  - Reinforcement learnings
KW  - Reinforcement learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Falaschetti, L.
AU  - Manoni, L.
AU  - Palma, L.
AU  - Pierleoni, P.
AU  - Turchetti, C.
TI  - Embedded Real-Time Vehicle and Pedestrian Detection Using a Compressed Tiny YOLO v3 Architecture
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2024.3447453
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204704255&doi=10.1109%2fTITS.2024.3447453&partnerID=40&md5=a9f1703bb3be9c96e4a4e19600235900
AB  - Vehicle and pedestrian detection (VaPD) is one of the most critical tasks in an advanced driver assistance system which help the driver to drive safely and save the pedestrian life. VaPD is a typical object detection problem that requires a trade-off among accuracy, speed, and memory consumption. Most existing methods focus on improving detection accuracy, while ignoring VaPD requires real-time detection speed with limited computational resources. Thus, it is of primary importance to study light-weight and real-time VaPD methods for embedded devices, that is hardware platforms with limited computation and memory resources. To deal with these issues, this paper proposes a low-rank (LR) Tiny YOLO v3 architecture that meets the requirements of real-time VaPD on embedded systems. The architecture has been developed starting from Tiny YOLO v3 adopting a convolutional neural network compression technique based on Tucker tensor decomposition, able to reduce the computational complexity of the network. A wide experimentation has been carried out on two embedded platforms, Raspberry Pi 4 and NVIDIA Jetson Nano 2 GB, and two datasets commonly used for VaPD, PASCAL VOC and KITTI dataset, showing the superiority of the LR Tiny YOLO v3 with respect to the state-of-the-art networks in obtaining the best compromise between inference time, accuracy and memory occupancy. Moreover, the proposed architecture meets the requirements of VaPD on embedded systems using only 22% of the memory required by the baseline Tiny YOLO v3 Darknet, and always providing better inference time (36.46 FPS) with only a marginal decrease in accuracy (~2%). © 2000-2011 IEEE.
KW  - Autonomous driving
KW  - CNN compression
KW  - embedded smart sensors
KW  - object detection
KW  - pedestrian detection
KW  - YOLO
KW  - Autonomous vehicles
KW  - Convolutional neural networks
KW  - Image segmentation
KW  - Magnetic levitation vehicles
KW  - Tensors
KW  - Autonomous driving
KW  - CNN compression
KW  - Critical tasks
KW  - Embedded smart sensor
KW  - Embedded-system
KW  - Objects detection
KW  - Pedestrian detection
KW  - Real- time
KW  - Vehicles detection
KW  - YOLO
KW  - Advanced driver assistance systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Wu, Y.
AU  - Gu, Z.
AU  - Wang, Z.
AU  - Watabe, D.
TI  - Enhancing Safe Driving of Autonomous Buses in Obstructed-View Conditions Using Distributed Monocular Cameras
PY  - 2024
T2  - IECON Proceedings (Industrial Electronics Conference)
DO  - 10.1109/IECON55916.2024.10905580
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000977610&doi=10.1109%2fIECON55916.2024.10905580&partnerID=40&md5=85a45ba9e6d29b862efa024a539e7540
AB  - With the rapid advancement of autonomous driving technology, the application of automated urban transit has become a key component of modern transportation systems. As a representative of automated urban transit, the safe operation of autonomous buses is of great significance, especially in complex and variable traffic situations. This study proposes a low-cost method that combines a distributed architecture configuration based on monocular cameras with the latest YOLOv8 model to enhance the perception capabilities of fixed-route autonomous buses under specific conditions. The primary aim is to reduce traffic accidents caused by pedestrians suddenly appearing near the bus, particularly in blind spots. The methodology involves advanced object detection and transmission techniques. Video data captured by the monocular camera is processed in real-time, and data transmission is facilitated via Wi-Fi modules. The system centrally renders the relative positions of pedestrians and the bus on the receiving end, thereby enhancing the safety decision-making capabilities of the bus’s autonomous driving system. This approach not only advances the development of automated urban transit systems but also promotes the practical implementation of low-cost autonomous driving solutions, particularly in scenarios where the view is obstructed for autonomous buses. © 2024 IEEE.
KW  - autonomous driving
KW  - distributed architecture
KW  - monocular vision
KW  - object detection
KW  - real-time systems
KW  - YOLOv8
KW  - Autonomous vehicles
KW  - Bus transportation
KW  - Buses
KW  - Light rail transit
KW  - Pedestrian safety
KW  - Rapid transit
KW  - Autonomous driving
KW  - Condition
KW  - Distributed architecture
KW  - Monocular cameras
KW  - Monocular vision
KW  - Objects detection
KW  - Real - Time system
KW  - Safe driving
KW  - Urban transit
KW  - YOLOv8
KW  - Urban transportation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Narayanan, P.
AU  - Nikhil, V.
AU  - Veluchamy, S.
TI  - Vehicle Object Detection in Traffic Environments Using Low-Light Image Enhancement
PY  - 2024
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies, ICCCNT 2024
DO  - 10.1109/ICCCNT61001.2024.10723924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212817116&doi=10.1109%2fICCCNT61001.2024.10723924&partnerID=40&md5=249c4e458d99e0ca0b123a6e249034a0
AB  - Scene depth and the corresponding ambient lights generally degrade low-light photos taken in a busy setting with uneven lighting.Vehicle object recognition is made more challenging by this degradation, which results in a significant loss of object details in the deteriorated picture format due to the poor contrast and the appearance of artificial light. On the other hand, current methods for identifying prominent objects rely on the unreasonable assumption that the photographs were captured in an adequately lit atmosphere. In this research, we refer to a method for improving images in dim light image detection of vehicle objects. The suggested approach immediately embeds the physical lighting model into the convolution neural network, where the traffic environment light is represented as a point-wise fluctuation that varies with local content, to explain the degradation of low light images. A Sensor Filter is also used to record the difference between an object’s local content and its local neighborhood hotspots. We generate a low light Images dataset with pixel-level human labeled ground-truth annotations for quantitative assessment, in addition to our benchmark dataset. We also show encouraging results on four other publically available datasets. ©2024 IEEE.
KW  - Low light image
KW  - Pixel-level
KW  - Sensor Filter
KW  - Vehicle object
KW  - Convolutional neural networks
KW  - Image annotation
KW  - Image enhancement
KW  - Photography
KW  - Ambient light
KW  - Local contents
KW  - Low light
KW  - Low-light images
KW  - Objects detection
KW  - Pixel level
KW  - Scene depths
KW  - Sensor filter
KW  - Traffic environment
KW  - Vehicle object
KW  - Pixels
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yao, S.
AU  - Guan, R.
AU  - Wu, Z.
AU  - Ni, Y.
AU  - Huang, Z.
AU  - Wen Liu, R.
AU  - Yue, Y.
AU  - Ding, W.
AU  - Gee Lim, E.
AU  - Seo, H.
AU  - Lok Man, K.
AU  - Ma, J.
AU  - Zhu, X.
AU  - Yue, Y.
TI  - WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmarks for Autonomous Driving on Water Surfaces
PY  - 2024
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2024.3415772
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197611307&doi=10.1109%2fTITS.2024.3415772&partnerID=40&md5=8af930fceef24e1e9468f43d2e92a705
AB  - Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivor rescue, environmental monitoring, hydrography mapping and waste cleaning. This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces. Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation. Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively. In addition to basic perception tasks, such as object detection, instance segmentation and semantic segmentation, we also provide annotations for free-space segmentation and waterline segmentation. Leveraging the multi-task and multi-modal data, we conduct benchmark experiments on the uni-modality of radar and camera, as well as the fused modalities. Experimental results demonstrate that 4D radar-camera fusion can considerably improve the accuracy and robustness of perception on water surfaces, especially in adverse lighting and weather conditions. WaterScenes dataset is public on https://waterscenes.github.io.  © 2024 IEEE.
KW  - 4D radar-camera fusion
KW  - Autonomous driving
KW  - multi-task
KW  - unmanned surface vehicle
KW  - Cameras
KW  - Job analysis
KW  - Modal analysis
KW  - Object detection
KW  - Object recognition
KW  - Radar imaging
KW  - Semantics
KW  - Unmanned surface vehicles
KW  - 4d radar-camera fusion
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Environmental Monitoring
KW  - Maritime-surveillance
KW  - Monocular cameras
KW  - Multi tasks
KW  - Task analysis
KW  - Waste cleaning
KW  - Water surface
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Nazeri, A.
AU  - Godwin, D.W.
AU  - Maria Panteleaki, A.
AU  - Anagnostopoulos, I.
AU  - Edidem, M.I.
AU  - Li, R.
AU  - Shu, T.
TI  - Exploration of TPU Architectures for the Optimized Transformer in Drainage Crossing Detection
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024
DO  - 10.1109/BigData62323.2024.10826077
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217988044&doi=10.1109%2fBigData62323.2024.10826077&partnerID=40&md5=0a274d49b6cc46635704256868e1de7e
AB  - Understanding hydrologic connectivity within landscapes is crucial for managing environmental challenges. Despite advancements in high-resolution Digital Elevation Models (DEMs) derived from Light Detection and Ranging (LiDAR) technology, accurately delineating hydrologic connectivity remains challenging due to disruptions caused by virtual flow barriers, such as roads and bridges. This study addresses this issue by enhancing the detection performance and reducing the latency of Transformer models for image detection of drainage crossings. We retrained a Detection Transformer (DETR) with a specialized recipe to improve culvert detection performance. Owing to the high susceptibility of LiDAR-based DEMs to measurement noise and varying data modalities, we conducted extensive data preprocessing to ensure DETR compatibility with the culvert dataset. Ablation studies on input size indicate that the model performs optimally with 800×800 pixel inputs, demonstrating its adaptability to new data modalities. Additionally, we employed Tensor Processing Units (TPUs) to decrease the model's latency. We developed a novel strategy to optimize TPU architecture, utilizing genetic algorithms to expedite the discovery of optimal TPU configurations for detection deployment. Our model surpasses the performance of previous models on the same task. This work not only addresses the computational complexities of deploying advanced object detection in environmental contexts but also significantly contributes to the precise and efficient monitoring of hydrologic connectivity. © 2024 IEEE.
KW  - culvert
KW  - Detection Transformer
KW  - DETR
KW  - Digital Elevation Model
KW  - hydrologic connectivit
KW  - LiDAR
KW  - Object detection
KW  - TPU
KW  - Geological surveys
KW  - Image coding
KW  - Image enhancement
KW  - Levees
KW  - Photomapping
KW  - Steganography
KW  - Virtual addresses
KW  - Detection performance
KW  - Detection transformer
KW  - Digital elevation model
KW  - Hydrologic connectivit
KW  - Hydrologic connectivity
KW  - Light detection and ranging
KW  - Objects detection
KW  - Processing units
KW  - Tensor processing unit
KW  - Culverts
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, W.
AU  - Cai, C.
AU  - Li, C.
AU  - Xu, H.
AU  - Shi, H.
TI  - AD-YOLO: A Real-Time YOLO Network With Swin Transformer and Attention Mechanism for Airport Scene Detection
PY  - 2024
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2024.3472805
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206328562&doi=10.1109%2fTIM.2024.3472805&partnerID=40&md5=3f0dd2194aa8a07312f4b2db3d2640fc
AB  - Real-time acquisition of airport scene information is crucial for airport safety and optimization of airport utilization efficiency. However, detecting airport objects is still a challenging task due to the small size of person and vehicle targets in the airport scene images, insufficient public airport data, and so on, which makes it difficult to achieve high accuracy and real-time detection methods in the airport scene simultaneously. This article proposes a novel airport object detection approach to address the challenge by integrating the advantages of improved you only look once (YOLO), Swin Transformer, and attention mechanism [airport detector - YOLO (AD-YOLO)]. Specifically, we introduce the Swin Transformer, which retains the Transformer's ability of global attention to obtain features and reduces the drawbacks of computational complexity, into the head network based on YOLOv7 to improve the high-dimensional information feature fusion. We also design an efficient channel spatial attention (ECSA) module and introduce a small object detection layer (SODL) to improve the detection accuracy of small targets in the airport scene. We test the proposed method on the self-constructed multiple airport surveillance dataset (MASD) containing 5736 images captured by actual airport and online airport video. The experimental results show that AD-YOLO achieves 71.6% mean average precision (mAP), exceeding the mAP of the baseline method by 4.4%. The proposed method has 101.4 frames/s (FPS) on the NVIDIA RTX3080 GPU and 17.8 FPS on the Jetson Orin NX, meeting the real-time and accuracy requirements of the airport scene. Finally, the experimental results on the public airport surface surveillance (ASS) dataset show that AD-YOLO outperforms other detection methods, demonstrating its effectiveness. © 1963-2012 IEEE.
KW  - Airport scene
KW  - attention mechanism
KW  - object detection
KW  - Swin Transformer
KW  - YOLOv7
KW  - Airport runways
KW  - Airport security
KW  - Clutter (information theory)
KW  - Electric transformer testing
KW  - Remote sensing
KW  - Security systems
KW  - Airport object
KW  - Airport scene
KW  - Attention mechanisms
KW  - Detection methods
KW  - Objects detection
KW  - Public airports
KW  - Real- time
KW  - Scene detection
KW  - Swin transformer
KW  - YOLOv7
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Fedorov, V.A.
TI  - Recognizing Railway Infrastructure Using CNN and Stereoscopic Vision
PY  - 2024
T2  - Proceedings - 2024 International Russian Smart Industry Conference, SmartIndustryCon 2024
DO  - 10.1109/SmartIndustryCon61328.2024.10516208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193273357&doi=10.1109%2fSmartIndustryCon61328.2024.10516208&partnerID=40&md5=426b108c6b4ecbf04122564a5189f35a
AB  - In the era of Industry 4.0, achieving Grade of Automation 4 (GoA 4) within the railway sphere is pivotal for the development of autonomous networks. This necessitates intelligent systems capable of real-time perception, precise object identification, and accurate determination of distances. This paper investigates railway object detection using Convolutional Neural Network (CNN), specifically the YOLOv8 architecture, extending prior research by integrating CNN with stereoscopic vision for distance determination to the detected objects. Implementing GoA 4 enables autonomous systems to interpret and react within railway environments. Our evaluation of YOLOv8 demonstrates consistent and robust object detection, achieving an mAP of 0.8 at IoU 0.5 and maintaining 0.54 across IoU 0.5-0.95. Utilizing stereoscopic vision, the determination of distances to detected objects within a 250-meter range exhibits an error margin of less than 10%, ensuring high precision in distance estimation. The integration of YOLOv8 with stereoscopic vision for accurate distance estimation, executed within 80 milliseconds, demonstrates remarkable computational efficiency, which is crucial for real-time applications. This efficient process, facilitated by the Nvidia RTX A5000 graphics accelerator, ensures both precision and high speed in dynamic railway settings. Our assessment emphasizes the adaptability and reliability of YOLOv8 in detecting railway infrastructure objects across diverse conditions, signifying its potential for real-world deployment. © 2024 IEEE.
KW  - CNN
KW  - computer vision
KW  - GoA4
KW  - machine vision
KW  - railway infrastructure
KW  - stereoscopic vision
KW  - YOLOv8
KW  - Computational efficiency
KW  - Convolutional neural networks
KW  - Intelligent systems
KW  - Object detection
KW  - Object recognition
KW  - Railroad transportation
KW  - Railroads
KW  - Real time systems
KW  - Autonomous networks
KW  - Convolutional neural network
KW  - Distance estimation
KW  - Goa4
KW  - Machine-vision
KW  - Network vision
KW  - Railway infrastructure
KW  - Real time perception
KW  - Stereoscopic vision
KW  - YOLOv8
KW  - Computer vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, H.
AU  - Zhang, S.
AU  - Peng, X.
AU  - Lu, Z.
AU  - Li, G.
TI  - Improved object detection method for autonomous driving based on DETR
PY  - 2024
T2  - Frontiers in Neurorobotics
DO  - 10.3389/fnbot.2024.1484276
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216516718&doi=10.3389%2ffnbot.2024.1484276&partnerID=40&md5=442dc18ae40eef21674fd538ffee0b3d
AB  - Object detection is a critical component in the development of autonomous driving technology and has demonstrated significant growth potential. To address the limitations of current techniques, this paper presents an improved object detection method for autonomous driving based on a detection transformer (DETR). First, we introduce a multi-scale feature and location information extraction method, which solves the inadequacy of the model for multi-scale object localization and detection. In addition, we developed a transformer encoder based on the group axial attention mechanism. This allows for efficient attention range control in the horizontal and vertical directions while reducing computation, ultimately enhancing the inference speed. Furthermore, we propose a novel dynamic hyperparameter tuning training method based on Pareto efficiency, which coordinates the training state of the loss functions through dynamic weights, overcoming issues associated with manually setting fixed weights and enhancing model convergence speed and accuracy. Experimental results demonstrate that the proposed method surpasses others, with improvements of 3.3%, 4.5%, and 3% in average precision on the COCO, PASCAL VOC, and KITTI datasets, respectively, and an 84% increase in FPS. Copyright © 2025 Zhao, Zhang, Peng, Lu and Li.
KW  - feature extraction
KW  - loss function
KW  - object detection
KW  - parameter tuning
KW  - transformer encoder
KW  - Encoding (symbols)
KW  - Autonomous driving
KW  - Based on detections
KW  - Critical component
KW  - Features extraction
KW  - Growth potential
KW  - Loss functions
KW  - Object detection method
KW  - Objects detection
KW  - Parameters tuning
KW  - Transformer encoder
KW  - article
KW  - controlled study
KW  - diagnosis
KW  - feature extraction
KW  - male
KW  - pareto optimality
KW  - velocity
KW  - Signal encoding
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Roopa Devi, E.M.
AU  - Sedhumadhavan, V.
AU  - Sudharsan, V.K.
AU  - Rakesh, M.
AU  - Surendhar, D.
TI  - Wild Animal Recognition using Deep Learning Models
PY  - 2024
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies, ICCCNT 2024
DO  - 10.1109/ICCCNT61001.2024.10725145
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211083650&doi=10.1109%2fICCCNT61001.2024.10725145&partnerID=40&md5=6fea304d456a5233fe1ec19679033be3
AB  - Wildlife-vehicle collisions present a significant challenge at the intersection of road networks and natural habitats, necessitating innovative solutions to enhance road safety and mitigate the impact on wildlife populations. Traditional detection methods often fall short in achieving real-time effectiveness, prompting the exploration of advanced deep learning models such as EfficientDet, Single Shot MultiBox Detector (SSD), and YOLOv9 for wild animal recognition. This research addresses the limitations of existing models by proposing a comprehensive recognition system that leverages the capabilities of these state-of-the-art models. EfficientDet, SSD, and YOLOv9 represent a paradigm shift in automatically extracting intricate features from raw data, offering potential for robust and adaptive recognition capabilities. Our The experimental findings illustrate that the suggested YOLOv9 model outperforms EfficientDet and SSD, showcasing superior detection accuracy, robustness, and computational efficiency. This discovery emphasizes the effectiveness of YOLOv9 in tasks related to detecting wild animals, underscoring its potential as a reliable and efficient solution for mitigating wildlife-vehicle collisions and enhancing road safety in natural environments. © 2024 IEEE.
KW  - Wild Animal Recognition System, Deep Learning Models, EfficientDet, Single Shot MultiBox Detector (SSD), YOLOv9
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Song, G.
AU  - Song, S.
TI  - A Rain Image Generation Approach for Autonomous Driving Perception Sensors
PY  - 2024
T2  - 2024 4th International Conference on Electronic Information Engineering and Computer Science, EIECS 2024
DO  - 10.1109/EIECS63941.2024.10800702
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216320748&doi=10.1109%2fEIECS63941.2024.10800702&partnerID=40&md5=88ea65c100c787e0fb54edf8ba7ab86b
AB  - Proper rain image generation is important to assess the safety of autonomous perception system in rainy environment and improve perception performance. Rain image generation methods for autonomous driving vision sensors have become a research hotspot. The diversity of data can be extended and the performance of vision-based perception can be improved by effective real rain image generation. Although recent deep learning-based image generation of real rain has good performance, it is still an open problem for autonomous driving scenarios. Firstly, the existing rain images dataset only considers the streak characteristics produced by rain in modeling real rain features, and does not consider the interference of raindrops and fog on the image. Secondly, the distribution of the generated rain streaks, raindrops and fog lacks the influence of physical characteristics brought about by vehicle speed, rainfall and gravity during vehicle operation. Moreover, the lack of a real-time and efficient method for generating realistic raindrop images makes data enhancement and safety testing of sensing systems challenging. To solve the problem of real rain images generation for autonomous driving scenarios, we propose an efficient data generation method that combines physical properties and robot operating system to realize real-time generation of real rain for dynamic scenes and real-time data publish.  © 2024 IEEE.
KW  - Autonomous driving
KW  - Physical properties
KW  - Rain image generation
KW  - Real-time generation
KW  - Autonomous driving
KW  - Generation method
KW  - Image generations
KW  - Perception systems
KW  - Performance
KW  - Property
KW  - Rain image generation
KW  - Real- time
KW  - Real-time generation
KW  - Vision sensors
KW  - Robot Operating System
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hu, J.
AU  - He, Y.
AU  - Zeng, M.
AU  - Qian, Y.
AU  - Zhang, R.
TI  - Smoke and Fire Detection Based on YOLOv7 With Convolutional Structure Reparameterization and Lightweighting
PY  - 2024
T2  - IEEE Sensors Letters
DO  - 10.1109/LSENS.2024.3420883
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197538624&doi=10.1109%2fLSENS.2024.3420883&partnerID=40&md5=a857e340e7ae86b1914b7c4e8ed72b75
AB  - This letter presents a YOLO-based detection model called Diverse branch block and slimneck-YOLO (DS-YOLO) [Diverse Branch Block (DBB) and SlimNeck], which enables fast and accurate identification of smoke and fire. First, Diverse branch block and partial convolution- efficient layer aggregation network (DP-ELAN) module in the backbone is constructed based on DBB and partial convolution. It is possible to enhance detection capability while reducing the number of parameters. Second, utilizing the SlimNeck based on ghost shuffle convolution to reduce model complexity. Finally, we use normalized Gaussian Wasserstein distance instead of the standard intersection over union to improve the detection capability for small-sized objects.The experimental results on our custom smoke and fire dataset indicate that the proposed DS-YOLO achieves a mean average precision of 70.1%, while reducing computational complexity. This represents an 1.3% improvement over the baseline model. © 2017 IEEE.
KW  - computer vision
KW  - Sensor applications
KW  - sensor networks
KW  - smoke and fire detection
KW  - YOLOv7
KW  - Complex networks
KW  - Computer vision
KW  - Convolution
KW  - Feature extraction
KW  - Fire detectors
KW  - Fires
KW  - Smoke
KW  - Accuracy
KW  - Computational modelling
KW  - Detection capability
KW  - Features extraction
KW  - Fire detection
KW  - Reparameterization
KW  - Sensors network
KW  - Smoke detection
KW  - YOLO
KW  - YOLOv7
KW  - Sensor networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Duraisamy, P.
AU  - Deepika, A.
AU  - Shivadharshini, G.
AU  - Swetha, S.
AU  - Kumar, N.S.
TI  - Implementation of YOLO V8 for Advanced Autonomous Vehicle Detection Techniques
PY  - 2024
T2  - 15th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209086445&partnerID=40&md5=3034cd10b911fe511e5bb38034693132
AB  - Addressing the escalating necessity for proficient and dependable autonomous vehicle systems, this research proposes an advanced object detection approach harnessing the YOLOv8 algorithm. Object detection plays a pivotal role in the functionality of autonomous driving systems, enabling vehicles to perceive and react to their surroundings promptly. YOLOv8, an upgraded iteration of the YOLO algorithm, is acclaimed for its rapidity and precision in object detection endeavors. Our proposed model, distinguished by enhancements in network architecture and training methodologies, surpasses existing models in terms of detection precision and computational efficacy. Through an exhaustive examination, we delve into the network architecture, training regimen, and assessment metrics of the YOLOv8-based model. Experimental findings showcase the model's commendable performance in both quantitative and qualitative assessments, highlighting its resilience in identifying pedestrians, vehicles, traffic signs, and other pertinent objects across varied driving scenarios. The model's predicted outputs attest to its adeptness in precisely localizing and categorizing objects of interest, thereby augmenting the safety and efficiency of autonomous driving systems. © Grenze Scientific Society, 2024.
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Object detection
KW  - Real-time systems
KW  - YOLOv8 algorithm
KW  - Advanced driver assistance systems
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Detection approach
KW  - Driving systems
KW  - In networks
KW  - Objects detection
KW  - Real - Time system
KW  - Vehicle system
KW  - Vehicles detection
KW  - YOLOv8 algorithm
KW  - Pedestrian safety
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, H.
AU  - Zhang, X.
AU  - He, J.
AU  - Yu, Y.
AU  - Cheng, Y.
TI  - Real-Time Volumetric Perception for Unmanned Surface Vehicles Through Fusion of Radar and Camera
PY  - 2024
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2024.3381690
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189156410&doi=10.1109%2fTIM.2024.3381690&partnerID=40&md5=2a0674dba5e8842c806080b24b6359c5
AB  - In recent years, unmanned surface vehicles (USVs) have played an increasingly important role in various applications. Due to the expansion of USV application scenes from common marine areas to inland waters with complex environments, environmental perception has become an essential requirement for autonomous navigation systems of USVs. Traditional perception methods utilize either light detection and ranging (LiDAR) or radar to construct volumetric maps for environmental perception. To improve the accuracy of perception systems and reduce deployment costs, this article proposes a novel radar and camera fusion volumetric map network named FVMNet for real-time volumetric perception. FVMNet is based on a novel radar and image fusion architecture and comprises four modules: 1) the radar and image encoders can extract different features; 2) only using in training stage without extra valid time costs, auxiliary segmentation head advances the image encoder; 3) to eliminate the representation difference between image features and radar features, the BEV spatial transformer module transfers image feature representations from the perspective view to BEV space; and 4) the fusion segmentation head predicts the volumetric perception results. Compared to other baseline methods that use a single modality, FVMNet achieves state-of-the-art accuracy in the public USVInland dataset and our collected wharf dataset. We conducted comprehensive ablation experiments to validate the efficacy of the designed modules. Moreover, the proposed method demonstrates generalization in zero-shot real-world scenarios and robustness under extreme weather conditions.  © 1963-2012 IEEE.
KW  - Millimeter-wave (MMW) radar
KW  - radar-camera fusion
KW  - supervised learning with light detection and ranging (LiDAR) data
KW  - unmanned surface vehicles (USVs) perception
KW  - volumetric perception
KW  - Cameras
KW  - Costs
KW  - Image segmentation
KW  - Navigation systems
KW  - Optical radar
KW  - Real time systems
KW  - Signal encoding
KW  - Space-based radar
KW  - Unmanned surface vehicles
KW  - MMW radar
KW  - Point cloud compression
KW  - Point-clouds
KW  - Radar-camera fusion
KW  - Real- time
KW  - Sea surfaces
KW  - Supervised learning with LiDAR data
KW  - Unmanned surface vehicle  perception
KW  - Volumetric perception
KW  - Volumetrics
KW  - Radar imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fei, T.
AU  - Mukhopadhyay, S.C.
AU  - da Costa, J.P.J.
AU  - RoyChaudhuri, C.
AU  - Lan, L.
AU  - Demitri, N.
TI  - Spatial Environment Perception and Sensing in Automated Systems: A Review
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3379222
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189538819&doi=10.1109%2fJSEN.2024.3379222&partnerID=40&md5=e69e0a0643c064ddfad3528072eb36bf
AB  - —This article provides a concise yet comprehensive review of spatial environment sensing and perception (SESP) in automated systems, crucial for intelligent applications such as smart cities, smart homes, navigation, automated driving, and industry 4.0. With a focus on achieving dependable performance even in harsh environments and enabling affordable mass deployment, the review explores sensor technologies, calibration, diagnostics, performance degradation analysis, and machine-learning (ML)-based perception algorithms. Our contributions aim to deepen understanding and drive progress in automated systems, addressing challenges across diverse application scenarios. Through a meticulous analysis of the pros and cons of state-of-the-art technologies, this article sheds light on future trends in research and development. This work serves as a valuable resource for researchers, practitioners, and industry professionals seeking insights into the evolving landscape of SESP. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
KW  - Automated sensor
KW  - sensor calibration
KW  - sensor perception
KW  - sensors
KW  - smart sensor
KW  - Intelligent buildings
KW  - Intelligent robots
KW  - Learning systems
KW  - Smart sensors
KW  - Automated sensors
KW  - Automated systems
KW  - Environment perceptions
KW  - Environment sensing
KW  - Intelligent sensors
KW  - Robot sensing system
KW  - Sensor calibration
KW  - Sensor perceptions
KW  - Sensor systems
KW  - Spatial environments
KW  - Calibration
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tang, G.
AU  - Ni, J.
AU  - Zhao, Y.
AU  - Gu, Y.
AU  - Cao, W.
TI  - A Survey of Object Detection for UAVs Based on Deep Learning
PY  - 2024
T2  - Remote Sensing
DO  - 10.3390/rs16010149
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181847264&doi=10.3390%2frs16010149&partnerID=40&md5=739dbe8348e6f04ea3c3019f056e0dd9
AB  - With the rapid development of object detection technology for unmanned aerial vehicles (UAVs), it is convenient to collect data from UAV aerial photographs. They have a wide range of applications in several fields, such as monitoring, geological exploration, precision agriculture, and disaster early warning. In recent years, many methods based on artificial intelligence have been proposed for UAV object detection, and deep learning is a key area in this field. Significant progress has been achieved in the area of deep-learning-based UAV object detection. Thus, this paper presents a review of recent research on deep-learning-based UAV object detection. This survey provides an overview of the development of UAVs and summarizes the deep-learning-based methods in object detection for UAVs. In addition, the key issues in UAV object detection are analyzed, such as small object detection, object detection under complex backgrounds, object rotation, scale change, and category imbalance problems. Then, some representative solutions based on deep learning for these issues are summarized. Finally, future research directions in the field of UAV object detection are discussed. © 2023 by the authors.
KW  - computer vision
KW  - deep learning
KW  - object detection
KW  - unmanned aerial vehicles
KW  - Aircraft detection
KW  - Antennas
KW  - Computer vision
KW  - Deep learning
KW  - Object recognition
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial Photographs
KW  - Aerial vehicle
KW  - Deep learning
KW  - Detection technology
KW  - Early warning
KW  - Geological exploration
KW  - Objects detection
KW  - Precision Agriculture
KW  - Recent researches
KW  - Unmanned aerial vehicle
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - McDonnell, K.J.
TI  - Leveraging the Academic Artificial Intelligence Silecosystem to Advance the Community Oncology Enterprise
PY  - 2023
T2  - Journal of Clinical Medicine
DO  - 10.3390/jcm12144830
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166326559&doi=10.3390%2fjcm12144830&partnerID=40&md5=4d9e360cc4be9af4aefa2698972f0062
AB  - Over the last 75 years, artificial intelligence has evolved from a theoretical concept and novel paradigm describing the role that computers might play in our society to a tool with which we daily engage. In this review, we describe AI in terms of its constituent elements, the synthesis of which we refer to as the AI Silecosystem. Herein, we provide an historical perspective of the evolution of the AI Silecosystem, conceptualized and summarized as a Kuhnian paradigm. This manuscript focuses on the role that the AI Silecosystem plays in oncology and its emerging importance in the care of the community oncology patient. We observe that this important role arises out of a unique alliance between the academic oncology enterprise and community oncology practices. We provide evidence of this alliance by illustrating the practical establishment of the AI Silecosystem at the City of Hope Comprehensive Cancer Center and its team utilization by community oncology providers. © 2023 by the author.
KW  - artificial intelligence
KW  - City of Hope
KW  - community practice
KW  - oncology
KW  - adult
KW  - article
KW  - artificial intelligence
KW  - cancer center
KW  - human
KW  - synthesis
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bai, Z.
AU  - Nayak, S.P.
AU  - Zhao, X.
AU  - Wu, G.
AU  - Barth, M.J.
AU  - Qi, X.
AU  - Liu, Y.
AU  - Sisbot, E.A.
AU  - Oguchi, K.
TI  - Cyber Mobility Mirror: A Deep Learning-Based Real-World Object Perception Platform Using Roadside LiDAR
PY  - 2023
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2023.3268281
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159796681&doi=10.1109%2fTITS.2023.3268281&partnerID=40&md5=911aa5a439b3e025478bca95fdf7d90c
AB  - Object perception plays a fundamental role in Cooperative Driving Automation (CDA) which is regarded as a revolutionary promoter for next-generation transportation systems. However, the vehicle-based perception may suffer from the limited sensing range and occlusion as well as low penetration rates in connectivity. In this paper, we propose Cyber Mobility Mirror (CMM), a next-generation real-world object perception system for 3D object detection, tracking, localization, and reconstruction, to explore the potential of roadside sensors for enabling CDA in the real world. The CMM system consists of six main components: i) the data pre-processor to retrieve and preprocess the raw data; ii) the roadside 3D object detector to generate 3D detection results; iii) the multi-object tracker to identify detected objects; iv) the global locator to generate geo-localization information; v) the mobile-edge-cloud-based communicator to transmit perception information to equipped vehicles, and vi) the onboard advisor to reconstruct and display the real-time traffic conditions. An automatic perception evaluation approach is proposed to support the assessment of data-driven models without human-labeling requirements and a CMM field-operational system is deployed at a real-world intersection to assess the performance of the CMM. Results from field tests demonstrate that our CMM prototype system can achieve 96.99% precision and 83.62% recall for detection and 73.55% ID-recall for tracking. High-fidelity real-time traffic conditions (at the object level) can be geo-localized with a root-mean-square error (RMSE) of 0.69m and 0.33m for lateral and longitudinal direction, respectively, and displayed on the GUI of the equipped vehicle with a frequency of 3-4 Hz.  © 2000-2011 IEEE.
KW  - 3D object detection
KW  - cooperative driving automation
KW  - deep learning
KW  - Field operational system
KW  - localization
KW  - multi-object tracking
KW  - Automation
KW  - Deep learning
KW  - Image reconstruction
KW  - Intelligent vehicle highway systems
KW  - Laser applications
KW  - Mean square error
KW  - Object detection
KW  - Object recognition
KW  - Roadsides
KW  - Three dimensional computer graphics
KW  - Three dimensional displays
KW  - Vehicle to vehicle communications
KW  - Vehicles
KW  - 3D object
KW  - 3d object detection
KW  - Cooperative driving
KW  - Cooperative driving automation
KW  - Deep learning
KW  - Field operational system
KW  - Localisation
KW  - Multi-object tracking
KW  - Objects detection
KW  - Operational systems
KW  - Point cloud compression
KW  - Point-clouds
KW  - Three-dimensional display
KW  - Optical radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yan, G.
AU  - Liu, K.
AU  - Liu, C.
AU  - Zhang, J.
TI  - Edge Intelligence for Internet of Vehicles: A Survey
PY  - 2024
T2  - IEEE Transactions on Consumer Electronics
DO  - 10.1109/TCE.2024.3378509
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188538736&doi=10.1109%2fTCE.2024.3378509&partnerID=40&md5=f08d3a4a28733eea4175050bf0b74b28
AB  - The Internet of Vehicles (IoV) has become a fundamental platform for advancing Intelligent Transportation Systems (ITSs) and Intelligent Connected Vehicles (ICVs). However, the increasing volume of data generated by vehicle sensors and the computational demands of Artificial Intelligence (AI) algorithms present significant challenges for the platform. Edge Intelligence (EI), which brings intelligent computing and data processing closer to vehicles, has emerged as a potential solution. In this survey, we provide a comprehensive overview of Edge Intelligence for the Internet of Vehicles. We begin by discussing the motivations behind employing EI in the IoV for typical AI computations. To fully exploit the potential of EI in heterogeneous IoV environments, we present a layered vehicular EI architecture and discuss its benefits and challenges. Furthermore, we provide a taxonomy of EI approaches for vehicular networks, focusing on cooperative inference, distributed training, and collaborative sensing, in terms of their schemas and advanced frameworks. Finally, we explore emerging trends and research directions in this field, including vehicle-road-cloud integration, generative AI-driven IoV, and vehicular cyber-physical fusion. By offering insights into state-of-the-art techniques and trends, this survey aims to enable researchers to develop innovative solutions for transforming the intelligent IoV ecosystem.  © 1975-2011 IEEE.
KW  - edge intelligence (EI)
KW  - inference
KW  - Internet of Vehicles (IoV)
KW  - sensing
KW  - training
KW  - Data handling
KW  - Distributed computer systems
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Job analysis
KW  - Network architecture
KW  - Vehicles
KW  - Collaboration
KW  - Computational demands
KW  - Edge intelligence
KW  - Inference
KW  - Intelligent transportation systems
KW  - Internet of vehicle
KW  - Sensing
KW  - Task analysis
KW  - Vehicle sensors
KW  - Computer architecture
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liang, M.
AU  - Meyer, F.
TI  - Neural Enhanced Belief Propagation for Multiobject Tracking
PY  - 2024
T2  - IEEE Transactions on Signal Processing
DO  - 10.1109/TSP.2023.3314275
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181092781&doi=10.1109%2fTSP.2023.3314275&partnerID=40&md5=34736fd0feed62faf0427cbbc94b2c18
AB  - Algorithmic solutions for multi-object tracking (MOT) are a key enabler for applications in autonomous navigation and applied ocean sciences. State-of-the-art MOT methods fully rely on a statistical model and typically use preprocessed sensor data as measurements. In particular, measurements are produced by a detector that extracts potential object locations from the raw sensor data collected at discrete time steps. This preparatory processing step reduces data flow and computational complexity but may result in a loss of information. State-of-the-art Bayesian MOT methods that are based on belief propagation (BP) systematically exploit graph structures of the statistical model to reduce computational complexity and improve scalability. However, as a fully model-based approach, BP can provide highly suboptimal estimates when there is a mismatch between the statistical model and the true data-generating process. Existing BP-based MOT methods can further only make use of preprocessed measurements. In this paper, we introduce a variant of BP that combines model-based with data-driven MOT. The proposed neural enhanced belief propagation (NEBP) method complements the statistical model of BP by information learned from raw sensor data. This approach conjectures that the learned information can reduce model mismatch and thus improve data association and false alarm rejection. Our NEBP method improves tracking performance compared to model-based methods. At the same time, it inherits the advantages of BP-based MOT, i.e., it scales only quadratically in the number of objects, and it can thus generate and maintain a large number of object tracks. We evaluate the performance of our NEBP approach for MOT on the nuScenes autonomous driving dataset and demonstrate that it can achieve state-of-the-art performance. In particular, an average multi-object tracking accuracy (AMOTA) of 0.683 was obtained and, compared with non-BP-based methods, identity switches (IDS) and track fragments (Frag) were reduced by 23% and 19%, respectively.  © 1991-2012 IEEE.
KW  - belief propagation
KW  - factor graphs
KW  - graph neural networks
KW  - Multiobject tracking
KW  - probabilistic data association
KW  - Backpropagation
KW  - Complex networks
KW  - Computational complexity
KW  - Data reduction
KW  - Graph neural networks
KW  - Graphic methods
KW  - Large dataset
KW  - Object detection
KW  - Tracking (position)
KW  - Belief propagation
KW  - Factor graphs
KW  - Graph neural networks
KW  - Multi-object tracking
KW  - Probabilistic data association
KW  - Raw sensor
KW  - Sensors data
KW  - State of the art
KW  - Statistic modeling
KW  - Tracking method
KW  - Belief propagation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yuan, M.
AU  - Yue, P.
AU  - Yang, C.
AU  - Li, J.
AU  - Yan, K.
AU  - Cai, C.
AU  - Wan, C.
TI  - Generating lane-level road networks from high-precision trajectory data with lane-changing behavior analysis
PY  - 2024
T2  - International Journal of Geographical Information Science
DO  - 10.1080/13658816.2023.2279977
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176562390&doi=10.1080%2f13658816.2023.2279977&partnerID=40&md5=b765100c2dbbd24e3c260fa8e3386d03
AB  - Abstract–: Recent advances in mobile mapping systems have facilitated the collection of high-precision trajectory data in centimeter positioning accuracy. It provides the potential to infer lane-level road networks, which are essential for autonomous driving navigation. This task is challenging due to the complicated lane merging and diverging structures as well as the lane-changing patterns in trajectory data. This paper presents a lane-level road network generation method from high-precision trajectory data with lane-changing behavior analysis. Trajectories are firstly partitioned by detecting road intersections and changes in lane structure. Subsequently, in regions with consistent lane structure, a principal curve fitting algorithm is developed to extract lane centerlines. Erroneous lanes generated by lane-changing behavior are pruned based on a constructed lane intersection graph. In regions with merging and diverging lanes, a lane-group fitting algorithm is designed. This algorithm estimates lane locations by incorporating a Gaussian mixture model with lane width prior knowledge and then infers lane-level topological structures using trajectory flow information. The proposed method is evaluated on a real-world high-precision trajectory dataset. Comprehensive experiments demonstrate that it outperforms state-of-the-art methods in four metrics. Under complex scenarios, the method is capable of generating lane-level road networks with higher completeness and fewer fragments. © 2023 Informa UK Limited, trading as Taylor & Francis Group.
KW  - high-precision trajectory data
KW  - lane-changing behavior
KW  - Lane-level road network
KW  - algorithm
KW  - Gaussian method
KW  - navigation
KW  - numerical model
KW  - precision
KW  - road
KW  - topology
KW  - trajectory
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, K.
AU  - Zhu, X.
AU  - Pei, Y.
AU  - Zhan, Y.
TI  - Research progress of agricultural automatic machinery obstacle detection
ST  - 农业自动化机械障碍物检测研究进展
PY  - 2023
T2  - Jiangsu Daxue Xuebao (Ziran Kexue Ban)/Journal of Jiangsu University (Natural Science Edition)
DO  - 10.3969/j.issn.1671-7775.2023.04.007
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171848645&doi=10.3969%2fj.issn.1671-7775.2023.04.007&partnerID=40&md5=0b08f965a05b74609bda0d7a534a1bb0
AB  - Automatic navigation technology of agricultural machinery is concerned in the field of intelligent agriculture, while the obstacle detection is an important part. The shortcomings of sensor detection technology in the early years were analyzed, and the application and application prospects of computer vision in agricultural machinery intelligent obstacle detection methods were summarized. The results show that sensor technology is now developed from single sensor to multi-sensor fusion at very mature stage, but it is still affected by obstacles surface and too high detection cost. In recent years, computer vision and deep learning have been used in agriculture of convolutional neural network, but it should be improved much in occlusion, remote detection, moving obstacle detection and other aspects. According to the agricultural obstacle detection techniques in the past 20 years, the existing problems are summarized, and a new idea for the future development of smart agriculture in China is proposed. © 2023 Journal of Jiangsu University (Natural Science Edition). All rights reserved.
KW  - agricultural machinery
KW  - computer vison
KW  - information fusion
KW  - obstacle detection
KW  - sensor
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yuan, Y.
AU  - Wu, Y.
AU  - Zhao, L.
AU  - Chen, J.
AU  - Zhao, Q.
TI  - Research progress of UAV aerial video multi⁃object detection and tracking based on deep learning
ST  - 基 于 深 度 学 习 的 无 人 机 航 拍 视 频 多 目 标 检 测 与跟 踪 研 究 进 展
PY  - 2023
T2  - Hangkong Xuebao/Acta Aeronautica et Astronautica Sinica
DO  - 10.7527/S1000-6893.2023.28334
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175035439&doi=10.7527%2fS1000-6893.2023.28334&partnerID=40&md5=119dda6ba471fdcf769d188e7ae660a4
AB  - With the increasing convenience of data acquisition for aerial photography of Unmanned Aerial Vehicle （UAV），the multi-target detection and tracking technology based on the UAV platform has developed rapidly and has broad prospects for applications in civil and military fields. In recent years，the rapid progress of in-depth learning has also provided a variety of more effective solutions. However，the challenging problems such as sudden changes in the appearance of the target，serious occlusion of the target area，and disappearance and reappearance of the target from the perspective of UAV have not been completely solved. In this paper，we summarize the algorithms for multi-target detection and tracking in UAV aerial video based on deep learning，and summarize the latest progress in this field，including multi-target detection and multi-object tracking. The multi-object detection module is divided into two parts：two-stage and one-stage detection. For the multi-object tracking module，according to the two classical frameworks of tracking-based detection and joint-detection tracking，the principles of the two algorithms are described and their advantages and disadvantages are analyzed. Then，the existing public data sets are statistically analyzed，and the optimal schemes of the benchmark challenge VisDrone Challenge in the field of multi-target detection and tracking based on UAV aerial video in recent years are compared and analyzed. Finally，the paper discusses the urgent problems of multi-object detection and tracking from the perspective of UAV and the possible research directions in the future，providing a reference for the follow-up researchers. © 2023 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.
KW  - deep learning
KW  - detection tracking
KW  - joint-detection tracking
KW  - multi object detection
KW  - multi target tracking
KW  - one-stage detection
KW  - two-stage detection
KW  - UAV aerial video
KW  - Aerial photography
KW  - Aircraft detection
KW  - Antennas
KW  - Data acquisition
KW  - Deep learning
KW  - Military applications
KW  - Military photography
KW  - Object recognition
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial video
KW  - Deep learning
KW  - Detection tracking
KW  - Joint-detection
KW  - Joint-detection tracking
KW  - Multi object detection
KW  - Multi-target-tracking
KW  - Multiobject
KW  - Objects detection
KW  - One-stage detection
KW  - Two-stage detections
KW  - UAV aerial video
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, L.
AU  - Hua, S.
AU  - Zhang, C.
AU  - Yang, G.
AU  - Ren, J.
AU  - Li, J.
TI  - YOLOdrive: A Lightweight Autonomous Driving Single-Stage Target Detection Approach
PY  - 2024
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2024.3439863
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002092407&doi=10.1109%2fJIOT.2024.3439863&partnerID=40&md5=b68860615f7797c981c54f02b0c28547
AB  - With the continuous development of autonomous driving, real-time target detection has become increasingly critical in autonomous driving systems. However, traditional target detection algorithms usually require huge computational resources, limiting their application on embedded autonomous driving platforms. To address this challenge, a lightweight single-stage target detection algorithm is proposed YOLOdrive. The inverted residual structure, linear bottleneck layer, and depth-separable convolution in MobileNetv2 are utilized to improve the YOLOv8 backbone network, while the spatial channel reconstructed convolution is used to improve the C2f module of YOLOv8, and the convolution of YOLOv8 neck and detection head is replaced by the depth-separable convolution. Experimental results verify that the average accuracy of YOLOdrive algorithm on MS COCO2017 data set and VOC2007 data set is improved compared with the baseline model YOLOv8-Nano. The amount of model parameters has been reduced by more than 50%, and the amount of computation in the model has been reduced by more than 70%. The algorithm drastically reduces the amount of parameters and computational complexity of the network, improves the operational efficiency, saves the storage space of the network, and maintains a high-detection performance. © 2014 IEEE.
KW  - Autonomous driving
KW  - deep learning
KW  - target detection
KW  - YOLOv8
KW  - Computational efficiency
KW  - Continuous time systems
KW  - Convolution
KW  - Deep learning
KW  - Feature extraction
KW  - Interactive computer systems
KW  - Internet of things
KW  - Object detection
KW  - Parameter estimation
KW  - Signal detection
KW  - Accuracy
KW  - Autonomous driving
KW  - Classification algorithm
KW  - Deep learning
KW  - Features extraction
KW  - Objects detection
KW  - Real - Time system
KW  - Single stage
KW  - Targets detection
KW  - YOLOv8
KW  - Real time systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - An, X.
AU  - Li, S.
AU  - Zhang, Y.
AU  - Zhang, A.
TI  - Review of Fault Diagnosis Techniques for UAV Flight Control Systems
ST  - 无人机飞控系统故障诊断技术研究综述
PY  - 2023
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2305-0137
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007336975&doi=10.3778%2fj.issn.1002-8331.2305-0137&partnerID=40&md5=82f94cb56d1baea612de1ec37c69d0f1
AB  - In recent years, unmanned aerial vehicles（UAVs）have been widely used in various complex fields of military and civilian applications due to their unique advantages such as low operating costs and high mobility. At the same time, the complex and diverse missions have put forward higher requirements for the reliability and safety of UAV systems. The UAV fault diagnosis technology can provide timely and accurate diagnosis results, which helps the maintenance, repair and servicing of UAVs, and is of great significance in enhancing the combat effectiveness of UAVs. Therefore, this paper firstly analyses UAV flight control systems, and classifies the faults. Secondly, the research methods and status quo of UAV fault diagnosis technology are analysed and summarised. Finally, the main challenges faced by UAV fault diagnosis technology are discussed and the future development direction is pointed out; the aim is to provide some reference for researchers in the field of UAV fault diagnosis technology and to promote the improvement of UAV fault diagnosis technology level in China. © 2016 Chinese Medical Journals Publishing House Co.Ltd. All rights reserved.
KW  - actuator fault
KW  - fault diagnosis
KW  - sensor fault
KW  - unmanned aerial vehicle
KW  - Aircraft control
KW  - Antennas
KW  - Failure analysis
KW  - Fault detection
KW  - Flight control systems
KW  - Military applications
KW  - Military vehicles
KW  - Operating costs
KW  - Repair
KW  - Actuator fault
KW  - Aerial vehicle
KW  - Complex fields
KW  - Fault diagnosis technique
KW  - Fault diagnosis technology
KW  - Faults diagnosis
KW  - Flight-control systems
KW  - Lower operating costs
KW  - Sensors faults
KW  - Unmanned aerial vehicle
KW  - Unmanned aerial vehicles (UAV)
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hao, L.-Y.
AU  - Yang, J.-R.
AU  - Zhang, Y.
AU  - Zhang, J.
TI  - Multi-target vehicle detection based on corner pooling with attention mechanism
PY  - 2023
T2  - Applied Intelligence
DO  - 10.1007/s10489-023-05084-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174607346&doi=10.1007%2fs10489-023-05084-4&partnerID=40&md5=cdcdb6f1b9adadc3e014f1be3ae4ea9e
AB  - Abstract: Multi-target detection based on corner pooling provides a distinctive framework without anchor boxes, which has achieved wide application in the area of intelligent transportation system. To effectively detect small vehicles in the distant view, we propose an improved detection network termed corner pooling with attention mechanism (CPAM). A newly devised network called Hourglass with Coordinate Attention(Hourglass-CA) is proposed as an alternative to the Hourglass-104 backbone network. This one incorporates a multi-level attention mechanism to optimize the efficiency of feature extraction. Additionally, a novel multi-level attention loss(MLA loss) is presented, which dynamically adjusts the offsets during the feature extraction process. The experimental results demonstrate that our proposed CPAM achieves lightweight detection, reducing the parameters from 201M to 117M with an FPS from 4.2 to 16.1. Moreover, the AP can reach 51.6 % , surpassing several existing detectors. Graphical abstract: [Figure not available: see fulltext.]. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Coordinate attention
KW  - Corner pooling
KW  - Intelligent transportation system
KW  - Small object detection
KW  - Vehicle detection
KW  - Edge detection
KW  - Extraction
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Object detection
KW  - Object recognition
KW  - Vehicles
KW  - Attention mechanisms
KW  - Coordinate attention
KW  - Corner pooling
KW  - Features extraction
KW  - Intelligent transportation systems
KW  - Multi-targets
KW  - Multilevels
KW  - Small object detection
KW  - Target vehicles
KW  - Vehicles detection
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xu, C.
AU  - Zhang, Y.
AU  - Chen, S.
TI  - A multi-strategy integrated improved Yolov8n algorithm and its application to automatic driving detection
PY  - 2024
T2  - 2024 IEEE 7th International Conference on Information Systems and Computer Aided Education, ICISCAE 2024
DO  - 10.1109/ICISCAE62304.2024.10761211
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214700452&doi=10.1109%2fICISCAE62304.2024.10761211&partnerID=40&md5=a7b3e106c10b9d8a93125e73ddb6b360
AB  - For the existing target detection algorithms with low detection accuracy and slow recognition efficiency, it is difficult to meet the needs of autonomous driving to enable vehicles to quickly and accurately identify and localize the surrounding objects. For this reason, this paper proposes a multi-strategy integrated and improved Yolov8n algorithm to meet the demand for fast and accurate identification and localisation of surrounding objects for autonomous driving. Firstly, for the problem of low detection accuracy of Yolov8n, the GAM attention mechanism is introduced to improve the model's ability to capture key information. Subsequently, for the problem of poor model robustness, the CIOU loss function is replaced with the GIOU loss function to improve the model robustness. Finally, the SGD optimizer is applied to optimize the hyperparameters of the model to improve the model training efficiency. The proposed improved algorithm is applied to the VOC2007 dataset for validation, and the experimental results show that the improved model on the validation set mAP@0.5 can be 86.5%. The effectiveness and applicability of the proposed algorithm is verified by the improved model in the validation set. The Precision, Recall and mAP@0.5 all have significantly improved. The proposed improved Yolov8n model was compared with other target detection models, mAP@0.5 improved 2.8%~12.3%, and the superiority of the model is verified. ©2024 IEEE.
KW  - GAM attention
KW  - GIOU
KW  - VOC2007
KW  - Yolov8n
KW  - Automatic driving
KW  - Autonomous driving
KW  - Detection accuracy
KW  - GAM attention
KW  - GIOU
KW  - ITS applications
KW  - Loss functions
KW  - Model robustness
KW  - Validation sets
KW  - Yolov8n
KW  - Automatic target recognition
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chang, I.-C.
AU  - Yen, C.-E.
AU  - Song, Y.-J.
AU  - Chen, W.-R.
AU  - Kuo, X.-M.
AU  - Liao, P.-H.
AU  - Kuo, C.
AU  - Huang, Y.-F.
TI  - An Effective YOLO-Based Proactive Blind Spot Warning System for Motorcycles
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12153310
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167783693&doi=10.3390%2felectronics12153310&partnerID=40&md5=e4ef30c333b95291e4717cd95f010c7a
AB  - Interest in advanced driver assistance systems (ADAS) is booming in recent years. One of the most effervescent ADAS features is blind spot detection (BSD), which uses radar sensors or cameras to detect vehicles in the blind spot area and alerts the driver to avoid a collision when changing lanes. However, this kind of BSD system fails to notify nearby vehicle drivers in this blind spot of the possible collision. The goal of this research is to design a proactive bus blind spot warning (PBSW) system that will immediately notify motorcyclists when they enter the blind spot or the area of the inner wheel difference of a target vehicle, i.e., a bus. This will increase the real-time functionality of BSD and can have a significant impact on enhancing motorcyclist safety. The proposed hardware is placed on the motorcycle and consists of a Raspberry Pi 3B+ and a dual-lens stereo camera. We use dual-lens cameras to capture and create stereoscopic images then transmit the images from the Raspberry Pi 3B+ to an Android phone via Wi-Fi and to a cloud server using a cellular network. At the cloud server, we use the YOLOv4 image recognition model to identify the position of the rear-view mirror of the bus and use the lens imaging principle to estimate the distance between the bus and the motorcyclist. Finally, the cloud server returns the estimated distance to the PBSW app on the Android phone. According to the received distance value, the app will display the visible area/blind spot, the area of the inner wheel difference of the bus, the position of the motorcyclist, and the estimated distance between the motorcycle and the bus. Hence, as soon as the motorcyclist enters the blind spot of the bus or the area of the inner wheel difference, the app will alert the motorcyclist immediately to enhance their real-time safety. We have evaluated this PBSW system implemented in real life. The results show that the average position accuracy of the rear-view mirror is 92.82%, the error rate of the estimated distance between the rear-view mirror and the dual-lens camera is lower than 0.2%, and the average round trip delay between the Android phone and the cloud server is about 0.5 s. To the best of our knowledge, this proposed system is one of few PBSW systems which can be applied in the real world to protect motorcyclists from the danger of entering the blind spot and the area of the inner wheel difference of the target vehicle in real time. © 2023 by the authors.
KW  - Android app
KW  - area of the inner wheel difference
KW  - blind spot
KW  - distance estimation
KW  - dual-lens camera
KW  - motorcycle
KW  - proactive warning system
KW  - Raspberry Pi
KW  - real-time
KW  - YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tallat, R.
AU  - Hawbani, A.
AU  - Wang, X.
AU  - Al-Dubai, A.
AU  - Zhao, L.
AU  - Liu, Z.
AU  - Min, G.
AU  - Zomaya, A.Y.
AU  - Hamood Alsamhi, S.
TI  - Navigating Industry 5.0: A Survey of Key Enabling Technologies, Trends, Challenges, and Opportunities
PY  - 2024
T2  - IEEE Communications Surveys and Tutorials
DO  - 10.1109/COMST.2023.3329472
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181841986&doi=10.1109%2fCOMST.2023.3329472&partnerID=40&md5=057d4174ea29dc6179afb0cfd66a385d
AB  - This century has been a major avenue for revolutionary changes in technology and industry. Industries have transitioned towards intelligent automation, relying less on human intervention, resulting in the fourth industrial revolution, Industry 4.0. That is why IoT has been the researcher's arena for quite some time. With Industry 4.0 still in motion, the world is on the verge of the 5{textit {th}} industrial revolution, a relatively new concept with many unclear opinions regarding its potential benefits, challenges, opportunities, trends, and impact on society. There is a dire need for a broader and more critical perspective. This research paints a bigger picture of 'What is happening?' and 'What to expect?' during the transition phase of Industry 5.0. In this comprehensive review, we have addressed the state-of-the-art practices in Industry 4.0 and the transitional phase of Industry 5.0. We have highlighted the most promising key enabling technologies, trends, research topics, rising challenges, and unfolding opportunities that can help prepare society for this paradigm shift. The paper then surveys the work toward the outstanding key enablers, challenges, trends, and opportunities in the IoT evolution for Industry 5.0. To spur further avenues for researchers and industrialists, the paper offers conclusive insights at the end. In addition, the article has a precise set of research questions answered in consequent sections and subsections for the reader's clarity.  © 1998-2012 IEEE.
KW  - 6G
KW  - blockchain
KW  - digital twin
KW  - federated learning
KW  - Industrial Internet of Things (IIoT)
KW  - industrial wireless sensor networks
KW  - industry 4.0
KW  - Industry 5.0
KW  - intelligent sensing
KW  - Internet of Robotic Things (IoRT)
KW  - Blockchain
KW  - E-learning
KW  - Internet of things
KW  - Wireless sensor networks
KW  - 6g
KW  - Block-chain
KW  - Federated learning
KW  - Fourth industrial revolution
KW  - Industrial internet of thing
KW  - Industrial revolutions
KW  - Industrial wireless sensor network
KW  - Industrial wireless sensors
KW  - Industry 5.0
KW  - Intelligent sensing
KW  - Internet of robotic thing
KW  - Robot sensing system
KW  - Service robots
KW  - Industry 4.0
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, Y.
AU  - Zhu, B.
AU  - Zhao, X.
AU  - Deng, W.
TI  - Pixel-wise content attention learning for single-image deraining of autonomous vehicles
PY  - 2023
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2023.119990
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151630648&doi=10.1016%2fj.eswa.2023.119990&partnerID=40&md5=2ec5dd6d441a54ceadd099a4a4b58d91
AB  - Improving the performance of autonomous vehicles in adverse weather conditions is vital for the commercialization of such automated systems. Existing synthetic datasets for developing rain-tolerant vision are of limited value. To address this deficiency, a closed environment capable of simulating different degrees of rainfall is constructed. And a new Closed Field Rain dataset is collected in 36 testing cycles. Inspired by the idea that human can infer the content of rainy images directly without removing the raindrops. A new single-image deraining method is proposed, that does not require ground truth images. This method incorporates an image content estimation module applied to predict the scene content representation, and a pixel-wise content attention block used to evaluate the significance of each pixel. After that, an encoder-decoder network is applied to complete the image. On the other hand, it is almost impossible to obtain the ground truth of rainy images because of the dynamic characteristics of real traffic environment. Thus, the model is trained by employing PatchGAN, using a patch-based loss. Using common no-reference and feature point metrics as performance indicators, this paper conducts a comprehensive evaluation on both synthetic and real-world datasets including Closed Field Rain dataset. Results show the effectiveness of our model quantitatively and qualitatively. © 2023 Elsevier Ltd
KW  - Closed field rain
KW  - Content representation
KW  - GAN
KW  - Pixel-wise content attention
KW  - Single-image deraining
KW  - Autonomous vehicles
KW  - Rain
KW  - Statistical tests
KW  - Adverse weather
KW  - Autonomous Vehicles
KW  - Closed field rain
KW  - Content representation
KW  - GAN
KW  - Ground truth
KW  - Performance
KW  - Pixel-wise content attention
KW  - Single images
KW  - Single-image deraining
KW  - Pixels
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, Y.
AU  - Yang, F.
AU  - Sun, L.
AU  - Xiang, T.
AU  - Lv, P.
TI  - Echoformer: Transformer Architecture Based on Radar Echo Characteristics for UAV Detection
PY  - 2023
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2023.3254525
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151392959&doi=10.1109%2fJSEN.2023.3254525&partnerID=40&md5=4c4e7d8712b24c808fa850c05ec641f7
AB  - While recent years have witnessed an increasing number of commercial applications of unmanned aerial vehicles (UAVs), an imperative problem people have to face is the rapid growth of malicious use. So, it is imperative for security agencies to develop anti-UAV technology. The introduction of deep learning (DL) has a positive influence on radar signal processing, but DL-based methodologies have yet to be widespread in radar target detection because of the lack of unique architecture based on radar echo characteristics and the annotation method of radar data. In this article, a novel Transformer-based architecture is proposed, which transforms the problem of UAV detection into a binary classification task in each range cell. The complex encoder architecture and the Transformer-based extractor are designed to extract the Doppler frequency shift feature and the micro-Doppler signature (mDS) of a UAV simultaneously. The well-designed architecture based on radar echo characteristics can achieve a combination training of echoes with different coherent processing intervals (CPIs). In addition, we provide an annotation method and a data augmentation skill for our real measured dataset. The results of the experiment demonstrate that the proposed method has better detection performance and measuring accuracy under different SNRs in comparison with traditional radar target detection and other DL-based methods.  © 2001-2012 IEEE.
KW  - Doppler frequency shift
KW  - micro-Doppler signature (mDS)
KW  - radar echo processing
KW  - Transformer
KW  - unmanned aerial vehicle (UAV) detection
KW  - Aircraft detection
KW  - Antennas
KW  - Clutter (information theory)
KW  - Commercial vehicles
KW  - Continuous wave radar
KW  - Deep learning
KW  - Forestry
KW  - Radar measurement
KW  - Tracking radar
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Clutter
KW  - Doppler frequency shift
KW  - Doppler signatures
KW  - Micro-dopple signature
KW  - Micro-Doppler
KW  - Radar detection
KW  - Radar echo processing
KW  - Radar echoes
KW  - Task analysis
KW  - Transformer
KW  - Unmanned aerial vehicle detection
KW  - Vehicles detection
KW  - Frequency modulation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Khalid, S.
AU  - Shah, J.H.
AU  - Sharif, M.
AU  - Dahan, F.
AU  - Saleem, R.
AU  - Masood, A.
TI  - A Robust Intelligent System for Text-Based Traffic Signs Detection and Recognition in Challenging Weather Conditions
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3401044
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193222591&doi=10.1109%2fACCESS.2024.3401044&partnerID=40&md5=45dcd3245f7a602f9e4e3e0aa60bb05c
AB  - Traffic signs have great importance regarding smooth traffic flow and safe driving. However, due to many distractions and capricious factors, spotting and perceiving them may become hazardous. Traffic sign detection and recognition have gained popularity to put an end or to lessen the issue, and massive efforts have been realized in this regard. Despite considerable endeavors put together for traffic sign detection and recognition, there is a lack of attention in this area where these traffic signs contain text in them. A handful of studies may be found in state-of-the-art (SOTA) methods for text-based traffic sign detection, and particularly lesser for text recognition of detected text. The proposed method focuses on developing a robust semi-pipeline intelligent system to detect and understand text from traffic road signs boards in various weather conditions. For this purpose, a customized YOLOv5s is deployed for initial panel detection. Subsequently, MSER with preprocessing techniques is used for localization of text. Finally, OCR with NLP is utilized to recognize the text. The proposed method employed the ASAYAR dataset for training and different datasets for testing. The proposed approach produced satisfactory outcomes on them in contrast with SOTA approaches.  © 2013 IEEE.
KW  - automated road signs/panels detection
KW  - deep learning
KW  - MSER
KW  - natural language processing
KW  - Text recognition
KW  - YOLOV5s
KW  - Character recognition
KW  - Deep learning
KW  - Intelligent systems
KW  - Meteorology
KW  - Natural language processing systems
KW  - Roads and streets
KW  - Statistical tests
KW  - Traffic signs
KW  - Automated road sign/panel detection
KW  - Automated roads
KW  - Deep learning
KW  - Features extraction
KW  - Language processing
KW  - MSER
KW  - Natural language processing
KW  - Natural languages
KW  - Road
KW  - Road signs
KW  - Shape
KW  - Symbol
KW  - Text detection
KW  - Text recognition
KW  - YOLOV5
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Elhanashi, A.
AU  - Dini, P.
AU  - Saponara, S.
AU  - Zheng, Q.
TI  - Integration of Deep Learning into the IoT: A Survey of Techniques and Challenges for Real-World Applications
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12244925
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180682394&doi=10.3390%2felectronics12244925&partnerID=40&md5=dd55893c500d3a712fde72b0cab5ea0e
AB  - The internet of things (IoT) has emerged as a pivotal technological paradigm facilitating interconnected and intelligent devices across multifarious domains. The proliferation of IoT devices has resulted in an unprecedented surge of data, presenting formidable challenges concerning efficient processing, meaningful analysis, and informed decision making. Deep-learning (DL) methodologies, notably convolutional neural networks (CNNs), recurrent neural networks (RNNs), and deep-belief networks (DBNs), have demonstrated significant efficacy in mitigating these challenges by furnishing robust tools for learning and extraction of insights from vast and diverse IoT-generated data. This survey article offers a comprehensive and meticulous examination of recent scholarly endeavors encompassing the amalgamation of deep-learning techniques within the IoT landscape. Our scrutiny encompasses an extensive exploration of diverse deep-learning models, expounding on their architectures and applications within IoT domains, including but not limited to smart cities, healthcare informatics, and surveillance applications. We proffer insights into prospective research trajectories, discerning the exigency for innovative solutions that surmount extant limitations and intricacies in deploying deep-learning methodologies effectively within IoT frameworks. © 2023 by the authors.
KW  - convolutional neural networks
KW  - deep learning
KW  - healthcare
KW  - internet of things
KW  - recurrent neural networks
KW  - surveillance
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, J.
AU  - Pan, Z.
AU  - Liu, Y.
AU  - Niu, B.
AU  - Lei, B.
TI  - Single Object Tracking in Satellite Videos Based on Feature Enhancement and Multi-Level Matching Strategy
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15174351
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170353774&doi=10.3390%2frs15174351&partnerID=40&md5=27a6e3df7a08d1c950ec775bd819266f
AB  - Despite significant advancements in remote sensing object tracking (RSOT) in recent years, achieving accurate and continuous tracking of tiny-sized targets remains a challenging task due to similar object interference and other related issues. In this paper, from the perspective of feature enhancement and a better feature matching strategy, we present a tracker SiamTM specifically designed for RSOT, which is mainly based on a new target information enhancement (TIE) module and a multi-level matching strategy. First, we propose a TIE module to address the challenge of tiny object sizes in satellite videos. The proposed TIE module goes along two spatial directions to capture orientation and position-aware information, respectively, while capturing inter-channel information at the global 2D image level. The TIE module enables the network to extract discriminative features of the targets more effectively from satellite images. Furthermore, we introduce a multi-level matching (MM) module that is better suited for satellite video targets. The MM module firstly embeds the target feature map after ROI Align into each position of the search region feature map to obtain a preliminary response map. Subsequently, the preliminary response map and the template region feature map are subjected to the Depth-wise Cross Correlation operation to get a more refined response map. Through this coarse-to-fine approach, the tracker obtains a response map with a more accurate position, which lays a good foundation for the prediction operation of the subsequent sub-networks. We conducted extensive experiments on two large satellite video single-object tracking datasets: SatSOT and SV248S. Without bells and whistles, the proposed tracker SiamTM achieved competitive results on both datasets while running at real-time speed. © 2023 by the authors.
KW  - feature enhancement
KW  - matching strategy
KW  - object tracking
KW  - satellite video
KW  - siamese network
KW  - Large dataset
KW  - Remote sensing
KW  - Target tracking
KW  - Feature enhancement
KW  - Feature map
KW  - Level matching
KW  - Matching strategy
KW  - Matchings
KW  - Multilevels
KW  - Object Tracking
KW  - Satellite video
KW  - Siamese network
KW  - Target information
KW  - Satellites
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xing, J.
AU  - Liu, Y.
AU  - Zhang, G.-Z.
TI  - Improved YOLOV5-Based UAV Pavement Crack Detection
PY  - 2023
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2023.3281585
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161564254&doi=10.1109%2fJSEN.2023.3281585&partnerID=40&md5=cf04b109702444e850d042cf9f0fc744
AB  - In terms of highway crack detection, the combination of unmanned aerial vehicles (UAVs) and deep learning networks has become a powerful detection means. However, in the actual detection, in order to take into account the detection efficiency, it is necessary to ensure that the detection area is large enough, which makes the crack occupy few pixels in the image, and the image background is complex. Therefore, in this article, DJI Mavic3 is used to establish the image data set of highway pavement cracks under complex background. And, the YOLOV5 deep learning model is improved by adding a swin transformer structure and bidirectional feature pyramid network (BIFPN) feature pyramid. The improved YOLOV5 model achieved real-time pixel-level detection with a detection accuracy of 90% and a detection speed of 43.5 FPS. In terms of crack detection ability, the accuracy of the improved YOLOV5 reaches four pixels, and cracks of 1.2 mm can be detected in the experiment. Compared with the YOLOV7 model, the detection accuracy of the improved YOLOV5 model is increased by 15.4%. Compared with the YOLOV6 model, the calculated parameters of the improved YOLOV5 model are reduced by 59.25%. The proposed model is superior to other advanced models in crack detection. © 2001-2012 IEEE.
KW  - Bidirectional feature pyramid network (BIFPN)
KW  - crack
KW  - swin transformer
KW  - unmanned aerial vehicle (UAV)
KW  - YOLOV5
KW  - Aircraft detection
KW  - Antennas
KW  - Autonomous vehicles
KW  - Complex networks
KW  - Crack detection
KW  - Deep learning
KW  - Pavements
KW  - Pixels
KW  - Aerial vehicle
KW  - BIFPN
KW  - Computational modelling
KW  - Optimisations
KW  - Proposal
KW  - Road transportation
KW  - Swin transformer
KW  - Transformer
KW  - Unmanned aerial vehicle
KW  - YOLOV5
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Selvaraj, R.
AU  - Kuthadi, V.M.
AU  - Duraisamy, A.
AU  - Selvaraj, B.
AU  - Pethuraj, M.S.
TI  - Learning Optimizer-Based Visual Analytics Method to Detect Targets in Autonomous Unmanned Aerial Vehicles
PY  - 2024
T2  - IEEE Intelligent Transportation Systems Magazine
DO  - 10.1109/MITS.2023.3271447
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160228003&doi=10.1109%2fMITS.2023.3271447&partnerID=40&md5=ea15d0daa74a5d39bf0b3099904d1267
AB  - Visual analytics is vital for identifying targets by differentiating their structure and texture from unmanned aerial vehicles (UAVs). Object sensing disturbance and swift texture differentiation are tedious due to the UAV displacements. For improving the accuracy of target detection, this article introduces a learning optimizer-based visual analytics method. The proposed method assimilates deep learning and a gradient descent algorithm for feature differentiation and error minimization concurrently. The captured images are identified using multiple structural feature variations and are correlated with similar stored images. The features are extracted at different displacement and structural changing instances for leveraging accuracy. The learning process trains the similarity features during different differentiation factors. In the feature extraction, the minimum slope points are identified using a gradient descent algorithm by assigning random weights. As the differentiation increases using similar features, the minimum similarity value is detection. Postdetection, the weights are incremental and linear across different feature slopes. Therefore, the accuracy increases under varying displacement instances, preventing target misdetection. The gradient function is invariable between the minimum and maximum values for identifying high-precision features. This ensures optimal detection of different buildings and structures with high accuracy.  © 2009-2012 IEEE.
KW  - Aircraft detection
KW  - Antennas
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Energy utilization
KW  - Extraction
KW  - Feature extraction
KW  - Interactive computer systems
KW  - Object detection
KW  - Object recognition
KW  - Optimization
KW  - Textures
KW  - Unmanned aerial vehicles (UAV)
KW  - Visualization
KW  - Aerial vehicle
KW  - Analytic method
KW  - Energy-consumption
KW  - Features extraction
KW  - Objects detection
KW  - Optimizers
KW  - Real - Time system
KW  - Surveillance
KW  - Targets tracking
KW  - Visual analytics
KW  - Real time systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Saini, V.
AU  - Kantipudi, M.P.
AU  - Meduri, P.
TI  - Enhanced SSD Algorithm-Based Object Detection and Depth Estimation for Autonomous Vehicle Navigation
PY  - 2023
T2  - International Journal of Transport Development and Integration
DO  - 10.18280/ijtdi.070408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182652677&doi=10.18280%2fijtdi.070408&partnerID=40&md5=40ec75062cae16029c72767e497bfb58
AB  - Autonomous vehicles necessitate robust stability and safety mechanisms for effective navigation, relying heavily upon advanced perception and precise environmental awareness. This study addresses the object detection challenge intrinsic to autonomous navigation, with a focus on the system architecture and the integration of cutting-edge hardware and software technologies. The efficacy of various object recognition algorithms, notably the Single Shot Detector (SSD) and You Only Look Once (YOLO), is rigorously compared. Prior research has indicated that SSD, when augmented with depth estimation techniques, demonstrates superior performance in real-time applications within complex environments. Consequently, this research proposes an optimized SSD algorithm paired with a Zed camera system. Through this integration, a notable improvement in detection accuracy is achieved, with a precision increase to 87%. This advancement marks a significant step towards resolving the critical challenges faced by autonomous vehicles in object detection and distance estimation, thereby enhancing their operational safety and reliability. © 2023 WITPress. All rights reserved.
KW  - autonomous vehicle
KW  - camera
KW  - LiDAR
KW  - object recognition
KW  - perception & vision
KW  - RADAR
KW  - safe driving performance
KW  - SSD
KW  - YOLO
KW  - ZED-Camera
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liao, L.
AU  - Luo, L.
AU  - Su, J.
AU  - Xiao, Z.
AU  - Zou, F.
AU  - Lin, Y.
TI  - Eagle-YOLO: An Eagle-Inspired YOLO for Object Detection in Unmanned Aerial Vehicles Scenarios
PY  - 2023
T2  - Mathematics
DO  - 10.3390/math11092093
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159216871&doi=10.3390%2fmath11092093&partnerID=40&md5=7606d68011da7b6d2192eb69a4e4da88
AB  - Object detection in images taken by unmanned aerial vehicles (UAVs) is drawing ever-increasing research interests. Due to the flexibility of UAVs, their shooting altitude often changes rapidly, which results in drastic changes in the scale size of the identified objects. Meanwhile, there are often many small objects obscured from each other in high-altitude photography, and the background of their captured images is also complex and variable. These problems lead to a colossal challenge with object detection in UAV aerial photography images. Inspired by the characteristics of eagles, we propose an Eagle-YOLO detection model to address the above issues. First, according to the structural characteristics of eagle eyes, we integrate the Large Kernel Attention Module (LKAM) to enable the model to find object areas that need to be focused on. Then, in response to the eagle’s characteristic of experiencing dramatic changes in its field of view when swooping down to hunt at high altitudes, we introduce a large-sized feature map with rich information on small objects into the feature fusion network. The feature fusion network adopts a more reasonable weighted Bi-directional Feature Pyramid Network (Bi-FPN). Finally, inspired by the sharp features of eagle eyes, we propose an IoU loss named Eagle-IoU loss. Extensive experiments are performed on the VisDrone2021-DET dataset to compare it with the baseline model YOLOv5x. The experiments showed that Eagle-YOLO outperformed YOLOv5x by 2.86% and 4.23% in terms of the mAP and AP50, respectively, which demonstrates the effectiveness of Eagle-YOLO for object detection in UAV aerial image scenes. © 2023 by the authors.
KW  - attentional mechanisms
KW  - Eagle-YOLO
KW  - object detection
KW  - unmanned aerial vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Leng, J.
AU  - Mo, M.
AU  - Zhou, Y.
AU  - Ye, Y.
AU  - Gao, C.
AU  - Gao, X.
TI  - Recent advances in drone-view object detection
ST  - 无人机视角下的目标检测研究进展
PY  - 2023
T2  - Journal of Image and Graphics
DO  - 10.11834/jig.220836
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172764983&doi=10.11834%2fjig.220836&partnerID=40&md5=a50918020831a26b73bf221f753b5f96
AB  - Given the support of artificial intelligence technology，drones have initially acquired intelligent sensing capabilities and have demonstrated efficient and flexible data collection in practical applications. Drone-view object detection，which aims to locate specific objects in aerial images，plays an irreplaceable role in many fields and has important research significance. For example，drones with highly mobile and flexible deployment have remarkable advantages in accident handling，order management，traffic guidance，and flow detection，making them irreplaceable in traffic monitoring. As for disaster emergency rescue，drones with aerial vision and high mobility can achieve efficient search and safe rescue in large areas，locate people quickly and accurately in distress，and help rescuers control the situation，thereby ensuring the safety of people in distress. This study provides a comprehensive summary of the challenges in object detection based on the unmanned aerial vehicle（UAV）perspective to portray further the development of drone-view object detection. The existing algorithms and related datasets are also introduced. First，this study briefly introduces the concept of object detection in drone view and summarizes the five imbalance challenges in object detection in drone view，such as scale imbalance，spatial imbalance，class imbalance，semantic imbalance，and objective imbalance. This study analyzes and summarizes the challenges of drone-view object detection based on the aforementioned imbalances by using quantitative data analysis and visual qualitative analysis. 1）Object scale imbalance is the most focused challenge in current research. It comes from the unique aerial view of drones. The changes in the drone’s height and angle bring drastic changes to the object scale in the acquired images. The distance of the lens from the photographed object under the drone view is often far. This scenario results in numerous small objects in the image and makes capturing useful features for object detection difficult for the existing detectors. 2）Different regions of drone-view images have great differences，and most objects are concentrated in the minor area of images，i. e. ，the spatial distribution of objects is enormously uneven. On the one hand，the clustering of dense objects in small areas generates occlusion. The detection model needs to devote considerable attention to this occlusion to distinguish different objects effectively. On the other hand，treating equally different areas wastes many computational resources in vanilla areas，limiting the improvement of object detection performance. 3）The problem of class imbalance in the drone view is divided into two categories. One is the positive-negative sample imbalance problem caused by the gap between the front and rear views shared in the image. The other is the imbalanced numbers of different categories caused by the number of samples in the real world. 4）The semantic pieces of information defined by different category labels in the drone-view object detection dataset are often similar，resulting in only subtle differences between different categories. However，significantly different representations of objects exist in the same category，which together form the semantic imbalance problem. 5）Drone-view object detection often faces the problem of unbalanced optimization targets，i. e. ，the contradiction between the high computational demand for high-resolution images and the limited computing power of low-power chips is difficult to balance. These unbalanced problems bring enormous challenges to object detection from the UAV viewpoint. However，even the most advanced object detection algorithms currently available can hardly achieve an average accuracy rate of 40% on aerial images，which is far below the performance of general object detection tasks. Therefore，many scholars have conducted many studies. These research methods can be summarized as optimization ideas to solve these imbalance problems. In this study，we collect relevant research works，which are sorted and analyzed according to the countries of authors，institutions，published journals or conferences，years，the category of methods，and the solved problem. The present study presents the challenging problems solved by previous research and the development trends of existing methods. This study also focuses on the methods of improving drone-view object detection performance in terms of data augmentation，multiscale feature fusion，region searching strategies，multitask learning，and lightweight model. The advantages and disadvantages of these methods for different problems are systematically summarized and analyzed. Besides introducing existing methods，the present study compiles and introduces the applications of drone-view object detection in practical scenarios，such as traffic monitoring，power inspection，crop analysis，and disaster rescue. These applications further emphasize the significance of object detection in drone view. Then，this study collects and organizes UAV datasets suitable for object detection tasks. These datasets are present from various perspectives，such as year，published journals or conferences，annotation information，and number of citations. In particular，the present study provides the performance evaluation of the existing algorithms on two commonly used public datasets. The presentation of these performance data is expected to help researchers understand the current state of development of drone-view object detection and promote further development in this field. Finally，this study provides an outlook on the future direction of drone-view object detection by considering the aforementioned imbalance problems. The promising research includes the following：1）data augmentation：providing the network with enough high-quality learning samples by considering the specific characteristics of drone-view images based on the conventional data augmentation strategy is a good idea；2）multiscale representation：how to avoid the interference of background noise in feature fusion and effectively extract information at different scales using an efficient fusion strategy is an urgent problem to be solved；3）visual inference：using information unique to the viewpoint of drones，mining contextual information from images to facilitate image recognition，and using easy-to-detect objects to improve the performance of difficult-to-detect objects are directions worthy of deep consideration. © 2023 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.
KW  - aerial image
KW  - computer vision
KW  - deep learning
KW  - object detection
KW  - review
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, H.
AU  - Liu, Y.
AU  - Cheng, Y.
TI  - DRIO: Robust Radar-Inertial Odometry in Dynamic Environments
PY  - 2023
T2  - IEEE Robotics and Automation Letters
DO  - 10.1109/LRA.2023.3301290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166746147&doi=10.1109%2fLRA.2023.3301290&partnerID=40&md5=c3f6974187aedaaedfeeeeffa5756474
AB  - Accurate and robust localization is essential for mobile robots. Recently, millimeter wave (mmWave) radars have been widely used for odometry, owing to their robustness to all-weather conditions, lightweight and low cost. However, existing radar-based odometry methods degrade severely in high-dynamic environments. In this letter, we propose a robust radar-inertial odometry method for high-dynamic environments (DRIO) by exploiting the ground, an ever-present static target that is unaffected by the dynamic environments. The points of the ground surface were traditionally treated as clutter points in previous works due to their unstable distribution. We overcome this limitation by detecting ground points using both Doppler and geometric characteristics. During the detection process, accurate radar velocity is jointly estimated, which is then fused with inertial data to obtain the odometry. The real-world evaluations indicate that the proposed method achieves robust and Lidar-level localization in complex dynamic environments. In addition to odometry, our method can effectively improve the quality of radar point clouds for subsequent perception tasks.  © 2016 IEEE.
KW  - Dynamic environments
KW  - localization
KW  - millimeter wave radar
KW  - odometry
KW  - Doppler radar
KW  - Millimeter waves
KW  - Optical radar
KW  - Tracking radar
KW  - Dynamic environments
KW  - Localisation
KW  - Millimeter-wave radar
KW  - Millimeterwave communications
KW  - Millimetre-wave radar
KW  - Odometry
KW  - Point cloud compression
KW  - Point-clouds
KW  - Radar measurement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Luo, T.
AU  - Wang, H.
AU  - Cai, Y.
AU  - Chen, L.
AU  - Wang, K.
AU  - Yu, Y.
TI  - Binary residual feature pyramid network: An improved feature fusion module based on double-channel residual pyramid structure for autonomous detection algorithm
PY  - 2023
T2  - IET Intelligent Transport Systems
DO  - 10.1049/itr2.12291
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139703301&doi=10.1049%2fitr2.12291&partnerID=40&md5=0fdc8e69ce21c9a4d5d7764368686e51
AB  - The vehicle detection algorithm based on visual perception has been applied in all types of automatic driving scenes. However, there are still flaws in the current detection algorithm model, especially for small objects. The detection effect of vehicle objects with small pixels in the image is often missed and wrongly detected. This research proposes an improved feature fusion module based on double-channel residual pyramid (DRP) structure for autonomous detection algorithm which named binary residual feature pyramid network (BiResFPN) to solve the above problems. Firstly, a DRP structure, which can effectively supplement the shallow information of the network, is proposed. The residual structure is added to the output feature map for further supplement. Then, an average sampling method of positive and negative samples based on intersection-over-union (IOU) value is proposed on the basis of this structure, aimed at the unbalanced sampling of positive and negative samples in the training stage of faster regions with CNN features (RCNN). It leads to the reduction of the interference of a large number of simple negative samples, which makes the learned model better. The experimental results based on the KITTI and BDD100K dataset datasets show that the capability of the feature fusion module based on DRP structure is strong for small object detection. Compared with Faster-RCNN (FPN), the detection algorithm of small object detection accuracy APsmall was increased by 2.6%, APmedium and APlarge was increased by 1.1% and 0.3%. © 2022 The Authors. IET Intelligent Transport Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.
KW  - Automobile drivers
KW  - Feature extraction
KW  - Intelligent systems
KW  - Object recognition
KW  - Signal detection
KW  - Traffic control
KW  - Autonomous detection
KW  - Detection algorithm
KW  - Double-channel
KW  - Feature pyramid
KW  - Features fusions
KW  - Fusion modules
KW  - Module-based
KW  - Negative samples
KW  - Pyramid network
KW  - Pyramid structure
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Y.
AU  - Fang, X.
AU  - Guo, J.
AU  - Wang, L.
AU  - Tian, H.
AU  - Yan, K.
AU  - Lan, Y.
TI  - CURI-YOLOv7: A Lightweight YOLOv7tiny Target Detector for Citrus Trees from UAV Remote Sensing Imagery Based on Embedded Device
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15194647
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174145889&doi=10.3390%2frs15194647&partnerID=40&md5=d21f3fd3372a46ce1f6ec6ebb0948256
AB  - Data processing of low-altitude remote sensing visible images from UAVs is one of the hot research topics in precision agriculture aviation. In order to solve the problems of large model size with slow detection speed that lead to the inability to process images in real time, this paper proposes a lightweight target detector CURI-YOLOv7 based on YOLOv7tiny which is suitable for individual citrus tree detection from UAV remote sensing imagery. This paper augmented the dataset with morphological changes and Mosica with Mixup. A backbone based on depthwise separable convolution and the MobileOne-block module was designed to replace the backbone of YOLOv7tiny. SPPF (spatial pyramid pooling fast) was used to replace the original spatial pyramid pooling structure. Additionally, we redesigned the neck by adding GSConv and depth-separable convolution and deleted its input layer from the backbone with a size of (80, 80) and its output layer from the head with a size of (80, 80). A new ELAN structure was designed, and the redundant convolutional layers were deleted. The experimental results show that the GFLOPs = 1.976, the parameters = 1.018 M, the weights = 3.98 MB, and the mAP = 90.34% for CURI-YOLOv7 in the UAV remote sensing imagery of the citrus trees dataset. The detection speed of a single image is 128.83 on computer and 27.01 on embedded devices. Therefore, the CURI-YOLOv7 model can basically achieve the function of individual tree detection in UAV remote sensing imagery on embedded devices. This forms a foundation for the subsequent UAV real-time identification of the citrus tree with its geographic coordinates positioning, which is conducive to the study of precise agricultural management of citrus orchards. © 2023 by the authors.
KW  - citrus trees
KW  - lightweight
KW  - remote sensing
KW  - target detector
KW  - YOLOv7
KW  - Aircraft detection
KW  - Chemical detection
KW  - Convolution
KW  - Data handling
KW  - Remote sensing
KW  - Citrus tree
KW  - Detection speed
KW  - Embedded device
KW  - Lightweight
KW  - Remote sensing imagery
KW  - Remote-sensing
KW  - Spatial pyramids
KW  - Target detectors
KW  - UAV remote sensing
KW  - YOLOv7
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Gao, X.
AU  - Kanu-Asiegbu, A.M.
AU  - Du, X.
TI  - MambaST: A Plug-and-Play Cross-Spectral Spatial-Temporal Fuser for Efficient Pedestrian Detection
PY  - 2024
T2  - IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC
DO  - 10.1109/ITSC58415.2024.10920115
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001671411&doi=10.1109%2fITSC58415.2024.10920115&partnerID=40&md5=8f672dafcbf01823eae019971ae2e56c
AB  - This paper proposes MambaST, a plug-and-play cross-spectral spatial-temporal fusion pipeline for efficient pedestrian detection. Several challenges exist for pedestrian detection in autonomous driving applications. First, it is difficult to perform accurate detection using RGB cameras under dark or low-light conditions. Cross-spectral systems must be developed to integrate complementary information from multiple sensor modalities, such as thermal and visible cameras, to improve the robustness of the detections. Second, pedestrian detection models are latency-sensitive. Efficient and easy-to-scale detection models with fewer parameters are highly desirable for real-time applications such as autonomous driving. Third, pedestrian video data provides spatial-temporal correlations of pedestrian movement. It is beneficial to incorporate temporal as well as spatial information to enhance pedestrian detection. This work leverages recent advances in the state space model (Mamba) and proposes a novel Multi-head Hierarchical Patching and Aggregation (MHHPA) structure to extract both fine-grained and coarse-grained information from both RGB and thermal imagery. Experimental results show that the proposed MHHPA is an effective and efficient alternative to a Transformer model for cross-spectral pedestrian detection. Our proposed model also achieves superior performance on small-scale pedestrian detection. The code is available at https://github.com/XiangboGaoBarry/MambaST © 2024 IEEE.
KW  - Image coding
KW  - Image enhancement
KW  - Video recording
KW  - Autonomous driving
KW  - Low light conditions
KW  - Multiple sensors
KW  - Pedestrian detection
KW  - Plug-and-play
KW  - RGB cameras
KW  - Sensor modality
KW  - Spatial temporals
KW  - Thermal camera
KW  - Visible cameras
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kan, X.
AU  - Shi, G.
AU  - Yang, X.
TI  - Dense Mapping Method for Indoor Dynamic Scenes Using an Improved ORB-SLAM2 Algorithm Based on RGB-D
PY  - 2024
T2  - Proceedings of 2024 IEEE 25th China Conference on System Simulation Technology and its Application, CCSSTA 2024
DO  - 10.1109/CCSSTA62096.2024.10691846
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207457782&doi=10.1109%2fCCSSTA62096.2024.10691846&partnerID=40&md5=2557be1aa8aef51887481990c4a71550
AB  - Leveraging instance segmentation techniques within Simultaneous Localization and Mapping (SLAM) frameworks effectively tackles the challenges associated with dynamic environments. While instance segmentation algorithms enhance SLAM capabilities, it may come at the expense of real-time performance. Additionally, camera motion-induced jitter may lead to inaccurate keyframe selection and tracking loss. To address the performance bottleneck, this work introduces a novel algorithm that integrates YOLOv5 with ORB-SLAM2, aiming to achieve both real-time operation and improved localization accuracy. In the tracking thread, dynamic objects are identified and their feature points are filtered out, striking a balance between real-time performance and localization accuracy. Furthermore, a keyframe selection criterion based on inter-frame relative motion is introduced to improve the accuracy of keyframe selection. Finally, the algorithm achieves the construction of dense static point cloud maps in dynamic scenes, thereby obtaining stable dense point cloud maps. Experimental results on the walking_xyz dataset show that compared to ORBSLAM2, the proposed algorithm reduces the average processing time by 38.5% and improves the root mean square accuracy by 95.2%. The experimental results indicate that compared to ORB-SLAM2, the proposed algorithm reduces the average processing time by 38.5% and improves the root mean square accuracy by 95.2% on the walking_xyz dataset. The proposed algorithm effectively addresses the aforementioned challenges, en hancing the localization accuracy and precision of SLAM algorithms, and thereby improving the usability of maps. © 2024 IEEE.
KW  - dynamic scene
KW  - keyframe selection
KW  - ORB-SLAM2
KW  - VSLAM
KW  - YOLOv5
KW  - Image coding
KW  - Image segmentation
KW  - SLAM robotics
KW  - Timing jitter
KW  - Cloud map
KW  - Dynamic scenes
KW  - Key frame selection
KW  - Localization accuracy
KW  - ORB-SLAM2
KW  - Point-clouds
KW  - Real time performance
KW  - Simultaneous localization and mapping
KW  - VSLAM
KW  - YOLOv5
KW  - Mapping
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Beg, M.S.
AU  - Ismail, M.Y.
AU  - Saef Ullah Miah, M.
AU  - Peeie, M.H.
TI  - Enhancing Driving Assistance System with YOLO V8-Based Normal Visual Camera Sensor
PY  - 2023
T2  - Journal of Advanced Research in Applied Sciences and Engineering Technology
DO  - 10.37934/ARASET.31.1.226236
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170053893&doi=10.37934%2fARASET.31.1.226236&partnerID=40&md5=ec998974a41a7df78c307b6aafbc999b
AB  - One of the safety features that can alert drivers to the presence of other vehicles and reduce the risk of collisions is vehicle detection. In this study, the objective is to setup a driving support system for detecting vehicles, motorcycles, and traffic signals on the roads near to Universiti Malaysia Pahang using object detection techniques. The video was taken through a direct camera to capture video footage of traffic objects on the roads in the district, which was then analysed using the YOLO-V8 deep learning algorithm. The system was trained on a primary dataset of 1,068 images, with 70% of the dataset used for training, 20% for testing and 10% for validation. After conducting a performance validation, the system achieved a mean average precision (mAP) of 88.2% on train dataset and was able to detect different types of vehicles such as cars, motorcycles, and traffic lights. The results of this study could be beneficial for road safety authorities and researchers interested in developing intelligent transportation systems. © 2023, Penerbit Akademia Baru. All rights reserved.
KW  - Deep Learning
KW  - driving assisting
KW  - image processing
KW  - Object detection
KW  - Yolo-V8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ma, N.
AU  - Han, T.
AU  - Xia, Q.
AU  - Han, Y.
TI  - Road Information Recognition and Decision Making Based on YOLO Algorithm
PY  - 2024
T2  - Proceedings - 2024 3rd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics, AIHCIR 2024
DO  - 10.1109/AIHCIR65563.2024.00041
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004930140&doi=10.1109%2fAIHCIR65563.2024.00041&partnerID=40&md5=431420ef36258ba3b8c820a580155689
AB  - Autonomous driving technology is a revolutionary technology in the field of modern transportation, which has developed rapidly in recent years. However, it is greatly affected by the environment, such as the change of lighting (the difference in detection performance under different lighting conditions) and weather conditions (the impact of bad weather such as rain, fog, and snow). Hence, we present an exclusively visual, real-time perception system that can simultaneously identify traffic signals, pedestrians, signage, road markings, vehicles and delineate areas suitable for driving. By integrating YOLOv5s with YOLOP, we have trained multiple datasets to develop a sophisticated detection model. Leveraging this refined model along with a monocular distance measurement algorithm, we have successfully implemented decision control. The training results demonstrate that the system has effectively mitigated the impact of environmental factors, exhibiting strong robustness. Furthermore, it has met the stringent requirements for speed and accuracy in autonomous driving, yielding commendable outcomes. © 2024 IEEE.
KW  - monocular distance measurement
KW  - vehicle behavior decision
KW  - yolo
KW  - Traffic signs
KW  - Vehicle performance
KW  - Autonomous driving
KW  - Behavior decision
KW  - Decisions makings
KW  - Detection performance
KW  - Information recognition
KW  - Monocular distance measurement
KW  - Revolutionary technology
KW  - Vehicle behavior
KW  - Vehicle behavior decision
KW  - Yolo
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Heo, J.
AU  - Novlan, T.
AU  - Akoum, S.
AU  - Gavrilovska, A.
TI  - GT-Craft: A Framework for Fast Prototyping Geospatial-Based Digital Twins in Unity 3D
PY  - 2024
T2  - Proceedings - 2024 IEEE/ACM Symposium on Edge Computing, SEC 2024
DO  - 10.1109/SEC62691.2024.00043
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216721694&doi=10.1109%2fSEC62691.2024.00043&partnerID=40&md5=cec19794eb4d92f8da2cc819ddaee055
AB  - A digital twin presents promising opportunities and potential benefits for various industrial use cases by enabling simulation and prediction on the virtual representation of the real-world environment. However, the implementation and maintenance costs for the digital twin are prohibitively high, restricting its widespread adoption. To address this issue, we present a framework, GT-Craft, which enables fast prototyping the geospatial-based digital twin at scale. GT-Craft automates the generation of the digital twin by using the streamed geospatial data and the semantic information extracted from deep neural network (DNN) models. As GT-Craft generates digital twins on the Unity game engine, the Unity-based simulators and game applications can seamlessly use the digital twins generated by GT-Craft. The presented framework is compatible with non-Unity-based applications and existing 3D software and simulation tools, e.g., Blender, Apple Reality Composer, and NVIDIA Omniverse, as it supports exporting the generated digital twin in the universal scene description (USD) format, which is an emerging industrial open standard for exchanging and editing 3D contents.  © 2024 IEEE.
KW  - cyber physical system
KW  - digital twin
KW  - edge computing
KW  - game engine
KW  - unity
KW  - universal scene description
KW  - Deep neural networks
KW  - Digital elevation model
KW  - Edge computing
KW  - Three dimensional computer graphics
KW  - Virtual reality
KW  - Cybe-physical systems
KW  - Cyber-physical systems
KW  - Edge computing
KW  - Fast prototyping
KW  - Game Engine
KW  - Geo-spatial
KW  - Potential benefits
KW  - Scene description
KW  - Unity
KW  - Universal scene description
KW  - Application programs
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Saltori, C.
AU  - Galasso, F.
AU  - Fiameni, G.
AU  - Sebe, N.
AU  - Poiesi, F.
AU  - Ricci, E.
TI  - Compositional Semantic Mix for Domain Adaptation in Point Cloud Segmentation
PY  - 2023
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
DO  - 10.1109/TPAMI.2023.3310261
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169678738&doi=10.1109%2fTPAMI.2023.3310261&partnerID=40&md5=22dab8de0d5024d171ec2f85b1a0623f
AB  - Deep-learning models for 3D point cloud semantic segmentation exhibit limited generalization capabilities when trained and tested on data captured with different sensors or in varying environments due to domain shift. Domain adaptation methods can be employed to mitigate this domain shift, for instance, by simulating sensor noise, developing domain-agnostic generators, or training point cloud completion networks. Often, these methods are tailored for range view maps or necessitate multi-modal input. In contrast, domain adaptation in the image domain can be executed through sample mixing, which emphasizes input data manipulation rather than employing distinct adaptation modules. In this study, we introduce compositional semantic mixing for point cloud domain adaptation, representing the first unsupervised domain adaptation technique for point cloud segmentation based on semantic and geometric sample mixing. We present a two-branch symmetric network architecture capable of concurrently processing point clouds from a source domain (e.g. synthetic) and point clouds from a target domain (e.g. real-world). Each branch operates within one domain by integrating selected data fragments from the other domain and utilizing semantic information derived from source labels and target (pseudo) labels. Additionally, our method can leverage a limited number of human point-level annotations (semi-supervised) to further enhance performance. We assess our approach in both synthetic-to-real and real-to-real scenarios using LiDAR datasets and demonstrate that it significantly outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.  © 1979-2012 IEEE.
KW  - Domain adaptation
KW  - point cloud
KW  - semantic segmentation
KW  - semi-supervised learning
KW  - unsupervised learning
KW  - Computer architecture
KW  - Deep learning
KW  - Mixing
KW  - Network architecture
KW  - Semantic Segmentation
KW  - Supervised learning
KW  - Compositional semantics
KW  - Domain adaptation
KW  - Point cloud compression
KW  - Point-clouds
KW  - Semantic segmentation
KW  - Semi-supervised learning
KW  - Task analysis
KW  - Three-dimensional display
KW  - article
KW  - human
KW  - Semantics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Li, L.
AU  - Xu, P.
TI  - Visual Sensing and Depth Perception for Welding Robots and Their Industrial Applications
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23249700
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180252930&doi=10.3390%2fs23249700&partnerID=40&md5=90d758248938af1214b013662d001018
AB  - With the rapid development of vision sensing, artificial intelligence, and robotics technology, one of the challenges we face is installing more advanced vision sensors on welding robots to achieve intelligent welding manufacturing and obtain high-quality welding components. Depth perception is one of the bottlenecks in the development of welding sensors. This review provides an assessment of active and passive sensing methods for depth perception and classifies and elaborates on the depth perception mechanisms based on monocular vision, binocular vision, and multi-view vision. It explores the principles and means of using deep learning for depth perception in robotic welding processes. Further, the application of welding robot visual perception in different industrial scenarios is summarized. Finally, the problems and countermeasures of welding robot visual perception technology are analyzed, and developments for the future are proposed. This review has analyzed a total of 2662 articles and cited 152 as references. The potential future research topics are suggested to include deep learning for object detection and recognition, transfer deep learning for welding robot adaptation, developing multi-modal sensor fusion, integrating models and hardware, and performing a comprehensive requirement analysis and system evaluation in collaboration with welding experts to design a multi-modal sensor fusion architecture. © 2023 by the authors.
KW  - 3D reconstruction
KW  - deep learning
KW  - depth perception
KW  - industrial applications
KW  - welding robot
KW  - welding sensor
KW  - Binocular vision
KW  - Image reconstruction
KW  - Industrial robots
KW  - Intelligent robots
KW  - Machine design
KW  - Object detection
KW  - Robot vision
KW  - Stereo image processing
KW  - Welding
KW  - 3D reconstruction
KW  - Deep learning
KW  - Multimodal sensor
KW  - Sensor fusion
KW  - Vision sensing
KW  - Visual depth
KW  - Visual perception
KW  - Visual sensing
KW  - Welding robots
KW  - Welding sensor
KW  - adaptation
KW  - artificial intelligence
KW  - binocular vision
KW  - computer
KW  - controlled study
KW  - deep learning
KW  - depth perception
KW  - human
KW  - monocular vision
KW  - review
KW  - robotics
KW  - sensor
KW  - welding
KW  - Deep learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Sun, W.
TI  - Analysis of aerial images for identification of houses using big data, UAV photography and neural network
PY  - 2023
T2  - Soft Computing
DO  - 10.1007/s00500-023-08967-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164938690&doi=10.1007%2fs00500-023-08967-3&partnerID=40&md5=ddc8e6a019bfc2e8a5446160f9af9d03
AB  - Computer vision has undergone significant transformation owing to deep learning in the last two decades. Deep convolutional networks have been successfully applied for various applications to learn different tasks related to vision, such as image classification, image segmentation, and object detection. Deep learning models can generate fine-tuned results by transferring knowledge to large generic datasets. This study aims to conduct an in-depth analysis of a big data tracking algorithm for aerial images of unmanned aerial vehicles (UAVs) to detect houses using neural networks to address the low accuracy and efficiency of manual detection in remote areas by mitigating the associated security risks. In the context of big data, a UAV-based preprocessing method is discussed for images using guided filtering. In order to reduce the impact of radiation distortion on the color and brightness of UAV-based aerial images of houses, a histogram matching method was applied. The guided filtering method is used to solve the problem of imaging details of houses that are not apparent after smoothing and denoising the aerial images. A house detection algorithm based on a deep neural network is then applied to the UAV images to detect the images of houses, and the time consumption of the deep learning operation is examined within the context of big data. Combining deep separation convolution and calculation optimization with YOLOv2 improves the house's image detection in real-time while preserving an accurate performance of UAV-based aerial images to detect houses by combining the YOLOv2 detection framework. The results of the experiments indicate that the proposed method can improve the efficiency and accuracy of house detection using aerial images and has certain practical applications. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
KW  - Big data tracking
KW  - Conventional neural network
KW  - Deep neural network detection algorithm
KW  - Guided filtering method
KW  - House inspection
KW  - UAV aerial photography
KW  - Aerial photography
KW  - Aircraft detection
KW  - Antennas
KW  - Convolution
KW  - Deep neural networks
KW  - Houses
KW  - Image enhancement
KW  - Image segmentation
KW  - Large dataset
KW  - Learning systems
KW  - Object detection
KW  - Risk assessment
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Big data tracking
KW  - Conventional neural network
KW  - Data-tracking
KW  - Deep neural network detection algorithm
KW  - Detection algorithm
KW  - Filtering method
KW  - Guided filtering
KW  - Guided filtering method
KW  - House inspection
KW  - Network detections
KW  - Neural-networks
KW  - Unmanned aerial vehicle aerial photography
KW  - Efficiency
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhu, J.
AU  - Yang, Y.
AU  - Cheng, Y.
TI  - A Millimeter-Wave Radar-Aided Vision Detection Method for Water Surface Small Object Detection
PY  - 2023
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse11091794
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172779700&doi=10.3390%2fjmse11091794&partnerID=40&md5=5376a7f079ba21d35da939450fcdea5f
AB  - Unmanned surface vehicles (USVs) have wide applications in marine inspection and monitoring, terrain mapping, and water surface cleaning. Accurate and robust environment perception ability is essential for achieving autonomy in USVs. Small object detection on water surfaces is an important environment perception task, typically achieved by visual detection using cameras. However, existing vision-based small object detection methods suffer from performance degradation in complex water surface environments. Therefore, in this paper, we propose a millimeter-wave (mmWave) radar-aided vision detection method that enables automatic data association and fusion between mmWave radar point clouds and images. Through testing on real-world data, the proposed method demonstrates significant performance improvement over vision-based object detection methods without introducing more computational costs, making it suitable for real-time application on USVs. Furthermore, the image–radar data association model in the proposed method can serve as a plug-and-play module for other object detection methods. © 2023 by the authors.
KW  - object detection
KW  - unmanned surface vehicle
KW  - visual–radar fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, T.
AU  - Pan, Y.
TI  - Real-time detection of a camouflaged object in unstructured scenarios based on hierarchical aggregated attention lightweight network
PY  - 2023
T2  - Advanced Engineering Informatics
DO  - 10.1016/j.aei.2023.102082
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243140&doi=10.1016%2fj.aei.2023.102082&partnerID=40&md5=9a1f8a0fb407d44dbcf2fd1828ef2141
AB  - Accurate and real-time grasping of household items is a challenge for home service robots in complex indoor environments. In this paper, using transparent cups as the recognition object, an accurate, reliable, and real-time visual algorithm is proposed for the elderly who needs nursing care to solve the recognition problems of overlapping, blurred, and small cups in complex unstructured indoor scenes. In order to meet the lightweight deployment of home service robots while considering both high-scale and low-scale semantic features, this paper designs a multi-scale fusion lightweight backbone network based on the Split Shuffle Block (SSB) and Group Shuffle Block (GSB) feature encoding units. Among them, depthwise separable convolution (DwConv) uses to reduce the amount of coding unit parameters, and feature shuffle uses to promote information exchange and the ability of information expression. In order to accurately identify the transparent cup camouflaged in complex backgrounds, this paper proposed a lightweight feature enhancement module that combines multi-scale hierarchical aggregation attention and multi-branch parallel convolution structure. The proposed module used an adaptive weighting strategy and a channel normalization weighting strategy to highlight the active regions of boundary features in each branch feature map, enhance the exchange of boundary information, and reduce the loss of detailed information. The experimental results on the IndoorCup and MobileCup datasets show that the detection accuracy of the proposed method is 93.6% and 92.6%, respectively, and the model calculation amount is only 0.91 M, which can lightweight deploy on indoor mobile robots for real-time detection. From the results of qualitative comparisons, the proposed method has strong robustness. It can effectively suppress false and missed detection caused by background interference. Likewise, it also effectively identifies camouflage objects and small objects in complex backgrounds. © 2023 Elsevier Ltd
KW  - Camouflaged object
KW  - Hierarchical aggregated attention
KW  - Indoor mobile robot
KW  - Lightweight backbone
KW  - Unstructured scenarios
KW  - Complex networks
KW  - Machine design
KW  - Mobile robots
KW  - Object detection
KW  - Semantics
KW  - Signal detection
KW  - Camouflaged object
KW  - Complex background
KW  - Hierarchical aggregated attention
KW  - Home service robot
KW  - Indoor mobile robots
KW  - Lightweight backbone
KW  - Real- time
KW  - Real-time detection
KW  - Unstructured scenario
KW  - Weighting strategies
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jia, F.
AU  - Afaq, M.
AU  - Ripka, B.
AU  - Huda, Q.
AU  - Ahmad, R.
TI  - Vision- and Lidar-Based Autonomous Docking and Recharging of a Mobile Robot for Machine Tending in Autonomous Manufacturing Environments
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app131910675
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174214905&doi=10.3390%2fapp131910675&partnerID=40&md5=66eb5ca7e384e1f5b3d27d06dbc54f33
AB  - Autonomous docking and recharging are among the critical tasks for autonomous mobile robots that work continuously in manufacturing environments. This requires robots to demonstrate the following abilities: (i) detecting the charging station, typically in an unstructured environment and (ii) autonomously docking to the charging station. However, the existing research, such as that on infrared range (IR) sensor-based, vision-based, and laser-based methods, identifies many difficulties and challenges, including lighting conditions, severe weather, and the need for time-consuming computation. With the development of deep learning techniques, real-time object detection methods have been widely applied in the manufacturing field for the recognition and localization of target objects. Nevertheless, those methods require a large amount of proper and high-quality data to achieve a good performance. In this study, a Hikvision camera was used to collect data from a charging station in a manufacturing environment; then, a dataset for the wireless charger was built. In addition, the authors of this paper propose an autonomous docking and recharging method based on the deep learning model and the Lidar sensor for a mobile robot operating in a manufacturing environment. In the proposed method, a YOLOv7-based object detection method was developed, trained, and evaluated to enable the robot to quickly and accurately recognize the charging station. Mobile robots can achieve autonomous docking to the charging station using the proposed Lidar-based approach. Compared to other methods, the proposed method has the potential to improve recognition accuracy and efficiency and reduce the computation costs for the mobile robot system in various manufacturing environments. The developed method was tested in real-world scenarios and achieved an average accuracy of 95% in recognizing the target charging station. This vision-based charger detection method, if fused with the proposed Lidar-based docking method, can improve the overall accuracy of the docking alignment process. © 2023 by the authors.
KW  - 3D Lidar
KW  - autonomous docking
KW  - autonomous recharging
KW  - computer vision
KW  - manufacturing environments
KW  - mobile robots
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Guo, Y.
AU  - Tian, X.
AU  - Xiao, Y.
TI  - DBCR-YOLO: Improved YOLOv5 based on double-sampling and broad-feature coordinate-attention residual module for water surface object detection
PY  - 2023
T2  - Journal of Electronic Imaging
DO  - 10.1117/1.JEI.32.4.043013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173242522&doi=10.1117%2f1.JEI.32.4.043013&partnerID=40&md5=f53d47cb832234161262d12167c33b61
AB  - Unmanned missions have become more and more popular in recent years. The related technologies of unmanned ground vehicles and unmanned aerial vehicles are growing rapidly, but research on unmanned surface vehicles (USVs) is rare. Water surface object detection algorithms play a crucial role in the field of USVs. However, achieving an object detection algorithm that balances speed and accuracy in the presence of interference is a difficult challenge. We proposed a network, DBCR-YOLO, that improved the detection accuracy while meeting real-time requirements. Based on YOLOv5, we added an additional detection head for detecting tiny objects. Then, we replaced the downsampling in YOLOv5's backbone network with the proposed double sampling mechanism to solve the problem that paying attention to the key features of objects cannot be done in the downsampling process of YOLOv5. Finally, we substituted the proposed BCR neck for YOLOv5's neck, thus improving the fusion of features between different scales based on fewer parameters and fewer calculations. We tested our network on the water surface object detection dataset. Compared with YOLOv5, DBCR-YOLO improved the detection accuracy by 3.4%. At the same time, DBCR-YOLO achieved the highest accuracy in comparison with other networks.  © 2023 SPIE and IS&T.
KW  - unmanned surface vehicles
KW  - water surface object detection
KW  - YOLOv5
KW  - Antennas
KW  - Ground vehicles
KW  - Object recognition
KW  - Signal detection
KW  - Signal sampling
KW  - Aerial vehicle
KW  - Detection accuracy
KW  - Double-sampling
KW  - Down sampling
KW  - Object detection algorithms
KW  - Objects detection
KW  - Real time requirement
KW  - Water surface
KW  - Water surface object detection
KW  - YOLOv5
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kim, M.
AU  - Kim, H.
AU  - Sung, J.
AU  - Park, C.
AU  - Paik, J.
TI  - High-resolution processing and sigmoid fusion modules for efficient detection of small objects in an embedded system
PY  - 2023
T2  - Scientific Reports
DO  - 10.1038/s41598-022-27189-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145645329&doi=10.1038%2fs41598-022-27189-5&partnerID=40&md5=b86d5a24c70196ffb8d4da471dd922f5
AB  - Recent advances in deep learning realized accurate, robust detection of various types of objects including pedestrians on the road, defect regions in the manufacturing process, human organs in medical images, and dangerous materials passing through the airport checkpoint. Specifically, small object detection implemented as an embedded system is gaining increasing attention for autonomous vehicles, drone reconnaissance, and microscopic imagery. In this paper, we present a light-weight small object detection model using two plug-in modules: (1) high-resolution processing module (HRPM) and (2) sigmoid fusion module (SFM). The HRPM efficiently learns multi-scale features of small objects using a significantly reduced computational cost, and the SFM alleviates mis-classification errors due to spatial noise by adjusting weights on the lost small object information. Combination of HRPM and SFM significantly improved the detection accuracy with a low amount of computation. Compared with the original YOLOX-s model, the proposed model takes a two-times higher-resolution input image for higher mean average precision (mAP) using 57% model parameters and 71% computation in Gflops. The proposed model was tested using real drone reconnaissance images, and provided significant improvement in detecting small vehicles. © 2023, The Author(s).
KW  - Airports
KW  - Autonomous Vehicles
KW  - Colon, Sigmoid
KW  - Commerce
KW  - Excipients
KW  - Humans
KW  - excipient
KW  - article
KW  - classification error
KW  - human
KW  - human experiment
KW  - noise
KW  - sigmoid
KW  - unmanned aerial vehicle
KW  - airport
KW  - commercial phenomena
KW  - sigmoid
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Chu, C.
AU  - Hui, S.
AU  - Liu, X.
AU  - Jia, X.
AU  - Zhong, C.
TI  - Optimization of YOLOv8 Vehicle Detection Algorithm in Autonomous Driving Technology
PY  - 2024
T2  - 2024 6th International Conference on Artificial Intelligence and Computer Applications, ICAICA 2024
DO  - 10.1109/ICAICA63239.2024.10823036
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216868403&doi=10.1109%2fICAICA63239.2024.10823036&partnerID=40&md5=c20e4c05c0997537a90b303935fb858c
AB  - In autonomous driving, accurate vehicle target perception matters a lot. This paper devises an enhanced vehicle target detection algorithm using multi-sensor fusion, seeking to address the high computational expense, weak generalization, and poor small target detection of traditional algorithms. Firstly, to bolster YOLOv8's global feature extraction, its backbone network was swapped for Swin Transformer. Then, a GSConv module paired with the channel attention SE replaced the regular convolutions in YOLOv8, trimming computational complexity and enhancing generalization. Finally, high-resolution feature layers were incorporated into the adjusted backbone network. These furnished higher spatial resolution and snagged more details. Multi-scale fusion of the added layers then upped small target detection. The improved YOLOv8 model's Precision, Recall, Map@0.5, and mAP@0.5:0.95 climbed by 3%, 5%, 3.5%, and 2.5% respectively, topping out at 88.3%, 86%, 89.9%, and 64.9%. Against traditional algorithms' drawbacks like high cost and weak detection, these gains were notable, validating the improved method. © 2024 IEEE.
KW  - Autonomous Driving
KW  - Grouped Shuffling Convolution
KW  - Squeeze-and-Excitation
KW  - Swin Transformer
KW  - YOLOv8
KW  - Autonomous driving
KW  - Back-bone network
KW  - Generalisation
KW  - Grouped shuffling convolution
KW  - Optimisations
KW  - Small target detection
KW  - Squeeze-and-excitation
KW  - Swin transformer
KW  - Vehicle targets
KW  - YOLOv8
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Olayode, I.O.
AU  - Du, B.
AU  - Severino, A.
AU  - Campisi, T.
AU  - Alex, F.J.
TI  - Systematic literature review on the applications, impacts, and public perceptions of autonomous vehicles in road transportation system
PY  - 2023
T2  - Journal of Traffic and Transportation Engineering (English Edition)
DO  - 10.1016/j.jtte.2023.07.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180446260&doi=10.1016%2fj.jtte.2023.07.006&partnerID=40&md5=0c7e945b42d29d59339a7a01fdaa0650
AB  - As the advancement of driverless technology, together with information and communication technology moved at a fast pace, autonomous vehicles have attracted great attention from both industries and academic sectors during the past decades. It is evident that this emerging technology has great potential to improve the pedestrian safety on roads, mitigate traffic congestion, increase fuel efficiency, and reduce greenhouse gas emissions. However, there is limited systematic research into the applications and public perceptions of autonomous vehicles in road transportation. The purpose of this systematic literature review is to synthesise and analyse existing research on the applications, implications, and public perceptions of autonomous vehicles in road transportation system. It is found that autonomous vehicles are the future of road transportation and that the negative perception of humans is rapidly changing towards autonomous vehicles. Moreover, to fully deploy autonomous vehicles in a road transportation system, the existing road transportation infrastructure needs significant improvement. This systematic literature review contributes to the comprehensive knowledge of autonomous vehicles and will assist transportation researchers and urban planners to understand the fundamental and conceptual framework of autonomous vehicle technologies in road transportation systems. © 2023 Periodical Offices of Changâ€™an University
KW  - Autonomous mobility
KW  - Autonomous vehicles
KW  - Public perception
KW  - Road transportation
KW  - Traffic safety
KW  - Gas emissions
KW  - Greenhouse gases
KW  - Motor transportation
KW  - Pedestrian safety
KW  - Roads and streets
KW  - Traffic congestion
KW  - Urban transportation
KW  - Vehicle to vehicle communications
KW  - Autonomous mobilities
KW  - Autonomous Vehicles
KW  - Driverless
KW  - Industry sectors
KW  - Information and Communication Technologies
KW  - Public perception
KW  - Road transportation
KW  - Systematic literature review
KW  - Traffic safety
KW  - Transportation system
KW  - Autonomous vehicles
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, W.
AU  - Han, D.
AU  - Han, B.
AU  - Wu, Z.
TI  - YOLOv8-FDF: A Small Target Detection Algorithm in Complex Scenes
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3448619
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201764379&doi=10.1109%2fACCESS.2024.3448619&partnerID=40&md5=af19ff2c22bdb8bbae26548b8bdca9e6
AB  - Synthetic Aperture Radar (SAR) finds widespread applications in environmental monitoring, disaster management, ship surveillance, and military intelligence. However, existing target detection methods are ineffective in SAR scenes due to the intricate background environments, small target displays, and irregular appearances. To address these challenges, this thesis introduces a target detection model named YOLOv8-FDF, tailored for SAR scenes based on the YOLOv8 architecture. The model effectively incorporates the FADC module to distinguish targets from complex backgrounds and integrates a deformable feature adaptive mechanism to focus on irregular targets. Additionally, this thesis devised a specialized detection head designed to identify small targets in SAR-wide scenes, thereby improving the effectiveness of detecting such targets. The proposed YOLOv8-FDF model is evaluated on the HRSID dataset. Experiment results show a 3.6% improvement in Map75 on both the training and test sets. Furthermore, under the COCO standard, the model achieves improvements of 4.1%, 2.9%, and 5.5% on AP, AP50, and AP75, along with 6.8%, 1.2%, and 1.2% improvements on small, medium, and large-sized ship detection. An accuracy enhancement of 6.8%, 1.0%, and 14.9% is achieved. These experimental findings validate the efficacy of the proposed YOLOv8-FDF model in SAR scenarios.  © 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.
KW  - complex environment
KW  - deep learning
KW  - SAR target detection
KW  - Small targets
KW  - YOLOv8
KW  - Deep learning
KW  - Marine applications
KW  - Marine radar
KW  - Military radar
KW  - Radar target recognition
KW  - Ships
KW  - Surveillance radar
KW  - Tracking radar
KW  - Accuracy
KW  - Adaptation models
KW  - Complex environments
KW  - Deep learning
KW  - Features extraction
KW  - Marine vehicles
KW  - Radar target detection
KW  - Small targets
KW  - Synthetic aperture radar target detection
KW  - YOLO
KW  - YOLOv8
KW  - Synthetic aperture radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, H.
AU  - Wang, H.
AU  - Zhang, X.
AU  - Ruan, R.
AU  - Wang, Y.
AU  - Yin, Y.
TI  - Multiagent Detection System Based on Spatial Adaptive Feature Aggregation
PY  - 2024
T2  - IEEE Systems Journal
DO  - 10.1109/JSYST.2024.3423752
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211468281&doi=10.1109%2fJSYST.2024.3423752&partnerID=40&md5=4e2be8c0fbedc12ec74ab88387143c8e
AB  - Detection systems based on computer vision play important roles in Large-Scale Multiagent Systems. In particular, it can automatically locate and identify key objects and enhance intelligent collaboration and coordination among multiple agents. However, classification and localization in object detection may produce inconsistent prediction results due to different learning focus. Therefore, we propose a Spatial Decoupling and Boundary Feature Aggregation Network (SDBA-Net) to achieve spatial decoupling and task alignment. SDBA-Net includes a spatially sensitive region-aware module (SSRM) and a boundary feature aggregation module (BFAM). SSRM predicts sensitive regions for each task while minimizing computational cost. BFAM extracts valuable boundary features within sensitive regions and aligns them with corresponding anchors. These two modules are combined to spatially decouple and align the features of two tasks. In addition, a significance dependency complementary module (SDCM) is introduced. It enables SSRM to quickly adjust the sensitive region of the classification task to the significant feature region. Experiments are conducted on a large-scale complex real-world dataset MS COCO (Lin et al., 2014). The results show that SDBA-Net achieves better results than the baselines. Using the ResNet-50 backbone, our method improves the average precision (AP) of the single-stage detector VFNet by 1.0 point (from 41.3 to 42.3). In particular, when using the Res2Net-101-DCN backbone, SDBA-Net achieves an AP of 51.8 on the MS COCO test-dev.  © 2007-2012 IEEE.
KW  - Complex scenarios
KW  - feature aggregation
KW  - large scale
KW  - multiagent
KW  - object detection
KW  - spatial decoupling
KW  - Classification (of information)
KW  - Computer vision
KW  - Object detection
KW  - Aggregation network
KW  - Complex scenario
KW  - Decouplings
KW  - Detection system
KW  - Feature aggregation
KW  - Large-scales
KW  - Multi agent
KW  - Objects detection
KW  - Sensitive regions
KW  - Spatial decoupling
KW  - Object recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Z.
AU  - Chen, S.
AU  - Wang, Z.
AU  - Yang, J.
TI  - PlaneSeg: Building a Plug-In for Boosting Planar Region Segmentation
PY  - 2024
T2  - IEEE Transactions on Neural Networks and Learning Systems
DO  - 10.1109/TNNLS.2023.3262544
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153406367&doi=10.1109%2fTNNLS.2023.3262544&partnerID=40&md5=46b521226cd471a6bd6c03f56a9963d8
AB  - Existing methods in planar region segmentation suffer the problems of vague boundaries and failure to detect small-sized regions. To address these, this study presents an end-to-end framework, named PlaneSeg, which can be easily integrated into various plane segmentation models. Specifically, PlaneSeg contains three modules, namely, the edge feature extraction module, the multiscale module, and the resolution-adaptation module. First, the edge feature extraction module produces edge-aware feature maps for finer segmentation boundaries. The learned edge information acts as a constraint to mitigate inaccurate boundaries. Second, the multiscale module combines feature maps of different layers to harvest spatial and semantic information from planar objects. The multiformity of object information can help recognize small-sized objects to produce more accurate segmentation results. Third, the resolution-adaptation module fuses the feature maps produced by the two aforementioned modules. For this module, a pairwise feature fusion is adopted to resample the dropped pixels and extract more detailed features. Extensive experiments demonstrate that PlaneSeg outperforms other state-of-the-art approaches on three downstream tasks, including plane segmentation, 3-D plane reconstruction, and depth prediction. Code is available at https://github.com/nku-zhichengzhang/PlaneSeg. © 2012 IEEE.
KW  - Deep learning
KW  - depth prediction
KW  - planar region segmentation
KW  - plane reconstruction
KW  - plug-in
KW  - Deep learning
KW  - Edge detection
KW  - Extraction
KW  - Image reconstruction
KW  - Job analysis
KW  - Semantic Segmentation
KW  - Semantics
KW  - Deep learning
KW  - Depth prediction
KW  - Features extraction
KW  - Image edge detection
KW  - Images reconstruction
KW  - Images segmentations
KW  - Planar region
KW  - Planar region segmentation
KW  - Plane reconstruction
KW  - Plug-ins
KW  - Region segmentation
KW  - Task analysis
KW  - article
KW  - feature extraction
KW  - prediction
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Gai, K.
AU  - Yu, J.
AU  - Zhu, L.
TI  - Introduction to Cybersecurity in the Internet of Things
PY  - 2024
T2  - Introduction to Cybersecurity in the Internet of Things
DO  - 10.1201/9781032694818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191244437&doi=10.1201%2f9781032694818&partnerID=40&md5=9e18fd814f7e4c8fe84c23a2044c8d67
AB  - This book addresses the security challenges facing the rapidly growing Internet of Things (IoT) industry, including the potential threats and risks arising from its complex architecture. The authors discuss the overall IoT architecture, covering networking, computing, and security threats and risks to hardware such as sensors, actuators, and portable devices, as well as infrastructure layers. They cover a range of technical concepts such as cryptography, distributed storage, and data transmission, and offer practical advice on implementing security solutions such as authentication and access control. By exploring the future of cybersecurity in the IoT industry, with insights into the importance of big data and the threats posed by data mining techniques, this book is an essential resource for anyone interested in, or working in, the rapidly evolving field of IoT security. © 2024 Keke Gai, Jing Yu and Liehuang Zhu.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xing, Z.
AU  - Zhao, S.
AU  - Guo, W.
AU  - Meng, F.
AU  - Guo, X.
AU  - Wang, S.
AU  - He, H.
TI  - Coal resources under carbon peak: Segmentation of massive laser point clouds for coal mining in underground dusty environments using integrated graph deep learning model
PY  - 2023
T2  - Energy
DO  - 10.1016/j.energy.2023.128771
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168139719&doi=10.1016%2fj.energy.2023.128771&partnerID=40&md5=1d1a800119fa22eb254a9ec8180b30e4
AB  - With the background of China's carbon peak, the low-carbon and sustainable development of the coal industry is vital to China's national energy security. Because the underground visibility is low and the dust is continuously spreading, coal mine point cloud segmentation can provide a key basis for underground environment perception, and then provides a premise for the construction of green coal mines. In this study, we propose to segment the coal mining face (CMF) point cloud under the harsh environment based on the advanced dynamic graph convolution neural network (DGCNN) and to obtain the information of the coal cutting roof line. The results show that the multi-level and series pooling DGCNN (ML&SP-DGCNN) which was constructed on the basis of a large number of previous studies shows the best performance. In this study, the coal cutting roof line obtained by segmenting the CMF point cloud provides a key basis for dynamically correcting the underground geological model and straightening the CMF. More importantly, the established CMF point cloud segmentation model lays a foundation for perceiving the underground environment, which is of great help to realize the sustainable green production of coal resources. © 2023 Elsevier Ltd
KW  - Coal
KW  - Deep learning
KW  - Environmental perception
KW  - Low-carbon development
KW  - Unmanned mining
KW  - China
KW  - Carbon
KW  - Coal deposits
KW  - Coal dust
KW  - Coal industry
KW  - Coal mines
KW  - Deep learning
KW  - Energy security
KW  - Mine roof control
KW  - Roofs
KW  - Carbon peaks
KW  - Coal mining face
KW  - Coal resources
KW  - Deep learning
KW  - Environmental perceptions
KW  - Low-carbon development
KW  - Point cloud segmentation
KW  - Point-clouds
KW  - Underground environment
KW  - Unmanned mining
KW  - carbon emission
KW  - coal industry
KW  - coal mine
KW  - coal mining
KW  - dust
KW  - sustainable development
KW  - Coal
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hanzla, M.
AU  - Yusuf, M.O.
AU  - Sadiq, T.
AU  - Mudawi, N.A.
AU  - Rahman, H.
AU  - Alazeb, A.
AU  - Alarfaj, A.A.
AU  - Algarni, A.
TI  - UAV Detection Using Template Matching and Centroid Tracking
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3450580
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202738342&doi=10.1109%2fACCESS.2024.3450580&partnerID=40&md5=92eb74642220c59b7a9aa93efb654382
AB  - In computer vision and image processing, vehicle detection and tracking in complicated aerial images have become important subjects. The need for automated systems that can precisely detect and track vehicles in aerial image data is growing due to the abundance of data coming from numerous sources, including drones and satellites. This study introduces a new method for lane extraction that relies on centroid tracking and template matching, followed by co-registration and geo-referencing. Our approach offers robust vehicle detection and tracking over a range of sizes and positions in complicated backgrounds, while also efficiently segmenting the region of interest. Our suggested method, which makes use of machine learning and feature extraction techniques, shows excellent precision and effectiveness when it comes to detecting and tracking vehicles in complex aerial images. This finding has important implications for traffic management and urban planning, going beyond computer vision and image processing. Our technology has the potential to transform traffic management procedures by making it simpler to detect traffic bottlenecks and monitor traffic flow. Furthermore, our method can help identify damaged vehicles in disaster response scenarios, which will help prioritize rescue and recovery activities. All things considered, our suggested approach is a significant addition to the domains of computer vision and image processing, with a broad range of uses in traffic control, urban planning, and disaster management. © 2013 IEEE.
KW  - Georeferencing
KW  - multiple object detection
KW  - segmentation
KW  - smart traffic monitoring
KW  - vehicle detection
KW  - Advanced traffic management systems
KW  - Aerial photography
KW  - Aircraft detection
KW  - Emergency traffic control
KW  - Highway administration
KW  - Highway traffic control
KW  - Image registration
KW  - Image segmentation
KW  - Intelligent systems
KW  - Motor transportation
KW  - Unmanned aerial vehicles (UAV)
KW  - Accuracy
KW  - Features extraction
KW  - Georeferencing
KW  - Images segmentations
KW  - Multiple-object detections
KW  - Real - Time system
KW  - Road
KW  - Segmentation
KW  - Smart traffic
KW  - Smart traffic monitoring
KW  - Traffic monitoring
KW  - Vehicles detection
KW  - Template matching
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, L.
AU  - Wen, F.
AU  - Zhang, Q.
AU  - Gui, G.
AU  - Sari, H.
AU  - Adachi, F.
TI  - Constrained Multiobjective Decomposition Evolutionary Algorithm for UAV-Assisted Mobile Edge Computing Networks
PY  - 2024
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2024.3417009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002093892&doi=10.1109%2fJIOT.2024.3417009&partnerID=40&md5=38b444e1f0c55ac0f0e49ef26b416b2c
AB  - The increasing significance of unmanned aerial vehicles (UAVs) in mobile edge computing (MEC) has captured considerable attention. Nevertheless, the effectiveness of UAVs-assisted MEC networks is hampered by challenges, such as limited communication capacity and onboard power. To tackle these issues, this study develops a constrained multiobjective optimization model designed to enhance the performance of UAVs-assisted MEC networks, focusing on system capacity, energy consumption, and task latency. As a result, this problem manifests as a complex constrained multiobjective optimization problem. The study then proposes a constrained multiobjective decomposition evolutionary algorithm (CMODEA) with low-computational complexity. This algorithm employs an adaptive individual comparison strategy, balancing diversity and convergence, and integrates an optimally guided differential evolution strategy for efficiently approximating optimal solutions. Additionally, it incorporates an adaptive constraint handling method, effectively managing existing constraints. The CMODEA aims to simultaneously optimize system capacity, energy consumption, and task latency while meeting the computational resource requirements of UAVs and ensuring acceptable user task latency levels. Simulation results demonstrate the algorithm's effectiveness in significantly enhancing capacity, reducing energy consumption and latency, without greatly increasing algorithm complexity. © 2014 IEEE.
KW  - 6G
KW  - constrained multiobjective optimization
KW  - evolutionary computation
KW  - mobile edge computing (MEC)
KW  - unmanned aerial vehicles (UAVs)
KW  - Antennas
KW  - Complex networks
KW  - Computational complexity
KW  - Constrained optimization
KW  - Evolutionary algorithms
KW  - Internet of things
KW  - Job analysis
KW  - Mobile edge computing
KW  - Multiobjective optimization
KW  - 6g
KW  - Aerial vehicle
KW  - Constrained multi-objective optimizations
KW  - Convergence
KW  - Delay
KW  - Energy-consumption
KW  - Multi objective
KW  - Optimisations
KW  - Task analysis
KW  - Unmanned aerial vehicle
KW  - Energy utilization
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Atik, S.T.
AU  - Brocanelli, M.
AU  - Grosu, D.
TI  - Are Turn-by-Turn Navigation Systems of Regular Vehicles Ready for Edge-Assisted Autonomous Vehicles?
PY  - 2023
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2023.3275367
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161055799&doi=10.1109%2fTITS.2023.3275367&partnerID=40&md5=32e97e881292a9a0f4b6baa71cc0786e
AB  - Private and public transportation will be dominated by Autonomous Vehicles (AV), which are safer than regular vehicles. However, ensuring good performance for the autonomous features requires fast processing of heavy tasks. Providing each AV with powerful computing resources may result in increased AV cost and decreased driving range. An alternative solution is to install low-power computing hardware on each AV and offload the heavy tasks to powerful nearby edge servers. In this case, the AV's reaction time depends on how quickly the navigation tasks are completed in the edge server. To reduce task completion latency, the edge servers must be equipped with enough network and computing resources to handle the vehicle demands, which show large spatio-temporal variations. Thus, deploying the same resources in different locations may lead to unnecessary resource over-provisioning. In this paper, we leverage simulations using real traffic data to discuss the implications of deploying heterogeneous resources in different city areas to sustain peak versus average demand of edge-assisted AVs. Our analysis indicates that a reduction in network bandwidth and computing cores of up to 60% and 50%, respectively, is achieved by deploying edge resources for the average demand rather than peak demand. We also investigate how the peak-hour demand affects the safe travel time of AVs and find that it can be reduced by approximately 20% if they would be rerouted to areas with a lower edge-resource load. Thus, future research must consider that traditional turn-by-turn navigation systems may not provide the fastest routes for edge-assisted AVs.  © 2000-2011 IEEE.
KW  - Autonomous vehicles
KW  - edge computing
KW  - navigation systems
KW  - Autonomous vehicles
KW  - Cost benefit analysis
KW  - Edge computing
KW  - Job analysis
KW  - Travel time
KW  - Autonomous Vehicles
KW  - Computing resource
KW  - Edge computing
KW  - Edge resources
KW  - Edge server
KW  - Private transportation
KW  - Public transportation
KW  - Task analysis
KW  - Urban areas
KW  - Navigation systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Feng, R.
AU  - Zhao, F.
AU  - Chen, S.
AU  - Zhang, S.
AU  - Zhu, S.
TI  - A handwritten ancient text detector based on improved feature pyramid network
PY  - 2023
T2  - Pattern Recognition Letters
DO  - 10.1016/j.patrec.2023.06.013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163837856&doi=10.1016%2fj.patrec.2023.06.013&partnerID=40&md5=ef9a0a6bfe61d350ee705b5d28d92889
AB  - Text detection is the primary task for digitization of ancient books. Different from the common scene text detection tasks (ICDAR, TotalText, etc.), the texts in handwritten ancient documents are more densely distributed and generally small objects; at the same time, the layout structure is also more complex, with problems such as mixed arrangement of pictures and texts and high background noise, all of which pose challenges for detection. According to the characteristics of ancient book images, this paper proposes a new fusion structure based on Feature Pyramid Networks, and takes FCOS as the baseline model to form a new detector (named RFCOS). We enhance the detection capability for dense and small text instances by adding bottom-up fusion paths, cross-layer connections and weighted fusion. Meanwhile, the loss of high-level feature maps during fusion is reduced by new upsampling method and lateral connections. We verified the effectiveness of our RFCOS on the HWAD (Handwritten Ancient Books Dataset), a dataset containing samples in four languages - Yi, Chinese, Tibetan and Tangut, and verify the generalization of RFCOS on another public dataset MTHv2. The results show that RFCOS outperformed most of the existing text detectors in terms of precision, recall and F-measure. © 2023 Elsevier B.V.
KW  - FCOS
KW  - Feature pyramid networks
KW  - Handwritten text detection
KW  - Small object detection
KW  - Feature extraction
KW  - Digitisation
KW  - FCOS
KW  - Feature pyramid
KW  - Feature pyramid network
KW  - Handwritten text detection
KW  - Handwritten texts
KW  - Primary task
KW  - Pyramid network
KW  - Small object detection
KW  - Text detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Azurmendi, I.
AU  - Zulueta, E.
AU  - Lopez-Guede, J.M.
AU  - González, M.
TI  - Simultaneous Object Detection and Distance Estimation for Indoor Autonomous Vehicles
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12234719
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179330617&doi=10.3390%2felectronics12234719&partnerID=40&md5=0c47d95bafc36a4f6d3dec170c5a4ad3
AB  - Object detection is an essential and impactful technology in various fields due to its ability to automatically locate and identify objects in images or videos. In addition, object-distance estimation is a fundamental problem in 3D vision and scene perception. In this paper, we propose a simultaneous object-detection and distance-estimation algorithm based on YOLOv5 for obstacle detection in indoor autonomous vehicles. This method estimates the distances to the desired obstacles using a single monocular camera that does not require calibration. On the one hand, we train the algorithm with the KITTI dataset, which is an autonomous driving vision dataset that provides labels for object detection and distance prediction. On the other hand, we collect and label 100 images from a custom environment. Then, we apply data augmentation and transfer learning to generate a fast, accurate, and cost-effective model for the custom environment. The results show a performance of mAP0.5:0.95 of more than 75% for object detection and 0.71 m of mean absolute error in distance prediction, which are easily scalable with the labeling of a larger amount of data. Finally, we compare our method with other similar state-of-the-art approaches. © 2023 by the authors.
KW  - AGV
KW  - autonomous vehicles
KW  - distance estimation
KW  - indoor navigation
KW  - object detection
KW  - YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pan, X.
AU  - Jia, N.
AU  - Mu, Y.
AU  - Gao, X.
TI  - Survey of small object detection
ST  - 小目标检测研究综述
PY  - 2023
T2  - Journal of Image and Graphics
DO  - 10.11834/jig.220455
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172780895&doi=10.11834%2fjig.220455&partnerID=40&md5=4879bef52890c03b019cfdb3508a3da3
AB  - In recent years，object detection has attracted increasing attention because of the rapid development of computer vision and artificial intelligence technology. Early traditional object detection methods，such as histogram of oriented gradient（HOG）and deformable parts model（DPM）usually adopt three steps：region selection，manual feature extraction，and classification regression. However，manual feature extraction has great limitations for small object detection. The object detection algorithm based on the convolutional neural network can be divided into two-stage and one-stage detection algorithms. Two-stage detection algorithms，such as faster region with convolutional neural network（Faster RCNN）and cascade region with convolutional neural network（Cascade RCNN），select candidate regions through the region proposal network. Then，they classify and regress these regions to obtain the detection results. However，the problem of low accuracy still exists in small object detection. One-stage detection algorithms，such as single shot MultiBox detector（SSD）and you only look once（YOLO），can directly locate the object and output the category detection information of the object，thereby improving the speed of object detection to a certain extent. However，small object detection has always been a huge challenge in the field of object detection because of the small proportion of small object pixels，little semantic information，and small objects that are easily disturbed by complex scenes. In particular，the challenges in object detection are as follows：First，the characteristics of small objects are few. Given the small scale of small objects and the small coverage area in data images，extracting favorable semantic feature information in network training is difficult. Second，small object detection is susceptible to interference. Most of the small objects have low resolution，blurred images，and little visual information. Thus，they are easily disturbed during difficult feature extraction. Thus，the detection model cannot easily locate and identify small objects accurately. Moreover，many false detections and missed detections exist. Third，a shortage of small object datasets exists. At present，most of the mainstream object datasets，such as PASCAL VOC and MS-COCO，are aimed at normal-scale objects. In particular，the proportion of small-scale objects is insufficient，and the distribution is uneven. However，some datasets mentioned in this study that can be used for small object detection are all aimed at specific scenes or tasks. These datasets include DOTA remote sensing object detection dataset，face detection dataset and benchmark，which are not universal for small object detection. Fourth，small objects are easy to gather and block. A serious occlusion problem occurs when small objects gather. After many downsampling and pooling operations，quite a lot of feature information is lost，resulting in some detection difficulties. At present，visual small object detection is increasingly important in all fields of life. Aiming at the problems in small object detection，this study combs the research status and achievements of small object detection at home and abroad to promote the development of small object detection further，improve the speed and accuracy of small object detection，and optimize its algorithm model. The methods of small object detection are analyzed and summarized from the aspects of data enhancement，super resolution，multiscale feature fusion，contextual semantic information，anchor frame mechanism，attention，and specific detection scenarios. Data enhancement is the method proposed for solving the problems of a few general small object datasets，a small number of small objects in public datasets，and uneven distribution of small objects in images. The earliest data enhancement strategy is to increase the number of object training and improve the performance of object detection by deforming，rotating，scaling，cutting，and translating object instances. Then，other effective data augmentation methods emerged，which included oversampling the images containing small objects in the experiment，scaling and rotating the small objects，and copying the objects to any new position in order to augment the data. Data enhancement helps improve the robustness of a model to a certain extent. Moreover，it solves the problems of unobvious visual features of small objects and less object information. It also achieves good results in the final detection performance. However，the improper design of data enhancement strategy in practical applications may lead to new noise，impairing the performance of feature extraction. This scenario also brings some challenges to the design of the algorithm. The small object detection method based on multiscale fusion needs to make full use of the detailed information in the image because the characteristic information of small-scale objects is little. In the existing convolutional neural network（CNN）model of general object detection，multiscale detection can help the model to obtain accurate positioning information and discriminating feature information by using a low-level feature layer. This scenario is conducive to the detection and recognition of small-scale objects. First，a feature pyramid network（FPN）with strong semantic features at all scales is introduced. Then，an fpn-based path aggregation network（PANet），which not only achieved good results in case segmentation but also improved the detection of small objects. In feature fusion，the residual feature enhancement method extracts the context information with a constant ratio to reduce the information loss of the highest pyramid feature map. At present，many methods are based on multiscale feature fusion，which uses the low-level high-resolution and high-level strong feature semantic information of the network to improve the accuracy of small objects. In small object detection，the target’s feature expression ability is weak. Thus，the network structure must be deepened to learn considerable feature information. Introducing an attention mechanism can often make the network model pay considerable attention to the channels and areas related to the task. In the object detection network，the shallow feature map lacks the contextual semantic information of small objects. By incorporating attention mechanisms into the SSD model，irrelevant information in feature fusion is suppressed，leading to an improvement in the detection accuracy of small objects. In general，the attention mechanism can reasonably allocate the used resources，quickly find the region of interest，and ignore disturbing information. However，the improper design in use increases the cost of network calculation and affects the extraction of object features by the model. Finally，the future research direction of small object detection is prospected. Visual small object detection is becoming increasingly important in all fields of life，and it will develop in other directions in the future. © 2023 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.
KW  - data enhancement
KW  - multiscale characteristic fusion
KW  - object detection
KW  - small object detection
KW  - super-resolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, L.
AU  - Zhang, D.
AU  - Zheng, Y.
TI  - Road Object Detection in Foggy Complex Scenes Based on Improved YOLOv8
PY  - 2024
T2  - IEEE Access
DO  - 10.1109/ACCESS.2024.3438612
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200825413&doi=10.1109%2fACCESS.2024.3438612&partnerID=40&md5=ccaf2df4837060e4f103c897e3906395
AB  - Focusing on the challenges of vehicle detection in foggy weather, especially the algorithm of low accuracy caused by small and incomplete targets in adverse weather conditions, a foggy weather vehicle detection algorithm based on improved lightweight YOLOv8 was proposed. Firstly, the dataset was processed through a combination of data transformation, Dehaze Formers and dark channel preprocessing. Secondly, in the main body of YOLOv8, the C2f component was replaced with the dynamic convolution C2f- DCN, enhancing its adaptability to geometric changes in the image. To further improve the detection performance of the classifier, an improved S5attention module based on S2-MLP was introduced. This module utilizes contextual information to capture long-range dependencies and assign weights to different channels based on their relevance to the task at hand. By considering non-local features, the S5attention module helps the model better capture important spatial relationships within the image. Additionally, the feature extraction module was updated to FasterNext, improving the differential convolution's feature extraction capabilities. The Involution module was also introduced to reduce FLOPs during feature channel fusion and reduce the model's parameter count. Experimental results show that on the RESIDE foggy weather dataset, the improved algorithm has an mAP50 increase of 4.1% compared with the original algorithm, and the model's parameter quantity is only 9.06m, with a computational cost reduced from 28.7G to 28.1G. The research model in this article will provide technical support for detecting vehicle targets in foggy weather, ensuring fast and accurate operation. © 2013 IEEE.
KW  - Deep learning
KW  - feature extraction
KW  - foggy weather vehicle detection
KW  - YOLOv8
KW  - Convolution
KW  - Deep learning
KW  - Extraction
KW  - Image enhancement
KW  - Metadata
KW  - Object detection
KW  - Scattering parameters
KW  - Vehicles
KW  - Accuracy
KW  - Computational modelling
KW  - Deep learning
KW  - Features extraction
KW  - Foggy weather vehicle detection
KW  - Kernel
KW  - Vehicles detection
KW  - YOLO
KW  - YOLOv8
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tang, C.
AU  - Hu, Q.
AU  - Zhou, G.
AU  - Yao, J.
AU  - Zhang, J.
AU  - Huang, Y.
AU  - Ye, Q.
TI  - Transformer Sub-Patch Matching for High-Performance Visual Object Tracking
PY  - 2023
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2023.3264664
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153373340&doi=10.1109%2fTITS.2023.3264664&partnerID=40&md5=ba9e8c6583023e3122a2051079af6f26
AB  - Visual tracking is a core component of intelligent transportation systems, especially for unmanned driving and road surveillance. Numerous convolutional neural network (CNN) trackers have achieved unprecedented performance. However, CNN features with regular spatial context relationships experience difficulty matching the rigid target templates when dramatic deformation and occlusion occur. In this paper, we propose a novel full Transformer Sub-patch Matching network for tracking (TSMtrack), which decomposes the tracked object into sub-patches, and interlaced matches the extracted sub-patches by leveraging the attention mechanism born with the Transformer. Roots in Transformer architecture, TSMtrack consists of image patch decomposition, sub-patch matching, and position prediction. Specifically, TSMtrack converts the whole frame into sub-patches and extracts the sub-patch features independently. By sub-patch matching and FFN-like prediction, TSMtrack enables independent similarity measurement between sub-patch features in an interlaced and iterative fashion. With a full Transformer pipeline implemented, we achieve a high-quality trade-off between tracking speed performance. Experiments on nine benchmarks demonstrate the effectiveness of our Transformer sub-patch matching framework. In particular, it realizes an AO of 75.6 on GOT-10K and SR of 57.9 on WebUAV-3M with 48 FPS on GPU RTX-2060s.  © 2000-2011 IEEE.
KW  - full transformer
KW  - siamese network
KW  - sub-patch matching
KW  - Visual tracking
KW  - Intelligent systems
KW  - Neural networks
KW  - Convolutional neural network
KW  - Full transformer
KW  - Matching networks
KW  - Patch-matching
KW  - Performance
KW  - Siamese network
KW  - Sub-patch matching
KW  - Sub-patches
KW  - Visual object tracking
KW  - Visual Tracking
KW  - Economic and social effects
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Han, T.
AU  - Dong, Q.
AU  - Sun, L.
TI  - SenseLite: A YOLO-Based Lightweight Model for Small Object Detection in Aerial Imagery
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23198118
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174024646&doi=10.3390%2fs23198118&partnerID=40&md5=1644148fe4179e7a58ac86692ef89174
AB  - In the field of aerial remote sensing, detecting small objects in aerial images is challenging. Their subtle presence against broad backgrounds, combined with environmental complexities and low image resolution, complicates identification. While their detection is crucial for urban planning, traffic monitoring, and military reconnaissance, many deep learning approaches demand significant computational resources, hindering real-time applications. To elevate the accuracy of small object detection in aerial imagery and cater to real-time requirements, we introduce SenseLite, a lightweight and efficient model tailored for aerial image object detection. First, we innovatively structured the YOLOv5 model for a more streamlined structure. In the backbone, we replaced the original structure with cutting-edge lightweight neural operator Involution, enhancing contextual semantics and weight distribution. For the neck, we incorporated GSConv and slim-Neck, striking a balance between reduced computational complexity and performance, which is ideal for rapid predictions. Additionally, to enhance detection accuracy, we integrated a squeeze-and-excitation (SE) mechanism to amplify channel communication and improve detection accuracy. Finally, the Soft-NMS strategy was employed to manage overlapping targets, ensuring precise concurrent detections. Performance-wise, SenseLite reduces parameters by 30.5%, from 7.05 M to 4.9 M, as well as computational demands, with GFLOPs decreasing from 15.9 to 11.2. It surpasses the original YOLOv5, showing a 5.5% mAP0.5 improvement, 0.9% higher precision, and 1.4% better recall on the DOTA dataset. Compared to other leading methods, SenseLite stands out in terms of performance. © 2023 by the authors.
KW  - aerial images
KW  - GSConv
KW  - involution
KW  - SE
KW  - small objects
KW  - Soft-NMS
KW  - YOLOv5
KW  - Aerial photography
KW  - Antennas
KW  - Deep learning
KW  - Military applications
KW  - Military photography
KW  - Object detection
KW  - Object recognition
KW  - Remote sensing
KW  - Semantics
KW  - Aerial imagery
KW  - Aerial images
KW  - Gsconv
KW  - Involution
KW  - Performance
KW  - Small object detection
KW  - Small objects
KW  - Soft-NMS
KW  - Squeeze-and-excitation
KW  - YOLOv5
KW  - Image resolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rathee, M.
AU  - Bačić, B.
AU  - Doborjeh, M.
TI  - Automated Road Defect and Anomaly Detection for Traffic Safety: A Systematic Review
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23125656
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163947663&doi=10.3390%2fs23125656&partnerID=40&md5=1e6b780f9710de80833fd3c38b016432
AB  - Recently, there has been a substantial increase in the development of sensor technology. As enabling factors, computer vision (CV) combined with sensor technology have made progress in applications intended to mitigate high rates of fatalities and the costs of traffic-related injuries. Although past surveys and applications of CV have focused on subareas of road hazards, there is yet to be one comprehensive and evidence-based systematic review that investigates CV applications for Automated Road Defect and Anomaly Detection (ARDAD). To present ARDAD’s state-of-the-art, this systematic review is focused on determining the research gaps, challenges, and future implications from selected papers (N = 116) between 2000 and 2023, relying primarily on Scopus and Litmaps services. The survey presents a selection of artefacts, including the most popular open-access datasets (D = 18), research and technology trends that with reported performance can help accelerate the application of rapidly advancing sensor technology in ARDAD and CV. The produced survey artefacts can assist the scientific community in further improving traffic conditions and safety. © 2023 by the authors.
KW  - ARDAD
KW  - computer vision
KW  - deep learning
KW  - machine learning
KW  - motorist safety
KW  - on-road anomaly detection
KW  - structural damage detection
KW  - transfer learning
KW  - Accidents, Traffic
KW  - Safety
KW  - Anomaly detection
KW  - Damage detection
KW  - Deep learning
KW  - Defects
KW  - Roads and streets
KW  - Structural analysis
KW  - Anomaly detection
KW  - Automated road defect and anomaly detection
KW  - Automated roads
KW  - Deep learning
KW  - Defect detection
KW  - Machine-learning
KW  - Motorist safety
KW  - On-road anomaly detection
KW  - Structural damage detection
KW  - Transfer learning
KW  - prevention and control
KW  - safety
KW  - traffic accident
KW  - Computer vision
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - He, W.
AU  - Deng, Z.
AU  - Ye, Y.
AU  - Pan, P.
TI  - ConCs-Fusion: A Context Clustering-Based Radar and Camera Fusion for Three-Dimensional Object Detection
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15215130
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176326926&doi=10.3390%2frs15215130&partnerID=40&md5=5a93f90784cb6fb5334379d90aef95f2
AB  - Multi-modality three-dimensional (3D) object detection is a crucial technology for the safe and effective operation of environment perception systems in autonomous driving. In this study, we propose a method called context clustering-based radar and camera fusion for 3D object detection (ConCs-Fusion) that combines radar and camera sensors at the intermediate fusion level to achieve 3D object detection. We extract features from heterogeneous sensors and input them as feature point sets into the fusion module. Within the fusion module, we utilize context cluster blocks to learn multi-scale features of radar point clouds and images, followed by upsampling and fusion of the feature maps. Then, we leverage a multi-layer perceptron to nonlinearly represent the fused features, reducing the feature dimensionality to improve model inference speed. Within the context cluster block, we aggregate feature points of the same object from different sensors into one cluster based on their similarity. All feature points within the same cluster are then fused into a radar–camera feature fusion point, which is self-adaptively reassigned to the originally extracted feature points from a simplex sensor. Compared to previous methods that only utilize radar as an auxiliary sensor to camera, or vice versa, the ConCs-Fusion method achieves a bidirectional cross-modal fusion between radar and camera. Finally, our extensive experiments on the nuScenes dataset demonstrate that ConCs-Fusion outperforms other methods in terms of 3D object detection performance. © 2023 by the authors.
KW  - autonomous driving
KW  - bidirectional cross-modal fusion
KW  - context clustering
KW  - radar–camera fusion
KW  - three-dimensional object detection
KW  - Autonomous vehicles
KW  - Cameras
KW  - Object recognition
KW  - Radar imaging
KW  - 3D object
KW  - Autonomous driving
KW  - Bidirectional cross-modal fusion
KW  - Context clustering
KW  - Cross-modal
KW  - Fusion modules
KW  - Objects detection
KW  - Radar–camera fusion
KW  - Three-dimensional object
KW  - Three-dimensional object detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, H.
AU  - Guo, E.
AU  - Chen, F.
AU  - Chen, P.
TI  - Depth Completion in Autonomous Driving: Adaptive Spatial Feature Fusion and Semi-Quantitative Visualization
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13179804
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170395317&doi=10.3390%2fapp13179804&partnerID=40&md5=3d1f3098f5158cdd84463871314880cc
AB  - The safety of autonomous driving is closely linked to accurate depth perception. With the continuous development of autonomous driving, depth completion has become one of the crucial methods in this field. However, current depth completion methods have major shortcomings in small objects. To solve this problem, this paper proposes an end-to-end architecture with adaptive spatial feature fusion by encoder–decoder (ASFF-ED) module for depth completion. The architecture is built on the basis of the network architecture proposed in this paper, and is able to extract depth information adaptively with different weights on the specified feature map, which effectively solves the problem of insufficient depth accuracy of small objects. At the same time, this paper also proposes a depth map visualization method with a semi-quantitative visualization, which makes the depth information more intuitive to display. Compared with the currently available depth map visualization methods, this method has stronger quantitative analysis and horizontal comparison ability. Through experiments of ablation study and comparison, the results show that the method proposed in this paper exhibits a lower root-mean-squared error (RMSE) and better small object detection performance on the KITTI dataset. © 2023 by the authors.
KW  - autonomous driving
KW  - computer vision
KW  - depth completion
KW  - image processing
KW  - multi-source information fusion for sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mohapatra, J.B.
AU  - Monikantan, J.
AU  - Nishchal, N.K.
TI  - Object recognition under bad weather conditions with wavelet-modified logarithmic fringe-adjusted joint transform correlator
PY  - 2024
T2  - Journal of Optics (India)
DO  - 10.1007/s12596-024-02065-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202467273&doi=10.1007%2fs12596-024-02065-9&partnerID=40&md5=9f01f8a4e03e97cb1f1b015d534fbcf7
AB  - In real world applications, pattern recognition can be truly challenging when objects are embedded with backgrounds, fog, haze, sun-glare, and dust etc. It becomes very difficult to identify the object in these scenarios as auto-correlation is heavily affected by the noise. In this paper, our objective is to enhance the efficiency of object recognition in noisy environments by incorporating wavelet transform into the logarithmic fringe-adjusted joint transform correlator (LFJTC). The proposed study includes experimental demonstration of object detection and its investigation under ambient noise and different weather conditions. Wavelet function is integrated to the joint power spectrum of LFJTC. For eliminating the undesired dc term, differential processing is carried out. To quantify the effectiveness of the wavelet processing on LFJTC, three performance measure parameters have been calculated; correlation peak intensity, peak-to-sidelobe ratio, and signal-to-clutter ratio. The findings of the correlation analysis have been investigated using the target with Gaussian noise, speckle noise, simulated fog, haze, sun-glare, and dust. © The Author(s), under exclusive licence to The Optical Society of India 2024.
KW  - Correlation
KW  - Fringe-adjusted filter
KW  - Joint transform correlator
KW  - Wavelet transform
KW  - Correlators
KW  - Gaussian noise (electronic)
KW  - Glare effects
KW  - Image coding
KW  - Laser beams
KW  - Radar clutter
KW  - Signal to noise ratio
KW  - Wavelet transforms
KW  - Auto correlation
KW  - Condition
KW  - Correlation
KW  - Fringe-adjusted filter
KW  - Fringe-adjusted joint transform correlator
KW  - Joint transform correlators
KW  - Objects recognition
KW  - Real-world
KW  - Sun glare
KW  - Wavelets transform
KW  - Glare
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, D.
AU  - Yan, X.
AU  - Shi, S.
TI  - Review on Detection and Identification Positioning Technology of Surface Unexploded Submunitions
ST  - 地表未爆子弹药检测与识别定位技术研究综述
PY  - 2023
T2  - Aero Weaponry
DO  - 10.12132/ISSN.1673-5048.2023.0069
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179999385&doi=10.12132%2fISSN.1673-5048.2023.0069&partnerID=40&md5=d944c8e7deb29fdccbe52989843cadca
AB  - Long-range multi-purpose cluster munitions with reconnaissance, identification, strike and evaluation functions are widely used, and a large number of unexploded submunitions will be generated on the surface, which will greatly hinder the battlefield maneuver, deployment and depth attack. Due to its special visual characteristics, surface unexploded submunitions use imaging technology and deep learning technology for long-distance, large-area, non-contact rapid and accurate detection and recognition. This paper introduces the common methods of unexploded submunition detection and their advantages and disadvantages, summarizes the imaging characteristics of unexploded submunitions under different imaging technology conditions and their identification and positioning methods, analyzes the signifi-cant advantages of deep learning in the field of unexploded submunition detection and recognition. Then, a UAV-borne rapid detection, identification and positioning method based on deep learning for surface unexploded submunitions is proposed. © The Author(s) 2023.
KW  - deep learning
KW  - identification and positioning
KW  - imaging technology
KW  - unexploded submunition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yuan, L.
AU  - Tang, H.
AU  - Chen, Y.
AU  - Gao, R.
AU  - Wu, W.
TI  - Improved YOLOv5 for Road Target Detection in Complex Environments
ST  - 改进 YOLOv5 的复杂环境道路目标检测方法
PY  - 2023
T2  - Computer Engineering and Applications
DO  - 10.3778/j.issn.1002-8331.2304-0251
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007363901&doi=10.3778%2fj.issn.1002-8331.2304-0251&partnerID=40&md5=dbc0a50e668304b15660e14fadba58fc
AB  - To solve the problem of missed detection in road object detection tasks in complex environments due to diverse target scale changes, dense occlusion and uneven lighting, an improved method for road object detection CTC-YOLO（context transformer and convolutional block attention module based on YOLOv5）is proposed. Firstly, for small targets, improve the network detection head structure, add a multi-scale target detection layer, and improve the accuracy of small target detection. Secondly, in order to fully utilize the input contextual information, introduce a context transformer networks（CoTNet）module in the feature extraction section, and design a CoT3 module to guide dynamic attention matrix learning and improve visual representation ability. Finally, the C3 module in the Neck section integrates the convolutional block attention module（CBAM）to locate attention regions in complex scenes. To further validate the CTC-YOLO method proposed in this paper, some useful strategies are adopted, such as model integration position selection and comparison with other attention mechanisms. The experimental results show that the mAP@0.5 on the publicly available datasets KITTI, Cityscapes and BDD100K reaches 89.6%, 46.1% and 57.0%, respectively, which are 3.1, 2.0 and 1.2 percentage points higher than the baseline model, respectively. Compared with other models, the detection efficiency is higher and effectively improves the problem of object detection in complex environments. © 2023 Journal of Computer Engineering and Applications Beijing Co., Ltd.; Science Press. All rights reserved.
KW  - attentional mechanism
KW  - complex environment
KW  - target detection
KW  - YOLOv5
KW  - Complex networks
KW  - Convolution
KW  - Object recognition
KW  - Roads and streets
KW  - Attentional mechanism
KW  - Complex environments
KW  - Detection tasks
KW  - Missed detections
KW  - Module-based
KW  - Objects detection
KW  - Road targets
KW  - Small targets
KW  - Targets detection
KW  - YOLOv5
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shih, C.-H.
AU  - Lin, C.-J.
AU  - Jhang, J.-Y.
TI  - Ackerman Unmanned Mobile Vehicle Based on Heterogeneous Sensor in Navigation Control Application
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23094558
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159165676&doi=10.3390%2fs23094558&partnerID=40&md5=6c860ad195e2cd4b293cbe9394a93ee1
AB  - With the advancement of science and technology, the development and application of unmanned mobile vehicles (UMVs) have emerged as topics of crucial concern in the global industry. The development goals and directions of UMVs vary according to their industrial uses, which include navigation, autonomous driving, and environmental recognition; these uses have become the priority development goals of researchers in various fields. UMVs employ sensors to collect environmental data for environmental analysis and path planning. However, the analysis function of a single sensor is generally affected by natural environmental factors, resulting in poor identification results. Therefore, this study introduces fusion technology that employs heterogeneous sensors in the Ackerman UMV, leveraging the advantages of each sensor to enhance accuracy and stability in environmental detection and identification. This study proposes a fusion technique involving heterogeneous imaging and LiDAR (laser imaging, detection, and ranging) sensors in an Ackerman UMV. A camera is used to obtain real-time images, and YOLOv4-tiny and simple online real-time tracking are then employed to detect the location of objects and conduct object classification and object tracking. LiDAR is simultaneously used to obtain real-time distance information of detected objects. An inertial measurement unit is used to gather odometry information to determine the position of the Ackerman UMV. Static maps are created using simultaneous localization and mapping. When the user commands the Ackerman UMV to move to the target point, the vehicle control center composed of the robot operating system activates the navigation function through the navigation control module. The Ackerman UMV can reach the destination and instantly identify obstacles and pedestrians when in motion. © 2023 by the authors.
KW  - Ackerman unmanned mobile vehicle
KW  - deep learning
KW  - heterogeneous sensor
KW  - navigation control
KW  - object detection
KW  - Deep learning
KW  - Environmental technology
KW  - Motion planning
KW  - Navigation
KW  - Optical radar
KW  - Robot Operating System
KW  - Robots
KW  - Ackerman unmanned mobile vehicle
KW  - Deep learning
KW  - Heterogeneous sensors
KW  - Imaging detections
KW  - Laser detection
KW  - Laser imaging
KW  - Laser ranging
KW  - Mobile vehicle
KW  - Navigation controls
KW  - Objects detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Qiu, Y.
AU  - Jiang, H.
AU  - Lu, Y.
TI  - Small object detection for autonomous driving under hazy conditions on mountain motorways
PY  - 2023
T2  - Optical Engineering
DO  - 10.1117/1.OE.62.11.113101
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179468303&doi=10.1117%2f1.OE.62.11.113101&partnerID=40&md5=a08e5e6e9b4989bb48f4f81d8951e0d2
AB  - To address the issue of high miss rates for distant small objects and the diminished system detection performance due to the influence of hazy when autonomous vehicles operate on mountain highways. We propose a framework for small object vehicle detection in hazy traffic environments (SHTDet). This framework aims to enhance small object detection for autonomous driving under hazy conditions on mountainous motorways. Specifically, to restore the clarity of hazy images, we designed an image enhancement (IE), and its parameters are predicted by a convolutional neural network [filter parameter estimation (FPE)]. In addition, to enhance the detection accuracy of small objects, we introduce a cascaded sparse query (CSQ) mechanism, which effectively utilizes high-resolution features while maintaining fast detection speed. We jointly optimize the IE and the detection network (CSQ-FCOS) in an end-to-end manner, ensuring that FPE module can learn a suitable IE. Our proposed SHTDet method is adept at adaptively handling sunny and hazy conditions. Extensive experiments demonstrate the efficacy of the SHTDet method in detecting small objects on hazy sections of mountain highways. © 2023 SPIE.
KW  - autonomous driving
KW  - hazy images
KW  - mountain motorways
KW  - small object detection
KW  - small object vehicle detection in hazy traffic environments
KW  - Autonomous vehicles
KW  - Convolutional neural networks
KW  - Landforms
KW  - Object detection
KW  - Object recognition
KW  - Autonomous driving
KW  - Condition
KW  - Hazy image
KW  - Mountain highway
KW  - Mountain motorway
KW  - Small object detection
KW  - Small object vehicle detection in hazy traffic environment
KW  - Small objects
KW  - Traffic environment
KW  - Vehicles detection
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ma, S.
AU  - Wang, W.
AU  - Pan, Z.
AU  - Hu, Y.
AU  - Zhou, G.
AU  - Wang, Q.
TI  - A Recognition Model Incorporating Geometric Relationships of Ship Components
PY  - 2024
T2  - Remote Sensing
DO  - 10.3390/rs16010130
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181871571&doi=10.3390%2frs16010130&partnerID=40&md5=be31227346dc6e637d342354349b83ef
AB  - Ship recognition with optical remote sensing images is currently widely used in fishery management, ship traffic surveillance, and maritime warfare. However, it currently faces two major challenges: recognizing rotated targets and achieving fine-grained recognition. To address these challenges, this paper presents a new model called Related-YOLO. This model utilizes the mechanisms of relational attention to stress positional relationships between the components of a ship, extracting key features more accurately. Furthermore, it introduces a hierarchical clustering algorithm to implement adaptive anchor boxes. To tackle the issue of detecting multiple targets at different scales, a small target detection head is added. Additionally, the model employs deformable convolution to extract the features of targets with diverse shapes. To evaluate the performance of the proposed model, a new dataset named FGWC-18 is established, specifically designed for fine-grained warship recognition. Experimental results demonstrate the excellent performance of the model on this dataset and two other public datasets, namely FGSC-23 and FGSCR-42. In summary, our model offers a new route to solve the challenging issues of detecting rotating targets and fine-grained recognition with remote sensing images, which provides a reliable foundation for the application of remote sensing images in a wide range of fields. © 2023 by the authors.
KW  - fine-grained ship dataset
KW  - optical remote sensing
KW  - rotated ship recognition
KW  - Clustering algorithms
KW  - Warships
KW  - Fine grained
KW  - Fine-grained ship dataset
KW  - Fisheries management
KW  - Geometric relationships
KW  - Optical remote sensing
KW  - Performance
KW  - Recognition models
KW  - Remote sensing images
KW  - Rotated ship recognition
KW  - Ship recognition
KW  - Optical remote sensing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, K.
AU  - Zhang, Y.
AU  - Bao, Y.
AU  - Zhao, Y.
AU  - Yan, Y.
TI  - Self-Enhanced Mixed Attention Network for Three-Modal Images Few-Shot Semantic Segmentation
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23146612
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165987993&doi=10.3390%2fs23146612&partnerID=40&md5=c50b8da3703f6f640ed0bd667a43306c
AB  - As an important computer vision technique, image segmentation has been widely used in various tasks. However, in some extreme cases, the insufficient illumination would result in a great impact on the performance of the model. So more and more fully supervised methods use multi-modal images as their input. The dense annotated large datasets are difficult to obtain, but the few-shot methods still can have satisfactory results with few pixel-annotated samples. Therefore, we propose the Visible-Depth-Thermal (three-modal) images few-shot semantic segmentation method. It utilizes the homogeneous information of three-modal images and the complementary information of different modal images, which can improve the performance of few-shot segmentation tasks. We constructed a novel indoor dataset VDT-2048-5i for the three-modal images few-shot semantic segmentation task. We also proposed a Self-Enhanced Mixed Attention Network (SEMANet), which consists of a Self-Enhanced module (SE) and a Mixed Attention module (MA). The SE module amplifies the difference between the different kinds of features and strengthens the weak connection for the foreground features. The MA module fuses the three-modal feature to obtain a better feature. Compared with the most advanced methods before, our model improves mIoU by 3.8% and 3.3% in 1-shot and 5-shot settings, respectively, which achieves state-of-the-art performance. In the future, we will solve failure cases by obtaining more discriminative and robust feature representations, and explore achieving high performance with fewer parameters and computational costs. © 2023 by the authors.
KW  - few-shot semantic segmentation
KW  - multi-modal images
KW  - three-modal registration
KW  - Image enhancement
KW  - Large dataset
KW  - Semantics
KW  - Computer vision techniques
KW  - Few-shot semantic segmentation
KW  - Images segmentations
KW  - Large datasets
KW  - Multimodal images
KW  - Performance
KW  - Semantic segmentation
KW  - Supervised methods
KW  - Thermal
KW  - Three-modal registration
KW  - article
KW  - attention
KW  - attention network
KW  - computer vision
KW  - illumination
KW  - image segmentation
KW  - Semantic Segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Saidani, T.
TI  - Deep Learning Approach: YOLOv5-based Custom Object Detection
PY  - 2023
T2  - Engineering, Technology and Applied Science Research
DO  - 10.48084/etasr.6397
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179989862&doi=10.48084%2fetasr.6397&partnerID=40&md5=79c4d352a900f822b64b43749ec2a1da
AB  - Object detection is of significant importance in the field of computer vision, since it has extensive applications across many sectors. The emergence of YOLO (You Only Look Once) has brought about substantial changes in this domain with the introduction of real-time object identification with exceptional accuracy. The YOLOv5 architecture is highly sought after because of its increased flexibility and computational efficiency. This research provides an in-depth analysis of implementing YOLOv5 for object identification. This research delves deeply into the architectural improvements and design ideas that set YOLOv5 apart from its predecessors to illuminate its unique benefits. This research examines the training process and the efficiency of transfer learning techniques, among other things. The detection skills of YOLOv5 may be greatly improved by including these features. This study suggests the use of YOLOv5, a state-of-the-art object identification framework, as a crucial tool in the field of computer vision for accurate object recognition. The results of the proposed framework demonstrate higher performance in terms of mAP (60.9%) when evaluated with an IoU criterion of 0.5 and when compared to current methodologies in terms of reliability, computing flexibility, and mean average precision. These advantages make it applicable in many real-world circumstances. © 2023, Dr D. Pylarinos. All rights reserved.
KW  - computer vision
KW  - deep learning
KW  - object detection
KW  - YOLOv5
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Zhu, X.
AU  - Kundu, S.K.
TI  - Road Anomaly Detection and Localization for Connected Vehicle Applications
PY  - 2023
T2  - SAE Technical Papers
DO  - 10.4271/2023-01-0719
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160731341&doi=10.4271%2f2023-01-0719&partnerID=40&md5=ee2ed70d51a4777f3a8efbcf12dd5ad1
AB  - Road anomalies pose significant challenges for on-road safety, ride comfort, and fuel economy. The recent advancement of Connected Vehicle technology has made it feasible to overcome this challenge by sharing the detected road hazards information with other vehicles and entities. However, localization accuracies of the detected road hazards are often very low due to noisy detection results and limited GPS sensor performances. In this paper, a cloud based data management system with in-vehicle and on-cloud data processing modules is presented for road hazards detection and localization. Stereo camera and a consumer-grade GPS sensor on a testing vehicle are used to detect road anomaly information, e.g., type, size, and location, where a novel in-vehicle data processing module is implemented based on Kalman Filter and Phase Adjustment. For hazards data shared from all connected vehicles, an on-cloud data processing module is designed to further improve anomaly localization accuracy based on clustering. The whole system was tested in a parking lot with potholes, debris, and road bumps. Experimental results show that the hazards localization accuracy could be significantly improved from 7.4m to 1.4m with 84% accuracy using the proposed system. The proposed real-time system could bring significant benefits for commercial vehicles, and transportation companies with improved safety and ride quality. © 2023 SAE International. All Rights Reserved.
KW  - bump
KW  - Connected Vehicle
KW  - debris
KW  - GPS
KW  - Kalman filter
KW  - pothole
KW  - road anomaly
KW  - sensor fusion
KW  - stereo camera
KW  - Anomaly detection
KW  - Cameras
KW  - Commercial vehicles
KW  - Data handling
KW  - Debris
KW  - Fuel economy
KW  - Hazards
KW  - Information management
KW  - Landforms
KW  - Motor transportation
KW  - Real time systems
KW  - Roads and streets
KW  - Stereo image processing
KW  - Anomaly localizations
KW  - Bump
KW  - Connected vehicle
KW  - Localization accuracy
KW  - Pothole
KW  - Processing modules
KW  - Road anomaly
KW  - Road hazards
KW  - Sensor fusion
KW  - Stereo cameras
KW  - Kalman filters
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, B.
AU  - Ma, G.
AU  - Sui, H.
AU  - Zhang, Y.
AU  - Zhang, H.
AU  - Zhou, Y.
TI  - Few-Shot Object Detection in Remote Sensing Imagery via Fuse Context Dependencies and Global Features
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15143462
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166259832&doi=10.3390%2frs15143462&partnerID=40&md5=684c1adc372ce90ed81e14c854a09b86
AB  - The rapid development of Earth observation technology has promoted the continuous accumulation of images in the field of remote sensing. However, a large number of remote sensing images still lack manual annotations of objects, which makes the strongly supervised deep learning object detection method not widely used, as it lacks generalization ability for unseen object categories. Considering the above problems, this study proposes a few-shot remote sensing image object detection method that integrates context dependencies and global features. The method can be used to fine-tune the model with a small number of sample annotations based on the model trained in the base class, as a way to enhance the detection capability of new object classes. The method proposed in this study consists of three main modules, namely, the meta-feature extractor (ME), reweighting module (RM), and feature fusion module (FFM). These three modules are respectively used to enhance the context dependencies of the query set features, improve the global features of the support set that contains annotations, and finally fuse the query set features and support set features. The baseline of the meta-feature extractor of the entire framework is based on the optimized YOLOv5 framework. The reweighting module of the support set feature extraction is based on a simple convolutional neural network (CNN) framework, and the foreground feature enhancement of the support sets was made in the preprocessing stage. This study achieved beneficial results in the two benchmark datasets NWPU VHR-10 and DIOR. Compared with the comparison methods, the proposed method achieved the best performance in the object detection of the base class and the novel class. © 2023 by the authors.
KW  - context dependencies
KW  - feature fusion
KW  - few-shot object detection (FSOD)
KW  - global features
KW  - graph convolutional unit (GCU)
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Feature extraction
KW  - Object recognition
KW  - Remote sensing
KW  - Base class
KW  - Context dependency
KW  - Features fusions
KW  - Few-shot object detection
KW  - Global feature
KW  - Graph convolutional unit
KW  - Metafeature
KW  - Object detection method
KW  - Objects detection
KW  - Remote sensing images
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, J.
AU  - Liu, X.
AU  - Chen, Q.
AU  - Niu, S.
TI  - A Traffic Parameter Extraction Model Using Small Vehicle Detection and Tracking in Low-Brightness Aerial Images
PY  - 2023
T2  - Sustainability (Switzerland)
DO  - 10.3390/su15118505
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161660499&doi=10.3390%2fsu15118505&partnerID=40&md5=94a5f95d31ea6cb778f9e4ed1a9f3e4b
AB  - It is still a challenge to detect small-size vehicles from a drone perspective, particularly under low-brightness conditions. In this context, a YOLOX-IM-DeepSort model was proposed, which improved the object detection performance in low-brightness conditions accurately and efficiently. At the stage of object detection, this model incorporates the data enhancement algorithm as well as an ultra-lightweight subspace attention module, and optimizes the number of detection heads and the loss function. Then, the ablation experiment was conducted and the analysis results showed that the YOLOX-IM model has better mAP than the baseline model YOLOX-s for multi-scale object detection. At the stage of object tracking, the DeepSort object-tracking algorithm is connected to the YOLOX-IM model, which can extract vehicle classification data, vehicle trajectory, and vehicle speed. Then, the VisDrone2021 dataset was adopted to verify the object-detection and tracking performance of the proposed model, and comparison experiment results showed that the average vehicle detection accuracy is 85.00% and the average vehicle tracking accuracy is 71.30% at various brightness levels, both of which are better than those of CenterNet, YOLOv3, FasterR-CNN, and CascadeR-CNN. Next, a field experiment using an in-vehicle global navigation satellite system and a DJI Phantom 4 RTK drone was conducted in Tianjin, China, and 12 control experimental scenarios with different drone flight heights and vehicle speeds were designed to analyze the effect of drone flight altitude on speed extraction accuracy. Finally, the conclusions and discussions were presented. © 2023 by the authors.
KW  - low brightness images
KW  - traffic information and control
KW  - unmanned aerial vehicles
KW  - vehicle detection and tracking
KW  - China
KW  - Tianjin
KW  - detection method
KW  - GNSS
KW  - image analysis
KW  - satellite imagery
KW  - tracking
KW  - unmanned vehicle
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hong, J.-W.
AU  - Kim, S.-H.
AU  - Han, G.-T.
TI  - Detection of Multiple Respiration Patterns Based on 1D SNN from Continuous Human Breathing Signals and the Range Classification Method for Each Respiration Pattern
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23115275
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161584072&doi=10.3390%2fs23115275&partnerID=40&md5=5b6723ff4d942f27f4b08ed12ae50efe
AB  - Human respiratory information is being used as an important source of biometric information that can enable the analysis of health status in the healthcare domain. The analysis of the frequency or duration of a specific respiration pattern and the classification of respiration patterns in the corresponding section for a certain period of time are important for the utilization of respiratory information in various ways. Existing methods require window slide processing to classify sections for each respiration pattern from the breathing data for a certain time period. In this case, when multiple respiration patterns exist within one window, the recognition rate can be lowered. To solve this problem, a 1D Siamese neural network (SNN)-based human respiration pattern detection model and a merge-and-split algorithm for the classification of multiple respiration patterns in each region for all respiration sections are proposed in this study. When calculating the accuracy based on intersection over union (IOU) for the respiration range classification result for each pattern, the accuracy was found to be improved by approximately 19.3% compared with the existing deep neural network (DNN) and 12.4% compared with a 1D convolutional neural network (CNN). The accuracy of detection based on the simple respiration pattern was approximately 14.5% higher than that of the DNN and 5.3% higher than that of the 1D CNN. © 2023 by the authors.
KW  - 1D SNN
KW  - MASRP
KW  - mmWave sensor
KW  - one-dimensional (1D) CNN
KW  - respiration patterns
KW  - Algorithms
KW  - Humans
KW  - Neural Networks, Computer
KW  - Recognition, Psychology
KW  - Respiration
KW  - Respiratory Rate
KW  - Classification (of information)
KW  - Convolutional neural networks
KW  - Frequency domain analysis
KW  - Pattern recognition
KW  - 1d siamese neural network
KW  - Breathing signals
KW  - Convolutional neural network
KW  - Human breathing
KW  - MASRP
KW  - Mm-wave sensors
KW  - Neural-networks
KW  - One-dimensional
KW  - One-dimensional (1d) convolutional neural network
KW  - Respiration pattern
KW  - algorithm
KW  - artificial neural network
KW  - breathing
KW  - breathing rate
KW  - human
KW  - recognition
KW  - Deep neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shen, Y.
AU  - Chandaka, B.
AU  - Lin, Z.-H.
AU  - Zhai, A.
AU  - Cui, H.
AU  - Forsyth, D.
AU  - Wang, S.
TI  - Sim-on-Wheels: Physical World in the Loop Simulation for Self-Driving
PY  - 2023
T2  - IEEE Robotics and Automation Letters
DO  - 10.1109/LRA.2023.3325689
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174804455&doi=10.1109%2fLRA.2023.3325689&partnerID=40&md5=4f49a72e9c1f4495852205ab83a900e7
AB  - We present Sim-on-Wheels, a safe, realistic, and vehicle-in-loop framework to test autonomous vehicles' performance in the real world under safety-critical scenarios. Sim-on-wheels runs on a self-driving vehicle operating in the physical world. It creates virtual traffic participants with risky behaviors and seamlessly inserts the virtual events into images perceived from the physical world in real-time. The manipulated images are fed into autonomy, allowing the self-driving vehicle to react to such virtual events. The full pipeline runs on the actual vehicle and interacts with the physical world, but the safety-critical events it sees are virtual. Sim-on-Wheels is safe, interactive, realistic, and easy to use. The experiments demonstrate the potential of Sim-on-Wheels to facilitate the process of testing autonomous driving in challenging real-world scenes with high fidelity and low risk.  © 2016 IEEE.
KW  - Autonomous agents
KW  - robot safety
KW  - simulation and animation
KW  - Autonomous agents
KW  - Interactive computer systems
KW  - Pipelines
KW  - Rendering (computer graphics)
KW  - Robots
KW  - Safety engineering
KW  - Three dimensional computer graphics
KW  - Three dimensional displays
KW  - Vehicle wheels
KW  - Physical world
KW  - Real - Time system
KW  - Real-world
KW  - Rendering (computer graphic)
KW  - Road
KW  - Robot safety
KW  - Robot sensing system
KW  - Self drivings
KW  - Simulation and animation
KW  - Three-dimensional display
KW  - Real time systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sithiwichankit, C.
AU  - Chanchareon, R.
TI  - Advanced Stiffness Sensing through the Pincer Grasping of Soft Pneumatic Grippers
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23136094
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164846358&doi=10.3390%2fs23136094&partnerID=40&md5=4b52d56587f5c65a05b4dea722b8701b
AB  - In this study, a comprehensive approach for sensing object stiffness through the pincer grasping of soft pneumatic grippers (SPGs) is presented. This study was inspired by the haptic sensing of human hands that allows us to perceive object properties through grasping. Many researchers have tried to imitate this capability in robotic grippers. The association between gripper performance and object reaction must be determined for this purpose. However, soft pneumatic actuators (SPA), the main components of SPGs, are extremely compliant. SPA compliance makes the determination of the association challenging. Methodologically, the connection between the behaviors of grasped objects and those of SPAs was clarified. A new concept of SPA modeling was then introduced. A method for stiffness sensing through SPG pincer grasping was developed based on this connection, and demonstrated on four samples. This method was validated through compression testing on the same samples. The results indicate that the proposed method yielded similar stiffness trends with slight deviations in compression testing. A main limitation in this study was the occlusion effect, which leads to dramatic deviations when grasped objects greatly deform. This is the first study to enable stiffness sensing and SPG grasping to be carried out in the same attempt. This study makes a major contribution to research on soft robotics by progressing the role of sensing for SPG grasping and object classification by offering an efficient method for acquiring another effective class of classification input. Ultimately, the proposed framework shows promise for future applications in inspecting and classifying visually indistinguishable objects. © 2023 by the authors.
KW  - object classification
KW  - pincer grasping
KW  - pneumatic gripper
KW  - sensible grasping
KW  - soft gripper
KW  - stiffness sensing
KW  - Equipment Design
KW  - Hand
KW  - Hand Strength
KW  - Humans
KW  - Pressure
KW  - Robotics
KW  - Association reactions
KW  - Compression testing
KW  - Grippers
KW  - Pneumatic actuators
KW  - Pneumatics
KW  - Grasped object
KW  - Haptic sensing
KW  - Human hands
KW  - Object classification
KW  - Object property
KW  - Pincer grasping
KW  - Pneumatic grippers
KW  - Sensible grasping
KW  - Soft gripper
KW  - Stiffness sensing
KW  - equipment design
KW  - hand
KW  - hand strength
KW  - human
KW  - pressure
KW  - procedures
KW  - robotics
KW  - Stiffness
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Yang, Q.
AU  - Zhu, H.
AU  - Zhao, H.
AU  - Tang, X.
AU  - Dai, S.
AU  - Chen, J.
TI  - CG-YOLO: A Lightweight, High-Accuracy Gesture Recognition Method for Human-Computer Interaction
PY  - 2024
T2  - Proceedings - 2024 China Automation Congress, CAC 2024
DO  - 10.1109/CAC63892.2024.10864609
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000752012&doi=10.1109%2fCAC63892.2024.10864609&partnerID=40&md5=6da1ffb2e5f8f0b503e6c26dc233e604
AB  - Although gesture recognition technology has made significant progress, its accuracy and robustness in practical applications are still insufficient due to factors such as complex environments, lighting variations, and background interference. This paper proposes a lightweight and high-accuracy human-computer interaction gesture recognition method called CG-YOLO to address these issues. We reconstruct the backbone network of YOLOv8s by introducing a Contextual Feature Aggregation Backbone Network (ContextRepViT) to enhance the model’s accuracy in recognizing small-sized hand targets, thus solving the problems of missed and false detections in complex backgrounds. Additionally, we propose a lightweight feature extraction module (GSRepNCSP) to redesign the Contextual 2D Features (C2f) modules in the Head network of YOLOv8s, improving parameters utilization and reducing computational load. Experimental results on the public HaGRID dataset show that, compared to YOLOv8s, the CG-YOLO reduces the number of parameters by 1.3M, and improves Precision, Recall, mAP@0.5, and mAP@0.5-0.95 by 3.2%, 1.5%, 2%, and 3.9%, respectively. Moreover, the human-robot interaction experiments with the quadruped robot also verified the effectiveness of the CG-YOLO. © 2024 IEEE.
KW  - Gesture recognition
KW  - Human-computer interaction
KW  - Lightweight
KW  - YOLOv8
KW  - Gesture recognition
KW  - Human robot interaction
KW  - Modular robots
KW  - Palmprint recognition
KW  - Back-bone network
KW  - Complex environments
KW  - Computer interaction
KW  - Environment lighting
KW  - Gesture recognition technologies
KW  - Gestures recognition
KW  - High-accuracy
KW  - Lightweight
KW  - Recognition methods
KW  - YOLOv8
KW  - Multipurpose robots
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, T.
AU  - Wang, G.
AU  - Wu, L.
AU  - Pu, H.
AU  - Luo, J.
AU  - Liu, H.
AU  - Zou, X.
AU  - Luo, J.
TI  - MD-TLCF: Miner Distance Detection Based on Trajectory-Based Low-Confidence Filter
PY  - 2024
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2024.3412212
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196118141&doi=10.1109%2fTIM.2024.3412212&partnerID=40&md5=32f56e6132b62a9c891c59203e8da922
AB  - — The confidence level of the detection result generally indicates the reliability of the detected object. However, in underground coal mines with low light intensity and uneven light distribution, the confidence level of detection results is low and exhibits significant fluctuation. Conventional 2-D miner detection methods are inadequate in delivering comprehensive 3-D miner positions to coal mine robots. This article proposes a real-time detection framework on RGBD images to generate a miner detection box and corresponding distance (miner state). Moreover, a trajectory-based low-confidence filter (TLCF) is proposed to refilter the miner state with a low confidence value based on the predicted state of the tracked miner trajectory. To evaluate the performance of distance detection methods in underground coal mines under various lighting conditions, a miner tracking dataset MINERTKRGBD is proposed. In the evaluation of this dataset, the proposed framework based on RGBD images outperforms the state-of-the-art vision methods. The miner distance detection method using TLCF (MD-TLCF) outperforms the method that solely relied on a confidence threshold filter (CTF). The Recall50 of the MD-TLCF method improved by approximately 4.2% compared to the latter, without any decrease in Precision50. Our code repository and dataset are publicly available at https://github.com/HT-hlf/MD-TLCF.git. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
KW  - Kalman filter
KW  - low confidence
KW  - miner distance detection
KW  - underground coal mine
KW  - Bandpass filters
KW  - Coal
KW  - Coal mines
KW  - Feature extraction
KW  - Miners
KW  - Object detection
KW  - Three dimensional displays
KW  - Trajectories
KW  - Coal-mining
KW  - Distances detections
KW  - Features extraction
KW  - Information filter
KW  - Low confidence
KW  - Miner distance detection
KW  - Point cloud compression
KW  - Point-clouds
KW  - Three-dimensional display
KW  - Underground coal mine
KW  - Kalman filters
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Mohammed, M.A.
AU  - Mohammed, M.H.
AU  - Abed Alsultani, H.A.
AU  - Kassim Ahmad, H.
AU  - Hikmat, R.
AU  - Migo, P.
AU  - Zhyrov, G.
TI  - Analyzing the Role of Arduino and LTE in IoT-Powered Adaptive Traffic Solutions
PY  - 2024
T2  - Conference of Open Innovation Association, FRUCT
DO  - 10.23919/FRUCT64283.2024.10749917
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210842923&doi=10.23919%2fFRUCT64283.2024.10749917&partnerID=40&md5=cee8be1d76d5440535434e52ea570d33
AB  - Background: Urban traffic demands efficient management solutions to reduce congestion and improve flow. Traditional traffic signal systems, mostly static, struggle to track urban activity.Objective: This article uses IoT technologies, Arduino microcontrollers, and LTE connection to create an adaptive traffic light system that constantly adjusts traffic signal lengths to maximize traffic flow.Methodology: We created a prototype adaptive traffic light system using Arduino microcontrollers with LTE modules and sensors. The sensors send Real-time traffic data over LTE to a cloud server. The technology uses machine learning algorithms to assess data and traffic conditions and remotely alter traffic signal timings via IoT.Results: The prototype improved traffic flow and reduced congestion during peak hours at chosen junctions. In quantitative terms, traffic throughput rose 25%, and intersection waiting times decreased by 35%. Idling time reduction was anticipated to lower vehicle emissions.Conclusion: Arduino and LTE connection in an IoT-based adaptive traffic signal system show promise for urban traffic management. Traffic flow, waiting times, and emissions improve, proving its scalability and enabling cities to a sustainable and effective traffic management plan as vehicle loads rise. Further study is needed to determine its efficacy in different metropolitan topologies and traffic patterns. © 2024 FRUCT Oy.
KW  - adaptive systems
KW  - Arduino
KW  - cloud computing
KW  - emissions reduction
KW  - IoT (Internet of Things)
KW  - LTE (Long-Term Evolution)
KW  - machine learning
KW  - real-time data
KW  - traffic management
KW  - vehicular flow
KW  - 4G mobile communication systems
KW  - Advanced traffic management systems
KW  - Air traffic control
KW  - Bioluminescence
KW  - Emission control
KW  - Highway administration
KW  - Highway traffic control
KW  - Image thinning
KW  - Information management
KW  - Long Term Evolution (LTE)
KW  - Luminescence of gases
KW  - Luminescence of liquids and solutions
KW  - Luminescence of solids
KW  - Motor transportation
KW  - Phosphorescence
KW  - Street traffic control
KW  - Traffic congestion
KW  - Urban transportation
KW  - Vehicle actuated signals
KW  - Arduino
KW  - Cloud-computing
KW  - Emission reduction
KW  - Internet of thing
KW  - Long-term evolution
KW  - Machine-learning
KW  - Real-time data
KW  - Traffic management
KW  - Vehicular flow
KW  - Microcontrollers
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ruan, J.
AU  - Cui, H.
AU  - Huang, Y.
AU  - Li, T.
AU  - Wu, C.
AU  - Zhang, K.
TI  - A review of occluded objects detection in real complex scenarios for autonomous driving
PY  - 2023
T2  - Green Energy and Intelligent Transportation
DO  - 10.1016/j.geits.2023.100092
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160397104&doi=10.1016%2fj.geits.2023.100092&partnerID=40&md5=8f750cc3bae9e34e146eaf769ca59501
AB  - Autonomous driving is a promising way to future safe, efficient, and low-carbon transportation. Real-time accurate target detection is an essential precondition for the generation of proper following decision and control signals. However, considering the complex practical scenarios, accurate recognition of occluded targets is a major challenge of target detection for autonomous driving with limited computational capability. To reveal the overlap and difference between various occluded object detection by sharing the same available sensors, this paper presents a review of detection methods for occluded objects in complex real-driving scenarios. Considering the rapid development of autonomous driving technologies, the research analyzed in this study is limited to the recent five years. The study of occluded object detection is divided into three parts, namely occluded vehicles, pedestrians and traffic signs. This paper provided a detailed summary of the target detection methods used in these three parts according to the differences in detection methods and ideas, which is followed by the comparison of advantages and disadvantages of different detection methods for the same object. Finally, the shortcomings and limitations of the existing detection methods are summarized, and the challenges and future development prospects in this field are discussed. © 2023 The Author(s)
KW  - Autonomous driving
KW  - Object detection
KW  - Occluded objects
KW  - Pedestrians
KW  - Traffic signs
KW  - Vehicles
KW  - Autonomous vehicles
KW  - Object recognition
KW  - Pedestrian safety
KW  - Traffic signs
KW  - Autonomous driving
KW  - Control signal
KW  - Detection methods
KW  - Low carbon transportations
KW  - Objects detection
KW  - Occluded objects
KW  - Pedestrian
KW  - Real- time
KW  - Targets detection
KW  - Time-accurate
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, G.
AU  - Yuan, X.
AU  - Yao, X.
AU  - Yan, K.
AU  - Zeng, Q.
AU  - Xie, X.
AU  - Han, J.
TI  - Towards Large-Scale Small Object Detection: Survey and Benchmarks
PY  - 2023
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
DO  - 10.1109/TPAMI.2023.3290594
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163766263&doi=10.1109%2fTPAMI.2023.3290594&partnerID=40&md5=8629804ba8256a873bf0f9f07799f64e
AB  - With the rise of deep convolutional neural networks, object detection has achieved prominent advances in past years. However, such prosperity could not camouflage the unsatisfactory situation of Small Object Detection (SOD), one of the notoriously challenging tasks in computer vision, owing to the poor visual appearance and noisy representation caused by the intrinsic structure of small targets. In addition, large-scale dataset for benchmarking small object detection methods remains a bottleneck. In this paper, we first conduct a thorough review of small object detection. Then, to catalyze the development of SOD, we construct two large-scale Small Object Detection dAtasets (SODA), SODA-D and SODA-A, which focus on the Driving and Aerial scenarios respectively. SODA-D includes 24828 high-quality traffic images and 278433 instances of nine categories. For SODA-A, we harvest 2513 high resolution aerial images and annotate 872069 instances over nine classes. The proposed datasets, as we know, are the first-ever attempt to large-scale benchmarks with a vast collection of exhaustively annotated instances tailored for multi-category SOD. Finally, we evaluate the performance of mainstream methods on SODA. We expect the released benchmarks could facilitate the development of SOD and spawn more breakthroughs in this field.  © 1979-2012 IEEE.
KW  - Benchmark
KW  - convolutional neural networks
KW  - deep learning
KW  - object detection
KW  - small object detection
KW  - Antennas
KW  - Benchmarking
KW  - Convolution
KW  - Deep neural networks
KW  - Job analysis
KW  - Object detection
KW  - Object recognition
KW  - Benchmark
KW  - Benchmark testing
KW  - Convolutional neural network
KW  - Deep learning
KW  - Features extraction
KW  - Objects detection
KW  - Pedestrian
KW  - Small object detection
KW  - Task analysis
KW  - article
KW  - catalysis
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Wang, C.
AU  - Shi, C.
AU  - Liu, Y.
AU  - Lu, M.
TI  - Mask-Guided Mamba Fusion for Drone-Based Visible-Infrared Vehicle Detection
PY  - 2024
T2  - IEEE Transactions on Geoscience and Remote Sensing
DO  - 10.1109/TGRS.2024.3452550
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202732974&doi=10.1109%2fTGRS.2024.3452550&partnerID=40&md5=e78e01bb92c9775596b23d87f270fd68
AB  - Drone-based vehicle detection is a critical task within intelligent transportation systems. The existing methods that rely solely on single visible or infrared modalities often struggle to achieve both precise and robust detection. Effectively integrating cross-modal information to assist in vehicle detection remains a significant challenge. In this article, we propose a mask-guided Mamba fusion (MGMF) method for visible-infrared vehicle detection in aerial scenes. The proposed MGMF framework consists of two key components: the masked regularization constraint module (MRCM) and the state-space fusion module (SSFM). First, in MAEM, we use candidate regions from one modality to cover corresponding regions of intermediate-level features from another modality, while a regularization constraint extracts cross-modal guidance. This design allows cross-modal features focused on vehicle areas to be extracted from both modalities for fusion. Second, in SSFM, we propose mapping cross-modal features into a shared hidden state for interaction. This reduces disparities between the cross-modal features and enhances the representation, enabling better perception of intermodal correlations. When evaluated on the DroneVehicle dataset, our MGMF achieves an 80.24% with respect to mAP, establishing a new benchmark for state-of-the-art performance. Ablation studies further demonstrate the effectiveness of our MAEM and SSFM in enhancing visible-infrared fusion for vehicle detection. © 2024 IEEE.
KW  - Cross-modal information
KW  - drone-based vehicle detection
KW  - masked regularization constraint module (MRCM)
KW  - state-space fusion module (SSFM)
KW  - Air navigation
KW  - Aircraft detection
KW  - Automatic guided vehicles
KW  - Conformal mapping
KW  - Job analysis
KW  - Magnetic levitation vehicles
KW  - Object tracking
KW  - Vehicle locating systems
KW  - Accuracy
KW  - Constraint module
KW  - Cross-modal
KW  - Cross-modal information
KW  - Drone-based vehicle detection
KW  - Features extraction
KW  - Fusion modules
KW  - Masked regularization constraint module
KW  - Objects detection
KW  - Regularisation
KW  - State space fusion module
KW  - State-space
KW  - Task analysis
KW  - Transformer
KW  - Vehicles detection
KW  - aerial photography
KW  - detection method
KW  - image analysis
KW  - information system
KW  - infrared imagery
KW  - intelligent transportation system
KW  - remotely operated vehicle
KW  - Vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xiao, Y.
AU  - Liu, Y.
AU  - Luan, K.
AU  - Cheng, Y.
AU  - Chen, X.
AU  - Lu, H.
TI  - Deep LiDAR-Radar-Visual Fusion for Object Detection in Urban Environments
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15184433
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172995795&doi=10.3390%2frs15184433&partnerID=40&md5=e5e1126de76b30347b28107be4ae6725
AB  - Robust environmental sensing and accurate object detection are crucial in enabling autonomous driving in urban environments. To achieve this goal, autonomous mobile systems commonly integrate multiple sensor modalities onboard, aiming to enhance accuracy and robustness. In this article, we focus on achieving accurate 2D object detection in urban autonomous driving scenarios. Considering the occlusion issues of using a single sensor from a single viewpoint, as well as the limitations of current vision-based approaches in bad weather conditions, we propose a novel multi-modal sensor fusion network called LRVFNet. This network effectively combines data from LiDAR, mmWave radar, and visual sensors through a deep multi-scale attention-based architecture. LRVFNet comprises three modules: a backbone responsible for generating distinct features from various sensor modalities, a feature fusion module utilizing the attention mechanism to fuse multi-modal features, and a pyramid module for object reasoning at different scales. By effectively fusing complementary information from multi-modal sensory data, LRVFNet enhances accuracy and robustness in 2D object detection. Extensive evaluations have been conducted on the public VOD dataset and the Flow dataset. The experimental results demonstrate the superior performance of our proposed LRVFNet compared to state-of-the-art baseline methods. © 2023 by the authors.
KW  - deep learning method
KW  - multi-modal sensing
KW  - multi-sensor fusion
KW  - object detection
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Learning systems
KW  - Object recognition
KW  - Optical radar
KW  - Sensor data fusion
KW  - Urban planning
KW  - 2D objects
KW  - Autonomous driving
KW  - Deep learning method
KW  - Learning methods
KW  - Multi-modal
KW  - Multi-sensor fusion
KW  - Multimodal sensing
KW  - Objects detection
KW  - Sensor modality
KW  - Urban environments
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, Q.
AU  - Li, D.
AU  - Jiang, R.
AU  - Liu, S.
AU  - Liu, H.
AU  - Li, S.
TI  - MT-FANet: A Morphology and Topology-Based Feature Alignment Network for SAR Ship Rotation Detection
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15123001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164147045&doi=10.3390%2frs15123001&partnerID=40&md5=2b6d6dd22b240884d7d9c2fa56670f14
AB  - In recent years, ship target detection in synthetic aperture radar (SAR) images has significantly progressed due to the rapid development of deep learning (DL). However, since only the spatial feature information of ship targets is utilized, the current DL-based SAR ship detection approaches cannot achieve a satisfactory performance, especially in the case of multiscale, rotations, or complex backgrounds. To address these issues, in this paper, a novel deep-learning network for SAR ship rotation detection, called a morphology and topology-based feature alignment network, is proposed which can better exploit the morphological features and inherent topological structure information. This network consists of the following three main steps: First, deformable convolution is introduced to improve the representational ability for irregularly shaped ship targets, and subsequently, a morphology and topology feature pyramid network is developed to extract inherent topological structure information. Second, based on the aforementioned features, a rotation alignment feature head is devised for fine-grained processing as well as aligning and distinguishing the features; to enable regression prediction of rotated bounding boxes; and to adopt a parameter-sharing mechanism to improve detection efficiency. Therefore, utilizing morphological and inherent topological structural information enables a superior detection performance to be achieved. Finally, we evaluate the effectiveness of the proposed method using the rotated ship detection dataset in SAR images (RSDD-SAR). Our method outperforms other DL-based algorithms with fewer parameters. The overall average precision is 90.84% and recall is 92.21%. In inshore and offshore scenarios, our method performs well for the detection of multi-scale and rotation-varying ship targets, with its average precision reaching 66.87% and 95.72%, respectively. © 2023 by the authors.
KW  - morphology features
KW  - rotating bounding boxes
KW  - ship target detection
KW  - synthetic aperture radar (SAR)
KW  - topological structure information
KW  - Alignment
KW  - Deep learning
KW  - Feature extraction
KW  - Offshore oil well production
KW  - Radar imaging
KW  - Rotation
KW  - Ships
KW  - Space-based radar
KW  - Topology
KW  - Tracking radar
KW  - Bounding-box
KW  - Morphology feature
KW  - Rotating bounding box
KW  - Ship target detection
KW  - Ship targets
KW  - Structure information
KW  - Synthetic aperture radar
KW  - Targets detection
KW  - Topological structure
KW  - Topological structure information
KW  - Synthetic aperture radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xu, X.
AU  - Song, Y.
AU  - Ge, Q.
AU  - Huang, Y.
TI  - Optimization of Ship Small Target Detection Based on YOLOv10 in Complex Ocean Environment
PY  - 2024
T2  - Proceedings of 2024 IEEE International Conference on Unmanned Systems, ICUS 2024
DO  - 10.1109/ICUS61736.2024.10839864
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217989659&doi=10.1109%2fICUS61736.2024.10839864&partnerID=40&md5=99ca24eba08608a6e07bf0dad1b3686d
AB  - Ship target detection technology plays a vital role in civilian maritime traffic monitoring and military maritime security protection, and it is the key to ensuring the safety and order of marine activities. In a complex ocean environment, the background information of optical images is complex. The ship images are missed and wrongly detected due to the change in UAV shooting height, so a CSCGhost target detection algorithm is proposed. Experiments denote that compared with the traditional YOLOv10, the mAP increases by 3.2%, the accuracy increases by 9.1%, and the recall increases by 3.2%, which has a good detection effect and is especially suitable for offshore operations. © 2024 IEEE.
KW  - Ghost
KW  - object detection
KW  - sea ships
KW  - SSFF
KW  - YOLOv10
KW  - Change detection
KW  - Military photography
KW  - Waterway transportation
KW  - Ghost
KW  - Objects detection
KW  - Ocean environment
KW  - Optimisations
KW  - Sea ship
KW  - Ship targets
KW  - Small target detection
KW  - SSFF
KW  - Targets detection
KW  - YOLOv10
KW  - Marine safety
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Sahu, S.
AU  - Sahu, S.P.
AU  - Dewangan, D.K.
TI  - Pedestrian detection using ResNet-101 based Mask R-CNN
PY  - 2023
T2  - AIP Conference Proceedings
DO  - 10.1063/5.0134276
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166934718&doi=10.1063%2f5.0134276&partnerID=40&md5=cee41ef11396a9f60f15f8e17e5447c3
AB  - Pedestrian detection is the most important process of any intelligent surveillance or advanced autonomous vehicle system. Autonomous vehicles observe the surroundings using camera, lidar, radar or sensor to detect the pedestrian from a certain distance so that vehicle can take the appropriate action. There are many frameworks that have been proposed by the researcher in the past years to make a better pedestrian detection model. The enhancement in the deep learning detection process becomes more accurate, but still, it is lacking in terms of accuracy and computational speed. To resolve this problem we are introducing a new deep learning model that uses the advantage of two most popular deep learning algorithms to detect the pedestrian more accurately in less time. In this paper we are using ResNet101 as a backbone of Mask R-CNN. Use of ResNet-101 here to extract the feature map as well as it overcomes the vanishing gradient and exploding gradient problem because of skipping connection features. Mask RCNN does masking on objects after classification to provide better visibility. The main aim of this model is to reduce the computational cost and increase the accuracy without affecting the robustness of the system. Based on the experimental result on the INRIA dataset the accuracy is reported 98.9% to 100% of the proposed model with 3.57% error rate that is less than the human error rate. We hope this model will get more improvement in the future to deal with the upcoming challenges in autonomous vehicles. © 2023 Author(s).
KW  - Autonomous vehicle
KW  - Classification
KW  - Deep learning
KW  - Pedestrian detection
KW  - ResNet-101
KW  - Segmentation
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ren, K.-Y.
AU  - Gu, M.-Y.
AU  - Yuan, Z.-Q.
AU  - Yuan, S.
TI  - 3D object detection algorithms in autonomous driving: A review
ST  - 自动驾驶3D目标检测研究综述
PY  - 2023
T2  - Kongzhi yu Juece/Control and Decision
DO  - 10.13195/j.kzyjc.2022.0618
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160825452&doi=10.13195%2fj.kzyjc.2022.0618&partnerID=40&md5=dbba5d1ca9eae452e72c782adb3f20e4
AB  - Accurate and real-time object detection is one of the important functions for autonomous vehicles to accurately perceive the surrounding complex environment. Nevertheless, how to get the accurate size, distance, position, posture and other 3D information of surrounding objects is a classic problem. 3D object detection for autonomous driving has become a popular research field in recent years. Main research progress in this field is reviewed. Firstly, the characteristics of relevant sensors in the surrounding environment of autonomous driving is introduced. Then, the development of object detection from 2D to 3D is introduced and the loss functions is applied for optimization. According to the type of data acquired by the sensor, 3D object detection algorithms is categorized into three types, which are algorithms based on monocular/stereo images, point clouds, image and point cloud fusion. Futhermore, the classic and improved algorithms for each type of 3D object detection are reviewed, analyzed, and compared in detail. Simultaneously, the mainstream autonomous driving datasets and the evaluation criteria of their 3D object detection algorithms are summarized. Extensive experiment results of KITTI and NuScenes datasets are also compared and analyzed, which is widely used inpresent literature, summarizing the difficulties and problems of the existing algorithms. Besides, the opportunities and challenges of 3D object detection in data processing, feature extraction strategy, multi-sensor fusion and data distribution problems are proposed in hope of inspiring more future work. © 2023 Northeast University. All rights reserved.
KW  - 3D object detection
KW  - autonomous driving
KW  - computer vision
KW  - deep learning
KW  - object detection
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Deep learning
KW  - Feature extraction
KW  - Object recognition
KW  - Sensor data fusion
KW  - Signal detection
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - Object detection algorithms
KW  - Objects detection
KW  - Point-clouds
KW  - Real- time
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Thoo, Y.-J.
AU  - Jeanneret Medina, M.
AU  - Froehlich, J.E.
AU  - Ruffieux, N.
AU  - Lalanne, D.
TI  - A Large-Scale Mixed-Methods Analysis of Blind and Low-vision Research in ACM and IEEE
PY  - 2023
T2  - ASSETS 2023 - Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility
DO  - 10.1145/3597638.3608412
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177838134&doi=10.1145%2f3597638.3608412&partnerID=40&md5=e3aa4a7dc9a1794ff1a53c616ac32a04
AB  - Technologies for blind and low-vision (BLV) people have long been a focus of Human-Computer Interaction (HCI) and accessibility (ASSETS) research. To map and assess this cross-disciplinary field, prior literature reviews have focused on specific BLV research areas (e.g., navigation assistance) or study methodologies (e.g., qualitative methods). In this paper, we provide a more holistic examination, combining both quantitative bibliometric analyses with qualitative assessments. Using keyword queries of terms focused on the human (e.g., people) and their visual status (e.g., blind, low-vision), we first derived a dataset of 880 papers published between 2010-2022 from ACM and IEEE conferences and journals. We then apply a programmatic analysis of this dataset followed by a qualitative analysis of the 100 most-cited papers. Our findings highlight four major research areas: Accessibility at Home & on the Go, Non-Visual Interaction, Orientation & Mobility, and Education. We also capture the diversity of denominations used to refer to the BLV community and their co-occurrences, as well as computer systems targeting both blind and low-vision users with a focus on visual substitution. We close by suggesting areas for future work and hope to stimulate discussions in our field.  © 2023 Owner/Author.
KW  - bibliographic coupling
KW  - blind
KW  - low-vision
KW  - systematic review
KW  - visual impairment
KW  - Bibliographic couplings
KW  - Blind
KW  - Large-scales
KW  - Low vision
KW  - Method analysis
KW  - Mixed method
KW  - Research areas
KW  - Systematic Review
KW  - Vision research
KW  - Visual impairment
KW  - Human computer interaction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Madan, C.R.
TI  - Memories That Matter: How We Remember Important Things
PY  - 2024
T2  - Memories That Matter: How We Remember Important Things
DO  - 10.4324/9780429032028
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191251146&doi=10.4324%2f9780429032028&partnerID=40&md5=50acd92e5f63db88a7921f8a9fbbcc71
AB  - What makes some experiences more memorable than others? How can you better remember specific information later? Memories That Matter addresses these questions and more. The book is divided into three main parts, with each part focusing on a different aspect of memory. After the introductory first part, Part II discusses everyday uses of memory and why we remember, establishing a foundation for how memory is structured and stored in the brain. Part III dives into what makes us remember. Emotional and rewarding experiences are both more memorable than mundane experiences but are often studied using different approaches. Self-relevance and objects we can interact with are remembered better than less relevant information. The author explores these motivation-related influences on memory and considers whether a common mechanism underlies them all. Part IV changes the focus, discussing how we sometimes want to remember specific information that does not automatically capture our attention. The book considers evidence-based learning strategies and memory strategies, whilst also exploring real-world applications, with discussion of professions that accomplish amazing memory feats daily. The book concludes with a reflection on how the role of memory is changing as our world makes information increasingly accessible, particularly with the ever-expanding influence of the internet. Drawing from a variety of literatures and perspectives, this important book will be relevant for all students of memory from psychology, cognitive neuroscience, and related health backgrounds. © 2023 Christopher R. Madan.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Luo, Y.
AU  - Xu, D.
AU  - Zhou, G.
AU  - Sun, Y.
AU  - Lu, S.
TI  - Impact of Raindrops on Camera-Based Detection in Software-Defined Vehicles
PY  - 2024
T2  - Proceedings - 2024 IEEE International Conference on Mobility, Operations, Services and Technologies, MOST 2024
DO  - 10.1109/MOST60774.2024.00028
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201224774&doi=10.1109%2fMOST60774.2024.00028&partnerID=40&md5=1f94d70a49c911fd28820403214a4aaa
AB  - Raindrops adhering to windshields or camera lenses substantially impair visibility, leading to significant camera-based detection challenges for software-defined vehicles in both daytime and nighttime conditions. Addressing the impact of raindrops is thus crucial. This work begins by classifying four prevalent types of raindrops within the BDD100K dataset, identifying microsphere raindrops as particularly impactful in rainy conditions. We then conduct a quantitative analysis focusing on the density and diameter of raindrops, underscoring the pronounced impacts of small-density raindrops on detection performance. To mitigate raindrop interference, we introduce and assess the SR3 model for raindrop removal, applying it to both synthetic raindrop-degraded data and real-world rainy data. Besides, we propose YOLO-RA, a novel and fast model to resolve the issues of missing small-size objects and erroneous detections in irrelevant regions. Next, a novel pipeline that combines SR3 with YOLO-RA markedly improves accuracy and processing speed. Finally, we discuss our experimental observations extensively and offer detailed explanations, contributing to understanding SDVs' operational effectiveness in adverse weather conditions.  © 2024 IEEE.
KW  - adverse weather
KW  - computer vision
KW  - impact mitigation
KW  - raindrops
KW  - software-defined vehicles
KW  - Cameras
KW  - Classification (of information)
KW  - Computer software
KW  - Computer vision
KW  - Drops
KW  - Object detection
KW  - Adverse weather
KW  - Camera-based
KW  - Condition
KW  - Detection performance
KW  - FAST model
KW  - Impact mitigation
KW  - Raindrop
KW  - Rainy conditions
KW  - Real-world
KW  - Software-defined vehicle
KW  - Vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bolbot, V.
AU  - Sandru, A.
AU  - Saarniniemi, T.
AU  - Puolakka, O.
AU  - Kujala, P.
AU  - Valdez Banda, O.A.
TI  - Small Unmanned Surface Vessels—A Review and Critical Analysis of Relations to Safety and Safety Assurance of Larger Autonomous Ships
PY  - 2023
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse11122387
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180642087&doi=10.3390%2fjmse11122387&partnerID=40&md5=489488d7fdbc9c698f6b0f11e49318ff
AB  - Autonomous ships represent an emerging paradigm within the maritime sector, poised to bring multiple advantages. Although numerous prototypes have been developed, the deployment of large autonomous ships has predominantly remained confined to domestic waters or specialized military applications. The extensive adoption of autonomous ships is hampered by several challenges, primarily centered around safety. However, the direct assessment of autonomous technologies on large-scale vessels can be very costly. Small-scale autonomy testing may provide a cheaper option. This study reviews the current small autonomous ship models used by maritime researchers and industry practitioners. It aims to evaluate how these autonomous models currently augment and can augment safety assurances on larger autonomous ships. The review identifies relevant very small Unmanned Surface Vessels (USVs), the main research groups behind them and their applications. Then, the current use of USVs for safety and safety assurance is analyzed. Finally, the paper suggests innovative strategies and research directions for using USVs for the safety assurance of larger autonomous ships. © 2023 by the authors.
KW  - applications
KW  - bibliometric analysis
KW  - cybersecurity
KW  - safety
KW  - systematic review
KW  - unmanned surface vessels
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jin, G.
AU  - Tan, L.
AU  - Wang, X.
AU  - Zhao, J.
TI  - Multi-scale SAR road extraction method based on Duda operator
ST  - 基于Duda算子的多尺度SAR道路提取方法
PY  - 2023
T2  - Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics
DO  - 10.12305/j.issn.1001-506X.2023.10.10
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176233659&doi=10.12305%2fj.issn.1001-506X.2023.10.10&partnerID=40&md5=fac0f78672e2cffd20a7685110188868
AB  - Aiming at the problem of extracting different levels of roads in synthetic aperture radar (SAR) images, an effective multi-scale extraction method for SAR roads from coarse to fine is proposed. Based on Duda operator, the method improves the extraction probability of different levels of roads and increases the link probability of potential road segments through multi-scale extraction and results, integration of road in SAR images. On this basis, the effective extraction of roads in SAR images is realized by combing the hierarchical design of scale filtering, morphological filtering, and fragment linking. The proposed method is tested on real SAR images, and the effectiveness of the method is verified by continuous and complete extraction results of road in SAR images. © 2023 Chinese Institute of Electronics. All rights reserved.
KW  - morphology
KW  - multi-scale
KW  - road extraction
KW  - synthetic aperture radar (SAR)
KW  - Extraction
KW  - Image enhancement
KW  - Radar imaging
KW  - Roads and streets
KW  - Synthetic aperture radar
KW  - Coarse to fine
KW  - Extraction method
KW  - Link probability
KW  - Multi-scales
KW  - Result integrations
KW  - Road extraction
KW  - Road extraction method
KW  - Road segments
KW  - Synthetic aperture radar
KW  - Synthetic aperture radar images
KW  - Morphology
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhao, H.
AU  - Morgenroth, J.
AU  - Pearse, G.
AU  - Schindler, J.
TI  - A Systematic Review of Individual Tree Crown Detection and Delineation with Convolutional Neural Networks (CNN)
PY  - 2023
T2  - Current Forestry Reports
DO  - 10.1007/s40725-023-00184-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151571890&doi=10.1007%2fs40725-023-00184-3&partnerID=40&md5=ee84152dddc835b185737212978c6747
AB  - Purpose of Review: Crown detection and measurement at the individual tree level provide detailed information for accurate forest management. To efficiently acquire such information, approaches to conduct individual tree detection and crown delineation (ITDCD) using remotely sensed data have been proposed. In recent years, deep learning, specifically convolutional neural networks (CNN), has shown potential in this field. This article provides a systematic review of the studies that used CNN for ITDCD and identifies major trends and research gaps across six perspectives: accuracy assessment methods, data types, platforms and resolutions, forest environments, CNN models, and training strategies and techniques. Recent Findings: CNN models were mostly applied to high-resolution red–green–blue (RGB) images. When compared with other state-of-the-art approaches, CNN models showed significant improvements in accuracy. One study reported an increase in detection accuracy of over 11%, while two studies reported increases in F1-score of over 16%. However, model performance varied across different forest environments and data types. Several factors including data scarcity, model selection, and training approaches affected ITDCD results. Summary: Future studies could (1) explore data fusion approaches to take advantage of the characteristics of different types of remote sensing data, (2) further improve data efficiency with customised sample approaches and synthetic samples, (3) explore the potential of smaller CNN models and compare their learning efficiency with commonly used models, and (4) evaluate impacts of pre-training and parameter tunings. © 2023, The Author(s).
KW  - Crown delineation
KW  - Deep learning
KW  - Forestry
KW  - Instance segmentation
KW  - Object detection
KW  - Remote sensing
KW  - Tree detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bai, C.
AU  - Bai, X.
AU  - Wu, K.
TI  - A Review: Remote Sensing Image Object Detection Algorithm Based on Deep Learning
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12244902
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180690509&doi=10.3390%2felectronics12244902&partnerID=40&md5=c69c6b84b7b736d5808db7464aea766a
AB  - Target detection in optical remote sensing images using deep-learning technologies has a wide range of applications in urban building detection, road extraction, crop monitoring, and forest fire monitoring, which provides strong support for environmental monitoring, urban planning, and agricultural management. This paper reviews the research progress of the YOLO series, SSD series, candidate region series, and Transformer algorithm. It summarizes the object detection algorithms based on standard improvement methods such as supervision, attention mechanism, and multi-scale. The performance of different algorithms is also compared and analyzed with the common remote sensing image data sets. Finally, future research challenges, improvement directions, and issues of concern are prospected, which provides valuable ideas for subsequent related research. © 2023 by the authors.
KW  - comparative analysis of performance
KW  - deep learning
KW  - object detection
KW  - optical remote sensing image
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sun, A.
AU  - Ding, J.
AU  - Liu, J.
AU  - Zhou, H.
AU  - Zhang, J.
AU  - Zhang, P.
AU  - Dong, J.
AU  - Sun, Z.
TI  - Improved Detector Based on Yolov5 for Typical Targets on the Sea Surfaces
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13137695
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165208248&doi=10.3390%2fapp13137695&partnerID=40&md5=3a9a71c4b4b5ae38355d9fd1be7f367a
AB  - Detection of targets on sea surfaces is an important area of application that can bring great benefits to the management and control systems in marine environments. However, there are few open-source datasets accessible for the purpose of object detection on seas and rivers. In this paper, a study is conducted on the improved detection algorithms based on the YOLOv5 model. The dataset for the tests contains ten categories of typical objects that are commonly seen in the contexts of seas, including ships, devices, and structures. Multiple augmentation methods are employed in the pre-processing of the input data, which are verified to be effective in enhancing the generalization ability of the algorithm. Moreover, a new form of the loss function is proposed that highlights the effects of the high-quality boxes during training. The results demonstrate that the adapted loss function contributes to a boost in the model performance. According to the ablation studies, the synthesized methods raise the inference accuracy by making up for several shortcomings of the baseline model for the detection tasks of single or multiple targets from varying backgrounds. © 2023 by the authors.
KW  - augmentation
KW  - loss function
KW  - marine applications
KW  - object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, W.
AU  - Wu, J.
AU  - Zhang, J.
AU  - Gao, K.
AU  - Du, R.
AU  - Wu, Z.
AU  - Firkat, E.
AU  - Li, D.
TI  - Deformable convolution and coordinate attention for fast cattle detection
PY  - 2023
T2  - Computers and Electronics in Agriculture
DO  - 10.1016/j.compag.2023.108006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235566&doi=10.1016%2fj.compag.2023.108006&partnerID=40&md5=9506ce73d2cc48ee4b7c48e76c691625
AB  - Cattle detection is an important task in precision livestock farming, but it remains challenging due to the varying appearance and poses of cattle in different scenarios. In this paper, we propose a novel approach for fast cattle detection using deformable convolution and coordinate attention within YOLOv8, a SOTA object detection model. Our proposed method enhances the YOLOv8 architecture by introducing deformable convolution to capture more fine-grained spatial information and coordinate attention to emphasize important features in the detection process. We evaluate our method on a cattle dataset collected in a cattle farm and achieve superior performance compared to the baseline YOLOv8 and several SOTA object detection models. Specifically, our approach achieves a mean average precision (mAP) of 72.9% at 62.5 frames per second (FPS), which demonstrates its effectiveness and efficiency for fast cattle detection. By deploying our method on the farm's monitoring computer, our proposed approach has the potential to facilitate the development of automated cattle monitoring systems for improving animal welfare and farm management. © 2023 Elsevier B.V.
KW  - Computer vision
KW  - Precision agriculture
KW  - YOLOv8
KW  - Computer vision
KW  - Farms
KW  - Object detection
KW  - Object recognition
KW  - Precision agriculture
KW  - Detection models
KW  - Detection process
KW  - Fine grained
KW  - Important features
KW  - Objects detection
KW  - Precision Agriculture
KW  - Precision livestock farming
KW  - Spatial coordinates
KW  - Spatial informations
KW  - YOLOv8
KW  - cattle
KW  - computer vision
KW  - detection method
KW  - livestock farming
KW  - precision agriculture
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, P.
AU  - Wang, S.
AU  - Chen, J.
AU  - Li, W.
AU  - Peng, X.
TI  - Lightweight Model for Pavement Defect Detection Based on Improved YOLOv7
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23167112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168732156&doi=10.3390%2fs23167112&partnerID=40&md5=875b84153b7c5e9f11c14382a69ca1bb
AB  - Existing pavement defect detection models face challenges in balancing detection accuracy and speed while being constrained by large parameter sizes, hindering deployment on edge terminal devices with limited computing resources. To address these issues, this paper proposes a lightweight pavement defect detection model based on an improved YOLOv7 architecture. The model introduces four key enhancements: first, the incorporation of the SPPCSPC_Group grouped space pyramid pooling module to reduce the parameter load and computational complexity; second, the utilization of the K-means clustering algorithm for generating anchors, accelerating model convergence; third, the integration of the Ghost Conv module, enhancing feature extraction while minimizing the parameters and calculations; fourth, introduction of the CBAM convolution module to enrich the semantic information in the last layer of the backbone network. The experimental results demonstrate that the improved model achieved an average accuracy of 91%, and the accuracy in detecting broken plates and repaired models increased by 9% and 8%, respectively, compared to the original model. Moreover, the improved model exhibited reductions of 14.4% and 29.3% in the calculations and parameters, respectively, and a 29.1% decrease in the model size, resulting in an impressive 80 FPS (frames per second). The enhanced YOLOv7 successfully balances parameter reduction and computation while maintaining high accuracy, making it a more suitable choice for pavement defect detection compared with other algorithms. © 2023 by the authors.
KW  - CBAM convolution module
KW  - defect detection
KW  - Ghost Conv module
KW  - K-means
KW  - pavement defect detection
KW  - SPPCSPC_Group
KW  - YOLOv7
KW  - Defects
KW  - K-means clustering
KW  - Parameter estimation
KW  - Pavements
KW  - Semantics
KW  - CBAM convolution module
KW  - Defect detection
KW  - Detection models
KW  - Detection speed
KW  - Ghost conv module
KW  - K-means
KW  - Pavement defect detection
KW  - SPPCSPC_group
KW  - YOLOv7
KW  - article
KW  - calculation
KW  - feature extraction
KW  - k means clustering
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, W.
AU  - Zhou, B.
AU  - Wang, Z.
AU  - Yu, G.
AU  - Yang, S.
TI  - FPPNet: A Fixed-Perspective-Perception Module for Small Object Detection Based on Background Difference
PY  - 2023
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2023.3263539
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153401485&doi=10.1109%2fJSEN.2023.3263539&partnerID=40&md5=1e8c9c3ce4c0932ea450a45b2ec94060
AB  - A roadside sensing unit can provide over-the-horizon perception information for autonomous vehicles due to its high perception perspective. However, numerous challenges need to be overcome such as the missing detection of small objects and occluded objects. To this end, this study proposed a fixed perspective perception (FPP) module, which considered background subtraction and a fixed camera for small object detection. The proposed FPP module was divided into two parts: a grayscale background subtraction (GBS) submodule and a background-current image fusion (BCF) submodule. Specifically, the GBS submodule introduces background spatial information into a current frame, and the BCF submodule combines feature maps of a current frame and background by using channel attention. Moreover, we designed an object detection network called FPPNet which uses the FPP module to facilitate small object detection. Experimental results demonstrate that the FPPNet achieved 39.8% average precision small ( AP) and 65.7% AP in a Dair-V2X-I dataset. Futhermore, we conducted an extension of the FPP module to mainstream object detection networks such as CenterNet, Faster-Rcnn, and RetinaNet. Experimental results show that the proposed module can effectively improve small object detection accuracy of the networks mentioned earlier.  © 2001-2012 IEEE.
KW  - Background difference
KW  - feature fusion
KW  - object detection
KW  - roadside sensor
KW  - Cameras
KW  - Image fusion
KW  - Object detection
KW  - Object recognition
KW  - Roadsides
KW  - Semantics
KW  - Background differences
KW  - Features extraction
KW  - Features fusions
KW  - Gray scale
KW  - Location awareness
KW  - Objects detection
KW  - Roadside sensor
KW  - Small object detection
KW  - Submodules
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, J.
AU  - Cai, Q.
AU  - Zou, F.
AU  - Zhu, Y.
AU  - Liao, L.
AU  - Guo, F.
TI  - BiGA-YOLO: A Lightweight Object Detection Network Based on YOLOv5 for Autonomous Driving
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12122745
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163856667&doi=10.3390%2felectronics12122745&partnerID=40&md5=597196b24393bf20f44ce8bbbc4cd2cb
AB  - Object detection in autonomous driving scenarios has become a popular task in recent years. Due to the high-speed movement of vehicles and the complex changes in the surrounding environment, objects of different scales need to be detected, which places high demands on the performance of the network model. Additionally, different driving devices have varying performance capabilities, and a lightweight model is needed to ensure the stable operation of devices with limited computing power. To address these challenges, we propose a lightweight network called BiGA-YOLO based on YOLOv5. We design the Ghost-Hardswish Conv module to simplify the convolution operations and incorporate spatial coordinate information into feature maps using Coordinate Attention. We also replace the PANet structure with the BiFPN structure to enhance the expression ability of features through different weights during the process of fusing multi-scale feature maps. Finally, we conducted extensive experiments on the KITTI dataset, and our BiGA-YOLO achieved a mAP@0.5 of 92.2% and a mAP@0.5:0.95 of 68.3%. Compared to the baseline model YOLOv5, our proposed model achieved improvements of 1.9% and 4.7% in mAP@0.5 and mAP@0.5:0.95, respectively, while reducing the model size by 15.7% and the computational cost by 16%. The detection speed was also increased by 6.3 FPS. Through analysis and discussion of the experimental results, we demonstrate that our proposed model is superior, achieving a balance between detection accuracy, model size, and detection speed. © 2023 by the authors.
KW  - attention mechanism
KW  - BiFPN
KW  - CA
KW  - ghost module
KW  - lightweight network
KW  - object detection
KW  - YOLOv5
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, P.
AU  - Hu, Y.
AU  - Peng, S.
AU  - Zhou, L.
TI  - EMANet: An Ancient Text Detection Method Based on Enhanced-EfficientNet and Multidimensional Scale Fusion
PY  - 2024
T2  - IEEE Internet of Things Journal
DO  - 10.1109/JIOT.2024.3423667
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198269841&doi=10.1109%2fJIOT.2024.3423667&partnerID=40&md5=0b1a97491460cc12352777d51ae48dfa
AB  - Digitization through the Internet of Things (IoT) is a key measure for preserving ancient books. As a crucial aspect of digitization, text detection plays an essential role in the preservation and dissemination of ancient culture. Nevertheless, common detectors for ancient books struggle to meet the dual demands of speed and accuracy required by the IoT. Moreover, the robustness of these text detectors to capture complex features still needs to be strengthened. To tackle these issues, we introduce a new detector called EMANet. First, we substituted the feature extraction module in the core of EfficientNet-B3 with an improved Feature extraction (MBConv++) module to better capture the dependencies between channels. By incorporating this module, the network can concentrate on crucial features within the entire data sets. This significantly contributes to fulfilling the requirements for both speed and accuracy in ancient text detection. Additionally, we devised a multidimensional scale fusion (MDSF) module, effectively bolstering the scale robustness of the network. Finally, we construct a mobile ancient text digitization app suitable for the IoT. The proposed EMANet achieves an f-measure of 94.9% on shoot handwritten ancient book data set data sets, demonstrating its effectiveness. Simultaneously, we evaluate the generalization capability of EMANet using the public MTHv2 data sets. Results reveal that EMANet outperforms the majority of existing text detectors. Furthermore, our model demonstrates exceptional computational speed and precision in detection when deployed within the IoT framework, offering significant contributions to the holistic digitization of ancient texts.  © 2014 IEEE.
KW  - Ancient text detection
KW  - Enhanced-EfficientNet
KW  - Internet of Things (IoT)
KW  - multidimensional scale fusion (MDSF)
KW  - Extraction
KW  - Feature extraction
KW  - Ancient cultures
KW  - Ancient text detection
KW  - Detection methods
KW  - Digitisation
KW  - Enhanced-efficientnet
KW  - Features extraction
KW  - Multi dimensional
KW  - Multi-dimensional scale fusion
KW  - New detectors
KW  - Text detection
KW  - Internet of things
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Tian, H.
AU  - Yin, T.
AU  - Song, Z.
AU  - Hauwa, A.S.
AU  - Zhang, H.
AU  - Gao, S.
AU  - Zhou, L.
TI  - The transmission line foreign body detection algorithm based on weighted spatial attention
PY  - 2024
T2  - Frontiers in Neurorobotics
DO  - 10.3389/fnbot.2024.1424158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198743784&doi=10.3389%2ffnbot.2024.1424158&partnerID=40&md5=547a8739a041e7bb0db267dd12820dc6
AB  - Introduction: The secure operation of electric power transmission lines is essential for the economy and society. However, external factors such as plastic film and kites can cause damage to the lines, potentially leading to power outages. Traditional detection methods are inefficient, and the accuracy of automated systems is limited in complex background environments. Methods: This paper introduces a Weighted Spatial Attention (WSA) network model to address the low accuracy in identifying extraneous materials within electrical transmission infrastructure due to background texture occlusion. Initially, in the model preprocessing stage, color space conversion, image enhancement, and improved Large Selective Kernel Network (LSKNet) technology are utilized to enhance the model's proficiency in detecting foreign objects in intricate surroundings. Subsequently, in the feature extraction stage, the model adopts the dynamic sparse BiLevel Spatial Attention Module (BSAM) structure proposed in this paper to accurately capture and identify the characteristic information of foreign objects in power lines. In the feature pyramid stage, by replacing the feature pyramid network structure and allocating reasonable weights to the Bidirectional Feature Pyramid Network (BiFPN), the feature fusion results are optimized, ensuring that the semantic information of foreign objects in the power line output by the network is effectively identified and processed. Results: The experimental outcomes reveal that the test recognition accuracy of the proposed WSA model on the PL (power line) dataset has improved by three percentage points compared to that of the YOLOv8 model, reaching 97.6%. This enhancement demonstrates the WSA model's superior capability in detecting foreign objects on power lines, even in complex environmental backgrounds. Discussion: The integration of advanced image preprocessing techniques, the dynamic sparse BSAM structure, and the BiFPN has proven effective in improving detection accuracy and has the potential to transform the approach to monitoring and maintaining power transmission infrastructure. Copyright © 2024 Wang, Tian, Yin, Song, Hauwa, Zhang, Gao and Zhou.
KW  - BiFPN
KW  - BSAM
KW  - LSKNet
KW  - transmission lines
KW  - WSA
KW  - Automation
KW  - Complex networks
KW  - Electric lines
KW  - Electric power transmission networks
KW  - Image enhancement
KW  - Object detection
KW  - Power transmission
KW  - Semantics
KW  - Signal detection
KW  - Statistical tests
KW  - Textures
KW  - Bidirectional feature pyramid network
KW  - Bilevel
KW  - Bilevel spatial attention module
KW  - Feature pyramid
KW  - Large selective kernel network
KW  - Pyramid network
KW  - Spatial attention
KW  - Transmission-line
KW  - Weighted spatial attention
KW  - accuracy
KW  - algorithm
KW  - Article
KW  - artificial neural network
KW  - body weight gain
KW  - computer model
KW  - foreign body
KW  - human
KW  - interneuron
KW  - learning
KW  - learning algorithm
KW  - receptive field
KW  - satellite imagery
KW  - spatial attention
KW  - speech intelligibility
KW  - three-dimensional imaging
KW  - training
KW  - visual field
KW  - Electric power transmission
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chung, M.-A.
AU  - Wang, T.-H.
AU  - Lin, C.-W.
TI  - Advancing ESG and SDGs Goal 11: Enhanced YOLOv7-Based UAV Detection for Sustainable Transportation in Cities and Communities
PY  - 2023
T2  - Urban Science
DO  - 10.3390/urbansci7040108
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180248486&doi=10.3390%2furbansci7040108&partnerID=40&md5=135e847b03c842ce7a8a368d90120820
AB  - Environmental, social, and governance issues have gained significant prominence recently, particularly with a growing emphasis on environmental protection. In the realm of heightened environmental concerns, unmanned aerial vehicles have emerged as pivotal assets in addressing transportation challenges with a sustainable perspective. This study focuses on enhancing unmanned aerial vehicles’ object detection proficiency within the realm of sustainable transportation. The proposed method refines the YOLOv7 E-ELAN model, tailored explicitly for traffic scenarios. Leveraging strides in deep learning and computer vision, the adapted model demonstrates enhancements in mean average precision, outperforming the original on the VisDrone2019 dataset. This approach, encompassing model component enhancements and refined loss functions, establishes an efficacious strategy for precise unmanned aerial vehicles object detection. This endeavor aligns seamlessly with environmental, social, and governance principles. Moreover, it contributes to the 11th Sustainable Development Goal by fostering secure urban spaces. As unmanned aerial vehicles have become integral to public safety and surveillance, enhancing detection algorithms cultivates safer environments for residents. Sustainable transport encompasses curbing traffic congestion and optimizing transportation systems, where unmanned aerial vehicle-based detection plays a pivotal role in managing traffic flow, thereby supporting extended Sustainable Development Goal 11 objectives. The efficient utilization of unmanned aerial vehicles in public transit significantly aids in reducing carbon footprints, corresponding to the “Environmental Sustainability” facet of Environmental, Social, and Governance principles. © 2023 by the authors.
KW  - environmental
KW  - object detection
KW  - social and governance (ESG)
KW  - sustainable development goals (SDGs)
KW  - sustainable transport
KW  - traffic monitoring
KW  - unmanned aerial vehicles (UAV)
KW  - YOLOv7
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, K.
AU  - Wang, Y.
AU  - Hu, Z.
TI  - Improved YOLOv7 for Small Object Detection Algorithm Based on Attention and Dynamic Convolution
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13169316
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169137554&doi=10.3390%2fapp13169316&partnerID=40&md5=c2856cf95ea4328f81b0bb91b277afe9
AB  - The rapid advancement of deep learning has significantly accelerated progress in target detection. However, the detection of small targets remains challenging due to their susceptibility to size variations. In this paper, we address these challenges by leveraging the latest version of the You Only Look Once (YOLOv7) model. Our approach enhances the YOLOv7 model to improve feature preservation and minimize feature loss during network processing. We introduced the Spatial Pyramid Pooling and Cross-Stage Partial Channel (SPPCSPC) module, which combines the feature separation and merging ideas. To mitigate missed detections in small target scenarios and reduce noise impact, we incorporated the Coordinate Attention for Efficient Mobile Network Design (CA) module strategically. Additionally, we introduced a dynamic convolutional module to address misdetection and leakage issues stemming from significant target size variations, enhancing network robustness. An experimental validation was conducted on the FloW-Img sub-dataset provided by Okahublot. The results demonstrated that our enhanced YOLOv7 model outperforms the original network, exhibiting significant improvement in leakage reduction, with a mean Average Precision (mAP) of 81.1%. This represents a 5.2 percentage point enhancement over the baseline YOLOv7 model. In addition, the new model also has some advantages over the latest small-target-detection algorithms such as FCOS and VFNet in some respects. © 2023 by the authors.
KW  - attention module
KW  - dynamic convolution
KW  - small target detection
KW  - target detection techniques
KW  - YOLOv7 network model
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhuang, L.
AU  - Jiang, T.
AU  - Qiu, M.
AU  - Wang, A.
AU  - Huang, Z.
TI  - Transformer Generates Conditional Convolution Kernels for End-to-End Lane Detection
PY  - 2024
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2024.3430234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199491645&doi=10.1109%2fJSEN.2024.3430234&partnerID=40&md5=4f384e9d177b71c07da754f1060a746a
AB  - Environmental perception is crucial in autonomous driving technology, providing essential prior information for vehicle control and decision-making. Lane detection is gaining increasing attention as a critical component of environmental perception tasks. Mainstream frameworks rely on CNN-based architectures that often require preprocessing and postprocessing, complicating the implementation of end-to-end detection. Although Transformer-based structures address this issue from a global perspective, achieving higher accuracy remains challenging. This article proposes a novel Transformer-based end-to-end architecture, CondFormer, that processes each lane line separately, enhancing both accuracy and speed. First, we design a new Transformer structure to generate lane-by-lane parameter matrices from a global perspective instead of extracting features, efficiently constructing per-lane conditional convolution kernels. Second, to fully utilize multiscale information, CondFormer performs conditional convolution on the fused feature data. Then, each lane is further processed to obtain lane detection results using the ROW-wise method. Extensive experiments on the CULane, TuSimple, and CurveLanes datasets demonstrate that our method outperforms all Transformer-based end-to-end methods, offering a superior tradeoff between accuracy and speed. Our code is available at https://github.com/Zhuanglong2/Condformer. © 2001-2012 IEEE.
KW  - Autonomous driving
KW  - end to end
KW  - environmental perception
KW  - lane detection
KW  - Autonomous vehicles
KW  - Control system synthesis
KW  - Convolution
KW  - Economic and social effects
KW  - Feature extraction
KW  - Job analysis
KW  - Accuracy
KW  - Autonomous driving
KW  - End to end
KW  - Environmental perceptions
KW  - Features extraction
KW  - Kernel
KW  - Lane detection
KW  - Task analysis
KW  - Transformer
KW  - Decision making
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Jagtap, S.
AU  - Chopade, N.B.
AU  - Deshmukh, V.
AU  - Dhavale, K.
AU  - Hange, P.
TI  - Object Detection, Tracking, and Prediction Analysis System for Sports Performance and Coaching
PY  - 2023
T2  - 2023 7th International Conference On Computing, Communication, Control And Automation, ICCUBEA 2023
DO  - 10.1109/ICCUBEA58933.2023.10392158
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187261408&doi=10.1109%2fICCUBEA58933.2023.10392158&partnerID=40&md5=85f82eafc291fee496f525f14fffc241
AB  - Object detection and tracking systems have diverse real-world applications, including surveillance and security, autonomous vehicles, robotics, sports analysis, and medical imaging. These systems can detect and track objects of interest, providing valuable insights into various domains and enabling autonomous operation. Current object analysis systems often have limitations in accuracy and, efficiency, and realtime processing capabilities. These limitations include occlusion, motion blur, lighting conditions, scale and orientation, computational complexity, false positives and false negatives, and limited object types. These limitations can affect the applicability of object analysis systems in various domains and limit their ability to detect and track objects accurately. The proposed system aims to overcome these limitations by leveraging the powerful processing capabilities of Jetson Xavier and combining the Kalman filter and polynomial regression techniques for more accurate and efficient object analysis. The proposed system uses state-of-the-art computer vision techniques for object tracking and trajectory prediction. The proposed approach can be valuable in sports analysis, providing accurate and efficient object detection, tracking, and prediction analysis.  © 2023 IEEE.
KW  - Image pre-processing
KW  - Jetson Xavier NX module
KW  - Kalman filter
KW  - NumPy
KW  - Object detection
KW  - OpenCv
KW  - Computer vision
KW  - Forecasting
KW  - Kalman filters
KW  - Medical imaging
KW  - Object recognition
KW  - Sports
KW  - Tracking (position)
KW  - Analysis system
KW  - Image preprocessing
KW  - Jetson xavy NX module
KW  - Numpy
KW  - Object analysis
KW  - Objects detection
KW  - Opencv
KW  - Processing capability
KW  - Sports analysis
KW  - Tracking and predictions
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kaur, R.
AU  - Singh, S.
TI  - A comprehensive review of object detection with deep learning
PY  - 2022
T2  - Digital Signal Processing: A Review Journal
DO  - 10.1016/j.dsp.2022.103812
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141923901&doi=10.1016%2fj.dsp.2022.103812&partnerID=40&md5=b525e123d0ca34e5a49bb4b79066534b
AB  - In the realm of computer vision, Deep Convolutional Neural Networks (DCNNs) have demonstrated excellent performance. Video Processing, Object Detection, Image Segmentation, Image Classification, Speech Recognition and Natural Language Processing are some of the application areas of CNN. Object detection is the most crucial and challenging task of computer vision. It has numerous applications in the field of security, military, transportation and medical sciences. In this review, object detection and its different aspects have been covered in detail. With the gradual increase in the evolution of deep learning algorithms for detecting objects, a significant improvement in the performance of object detection models has been observed. However, this does not imply that the conventional object detection methods, which had been evolving for decades prior to the emergence of deep learning, had become outdated. There are some cases where conventional methods with global features are superior choice. This review paper starts with a quick overview of object detection followed by object detection frameworks, backbone convolutional neural network, and an overview of common datasets along with the evaluation metrics. Object detection problems and applications are also studied in detail. Some future research challenges in designing deep neural networks are discussed. Lastly, the performance of object detection models on PASCAL VOC and MS COCO datasets is compared and conclusions are drawn. © 2022 Elsevier Inc.
KW  - Computer vision
KW  - Conventional methods
KW  - Deep convolutional neural network
KW  - Deep learning
KW  - Object detection
KW  - Computer vision
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Image segmentation
KW  - Learning algorithms
KW  - Military applications
KW  - Natural language processing systems
KW  - Object recognition
KW  - Speech recognition
KW  - Video signal processing
KW  - Conventional methods
KW  - Convolutional neural network
KW  - Deep convolutional neural network
KW  - Deep learning
KW  - Detection models
KW  - Images segmentations
KW  - Objects detection
KW  - Performance
KW  - Segmentation images
KW  - Video processing
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kochan, O.
AU  - Osolinskyi, O.
AU  - Sachenko, A.
AU  - Kochan, V.
AU  - Romanets, I.
TI  - Simulator of Microcontroller's Power Consumption
PY  - 2023
T2  - Proceedings of the IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS
DO  - 10.1109/IDAACS58523.2023.10348884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184811472&doi=10.1109%2fIDAACS58523.2023.10348884&partnerID=40&md5=779194e8d5542d7570a823d8ea5b1baa
AB  - This paper proposed a simulator of power consumption for pulse consumers (in particular, microcontrollers, henceforth MC). It simulates the character of their power consumption. Due to the function of self-correction of error of circuit components, the simulator provides high accuracy. At the same time, the simulator provides a simulation of the MC operation at frequencies that considerably exceed the frequency of the MC clock generator. This eliminates frequency errors when calibrating the measuring channels of devices and systems designed to measure the MC's power consumption. Due to the calibration, these measurement channels can provide traceability of measurements when studying the MC's power consumption when executing instructions, commands, programs, and their fragments. The simulator can be a reference for creating the system of metrological service of the specified above measurement channels. © 2023 IEEE.
KW  - MCU
KW  - power consumption
KW  - pulse consumer
KW  - simulator
KW  - Clocks
KW  - Microcontrollers
KW  - Circuit components
KW  - Clock generator
KW  - Correction of errors
KW  - Frequency errors
KW  - High-accuracy
KW  - MCU
KW  - Measurement channels
KW  - Measuring channel
KW  - Pulse consumer
KW  - Self-correction
KW  - Electric power utilization
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Sai Haritha, K.S.D.V.N.
AU  - Ravulapalli, G.
AU  - Sunny, N.
TI  - Deep Learning Virtual Assistant for Visually Impaired with Object Detection and Distance Estimation
PY  - 2023
T2  - 14th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174259330&partnerID=40&md5=a4ae18e35bb029ff149c9bb7034098f8
AB  - According to WHO (World Health Organization) 285 million individuals worldwide suffer from vision impairment. These metrics may triple in the coming 30 years. Thus, there is a need to develop a cost-effective guiding system for those people. This system provided a solution using large dataset COCO (Common Objects in Context) dataset which consists of 90 classes of real-time objects to cover all the day-to-day life objects It used the Single Shot Detector algorithm for object detection. It makes the work of blind people easier, and more efficient by transferring wireless voice-based feedback about whether an object is too close to them or at a safer distance. Google's text-to-speech engine is being used to provide audio-based output. It estimates the distance from person to object using depth extraction methodology with focal length as a scaling factor, real object width. This model provides a guiding system to help visually impaired people. © Grenze Scientific Society, 2023.
KW  - Depth Estimation
KW  - Google Text to Speech engine(gTTS)
KW  - Object detection
KW  - Scaling Factor
KW  - Single Shot Detector (SSD)
KW  - Cost effectiveness
KW  - Deep learning
KW  - E-learning
KW  - Engines
KW  - Large dataset
KW  - Object recognition
KW  - Speech recognition
KW  - Depth Estimation
KW  - Google text to speech engine
KW  - Google+
KW  - Guiding systems
KW  - Objects detection
KW  - Scaling factors
KW  - Single shot detector
KW  - Single-shot
KW  - Text-to-speech engines
KW  - Virtual assistants
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Borade, J.L.
AU  - Muddana, A.
TI  - Performance Analysis of Different Optimization Algorithms for Multi-Class Object Detection
PY  - 2023
T2  - International Journal on Recent and Innovation Trends in Computing and Communication
DO  - 10.17762/ijritcc.v11i4.6400
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163661741&doi=10.17762%2fijritcc.v11i4.6400&partnerID=40&md5=6b1e62af58f0f5316b7ccbcbae8b78b1
AB  - Object recognition is a significant approach employed for recognizing suitable objects from the image. Various improvements, particularly in computer vision, are probable to diagnose highly difficult tasks with the assistance of local feature detection methodologies. Detecting multi-class objects is quite challenging, and many existing researches have worked to enhance the overall accuracy. But because of certain limitations like higher network loss, degraded training ability, improper consideration of features, less convergent and so on. The proposed research introduced a hybrid convolutional neural network (H-CNN) approach to overcome these drawbacks. The collected input images are pre-processed initially through Gaussian filtering to eradicate the noise and enhance the image quality. Followed by image preprocessing, the objects present in the images are localized using Grid Guided Localization (GGL I. The effective features are extracted from the localized objects using the AlexNet model. Different objects are classified by replacing the concluding softmax layer of AlexNet with Support Vector Regression (SVR) model. The losses present in the network model are optimized using the Improved Grey Wolf (IGW) optimization procedure. The performances of the proposed model are analyzed using PYTHON. Various datasets are employed, including MIT-67. PASCAL VOC2010. Microsoft (MS)-COCO and MSRC. The performances are analyzed by varying the loss optimization algorithms like improved Particle Swarm Optimization (IPSO), unproved Genetic Algorithm (IGA). and improved dragon fly algorithm (IDFA). unproved simulated annealing algorithm (ISAA) and improved bacterial foraging algorithm iIBFA). to choose the best algorithm. The proposed accuracy outcomes are attained as PASCAL VOC2010 (95.04%), MIT-67 dataset (96.02%). MSRC (97.37%), and MS COCO (94.53%), respectively. © 2023 Auricle Global Society of Education and Research. All rights reserved.
KW  - Gaussian filtering
KW  - Grey wolf optimization
KW  - Hybrid deep learning
KW  - improved optimization algorithms
KW  - Multi-class classification
KW  - Object recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, P.
AU  - Xie, Z.
AU  - Li, T.
TI  - UCN-YOLOv5: Traffic Sign Object Detection Algorithm Based on Deep Learning
PY  - 2023
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3322371
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174815830&doi=10.1109%2fACCESS.2023.3322371&partnerID=40&md5=fdffa952bf57cd77e800049433c8b7c8
AB  - Traffic sign detection plays an important role in traffic safety and traffic management. In view of the complex and changeable environment and detection accuracy of traffic sign detection, this paper proposes UCN-YOLOv5 model based on the framework of YOLOv5.This model first replaces a new backbone network, which uses the core module RSU of U2Net to enhance the feature extraction of the network. Then, ConvNeXt-V2 is integrated, and the C3 module of its Block and YOLOv5 network is used to construct the C3_CN2 structure. The utilization of the proposed lightweight receptive field attention module LPFAConv in the Head Section represents a potential enhancement for the extraction of receptive field features. Finally, for small targets in traffic signs, Normalized Wasserstein Distance (NWD), which is insensitive to targets of different scales, is added to calculate the position loss function to replace the IoU metric to a certain extent, which further improves the detection ability of our model for traffic signs. Experiments on the TT100K dataset show that UCNYOLOv5 has excellent detection performance. Compared with the baseline model (Y0Lov5s, YOLOV5m, YOLOV5l), it improves the Map.5 index by 5.9 %, 4.9 % and 4.6 %; in the Map.5:.95 index, it is 4.4 %, 3.5 % and 2.8 % better. Moreover, the enhanced algorithm demonstrated favorable performance on the LISA and CCTSDB2021 traffic sign datasets. This research has important value for the accurate detection of traffic sign detection, and has guiding significance for in-depth research in related fields. © 2013 IEEE.
KW  - Convnext
KW  - Object detection
KW  - RFAConv
KW  - traffic sign detection
KW  - U2Net
KW  - YOLO
KW  - YOLOv5
KW  - Accident prevention
KW  - Deep learning
KW  - Extraction
KW  - Indium compounds
KW  - Job analysis
KW  - Object detection
KW  - Object recognition
KW  - Traffic signs
KW  - Convnext
KW  - Features extraction
KW  - Objects detection
KW  - RFAConv
KW  - Road
KW  - Task analysis
KW  - Traffic sign detection
KW  - U2net
KW  - Vehicle safety
KW  - YOLO
KW  - YOLOv5
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Heslinga, F.G.
AU  - Ruis, F.
AU  - Ballan, L.
AU  - van Leeuwen, M.C.
AU  - Masini, B.
AU  - van Woerden, J.E.
AU  - den Hollander, R.J.M.
AU  - Berndsen, M.
AU  - Baan, J.
AU  - Dijk, J.
AU  - Huizinga, W.
TI  - Leveraging temporal context in deep learning methodology for small object detection
PY  - 2023
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.2675589
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176500066&doi=10.1117%2f12.2675589&partnerID=40&md5=0edc05defad94692df8a72db88ee2ee9
AB  - Automated object detection is becoming more relevant in a wide variety of applications in the military domain. This includes the detection of drones, ships, and vehicles in video and IR video. In recent years, deep learning-based object detection methods, such as YOLO, have shown to be promising in many applications for object detection. However, current methods have limited success when objects of interest are small in number of pixels, e.g. objects far away or small objects closer by. This is important, since accurate small object detection translates to early detection and the earlier an object is detected the more time is available for action. In this study, we investigate novel image analysis techniques that are designed to address some of the challenges of (very) small object detection by taking into account temporal information. We implement six methods, of which three are based on deep learning and use the temporal context of a set of frames within a video. The methods consider neighboring frames when detecting objects, either by stacking them as additional channels or by considering difference maps. We compare these spatio-temporal deep learning methods with YOLO-v8 that only considers single frames and two traditional moving object detection methods. Evaluation is done on a set of videos that encompasses a wide variety of challenges, including various objects, scenes, and acquisition conditions to show real-world performance. © 2023 SPIE. All rights reserved.
KW  - 3D U-Net
KW  - Deep learning
KW  - Moving object detection
KW  - Small object detection
KW  - Spatio-temporal
KW  - T2-YOLO
KW  - YOLO
KW  - Deep learning
KW  - Learning systems
KW  - Military applications
KW  - Military vehicles
KW  - Object recognition
KW  - 3d U-net
KW  - Deep learning
KW  - Military domains
KW  - Moving-object detection
KW  - Object detection method
KW  - Objects detection
KW  - Small object detection
KW  - Spatio-temporal
KW  - T2-YOLO
KW  - YOLO
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Allebosch, G.
AU  - Van Hamme, D.
AU  - Veelaert, P.
AU  - Philips, W.
TI  - Efficient detection of crossing pedestrians from a moving vehicle with an array of cameras
PY  - 2023
T2  - Optical Engineering
DO  - 10.1117/1.OE.62.3.031210
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151674361&doi=10.1117%2f1.OE.62.3.031210&partnerID=40&md5=068e7316c94ae84dc1853ad9fe889ab7
AB  - We describe a method for detecting crossing pedestrians and, in general, any object that is moving perpendicular to the driving direction of the vehicle. This is achieved by combining video snapshots from multiple cameras that are placed in a linear configuration and from multiple time instances. We demonstrate that the proposed array configuration imposes tight constraints on the expected disparity of static objects in a certain image region for a given camera pair. These regions are distinct for different camera pairs. In that manner, static regions can generally be distinguished from moving targets throughout the entire field of view when analyzing enough pairs, requiring only straightforward image processing techniques. On a self-captured dataset with crossing pedestrians, our proposed method reaches an F1 detection score of 83.66% and a mean average precision (MAP) of 84.79% on an overlap test when used stand-alone, being processed at 59 frames per second without GPU acceleration. When combining it with the Yolo V4 object detector in cooperative fusion, the proposed method boosts the maximal F1 scores of this detector on this same dataset from 87.86% to 92.68% and the MAP from 90.85% to 94.30%. Furthermore, combining it with the lower power Yolo-Tiny V4 detector in the same way yields F1 and MAP increases from 68.57% to 81.16% and 72.32% to 85.25%, respectively.  © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.
KW  - autonomous vehicles
KW  - epipolar geometry
KW  - motion segmentation
KW  - multicamera
KW  - pedestrian detection
KW  - Acceleration
KW  - Cameras
KW  - Motion analysis
KW  - Object detection
KW  - Pedestrian safety
KW  - Statistical tests
KW  - Autonomous Vehicles
KW  - Efficient detection
KW  - Epipolar geometry
KW  - Linear configuration
KW  - Motion segmentation
KW  - Moving vehicles
KW  - Multi-cameras
KW  - Multiple cameras
KW  - Pedestrian detection
KW  - Video snapshots
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xie, B.
AU  - Yang, L.
AU  - Wei, A.
AU  - Weng, X.
AU  - Li, B.
TI  - MuTrans: Multiple Transformers for Fusing Feature Pyramid on 2D and 3D Object Detection
PY  - 2023
T2  - IEEE Transactions on Image Processing
DO  - 10.1109/TIP.2023.3299190
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166734491&doi=10.1109%2fTIP.2023.3299190&partnerID=40&md5=1f3a190645da6b52e32eb9df5036590c
AB  - One of the major components of the neural network, the feature pyramid plays a vital part in perception tasks, like object detection in autonomous driving. But it is a challenge to fuse multi-level and multi-sensor feature pyramids for object detection. This paper proposes a simple yet effective framework named MuTrans (Mu ltiple Trans formers) to fuse feature pyramid in single-stream 2D detector or two-stream 3D detector. The MuTrans based on encoder-decoder focuses on the significant features via multiple Transformers. MuTrans encoder uses three innovative self-attention mechanisms: S patial-wise B oxAlign attention (SB) for low-level spatial locations, C ontext-wise A ffinity attention (CA) for high-level context information, and high-level attention for multi-level features. Then MuTrans decoder processes these significant proposals including the RoI and context affinity. Besides, the L ow and H igh-level F usion (LHF) in the encoder reduces the number of computational parameters. And the Pre-LN is utilized to accelerate the training convergence. LHF and Pre-LN are proven to reduce self-attention's computational complexity and slow training convergence. Our result demonstrates the higher detection accuracy of MuTrans than that of the baseline method, particularly in small object detection. MuTrans demonstrates a 2.1 higher detection accuracy on APS index in small object detection on MS-COCO 2017 with ResNeXt-101 backbone, a 2.18 higher 3D detection accuracy (moderate difficulty) for small object-pedestrian on KITTI, and 6.85 higher RC index (Town05 Long) on CARLA urban driving simulator platform.  © 1992-2012 IEEE.
KW  - feature pyramid
KW  - object detection
KW  - sensor fusion
KW  - Transformers
KW  - C (programming language)
KW  - Decoding
KW  - Object detection
KW  - Object recognition
KW  - Signal encoding
KW  - Three dimensional displays
KW  - 2D objects
KW  - Decoding
KW  - Detection accuracy
KW  - Feature pyramid
KW  - Features extraction
KW  - Multilevels
KW  - Objects detection
KW  - Small object detection
KW  - Three-dimensional display
KW  - Transformer
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bożko, A.
AU  - Ambroziak, L.
TI  - Influence of Insufficient Dataset Augmentation on IoU and Detection Threshold in CNN Training for Object Detection on Aerial Images
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22239080
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143817129&doi=10.3390%2fs22239080&partnerID=40&md5=1f839fcaeecc1748a3467609c22a3581
AB  - The objects and events detection tasks are being performed progressively often by robotic systems like unmanned aerial vehicles (UAV) or unmanned surface vehicles (USV). Autonomous operations and intelligent sensing are becoming standard in numerous scenarios such as supervision or even search and rescue (SAR) missions. The low cost of autonomous vehicles, vision sensors and portable computers allows the incorporation of the deep learning, mainly convolutional neural networks (CNN) in these solutions. Many systems meant for custom purposes rely on insufficient training datasets, what may cause a decrease of effectiveness. Moreover, the system’s accuracy is usually dependent on the returned bounding boxes highlighting the supposed targets. In desktop applications, precise localisation might not be particularly relevant; however, in real situations, with low visibility and non-optimal camera orientation, it becomes crucial. One of the solutions for dataset enhancement is its augmentation. The presented work is an attempt to evaluate the influence of the training images augmentation on the detection parameters important for the effectiveness of neural networks in the context of object detection. In this research, network appraisal relies on the detection confidence and bounding box prediction accuracy (IoU). All the applied image modifications were simple pattern and colour alterations. The obtained results imply that there is a measurable impact of the augmentation process on the localisation accuracy. It was concluded that a positive or negative influence is related to the complexity and variability of the objects classes. © 2022 by the authors.
KW  - aerial images
KW  - data augmentation
KW  - deep neural networks
KW  - image classification
KW  - object detection
KW  - unmanned aerial vehicle
KW  - Neural Networks, Computer
KW  - Aircraft detection
KW  - Antennas
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Image classification
KW  - Microcomputers
KW  - Object recognition
KW  - Aerial images
KW  - Aerial vehicle
KW  - Bounding-box
KW  - Convolutional neural network
KW  - Data augmentation
KW  - Detection threshold
KW  - Images classification
KW  - Neural networks trainings
KW  - Objects detection
KW  - Unmanned aerial vehicle
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cao, S.
AU  - Jin, Y.
AU  - Trautmann, T.
AU  - Liu, K.
TI  - Design and Experiments of Autonomous Path Tracking Based on Dead Reckoning
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13010317
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145691058&doi=10.3390%2fapp13010317&partnerID=40&md5=a8ac1bbba3b83594be2102bf7289cb52
AB  - Path tracking is an important component of autonomous driving and most current path tracking research is based on different positioning sensors, such as GPS, cameras, and LIDAR. However, in certain extreme cases (e.g., in tunnels or indoor parking lots), if these sensors are unavailable, achieving accurate path tracking remains a problem that is worthy of study. This paper addresses this problem by designing a dead reckoning method that is solely reliant on wheel speed for localization. Specifically, a differential drive model is first used for estimating the current relative vehicle position in real time by rear wheel speed, and the deviation between the current path and the reference path is then calculated using the pure pursuit algorithm as a means of obtaining the target steering wheel angle and vehicle speed. The steering wheel and vehicle speed signals are then output by two PID controllers in order to control the vehicle, and the automatic driving path tracking is ultimately realized. Through exhaustive tests and experiments, the stop position error and tracking process error are compared under different conditions, and the effects of vehicle speed, look-ahead distance, starting position angle, and driving mode on tracking accuracy are analyzed. The experimental results show the average error of the end position to be 0.26 m, 0.383 m, and 0.505 m when using BMW-i3 to drive one lap automatically at speeds of 5 km/h, 10 km/h, and 15 km/h in a test area with a perimeter of approximately 200 m. © 2022 by the authors.
KW  - autonomous driving
KW  - dead reckoning
KW  - differential drive kinematics
KW  - pure pursuit
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lele, A.S.
AU  - Fang, Y.
AU  - Anwar, A.
AU  - Raychowdhury, A.
TI  - Bio-mimetic high-speed target localization with fused frame and event vision for edge application
PY  - 2022
T2  - Frontiers in Neuroscience
DO  - 10.3389/fnins.2022.1010302
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143730328&doi=10.3389%2ffnins.2022.1010302&partnerID=40&md5=ce82a68f26347a16db4e930f2a4cd799
AB  - Evolution has honed predatory skills in the natural world where localizing and intercepting fast-moving prey is required. The current generation of robotic systems mimics these biological systems using deep learning. High-speed processing of the camera frames using convolutional neural networks (CNN) (frame pipeline) on such constrained aerial edge-robots gets resource-limited. Adding more compute resources also eventually limits the throughput at the frame rate of the camera as frame-only traditional systems fail to capture the detailed temporal dynamics of the environment. Bio-inspired event cameras and spiking neural networks (SNN) provide an asynchronous sensor-processor pair (event pipeline) capturing the continuous temporal details of the scene for high-speed but lag in terms of accuracy. In this work, we propose a target localization system combining event-camera and SNN-based high-speed target estimation and frame-based camera and CNN-driven reliable object detection by fusing complementary spatio-temporal prowess of event and frame pipelines. One of our main contributions involves the design of an SNN filter that borrows from the neural mechanism for ego-motion cancelation in houseflies. It fuses the vestibular sensors with the vision to cancel the activity corresponding to the predator's self-motion. We also integrate the neuro-inspired multi-pipeline processing with task-optimized multi-neuronal pathway structure in primates and insects. The system is validated to outperform CNN-only processing using prey-predator drone simulations in realistic 3D virtual environments. The system is then demonstrated in a real-world multi-drone set-up with emulated event data. Subsequently, we use recorded actual sensory data from multi-camera and inertial measurement unit (IMU) assembly to show desired working while tolerating the realistic noise in vision and IMU sensors. We analyze the design space to identify optimal parameters for spiking neurons, CNN models, and for checking their effect on the performance metrics of the fused system. Finally, we map the throughput controlling SNN and fusion network on edge-compatible Zynq-7000 FPGA to show a potential 264 outputs per second even at constrained resource availability. This work may open new research directions by coupling multiple sensing and processing modalities inspired by discoveries in neuroscience to break fundamental trade-offs in frame-based computer vision1. Copyright © 2022 Lele, Fang, Anwar and Raychowdhury.
KW  - accuracy-speed tradeoff
KW  - design space exploration
KW  - ego-motion cancelation
KW  - event camera
KW  - high-speed target tracking
KW  - hybrid neural network
KW  - neuromorphic vision
KW  - retinomorphic systems
KW  - accuracy speed tradeoff
KW  - Article
KW  - artificial neural network
KW  - biomimetics
KW  - computer vision
KW  - controlled study
KW  - convolutional neural network
KW  - deep learning
KW  - design space exploration
KW  - drone
KW  - ego motion cancelation
KW  - feature extraction
KW  - frame based computer vision
KW  - high speed target localization
KW  - high speed target tracking
KW  - house fly
KW  - hybrid neural network
KW  - insect
KW  - neuromorphic vision
KW  - noise
KW  - nonhuman
KW  - predation
KW  - predator
KW  - prey
KW  - primate
KW  - retinomorphic system
KW  - robotics
KW  - simulation
KW  - spatiotemporal analysis
KW  - spiking neural network
KW  - temporal analysis
KW  - vestibular system
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, G.
AU  - Zhu, D.
TI  - Research on road defect detection based on improved YOLOv8
PY  - 2023
T2  - IEEE Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
DO  - 10.1109/ITAIC58329.2023.10408744
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186065191&doi=10.1109%2fITAIC58329.2023.10408744&partnerID=40&md5=3103eecf1c217e96e7acbbaf6606e8cc
AB  - Currently, the use of autonomous driving technology in China is generally limited by strict lane division and road conditions. In contrast to Tesla's vision-based autonomous driving technology, autonomous driving in China generally relies on radar signal reflection for positioning. As target detection algorithms continue to advance, it is a future trend to introduce computer vision methods into autonomous driving. This article proposes an improved YOLOv8 network model for research using relevant Chinese road datasets. By introducing a better backbone network, convnextv2, based on YOLOv8, it can effectively extract features from the data. Additionally, by introducing the novel WIOU computation function, it can better calculate the rectangular boxes. In this experiment, the improved YOLOv8-ATE achieved an average precision of mAPO.5 and mAPO.5:0.95, which were 18.1 % and 19% higher, respectively, compared to YOLOv8-base. The proposed YOLOv8-ATE model can effectively detect road defects and provide theoretical and technical support for future visual research in autonomous driving. © 2023 IEEE.
KW  - Attention mechanism
KW  - Computer vision
KW  - Object detection
KW  - Road defect detection
KW  - YOLOv8
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Defects
KW  - Roads and streets
KW  - Attention mechanisms
KW  - Autonomous driving
KW  - Defect detection
KW  - Objects detection
KW  - Radar signals
KW  - Road condition
KW  - Road defect detection
KW  - Signal reflection
KW  - Vision based
KW  - YOLOv8
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nahata, D.
AU  - Othman, K.
TI  - Exploring the challenges and opportunities of image processing and sensor fusion in autonomous vehicles: A comprehensive review
PY  - 2023
T2  - AIMS Electronics and Electrical Engineering
DO  - 10.3934/electreng.2023016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177487365&doi=10.3934%2felectreng.2023016&partnerID=40&md5=454aadc5174e3a72b39be0b6923c9f26
AB  - Autonomous vehicles are at the forefront of future transportation solutions, but their success hinges on reliable perception. This review paper surveys image processing and sensor fusion techniques vital for ensuring vehicle safety and efficiency. The paper focuses on object detection, recognition, tracking, and scene comprehension via computer vision and machine learning methodologies. In addition, the paper explores challenges within the field, such as robustness in adverse weather conditions, the demand for real-time processing, and the integration of complex sensor data. Furthermore, we examine localization techniques specific to autonomous vehicles. The results show that while substantial progress has been made in each subfield, there are persistent limitations. These include a shortage of comprehensive large-scale testing, the absence of diverse and robust datasets, and occasional inaccuracies in certain studies. These issues impede the seamless deployment of this technology in real-world scenarios. This comprehensive literature review contributes to a deeper understanding of the current state and future directions of image processing and sensor fusion in autonomous vehicles, aiding researchers and practitioners in advancing the development of reliable autonomous driving systems. © 2023 the Author(s), licensee AIMS Press.
KW  - adverse weather
KW  - autonomous parking
KW  - autonomous vehicles
KW  - image processing
KW  - localization
KW  - sensor fusion
KW  - Computer vision
KW  - Large datasets
KW  - Object detection
KW  - Object recognition
KW  - Vehicle safety
KW  - Adverse weather
KW  - Autonomous Parking
KW  - Autonomous Vehicles
KW  - Fusion techniques
KW  - Images processing
KW  - Localisation
KW  - Paper surveys
KW  - Review papers
KW  - Sensor fusion
KW  - Vehicle efficiency
KW  - Autonomous vehicles
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, R.
AU  - Li, H.
AU  - Lv, Z.
TI  - Modeling Methods of 3D Model in Digital Twins
PY  - 2023
T2  - CMES - Computer Modeling in Engineering and Sciences
DO  - 10.32604/cmes.2023.023154
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148225357&doi=10.32604%2fcmes.2023.023154&partnerID=40&md5=ecef5d3967c4f5d9993f51d700242105
AB  - To understand the current application and development of 3D modeling in Digital Twins (DTs), abundant literatures on DTs and 3D modeling are investigated by means of literature review. The transition process from 3D modeling to DTs modeling is analyzed, as well as the current application of DTs modeling in various industries. The application of 3D DTs modeling in the fields of smart manufacturing, smart ecology, smart transportation, and smart buildings in smart cities is analyzed in detail, and the current limitations are summarized. It is found that the 3D modeling technology in DTs has broad prospects for development and has a huge impact on all walks of life and even human lifestyles. At the same time, the development of DTs modeling relies on the development and support capabilities of mature technologies such as Big Data, Internet of Things, Cloud Computing, Artificial Intelligence, and game technology. Therefore, although some results have been achieved, there are still limitations. This work aims to provide a good theoretical support for the further development of 3D DTs modeling. © 2023 Tech Science Press. All rights reserved.
KW  - 3D modeling
KW  - Digital twins
KW  - smart buildings
KW  - smart city
KW  - smart manufacturing
KW  - Computer games
KW  - Smart city
KW  - Three dimensional computer graphics
KW  - 'current
KW  - 3d modeling technologies
KW  - 3D models
KW  - 3d-modeling
KW  - Current limitation
KW  - Literature reviews
KW  - Model method
KW  - Smart manufacturing
KW  - Support capability
KW  - Transition process
KW  - 3D modeling
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CHAP
AU  - Balasubramaniam, A.
AU  - Pasricha, S.
TI  - Object Detection in Autonomous Cyber-Physical Vehicle Platforms: Status and Open Challenges
PY  - 2023
T2  - Machine Learning and Optimization Techniques for Automotive Cyber-Physical Systems
DO  - 10.1007/978-3-031-28016-0_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195994005&doi=10.1007%2f978-3-031-28016-0_17&partnerID=40&md5=c6b2756d5081a2259123a809a643b55e
AB  - Object detection is a computer vision task that has become an integral part of many consumer applications today such as surveillance and security systems, mobile text recognition, and diagnosing diseases from MRI/CT scans. Object detection is also one of the critical components to support autonomous driving. Autonomous vehicles rely on the perception of their surroundings to ensure safe and robust driving performance. This perception system uses object detection algorithms to accurately determine objects such as pedestrians, vehicles, traffic signs, and barriers in the vehicle’s vicinity. Deep learning-based object detectors play a vital role in finding and localizing these objects in real-time. This chapter discusses the state-of-the-art in object detectors and open challenges for their integration into autonomous vehicles. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Model compression
KW  - Object detection
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ji, W.
AU  - Li, J.
AU  - Bian, C.
AU  - Zhou, Z.
AU  - Zhao, J.
AU  - Yuille, A.
AU  - Cheng, L.
TI  - Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline
PY  - 2023
T2  - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
DO  - 10.1109/CVPR52729.2023.00112
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173930754&doi=10.1109%2fCVPR52729.2023.00112&partnerID=40&md5=37aa867d380fe2a242b0cfffd0c4755b
AB  - Robust and reliable semantic segmentation in complex scenes is crucial for many real-life applications such as autonomous safe driving and nighttime rescue. In most approaches, it is typical to make use of RGB images as input. They however work well only in preferred weather conditions; when facing adverse conditions such as rainy, overexposure, or low-light, they often fail to deliver satisfactory results. This has led to the recent investigation into multispectral semantic segmentation, where RGB and thermal infrared (RGBT) images are both utilized as input. This gives rise to significantly more robust segmentation of image objects in complex scenes and under adverse conditions. Nevertheless, the present focus in single RGBT image input restricts existing methods from well addressing dynamic real-world scenes. Motivated by the above observations, in this paper, we set out to address a relatively new task of semantic segmentation of multispectral video input, which we refer to as Multispectral Video Semantic Segmentation, or MVSS in short. An in-house MVSeg dataset is thus curated, consisting of 738 calibrated RGB and thermal videos, accompanied by 3,545 fine-grained pixel-level semantic annotations of 26 categories. Our dataset contains a wide range of challenging urban scenes in both daytime and nighttime. Moreover, we propose an effective MVSS baseline, dubbed MVNet, which is to our knowledge the first model to jointly learn semantic representations from multispectral and temporal contexts. Comprehensive experiments are conducted using various semantic segmentation models on the MVSeg dataset. Empirically, the engagement of multispectral video input is shown to lead to significant improvement in semantic segmentation; the effectiveness of our MVNet baseline has also been verified. © 2023 IEEE.
KW  - Scene analysis and understanding
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Thirumurugan, S.
AU  - Pradeepan, M.L.
AU  - Sundhar, K.
AU  - Dhanapal, R.
TI  - Literature Review of Drowsiness Identification and Alert System for Real Time Network
PY  - 2023
T2  - 6th International Conference on Inventive Computation Technologies, ICICT 2023 - Proceedings
DO  - 10.1109/ICICT57646.2023.10134489
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163533810&doi=10.1109%2fICICT57646.2023.10134489&partnerID=40&md5=4c7057987ef3f2f848630f84c9f87566
AB  - Road mishaps are more likely to be caused by tired drivers. Driving for extended periods of time, driving while sleep deprived, and driving while under the influence of drugs, alcohol, or medication are just a few of the factors that contribute to driver fatigue. In developing nations, where the death rate from traffic injuries is above 85% and the disability-adjusted life expectancy is above 90%, traffic injuries and fatalities are a major public health concern. One of the safeguards to avoid accidents is drowsiness detection. Based on the parameters each method uses, drowsiness detection techniques created for defence purposes are divided into four groups. These groups are: Subjective-based, Vehicle-based, Behavioral or Visual-based, Mouth Tracking, Physiological or Non visual-based. The remote healthcare monitoring and drowsiness alert system is a network of interconnected vehicles that communicate with each other and with the surrounding environment using advanced communication technologies. It leverages this network to collect and transmit vital health data from drivers to healthcare professionals who can then use the information's to monitor their health status and provide timely medical interventions when necessary. © 2023 IEEE.
KW  - Alert system
KW  - differential algorithm
KW  - Drowsiness Detection
KW  - Healthcare monitoring
KW  - Quality of Service
KW  - Health care
KW  - Vehicle to vehicle communications
KW  - Alert systems
KW  - Differential algorithms
KW  - Driver fatigue
KW  - Drowsiness detection
KW  - Healthcare monitoring
KW  - Literature reviews
KW  - Quality-of-service
KW  - Real time network
KW  - Time drivings
KW  - Traffic injuries
KW  - Quality of service
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Choudhary, A.
AU  - Mishra, R.K.
AU  - Fatima, S.
AU  - Panigrahi, B.K.
TI  - Multi-input CNN based vibro-acoustic fusion for accurate fault diagnosis of induction motor
PY  - 2023
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2023.105872
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147193484&doi=10.1016%2fj.engappai.2023.105872&partnerID=40&md5=23971dd79737e02a29853d04f1102ca9
AB  - Induction motor (IM) is a highly efficient prime mover in industrial applications. To maintain an uninterrupted operation, accurate fault diagnosis system of IM is required. It can help to improve operational safety and prevent unexpected economic losses. The traditional diagnosis methods are less capable of dealing with real-time and varying working environments. This paper presents a vibro-acoustic fusion technique for an accurate fault diagnosis under varying working conditions. The suggested method fuses the features of vibration and acoustic signals using Multi Input-Convolutional Neural Network (MI-CNN) technique. At first, raw vibration and acoustic signals are acquired at varying speeds and converted into a time–frequency spectrum using the Constant Q-Non-Stationary Gabor Transform (CQ-NSGT). Thereafter, a MI-CNN-based vibro-acoustic fusion is adopted for the fusion of vibration and acoustic features. Six distinct motor conditions are utilized to compute the effectiveness of the suggested MI-CNN model. Further, two additional datasets, i.e., bearing and the gearbox datasets, are employed to validate the suggested approach. The experimental results demonstrate that the suggested methodology is accurate and reliable for IMs and other components of rotating machine. © 2023 Elsevier Ltd
KW  - CQ-NSGT
KW  - Fault detection
KW  - Induction Motor
KW  - MI-CNN
KW  - Vibro-acoustic fusion
KW  - Acoustic waves
KW  - Failure analysis
KW  - Induction motors
KW  - Losses
KW  - Constant Q-non-stationary gabor transform
KW  - Convolutional neural network
KW  - Faults detection
KW  - Gabor transform
KW  - Inductions motors
KW  - Multi input-convolutional neural network
KW  - Multiinput
KW  - Nonstationary
KW  - Vibro-acoustic fusion
KW  - Vibroacoustics
KW  - Fault detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kurian, M.Z.
TI  - Methodological Insights Towards Leveraging Performance in Video Object Tracking and Detection
PY  - 2023
T2  - International Journal of Advanced Computer Science and Applications
DO  - 10.14569/IJACSA.2023.0140851
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170647212&doi=10.14569%2fIJACSA.2023.0140851&partnerID=40&md5=4120aed07d5cefd594a6a349ab5d6de0
AB  - Video Object Detection and Tracking (VODT), one of its integral operations of surveillance system in present time, mechanizes a way to identify and track the target object autonomously and seamlessly within its visual field. However, the challenges associated with video feeding are immensely high, and the scene context is out of human control, posing an impediment to a successful model of VODT. The presented work has discussed about effectiveness of existing VODT approaches considering its identified taxonomies viz. satellite based, remote sensing-based, unmanned-based, Real-time Tracking based, behavioral analysis and event detection based, integration of multiple data sources, and privacy and ethics. Further, research trend associated with cumulative publications and evolving methods to realize the frequently used methodologies in VODT. Further, the results of review showcase that there is prominent research gap of manifold attributes that demands to be addressed for improving performance of VODT. © (2023), (Science and Information Organization). All Rights Reserved.
KW  - Object detection
KW  - object tracking
KW  - surveillance system
KW  - video
KW  - video feed
KW  - visual field
KW  - Object recognition
KW  - Remote sensing
KW  - Security systems
KW  - Tracking (position)
KW  - Vision
KW  - Object detection and tracking
KW  - Object Tracking
KW  - Objects detection
KW  - Performance
KW  - Surveillance systems
KW  - Video
KW  - Video feed
KW  - Video object detections
KW  - Video object tracking
KW  - Visual fields
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Iftikhar, S.
AU  - Asim, M.
AU  - Zhang, Z.
AU  - Muthanna, A.
AU  - Chen, J.
AU  - El-Affendi, M.
AU  - Sedik, A.
AU  - Abd El-Latif, A.A.
TI  - Target Detection and Recognition for Traffic Congestion in Smart Cities Using Deep Learning-Enabled UAVs: A Review and Analysis
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13063995
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152262021&doi=10.3390%2fapp13063995&partnerID=40&md5=e5e48d87855d72be72b9d060fd7b64c8
AB  - In smart cities, target detection is one of the major issues in order to avoid traffic congestion. It is also one of the key topics for military, traffic, civilian, sports, and numerous other applications. In daily life, target detection is one of the challenging and serious tasks in traffic congestion due to various factors such as background motion, small recipient size, unclear object characteristics, and drastic occlusion. For target examination, unmanned aerial vehicles (UAVs) are becoming an engaging solution due to their mobility, low cost, wide field of view, accessibility of trained manipulators, a low threat to people’s lives, and ease to use. Because of these benefits along with good tracking effectiveness and resolution, UAVs have received much attention in transportation technology for tracking and analyzing targets. However, objects in UAV images are usually small, so after a neural estimation, a large quantity of detailed knowledge about the objects may be missed, which results in a deficient performance of actual recognition models. To tackle these issues, many deep learning (DL)-based approaches have been proposed. In this review paper, we study an end-to-end target detection paradigm based on different DL approaches, which includes one-stage and two-stage detectors from UAV images to observe the target in traffic congestion under complex circumstances. Moreover, we also analyze the evaluation work to enhance the accuracy, reduce the computational cost, and optimize the design. Furthermore, we also provided the comparison and differences of various technologies for target detection followed by future research trends. © 2023 by the authors.
KW  - cascade R-CNN
KW  - deep learning
KW  - faster R-CNN
KW  - target detection
KW  - traffic congestion
KW  - unmanned aerial vehicles
KW  - YOLO versions
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Doly, M.
AU  - Al-Khowarizmi, N.
AU  - Rahmat, R.F.
AU  - Lubis, A.R.
AU  - Lubis, M.
TI  - The Role of Faster R-CNN Algorithm in the Internet of Things to Detect Mask Wearing: The Endemic Preparations
PY  - 2023
T2  - International Journal of Electronics and Telecommunications
DO  - 10.24425/ijet.2023.147689
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179004846&doi=10.24425%2fijet.2023.147689&partnerID=40&md5=8ddb131edf3c17cd26703fe2ca972e30
AB  - Faster R-CNN is an algorithm development that continuously starts from CNN then R-CNN and Faster R-CNN. The development of the algorithm is needed to test whether the heuristic algorithm has optimal provisions. Broadly speaking, faster R-CNN is included in algorithms that are able to solve neural network and machine learning problems to detect a moving object. One of the moving objects in the current phenomenon is the use of masks. Where various countries in the world have issued endemic orations after the Covid 19 pandemic occurred. Detection tool has been prepared that has been tested at the mandatory mask door, namely for mask users. In this paper, the role of the Faster R-CNN algorithm has been carried out to detect masks poured on Internet of Thinks (IoT) devices to automatically open doors for standard mask users. From the results received that testing on the detection of moving mask objects when used reaches 100% optimal at a distance of 0.5 to 1 meter and 95% at a distance of 1.5 to 2 meters so that the process of sending detection signals to IoT devices can be carried out at a distance of 1 meter at the position mask users to automatic doors. © 2023 Polish Academy of Sciences. All rights reserved.
KW  - Endemic
KW  - Faster R-CNN
KW  - IoT
KW  - Mask
KW  - Heuristic algorithms
KW  - Internet of things
KW  - Machine learning
KW  - Object detection
KW  - Wear of materials
KW  - 'current
KW  - Algorithms development
KW  - Detection tools
KW  - Endemic
KW  - Fast R-CNN
KW  - Heuristics algorithm
KW  - Internet of think
KW  - Machine learning problem
KW  - Moving objects
KW  - Neural network learning
KW  - Optimization
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Lai, H.
AU  - Liu, B.
AU  - Kan, H.Y.
AU  - Lam, C.-T.
AU  - Im, S.K.
TI  - YOLOv8-lite: An Interpretable Lightweight Object Detector for Real-Time UAV Detection
PY  - 2023
T2  - 2023 9th International Conference on Computer and Communications, ICCC 2023
DO  - 10.1109/ICCC59590.2023.10507293
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192976083&doi=10.1109%2fICCC59590.2023.10507293&partnerID=40&md5=b40b0f39e16c18901a826be516f5f9e5
AB  - UAV detection is an important problem in sensitive areas involving security and privacy. This paper proposes an interpretable lightweight model designed explicitly for the real-time detection of UAVs, called YOLOv8-lite. By employing a high-speed YOLOv8 model and Depthwise convolution, the model performs better than the original YOLOv8 with fewer parameters in the Det-fly dataset. The proposed YOLOv8-lite achieves impressive results with 0.98 AP50 and 0.68 AP0.5:0.95 on the test set, using only 2 million parameters. Meanwhile, YOLOv8-lite shows good results in solving the challenges of detecting UAVs against various environmental backgrounds. In addition, interpretability methods are applied to illustrate the factors contributing to the effectiveness and generalization capability of the model. The code for the model is available: https://github.com/hawkinglai/uav-det.  © 2023 IEEE.
KW  - Depthwise convolution
KW  - Interpretable machine learning
KW  - Object detection
KW  - UAV detection
KW  - YOLOv8
KW  - Aircraft detection
KW  - Machine learning
KW  - Object detection
KW  - Object recognition
KW  - Unmanned aerial vehicles (UAV)
KW  - Depthwise convolution
KW  - Interpretable machine learning
KW  - Machine-learning
KW  - Object detectors
KW  - Objects detection
KW  - Real- time
KW  - Security and privacy
KW  - Sensitive area
KW  - UAV detection
KW  - YOLOv8
KW  - Convolution
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Mahaur, B.
AU  - Kumar, A.
TI  - Designing an Efficient Object Detection Model for Autonomous Driving Applications
PY  - 2023
T2  - Proceedings of the 10th International Conference on Signal Processing and Integrated Networks, SPIN 2023
DO  - 10.1109/SPIN57001.2023.10117387
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160006276&doi=10.1109%2fSPIN57001.2023.10117387&partnerID=40&md5=fa8fb962a1fa589d1bdca7342f59fba7
AB  - The detection of road objects plays an essential role in the development of autonomous vehicles in intelligent transportation systems, which has become an emerging field in deep learning. Several objects on the road, like vehicles, pedestrians, etc., are necessary to be accurately identified, which guarantees the safety of other people and vehicles in the surroundings. In this article, we aim to design an efficient object detection model for autonomous driving systems. To achieve this, we investigate the recently developed YOLOv7 and optimize the same for improving the detection performance to satisfy the realtime safety requirements of autonomous vehicles. We perform extensive experimentation and demonstrate the effectiveness of our method on the BDDIOOK dataset. Experimental results show that our proposed method increases the detection accuracy to 82.6% and inference speed to 97.2 FPS compared to the baselines, with no additional increase in model complexity.  © 2023 IEEE.
KW  - Architectural Design
KW  - Autonomous Vehicles
KW  - Efficient Model
KW  - Object Detection
KW  - YOLOv7
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Object recognition
KW  - Pedestrian safety
KW  - Roads and streets
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Detection models
KW  - Detection performance
KW  - Driving systems
KW  - Efficient model
KW  - Efficient object detections
KW  - Intelligent transportation systems
KW  - Objects detection
KW  - YOLOv7
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Oliskevich, M.
AU  - Danchuk, V.
AU  - Bakulich, O.
TI  - Information System for Energy-Saving Vehicle Traffic Control on the Highway
PY  - 2023
T2  - Lecture Notes in Networks and Systems
DO  - 10.1007/978-3-031-46874-2_31
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180622824&doi=10.1007%2f978-3-031-46874-2_31&partnerID=40&md5=76dc3d863ae9e2287e491a21fb738167
AB  - The paper is devoted to the problem of information provision of energy-efficient automatic control of vehicles on highways. Comprehensive measures are proposed for the transmission of data flow to on-board controllers of vehicles, which makes it possible to avoid radio signal interference. The main technical means of the proposed system are unmanned aerial vehicles equipped with emitters and direction finders of radio signals, which are able to communicate with motor vehicles, with each other, as well as with immovable roadside infrastructure objects. These objects play the role of markers for the automatic flight of drones along the highway and means of saving fragments of the data stream. Every vehicle on the highway has access to the 3D trajectory and road conditions at least 4 km ahead on the chosen route, thanks to the use of markers with a constant distance between them and drones that fly precisely along the highway, against the direction of traffic. Simulation modeling of information system functioning with different numbers of markers was carried out. The deviation of the average cruising speed forecast from the value obtained as a result of simulation was determined. It was established that the accuracy of forecasting significantly depends on the horizon and the number of markers. Reducing the horizon to less than 1 km and the number of markers to less than 25 is impractical, as the error can exceed 50%. At the same time, the highest prediction accuracy was achieved at the level of 2.8%. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.
KW  - Autonomous vehicles
KW  - Drones
KW  - Energy saving
KW  - Radars
KW  - Antennas
KW  - Autonomous vehicles
KW  - Energy efficiency
KW  - Forecasting
KW  - Information systems
KW  - Information use
KW  - Radio transmission
KW  - Traffic control
KW  - Vehicle transmissions
KW  - Aerial vehicle
KW  - Autonomous Vehicles
KW  - Dataflow
KW  - Energy  savings
KW  - Energy efficient
KW  - Energy-savings
KW  - Information provision
KW  - Radio signals
KW  - Transmission of data
KW  - Vehicle traffic
KW  - Drones
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Khan, M.U.
AU  - Dil, M.
AU  - Alam, M.Z.
AU  - Orakazi, F.A.
AU  - Almasoud, A.M.
AU  - Kaleem, Z.
AU  - Yuen, C.
TI  - SafeSpace MFNet: Precise and Efficient MultiFeature Drone Detection Network
PY  - 2023
T2  - IEEE Transactions on Vehicular Technology
DO  - 10.1109/TVT.2023.3323313
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174846836&doi=10.1109%2fTVT.2023.3323313&partnerID=40&md5=7a46286625299b9048e50a2cfb6d45ed
AB  - The increasing prevalence of unmanned aerial vehicles (UAVs), commonly known as drones, has generated a demand for reliable detection systems. The inappropriate use of drones presents potential security and privacy hazards, particularly concerning sensitive facilities. Consequently, a critical necessity revolves around the development of a proficient system with the capability to precisely identify UAVs and other flying objects even in challenging scenarios. Although advancements have been made in deep learning models, obstacles such as computational intricacies, precision limitations, and scalability issues persist. To overcome those obstacles, we proposed the concept of MultiFeatureNet (MFNet), a solution that enhances feature representation by capturing the most concentrated feature maps. Additionally, we present MultiFeatureNet-Feature Attention (MFNet-FA), a technique that adaptively weights different channels of the input feature maps. To meet the requirements of multi-scale detection, we presented the versions of MFNet and MFNet-FA, namely the small (S), medium (M), and large (L). The outcomes reveal notable performance enhancements. For optimal bird detection, MFNet-M (Ablation study 2) achieves an impressive precision of 99.8%, while for UAV detection, MFNet-L (Ablation study 2) achieves a precision score of 97.2%. Among the options, MFNet-FA-S (Ablation study 3) emerges as the most resource-efficient alternative, considering its small feature map size, computational demands (GFLOPs), and operational efficiency (in frame per second). This makes it particularly suitable for deployment on hardware with limited capabilities. Additionally, MFNet-FA-S (Ablation study 3) stands out for its swift real-time inference and multiple-object detection due to the incorporation of the FA module. The proposed MFNet-L with the focus module (Ablation study 2) demonstrates the most remarkable classification outcomes, boasting an average precision of 98.4%, average recall of 96.6%, average mean average precision (mAP) of 98.3%, and average intersection over union (IoU) of 72.8%. © 2023 IEEE.
KW  - Birds
KW  - feature attention
KW  - multi-scale detection
KW  - multifeaturenet (MFNet)
KW  - MultiFeatureNet-Feature Attention (MFNet-FA)
KW  - unmanned aerial vehicle (UAV) detection
KW  - YOLOv5s
KW  - Ablation
KW  - Aircraft detection
KW  - Antennas
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Drones
KW  - Feature extraction
KW  - Object detection
KW  - Aerial vehicle
KW  - Feature attention
KW  - Features extraction
KW  - Multi-scale detection
KW  - Multi-scales
KW  - Multifeaturenet
KW  - Multifeaturenet</italic>
KW  - Multifeaturenet</italic>-FA
KW  - Radiofrequencies
KW  - Security
KW  - Unmanned aerial vehicle detection
KW  - Vehicles detection
KW  - YOLOv5
KW  - Birds
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kaur, J.
AU  - Singh, W.
TI  - Tools, techniques, datasets and application areas for object detection in an image: a review
PY  - 2022
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-022-13153-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128750331&doi=10.1007%2fs11042-022-13153-y&partnerID=40&md5=e3b3a270fd6692be27bcd984d7dc91c9
AB  - Object detection is one of the most fundamental and challenging tasks to locate objects in images and videos. Over the past, it has gained much attention to do more research on computer vision tasks such as object classification, counting of objects, and object monitoring. This study provides a detailed literature review focusing on object detection and discusses the object detection techniques. A systematic review has been followed to summarize the current research work’s findings and discuss seven research questions related to object detection. Our contribution to the current research work is (i) analysis of traditional, two-stage, one-stage object detection techniques, (ii) Dataset preparation and available standard dataset, (iii) Annotation tools, and (iv) performance evaluation metrics. In addition, a comparative analysis has been performed and analyzed that the proposed techniques are different in their architecture, optimization function, and training strategies. With the remarkable success of deep neural networks in object detection, the performance of the detectors has improved. Various research challenges and future directions for object detection also has been discussed in this research paper. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Computer vision
KW  - Dataset
KW  - Deep learning
KW  - Object detection
KW  - Deep neural networks
KW  - Object detection
KW  - Object recognition
KW  - 'current
KW  - Application area
KW  - Dataset
KW  - Deep learning
KW  - Literature reviews
KW  - Object classification
KW  - Object monitoring
KW  - Objects detection
KW  - Research questions
KW  - Systematic Review
KW  - Computer vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Pang, F.
AU  - Chen, X.
TI  - MS-YOLOv5: a lightweight algorithm for strawberry ripeness detection based on deep learning
PY  - 2023
T2  - Systems Science and Control Engineering
DO  - 10.1080/21642583.2023.2285292
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178227208&doi=10.1080%2f21642583.2023.2285292&partnerID=40&md5=49feafebc1c61dd02fd9e54d745a5c6a
AB  - The existing ripeness detection algorithm for strawberries suffers from low detection accuracy and high detection error rate. Considering these problems, we propose an improvement method based on YOLOv5, named MS-YOLOv5. The first step is to reconfigure the feature extraction network of MS-YOLOv5 by replacing the standard convolution with the depth hybrid deformable convolution (Ms-MDconv). In the second step, a double cooperative attention mechanism (Bc-attention) is constructed and implemented in the CSP2 module to improve the feature representation in complex environments. Finally, the Neck section of MS-YOLOv5 has been enhanced to use the fast-weighted fusion of cross-scale feature pyramid networks (FW-FPN) to replace the CSP2 module. It not only integrates multi-scale target features but also significantly reduces the number of parameters. The method was tested on the strawberry ripeness dataset, the mAP reached 0.956, the FPS reached 76, and the model size was 7.44M. The mAP and FPS are 8.4 and 1.3 percentage higher than the baseline network, respectively. The model size is reduced by 6.28M. This method is superior to mainstream algorithms in detection speed and accuracy. The system can accurately identify the ripeness of strawberries in complex environments, which could provide technical support for automated picking robots. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.
KW  - attention mechanism
KW  - Cross-scale feature fusion
KW  - deformable convolution
KW  - object detection
KW  - YOLO
KW  - Complex networks
KW  - Convolution
KW  - Deep learning
KW  - Feature extraction
KW  - Fruits
KW  - Attention mechanisms
KW  - Complex environments
KW  - Cross-scale feature fusion
KW  - Deformable convolution
KW  - Detection accuracy
KW  - Detection algorithm
KW  - Features fusions
KW  - Model size
KW  - Objects detection
KW  - YOLO
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xiao, M.
TI  - Research on the Application of Improved SSD Algorithm in the Intelligent Recognition Model of Anime Characters
PY  - 2023
T2  - Proceedings of 2023 7th Asian Conference on Artificial Intelligence Technology, ACAIT 2023
DO  - 10.1109/ACAIT60137.2023.10528481
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194197263&doi=10.1109%2fACAIT60137.2023.10528481&partnerID=40&md5=2c36f19e04225f7aa310f09fe885a634
AB  - In order to apply object detection techniques with better detection performance to anime character detection, an improved Single Shot MulmtiBox Detector (SSD) algorithm is proposed. Based on the basic SSD algorithm, the research introduces a Feature Pyramid Network (FPN) upsampling module in feature layer prediction, and replaces the VGGNet network structure in the basic algorithm with ResNet network structure as the front-end network structure. And introduce weight parameters to adjust the proportion balance in the samples, optimize the training strategy of the algorithm, and obtain the final improved SSD algorithm, further improving the detection accuracy and speed of the algorithm. The study compared the performance of the improved SSD algorithm with other algorithms on the COCO dataset and the anime character dataset. The results showed that the improved SSD algorithm achieved an average accuracy (mAP) of 0.78, a positioning error of 0.04, a recall rate of 0.87, and a processing speed of 18 FPS on the COCO dataset, all of which were the best among the algorithms. The algorithm achieved a detection AP value of 0.82 for character 30. The average detection time of the improved SSD algorithm is 32ms, and the MAP reaches 0.74, which is the highest among all algorithms. At the same time, experiments were conducted on the value of the weight parameter. When the weight parameter value was 1, the detection mAP value remained stable at 0.77, achieving the best detection effect. The results show that the improved SSD algorithm proposed in the study has superior detection performance, and has further improved detection accuracy and speed, providing a certain experimental basis for the detection research of anime characters.  © 2023 IEEE.
KW  - anime
KW  - character
KW  - CNN
KW  - SSD
KW  - target detection
KW  - YOLO
KW  - Anime
KW  - Character
KW  - Detection performance
KW  - Detection speed
KW  - Network structures
KW  - Single shot mulmtibox detector
KW  - Single-shot
KW  - Targets detection
KW  - Weight parameters
KW  - YOLO
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kukolj, D.
AU  - Marinović, I.
AU  - Nemet, S.
TI  - Road edge detection based on combined deep learning and spatial statistics of LiDAR data
PY  - 2023
T2  - Journal of Spatial Science
DO  - 10.1080/14498596.2021.1960912
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112147929&doi=10.1080%2f14498596.2021.1960912&partnerID=40&md5=e0a82ba3ca529c3c6268b3e428d875ef
AB  - Mobile laser scanning data can be used for effective extraction of road edge information, which is important in the domain of road maintenance and intelligent transportation. This paper proposes a road edge detection method that combines a deep learning and spatial statistics of point cloud data. Semantic segmentation using a deep neural network enables the effective extraction of point cloud fragments recognized as road. The process continues with the spatial statistical analysis of voxel features of data organized into a 3D voxel grid. Filtered voxels are clustered into spatially proximate clusters of similar shape, i.e. straight or curved edges. © 2021 Mapping Science Institute, Australia and Surveying and Spatial Science Institute.
KW  - LiDAR
KW  - machine learning
KW  - neural network
KW  - point cloud
KW  - road edge
KW  - artificial neural network
KW  - detection method
KW  - learning
KW  - lidar
KW  - machine learning
KW  - road
KW  - spatial analysis
KW  - statistical analysis
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Barnawi, A.
AU  - Budhiraja, I.
AU  - Kumar, K.
AU  - Kumar, N.
AU  - Alzahrani, B.
AU  - Almansour, A.
AU  - Noor, A.
TI  - A comprehensive review on landmine detection using deep learning techniques in 5G environment: open issues and challenges
PY  - 2022
T2  - Neural Computing and Applications
DO  - 10.1007/s00521-022-07819-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138329430&doi=10.1007%2fs00521-022-07819-9&partnerID=40&md5=b1aadf47f5d822fdc98fe9b474f7c600
AB  - Detection of Landmines, especially anti-tank mines, bombs, and unexploded substances, is one of the major challenges facing humanity. The devastation and human tragedy associated with undetected explosives are self-evident in war-torn communities. To deal with this problem, we are only left with proactive measures that such substances must be detected and dealt with before the fallout. Most available solutions have major shortcomings, such as cost, efficiency, and accuracy, where the trade-offs among them are inversely related. On the other hand, advances in deep learning, unmanned aerial vehicle, and sensing are making their way as potential technologies to revolutionize the detection and removal of landmines. In this paper, we go through the literature reviewing the most recent work featuring computerized technologies to detect landmines. To our knowledge, no such study has taken place in this respect. Our aim is to find out how deep learning can be integrated with landmine detection. We identify open challenges toward viable automated solutions that enable deep learning to optimize performance effectively. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.
KW  - Applied artificial intelligence
KW  - Deep learning
KW  - Ground penetrating radar
KW  - Hyperspectral imaging
KW  - Landmine
KW  - Magnetometery
KW  - UAV
KW  - 5G mobile communication systems
KW  - Aircraft detection
KW  - Antennas
KW  - Bombs (ordnance)
KW  - Deep learning
KW  - Economic and social effects
KW  - Explosives
KW  - Geological surveys
KW  - Ground penetrating radar systems
KW  - Hyperspectral imaging
KW  - Spectroscopy
KW  - Unmanned aerial vehicles (UAV)
KW  - Anti-tank mines
KW  - Applied artificial intelligence
KW  - Cost-efficiency
KW  - Deep learning
KW  - Ground Penetrating Radar
KW  - Issues and challenges
KW  - Landmine
KW  - Learning techniques
KW  - Magnetometery
KW  - Proactive measures
KW  - Landmine detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Dai, H.
AU  - Liu, B.
AU  - Wan, G.
AU  - Qi, J.
AU  - Wan, L.
TI  - Research on Lightweight Small Object Detection Method with Fusion Attention Mechanism
PY  - 2023
T2  - Proceedings - 2023 China Automation Congress, CAC 2023
DO  - 10.1109/CAC59555.2023.10451036
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189335689&doi=10.1109%2fCAC59555.2023.10451036&partnerID=40&md5=347cdf524554b13e3ccecaa59722c387
AB  - In the task of small object detection, due to the relatively small size of the objects, complex situations such as occlusion and overlap may arise in high-density scenes. These factors can significantly increase the issues of missed detections and false detections in object detection tasks. To address the aforementioned issues, this paper proposes a lightweight small object detection model, YOLOv5s-MGC. Firstly, we introduce an improved Mobilenetv3 network to replace the backbone network of YOLOv5s, aiming to improve the network's capability for effective feature extraction. Secondly, we incorporate the GSConv module into the feature fusion network to reduce model parameters and strengthen the network's ability in fusing feature information. Finally, we have integrated Convolutional Block Attention Module (CBAM) at the forefront of the detection heads, achieving the fusion of fine-grained features. We assessed our approach on the openly accessible dataset 'SHWD' and contrasted the results with the baseline YOLOv5s and various SOTA object detection models. Precisely, our method attains an accuracy of 91.1%, exceeding the baseline module by 1.6%. Furthermore, the module size is reduced to 1.97MB and it achieves a 52.4 FPS. In comparison to other prevailing algorithms, YOLOv5s-MGC enables efficient and accurate small object detection.  © 2023 IEEE.
KW  - CBAM
KW  - GSConv
KW  - Mobilenetv3
KW  - Small object detection
KW  - YOLOv5s
KW  - Computer vision
KW  - Feature extraction
KW  - Object recognition
KW  - Attention mechanisms
KW  - Convolutional block attention module
KW  - Detection models
KW  - Gsconv
KW  - Missed detections
KW  - Mobilenetv3
KW  - Object detection method
KW  - Objects detection
KW  - Small object detection
KW  - YOLOv5
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Wang, Y.
AU  - Deng, J.
AU  - Li, Y.
AU  - Hu, J.
AU  - Liu, C.
AU  - Zhang, Y.
AU  - Ji, J.
AU  - Ouyang, W.
AU  - Zhang, Y.
TI  - Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection
PY  - 2023
T2  - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
DO  - 10.1109/CVPR52729.2023.01287
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215237226&doi=10.1109%2fCVPR52729.2023.01287&partnerID=40&md5=399d49f49f059d5c4a336b7a66fbd0e0
AB  - LiDAR and Radar are two complementary sensing approaches in that LiDAR specializes in capturing an object's 3D shape while Radar provides longer detection ranges as well as velocity hints. Though seemingly natural, how to efficiently combine them for improved feature representation is still unclear. The main challenge arises from that Radar data are extremely sparse and lack height information. Therefore, directly integrating Radar features into LiDAR-centric detection networks is not optimal. In this work, we introduce a bi-directional LiDAR-Radar fusion framework, termed Bi-LRFusion, to tackle the challenges and improve 3D detection for dynamic objects. Technically, Bi-LRFusion involves two steps: first, it enriches Radar's local features by learning important details from the LiDAR branch to alleviate the problems caused by the absence of height information and extreme sparsity; second, it combines LiDAR features with the enhanced Radar features in a unified bird's-eye-view representation. We conduct extensive experiments on nuScenes and ORR datasets, and show that our Bi-LRFusion achieves state-of-the-art performance for detecting dynamic objects. Notably, Radar data in these two datasets have different formats, which demonstrates the generalizability of our method. Codes will be published.  © 2023 IEEE.
KW  - 3D from multi-view and sensors
KW  - Human computer interaction
KW  - Image segmentation
KW  - Object detection
KW  - Object recognition
KW  - Object tracking
KW  - 3-D shape
KW  - 3d from multi-view and sensor
KW  - Bi-directional
KW  - Detection range
KW  - Dynamic objects
KW  - Feature representation
KW  - Multi sensor
KW  - Multi-views
KW  - Objects detection
KW  - Radar data
KW  - Optical radar
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Li, Y.
AU  - Hoi, K.I.
AU  - Mok, K.M.
AU  - Yuen, K.V.
TI  - Air Quality Monitoring and Advanced Bayesian Modeling
PY  - 2023
T2  - Air Quality Monitoring and Advanced Bayesian Modeling
DO  - 10.1016/C2020-0-03496-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150086501&doi=10.1016%2fC2020-0-03496-1&partnerID=40&md5=17487e20a0741d6cb9e5547c6cbad448
AB  - Air Quality Monitoring and Advanced Bayesian Modeling introduces recent developments in urban air quality monitoring and forecasting. The book presents concepts, theories, and case studies related to monitoring methods of criteria air pollutants, advanced methods for real-time characterization of chemical composition of PM and VOCs, and emerging strategies for air quality monitoring. The book illustrates concepts and theories through case studies about the development of common statistical air quality forecasting models. Readers will also learn advanced topics such as the Bayesian model class selection, adaptive forecasting model development with Kalman filter, and the Bayesian model averaging of multiple adaptive forecasting models. © 2023 Elsevier Inc. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Mian, T.
AU  - Choudhary, A.
AU  - Fatima, S.
TI  - A sensor fusion based approach for bearing fault diagnosis of rotating machine
PY  - 2022
T2  - Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability
DO  - 10.1177/1748006X211044843
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114441519&doi=10.1177%2f1748006X211044843&partnerID=40&md5=3d7858276e4f6ff4d469fa5a607301a3
AB  - Fault diagnosis in rotating machines plays a vital role in various industries. Bearing is the essential element of rotating machines, and early fault detection can reduce the maintenance cost and enhance machine availability. In complex industrial machinery, a single sensor has a limitation to capture complete information about fault conditions. Hence, there is a need to involve multiple sensors to diagnose all possible fault conditions effectively. In such situations, an efficient fusion of information is required to develop a reliable fault diagnosis system. In this work, a feature fusion approach is implemented using two different sensors, that is, a contact type vibration sensor and a non-invasive thermal imaging camera. Hilbert transform is applied to decompose raw vibration and thermal image data, and subsequently, features are extracted and fused into a single feature vector. However, the features are fused in a concatenation manner, but this stage has high dimensionality. Neighborhood component analysis (NCA) is applied to reduce this high dimensionality of the feature vector, followed by a relief algorithm (RA) to compute the relevance level to find the optimal features. Finally, these optimal features are used as an input feature vector to the support vector machine (SVM) to classify the faults. The proposed approach resulted in considerably improved classification accuracy and detection quality than individual sensors. Also, the relevance of the proposed approach is proved by comparing its performance with other prevalent feature fusion techniques. © IMechE 2021.
KW  - fault classification
KW  - Hilbert transform
KW  - neighborhood component analysis
KW  - relief algorithm
KW  - Bearings (machine parts)
KW  - Failure analysis
KW  - Infrared imaging
KW  - Mathematical transformations
KW  - Rotating machinery
KW  - Support vector machines
KW  - Vectors
KW  - Bearing fault diagnosis
KW  - Classification accuracy
KW  - Complete information
KW  - Fault diagnosis systems
KW  - Industrial machinery
KW  - Machine availability
KW  - Neighborhood component analysis
KW  - Thermal imaging cameras
KW  - Fault detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Khan, M.
AU  - Raza, M.A.
AU  - Abbas, G.
AU  - Othmen, S.
AU  - Yousef, A.
AU  - Jumani, T.A.
TI  - Pothole detection for autonomous vehicles using deep learning: a robust and efficient solution
PY  - 2023
T2  - Frontiers in Built Environment
DO  - 10.3389/fbuil.2023.1323792
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183611995&doi=10.3389%2ffbuil.2023.1323792&partnerID=40&md5=311ecedf382ee477b21e4fc8697e9bed
AB  - Autonomous vehicles can transform the transportation sector by offering a safer and more effective means of travel. However, the success of self-driving cars depends on their ability to navigate complex road conditions, including the detection of potholes. Potholes pose a substantial risk to vehicles and passengers, leading to potential damage and safety hazards, making their detection a critical task for autonomous driving. In this work, we propose a robust and efficient solution for pothole detection using the “you look only once (YOLO) algorithm of version 8, the newest deep learning object detection algorithm.” Our proposed system employs a deep learning methodology to identify real-time potholes, enabling autonomous vehicles to avoid potential hazards and minimise accident risk. We assess the effectiveness of our system using publicly available datasets and show that it outperforms existing state-of-the-art approaches in terms of accuracy and efficiency. Additionally, we investigate different data augmentation methods to enhance the detection capabilities of our proposed system. Our results demonstrate that YOLO V8-based pothole detection is a promising solution for autonomous driving and can significantly improve the safety and reliability of self-driving vehicles on the road. The results of our study are also compared with the results of YOLO V5. Copyright © 2024 Khan, Raza, Abbas, Othmen, Yousef and Jumani.
KW  - autonomous vehicles
KW  - deep learning
KW  - image classification
KW  - intelligent technologies and cities
KW  - pothole detection
KW  - YOLO V8
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Chen, A.
AU  - Wang, X.
AU  - Shi, K.
AU  - Zhu, S.
AU  - Fang, B.
AU  - Chen, Y.
AU  - Chen, J.
AU  - Huo, Y.
AU  - Ye, Q.
TI  - ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions
PY  - 2023
T2  - Proceedings - IEEE International Conference on Robotics and Automation
DO  - 10.1109/ICRA48891.2023.10161428
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168693333&doi=10.1109%2fICRA48891.2023.10161428&partnerID=40&md5=6bf271fb0a7d31487c4d0914254fa8b6
AB  - 3D human reconstruction from RGB images achieves decent results in good weather conditions but degrades dramatically in rough weather. Complementary, mmWave radars have been employed to reconstruct 3D human joints and meshes in rough weather. However, combining RGB and mmWave signals for robust all-weather 3D human reconstruction is still an open challenge, given the sparse nature of mmWave and the vulnerability of RGB images. In this paper, we present ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies in all weather conditions robustly. Specifically, our ImmFusion consists of image and point backbones for token feature extraction and a Transformer module for token fusion. The image and point backbones refine global and local features from original data, and the Fusion Transformer Module aims for effective information fusion of two modalities by dynamically selecting informative tokens. Extensive experiments on a large-scale dataset, mmBody, captured in various environments demonstrate that ImmFusion can efficiently utilize the information of two modalities to achieve a robust 3D human body reconstruction in all weather conditions. In addition, our method's accuracy is significantly superior to that of state-of-the-art Transformer-based LiDAR-camera fusion methods. © 2023 IEEE.
KW  - Computer vision
KW  - Large dataset
KW  - Meteorology
KW  - Millimeter waves
KW  - 3D human body
KW  - Body reconstruction
KW  - Condition
KW  - Features extraction
KW  - Global feature
KW  - Human joints
KW  - Large-scale datasets
KW  - Local feature
KW  - Mm waves
KW  - RGB images
KW  - Image reconstruction
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Naf’an, E.
AU  - Sulaiman, R.
AU  - Ali, N.M.
TI  - Optimization of Trash Identification on the House Compound Using a Convolutional Neural Network (CNN) and Sensor System
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23031499
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147894666&doi=10.3390%2fs23031499&partnerID=40&md5=0983f2f48ab7854b1aa2b96bc63f3c63
AB  - This study aims to optimize the object identification process, especially identifying trash in the house compound. Most object identification methods cannot distinguish whether the object is a real image (3D) or a photographic image on paper (2D). This is a problem if the detected object is moved from one place to another. If the object is 2D, the robot gripper only clamps empty objects. In this study, the Sequential_Camera_LiDAR (SCL) method is proposed. This method combines a Convolutional Neural Network (CNN) with LiDAR (Light Detection and Ranging), with an accuracy of ±2 mm. After testing 11 types of trash on four CNN architectures (AlexNet, VGG16, GoogleNet, and ResNet18), the accuracy results are 80.5%, 95.6%, 98.3%, and 97.5%. This result is perfect for object identification. However, it needs to be optimized using a LiDAR sensor to determine the object in 3D or 2D. Trash will be ignored if the fast scanning process with the LiDAR sensor detects non-real (2D) trash. If Real (3D), the trash object will be scanned in detail to determine the robot gripper position in lifting the trash object. The time efficiency generated by fast scanning is between 13.33% to 59.26% depending on the object’s size. The larger the object, the greater the time efficiency. In conclusion, optimization using the combination of a CNN and a LiDAR sensor can identify trash objects correctly and determine whether the object is real (3D) or not (2D), so a decision may be made to move the trash object from the detection location. © 2023 by the authors.
KW  - Convolutional Neural Network (CNN)
KW  - identification
KW  - optimization
KW  - sensor
KW  - trash
KW  - Convolutional neural networks
KW  - Efficiency
KW  - Grippers
KW  - Object detection
KW  - Optical radar
KW  - Photography
KW  - Convolutional neural network
KW  - Detection sensors
KW  - Identification
KW  - Light detection and ranging
KW  - Object identification
KW  - Optimisations
KW  - Ranging sensors
KW  - Robot gripper
KW  - Trash
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Qi, L.
AU  - Gao, J.
TI  - Small Object Detection Based on Improved YOLOv7
PY  - 2023
T2  - Jisuanji Gongcheng/Computer Engineering
DO  - 10.19678/j.issn.1000-3428.0065942
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147041053&doi=10.19678%2fj.issn.1000-3428.0065942&partnerID=40&md5=6638c6e7f6d6742daedcdf6d24ed19d0
AB  - Despite advancements in object detection technology, Small Object Detection (SOD) is still difficult to research.To address the challenge of easily missing detection in the process of object detection, this study proposes an improved YOLOv7 object detection model. Firstly, the MPConv module in the YOLOv7 model is improved by combining feature separation with merge, to reduce the feature loss caused by the process of network feature processing. The optimal position of the improved MPConv module is determined through experiments. Secondly, due to the phenomenon of missing detection in SOD, the algorithm uses the ACmix attention module to increase the sensitivity of the network to small-scale targets and reduce the influence caused by noise.Finally, SIoU is used to replace CIoU in the original YOLOv7 network model to optimize the loss function, reduce the freedom of the loss function, and improve the network robustness. Compared with the original network, the improved YOLOv7 network model can improve the missing detection situation of the images in the data set of the dense, small target, and ultra-small target by experimental comparison with the FloW-Img sub-dataset published by Okahublot. The results show that the mAP of the improved YOLOv7 network model can reach 71.1%, 4 percentage points higher than that of the baseline YOLOv7 network model, and the detection effect is better than that of the original network model and traditional classical target detection networks model. © 2023, Editorial Office of Computer Engineering. All rights reserved.
KW  - Attention module
KW  - Loss function
KW  - Object detection technology
KW  - Small Object Detection (SOD)
KW  - YOLOv7 network model
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, Y.
AU  - Zhu, J.
AU  - Jiang, C.
AU  - Xie, T.
AU  - Liu, R.
AU  - Wang, Y.
TI  - Adaptive Suppression Method of LiDAR Background Noise Based on Threshold Detection
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13063772
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151527251&doi=10.3390%2fapp13063772&partnerID=40&md5=32e4cb4b8a0886f4b70c080ce3f765e3
AB  - Background radiation in the LiDAR detection field of view is complex and variable, and the background noise generated can easily cause false alarms in the receiver, which affects the effective detection of the system. Through the analysis of the influence of background radiation noise of LiDAR on the detection performance, an adaptive suppression method of LiDAR background noise is proposed. This method realizes the rapid suppression of background noise in the instantaneous field of view through an adaptive threshold adjustment of current steering architecture with a back-end digital-to-analog converter (DAC) correction based on the principle of constant false alarm rate (CFAR) control. Aiming at the problem of accurate noise detection and quantification in a very short time, a dynamic comparator is used to replace the traditional continuous comparator. While detecting the number of noise pulses, the measurement of the pulse duration of noise is realized, which improves the accuracy of short-time noise detection. In order to verify the actual effect of the adaptive method, experiments were carried out based on the team’s self-developed LiDAR. The experimental results show that the measured noise ratio of the adaptive mode by using this method is the lowest. Even at 12 a.m., the noise ratio of the point cloud obtained by the adaptive mode is 0.012%, compared with 0.08% obtained by the traditional mode, which proves that this method has a good ability to suppress background noise. The proportion of noise reduction of the adaptive mode is more than 80% compared with the traditional mode. It achieves noise suppression through hardware at each detection, and each adjustment can be completed within a single period of pulse detection. Therefore, it has great advantages in real-time detection compared with the back-end software noise reduction processing method, and it is suitable for the application of LiDAR in the complex background environment. © 2023 by the authors.
KW  - background radiation
KW  - constant false alarm rate (CFAR)
KW  - LiDAR
KW  - noise detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hu, D.
AU  - Tong, Q.
AU  - Chai, G.
AU  - Wang, K.
AU  - Mu, Y.
AU  - Su, S.
TI  - Two-Stage Progressive Image Deraining Algorithm for Vehicle Detection in Rainy Days
ST  - 雨天车辆检测的两阶段渐进式图像去雨算法
PY  - 2023
T2  - Laser and Optoelectronics Progress
DO  - 10.3788/LOP231053
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181700401&doi=10.3788%2fLOP231053&partnerID=40&md5=b4d43bd29b3fd57cf888ad3bd8f9ea2d
AB  - This study proposes a two-stage progressive image deraining algorithm for vehicle detection in rainy days. The proposed algorithm aims to improve the accuracy of vehicle detection in rainy days and solve the problem of accuracy degradation caused by rain streak interference in the vehicle detection system of intelligent and connected vehicles. For the algorithm, a two-stage progressive deraining network was developed with light feature extraction and weighting block along with efficient feature transfer and fuse block as the core to realize the mining and capture of rain streak information and achieve accurate deraining. The deraining images were input to benchmark vehicle detector YOLOv5 for verifying the effectiveness of the proposed algorithm. Furthermore, a mix vehicle dataset was constructed based on the working environment of intelligent and connected vehicles. The gains of the proposed deraining algorithm on the precision, recall, and mAP@0. 5 of the benchmark vehicle detector YOLOv5 are 3. 0 percentage points, 8. 9 percentage points, and 7. 6 percentage points, respectively, under a rainy traffic scenario compared with other algorithms. The results prove that the proposed deraining algorithm considerably improves the accuracy of vehicle detection in rainy days and can be used in practice. © 2023 Universitat zu Koln. All rights reserved.
KW  - image deraining
KW  - image enhancing
KW  - image processing
KW  - two-stage progressive image deraining algorithm
KW  - vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cheng, Y.
AU  - Su, J.
AU  - Jiang, M.
AU  - Liu, Y.
TI  - A Novel Radar Point Cloud Generation Method for Robot Environment Perception
PY  - 2022
T2  - IEEE Transactions on Robotics
DO  - 10.1109/TRO.2022.3185831
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134272094&doi=10.1109%2fTRO.2022.3185831&partnerID=40&md5=afe77d5aa0f1b3a73a03fceaee7fb8bb
AB  - Millimeter-wave (mmWave) radar has been widely used in autonomous driving due to its good performance under harsh weather conditions. In recent years, with the development of mmWave radar hardware performance, radar point clouds, as an important data format of mmWave radar, have been widely used in high-level perception tasks of mobile robots and autonomous driving. However, at present, compared to LiDAR point clouds, in common application scenes of mobile robots, mmWave radar point clouds have shortcomings such as sparsity and containing many 'ghost' targets. Therefore, in this article, we analyze the reasons that cause these problems and propose a new method for point cloud generation as well as a new evaluation metric. After building a new dataset and carrying out experiments in real-world scenes, our method shows better performance on the quality of radar point clouds compared to other methods. In addition, by evaluating the performance of applying the high-quality radar point clouds to object detection tasks as well as localization and mapping tasks, the result shows that radar point clouds generated using our method can significantly improve the environment perception ability of mobile robots.  © 2004-2012 IEEE.
KW  - Millimeter-wave (mmWave) radar
KW  - point cloud
KW  - radar detector
KW  - robot environment perception
KW  - Autonomous vehicles
KW  - Doppler radar
KW  - Millimeter waves
KW  - Object detection
KW  - Optical radar
KW  - Radar antennas
KW  - Tracking radar
KW  - Environment perceptions
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Point cloud compression
KW  - Point-clouds
KW  - Radar applications
KW  - Radar detection
KW  - Radar detectors
KW  - Radars antennas
KW  - Robot environment
KW  - Robot environment perception
KW  - Mobile robots
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, M.
AU  - Zhao, D.
AU  - Sheng, C.
AU  - Liu, Z.
AU  - Cai, W.
TI  - Long-Strip Target Detection and Tracking with Autonomous Surface Vehicle
PY  - 2023
T2  - Journal of Marine Science and Engineering
DO  - 10.3390/jmse11010106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146785223&doi=10.3390%2fjmse11010106&partnerID=40&md5=c1ebacbf015020dfdf45c90cd38706ac
AB  - As we all know, target detection and tracking are of great significance for marine exploration and protection. In this paper, we propose one Convolutional-Neural-Network-based target detection method named YOLO-Softer NMS for long-strip target detection on the water, which combines You Only Look Once (YOLO) and Softer NMS algorithms to improve detection accuracy. The traditional YOLO network structure is improved, the prediction scale is increased from threeto four, and a softer NMS strategy is used to select the original output of the original YOLO method. The performance improvement is compared totheFaster-RCNN algorithm and traditional YOLO methodin both mAP and speed, and the proposed YOLO–Softer NMS’s mAP reaches 97.09%while still maintaining the same speed as YOLOv3. In addition, the camera imaging model is used to obtain accurate target coordinate information for target tracking. Finally, using the dicyclic loop PID control diagram, the Autonomous Surface Vehicle is controlled to approach the long-strip target with near-optimal path design. The actual test results verify that our long-strip target detection and tracking method can achieve gratifying long-strip target detection and tracking results. © 2023 by the authors.
KW  - autonomous surface vehicle
KW  - softer NMS
KW  - targetdetection
KW  - YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Qiao, Y.
AU  - Yin, J.
AU  - Wang, W.
AU  - Duarte, F.
AU  - Yang, J.
AU  - Ratti, C.
TI  - Survey of Deep Learning for Autonomous Surface Vehicles in Marine Environments
PY  - 2023
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2023.3235911
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147265791&doi=10.1109%2fTITS.2023.3235911&partnerID=40&md5=6ff9cfbb9c4e4b513131b5c8106363a0
AB  - Within the next several years, there will be a high level of autonomous technology that will be available for widespread use, which will reduce labor costs, increase safety, save energy, enable difficult unmanned tasks in harsh environments, and eliminate human error. Compared to software development for other autonomous vehicles, maritime software development, especially in aging but still functional fleets, is described as being in a very early and emerging phase. This presents great challenges and opportunities for researchers and engineers to develop maritime autonomous systems. Recent progress in sensor and communication technology has introduced the use of autonomous surface vehicles (ASVs) in applications such as coastline surveillance, oceanographic observation, multi-vehicle cooperation, and search and rescue missions. Advanced artificial intelligence technology, especially deep learning (DL) methods that conduct nonlinear mapping with self-learning representations, has brought the concept of full autonomy one step closer to reality. This article reviews existing work on the implementation of DL methods in fields related to ASV. First, the scope of this work is described after reviewing surveys on ASV developments and technologies, which draws attention to the research gap between DL and maritime operations. Then, DL-based navigation, guidance, control (NGC) systems and cooperative operations are presented. Finally, this survey is completed by highlighting current challenges and future research directions.  © 2000-2011 IEEE.
KW  - Autonomous surface vehicle
KW  - deep learning
KW  - intelligent autonomous systems
KW  - neural network
KW  - NGC system
KW  - Autonomous vehicles
KW  - Control system synthesis
KW  - Deep learning
KW  - Job analysis
KW  - Software design
KW  - Surface waters
KW  - Vehicle to vehicle communications
KW  - Wages
KW  - Autonomous surface vehicles
KW  - Deep learning
KW  - Guidance control
KW  - Intelligent autonomous systems
KW  - Marine vehicles
KW  - Navigation guidance
KW  - Navigation, guidance, control system
KW  - Neural-networks
KW  - Sea surfaces
KW  - Sensor systems
KW  - Task analysis
KW  - Unmanned surface vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, K.
AU  - Zhou, T.
AU  - Li, X.
AU  - Ren, F.
TI  - Performance and Challenges of 3D Object Detection Methods in Complex Scenes for Autonomous Driving
PY  - 2023
T2  - IEEE Transactions on Intelligent Vehicles
DO  - 10.1109/TIV.2022.3213796
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139850424&doi=10.1109%2fTIV.2022.3213796&partnerID=40&md5=b0fc26d4893ca8737fb5a61d55ddad83
AB  - How to ensure robust and accurate 3D object detection under various environment is essential for autonomous driving (AD) environment perception. While, until now, most of the existing 3D object detection methods are based on the ordinary driving scenes provided by the mainstream dataset. The researches on actual complex scenes (adverse illumination, inclement weather, distant or small objects, etc.) have been ignored and there is still no comprehensive review of the recent progress in this field. Thence, this paper aims to gain a deep insight on the performance and challenges of 3D object detection methods under complex scenes for AD. Firstly, we discuss the complex driving environments in actual and the perception limitations of mainstream sensors (LIDAR and camera). Then we analyze the performance and challenges of single-modality 3D object detection methods. Therefore, in order to improve the accuracy and robustness of 3D object detection methods in some complex AD scenes, the fusion of L-C (LIDAR-camera) is recommended and systematically analyzed. Finally, some suitable datasets and potential directions are comparatively summarized to support the relative research in complex driving scenes. We hope that this review could facilitate people's research and look forward to more progress in this timely and crucial problem field. © 2016 IEEE.
KW  - 3D object detection
KW  - autonomous driving
KW  - Complex scenes
KW  - datasets
KW  - multimodal fusion
KW  - Automobile drivers
KW  - Autonomous vehicles
KW  - Lighting
KW  - Object detection
KW  - Object recognition
KW  - Optical radar
KW  - Three dimensional displays
KW  - 3D object
KW  - 3d object detection
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Complex scenes
KW  - Dataset
KW  - Multi-modal fusion
KW  - Objects detection
KW  - Three-dimensional display
KW  - Cameras
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cao, L.
AU  - Zheng, X.
AU  - Fang, L.
TI  - The Semantic Segmentation of Standing Tree Images Based on the Yolo V7 Deep Learning Algorithm
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12040929
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149225665&doi=10.3390%2felectronics12040929&partnerID=40&md5=ca0de19873922dea4b75533fbcc8064a
AB  - The existence of humans and the preservation of the natural ecological equilibrium depend greatly on trees. The semantic segmentation of trees is very important. It is crucial to learn how to properly and automatically extract a tree’s elements from photographic images. Problems with traditional tree image segmentation include low accuracy, a sluggish learning rate, and a large amount of manual intervention. This research suggests the use of a well-known network segmentation technique based on deep learning called Yolo v7 to successfully accomplish the accurate segmentation of tree images. Due to class imbalance in the dataset, we use the weighted loss function and apply various types of weights to each class to enhance the segmentation of the trees. Additionally, we use an attention method to efficiently gather feature data while reducing the production of irrelevant feature data. According to the experimental findings, the revised model algorithm’s evaluation index outperforms other widely used semantic segmentation techniques. In addition, the detection speed of the Yolo v7 model is much faster than other algorithms and performs well in tree segmentation in a variety of environments, demonstrating the effectiveness of this method in improving the segmentation performance of the model for trees in complex environments and providing a more effective solution to the tree segmentation issue. © 2023 by the authors.
KW  - deep learning
KW  - fast segmentation
KW  - semantic segmentation
KW  - tree segmentation
KW  - Yolo v7
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Deng, K.
AU  - Zhao, D.
AU  - Han, Q.
AU  - Wang, S.
AU  - Zhang, Z.
AU  - Zhou, A.
AU  - Ma, H.
TI  - Geryon: Edge Assisted Real-time and Robust Object Detection on Drones via mmWave Radar and Camera Fusion
PY  - 2022
T2  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
DO  - 10.1145/3550298
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139166067&doi=10.1145%2f3550298&partnerID=40&md5=e00b14dd7569a8461bac6c43f4301e89
AB  - Vision-based drone-view object detection suffers from severe performance degradation under adverse conditions (e.g., foggy weather, poor illumination). To remedy this, leveraging complementary mmWave radar has become a trend. However, existing fusion approaches seldom apply to drones due to i) the aggravated sparsity and noise of point clouds from low-cost commodity radars, and ii) explosive sensing data and intensive computations leading to high latency. To address these issues, we design Geryon, an edge assisted object detection system on drones, which utilizes a suit of approaches to fully exploit the complementary advantages of camera and mmWave radar on three levels: (i) a novel multi-frame compositing approach utilizes camera to assist radar to address the aggravated sparsity and noise of radar point clouds; (ii) a saliency area extraction and encoding approach utilizes radar to assist camera to reduce the bandwidth consumption and offloading latency; (iii) a parallel transmission and inference approach with a lightweight box enhancement scheme further reduces the offloading latency while ensuring the edge-side accuracy-latency trade-off by the parallelism and better camera-radar fusion. We implement and evaluate Geryon with four datasets we collect under foggy/rainy/snowy weather and poor illumination conditions, demonstrating its great advantages over other state-of-the-art approaches in terms of both accuracy and latency.  © 2022 ACM.
KW  - drone
KW  - edge network orchestration
KW  - mmWave radar sensing
KW  - multimodal fusion
KW  - real-time object detection
KW  - Aircraft detection
KW  - Cameras
KW  - Drones
KW  - Economic and social effects
KW  - Millimeter waves
KW  - Object recognition
KW  - Tracking radar
KW  - Edge network orchestration
KW  - EDGE Networks
KW  - Mm waves
KW  - Mmwave radar sensing
KW  - Multi-modal fusion
KW  - Objects detection
KW  - Point-clouds
KW  - Radar sensing
KW  - Real- time
KW  - Real-time object detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Z.
AU  - Hanwen, G.
AU  - Wu, X.
TI  - Detection of pedestrians and vehicles in autonomous driving with selective kernel networks
PY  - 2023
T2  - Cognitive Computation and Systems
DO  - 10.1049/ccs2.12078
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149452252&doi=10.1049%2fccs2.12078&partnerID=40&md5=9e931703e8eee0f5b8b60a56b962877b
AB  - Accurate detection of pedestrians and vehicles on the road is an important content in autonomous driving technology. In this article, a method to optimise the object detection network using the channel attention mechanism is proposed. In general, small object detection problems and difficult sample detection problems in object detection tasks can be solved by using feature pyramids. Different from building a feature pyramid, the authors did not make extensive changes to the network, but used the channel attention mechanism to dynamically adjust the output of a layer during the feature extraction process, allowing each neuron to adjust its receptive field size adaptively according to multiple scales of the input information, so that the network pays attention to the extraction of important features, especially the features of small objects and difficult samples. In order to evaluate the performance of the proposed method, experiments were conducted on standard benchmark data sets. It has been observed that the proposed method is superior to the original object detection network in terms of the detection accuracy of pedestrians and vehicles, especially the detection of small objects. © 2023 The Authors. Cognitive Computation and Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Shenzhen University.
KW  - artificial intelligence
KW  - computer vision
KW  - image processing
KW  - Autonomous vehicles
KW  - Benchmarking
KW  - Cognitive systems
KW  - Computer vision
KW  - Extraction
KW  - Feature extraction
KW  - Object recognition
KW  - Pedestrian safety
KW  - Attention mechanisms
KW  - Autonomous driving
KW  - Detection networks
KW  - Detection problems
KW  - Detection tasks
KW  - Feature pyramid
KW  - Images processing
KW  - Objects detection
KW  - Small object detection
KW  - Small objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shin, H.
AU  - Yeom, T.
AU  - Lee, S.
TI  - Real-Time Lane/Pothole Detection and Collision Avoidance Algorithm for Low-cost Autonomous Driving Systems
ST  - 저사양 소형 자율주행 시스템을 위한 실시간 차선 인식과 포트홀 탐지 기반 회피 주행 알고리즘 개발
PY  - 2023
T2  - Transactions of the Korean Society of Mechanical Engineers, A
DO  - 10.3795/KSME-A.2023.47.11.901
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179752991&doi=10.3795%2fKSME-A.2023.47.11.901&partnerID=40&md5=cfdef2f577da91604921bcd72b709f9b
AB  - Lane detection and object detection represent pivotal technologies in modern autonomous driving systems. Therefore, this study introduces a lightweight model that encompasses both object detection and lane detection, tailored for utilization with Raspberry Pi and Arduino platforms. The proposed method incorporates an algorithm designed for lane-change maneuvers, which involves the real-time identification of potholes-localized depressions in the road surface-by processing camera data. Moreover, effective training was administered via data augmentation techniques to establish a brightness-robust object detection network. This approach facilitated successful pothole detection, even in low-light conditions. Experimental trials conducted on a test track validated the functionality of the model, demonstrating its effectiveness through a range of evaluation metrics. © 2023 The Korean Society of Mechanical Engineers.
KW  - Autonomous Driving
KW  - Computer Vision
KW  - Deep Learning
KW  - Lane Detection
KW  - Object Detection
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Data handling
KW  - Deep learning
KW  - Landforms
KW  - Object recognition
KW  - Autonomous driving
KW  - Collisions avoidance
KW  - Deep learning
KW  - Driving systems
KW  - Lane change
KW  - Lane detection
KW  - Low-costs
KW  - Objects detection
KW  - Real- time
KW  - Real-time identification
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Alam, A.
AU  - Singh, L.
AU  - Jaffery, Z.A.
AU  - Verma, Y.K.
AU  - Diwakar, M.
TI  - Distance-based confidence generation and aggregation of classifier for unstructured road detection
PY  - 2022
T2  - Journal of King Saud University - Computer and Information Sciences
DO  - 10.1016/j.jksuci.2021.09.020
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118785580&doi=10.1016%2fj.jksuci.2021.09.020&partnerID=40&md5=049f634a0a1700077db7f7444df14594
AB  - Road region and non-road region separation in the unstructured road intends to be an important task for safe navigation and collision avoidance for autonomous driving vehicles. The road that connects rural areas and cities to the national highways are considered as unstructured roads. Absence of clear lane marking on these unstructured roads makes them more prone to accidents in comparison to highways, which have clear lane marking for indication of road and non-road regions. However, the unstructured roads have different color information from its background that paves an easy way for design and development of an efficient road detection system for recognition and classification of road and non-road regions. Hence, in this paper, we propose an efficient road detection system for the classification of unstructured roads into road and non-road regions using multiple nearest neighbors (NN) classifier and soft voting aggregation approach. The proposed system utilized the chromatic information (i.e. *a,*b, and Hue) to train the NN classifiers and aggregated their output using soft voting (SV) approach for final output response. The output results of multiple classifiers were aggregated using SV approach based on the confidence score obtained by each individual classifier. The performance of the proposed system is evaluated in terms of precision, recall, accuracy, intersection over union (IOU), true positive rate (TPR), and processing time and compared with current state of art methods reported in the literature. The proposed system achieved precision, recall, accuracy, IOU, and TPR of 96.79%, 96.92%, 97.8%, 96.08% and 96%, respectively with the processing time three times smaller than those of the existing state of art methods. The experimental results demonstrate that the proposed system can provide an effective guidance to the autonomous vehicles through recognition and classification of road and non-road regions in rural, urban, and city areas, wherein single unstructured roads connect the national highways. © 2021
KW  - Autonomous vehicle
KW  - L*a*b color space
KW  - Nearest neighbor classifier
KW  - Soft voting aggregation
KW  - Unstructured road detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Brophy, T.
AU  - Mullins, D.
AU  - Parsi, A.
AU  - Horgan, J.
AU  - Ward, E.
AU  - Denny, P.
AU  - Eising, C.
AU  - Deegan, B.
AU  - Glavin, M.
AU  - Jones, E.
TI  - A Review of the Impact of Rain on Camera-Based Perception in Automated Driving Systems
PY  - 2023
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3290143
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163495381&doi=10.1109%2fACCESS.2023.3290143&partnerID=40&md5=83e6ff2e39fec48a22a80e4adb7f9d9d
AB  - Automated vehicles rely heavily on image data from visible spectrum cameras to perform a wide range of tasks from object detection, classification, and avoidance to path planning. The availability and reliability of these sensors in adverse weather is therefore of critical importance to the safe and continuous operation of an automated vehicle. This review paper presents a data communication-inspired Image Formation Framework that characterizes the data flow from object through channel to sensor, and subsequent processing of the data. This framework is used to explore the degree to which adverse weather conditions affect the cameras used in automated vehicles for sensing and perception. The effects of rain on each element of the model are reviewed. Furthermore, the prevalence of these rain-induced changes in publicly available open-source datasets is reviewed. The degree to which synthetic rain generation techniques can accurately capture these changes is also examined. Finally, this paper offers some suggestions on how future adverse weather automotive datasets should be collected.  © 2013 IEEE.
KW  - Adverse weather
KW  - automated vehicles
KW  - computer vision
KW  - rain
KW  - sensor availability
KW  - Computer vision
KW  - Data handling
KW  - Motion planning
KW  - Object detection
KW  - Rain
KW  - Vehicles
KW  - Adverse weather
KW  - Automated driving systems
KW  - Automated vehicles
KW  - Camera-based
KW  - Image color analysis
KW  - Image data
KW  - Road
KW  - Sensor availability
KW  - Sensor phenomenon and characterizations
KW  - Visible spectrums
KW  - Cameras
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Krishna Rao Muvva, V.V.R.M.
AU  - Samal, K.
AU  - Bradley, J.
AU  - Wolf, M.
TI  - A Closed Loop Perception Subsystem for small Unmanned Aerial Systems
PY  - 2023
T2  - AIAA SciTech Forum and Exposition, 2023
DO  - 10.2514/6.2023-2673
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196498398&doi=10.2514%2f6.2023-2673&partnerID=40&md5=5ab91187001146f5645255e8adf45ff7
AB  - The perception subsystem in modern autonomous systems use convolutional neural networks (CNNs) for their high accuracy. The structure of such networks is static and there is an inverse relationship between their accuracy and resource consumption, such as inference speed and energy utilized. This poses a challenge while designing the perception subsystem for agile autonomous systems such as small unmanned aerial systems which have limited resources while operating in real world dynamic scenarios. Existing approaches design the perception subsystems for the worst case scenario which can lead to inefficient resource utilization that can hamper the mission capabilities of such systems. At the same time it is difficult to predict the worst case condition during design time especially for systems that operate in stochastic and dynamic real world scenarios. Therefore, we have developed a closed-loop perception subsystem that can dynamically change the structure of its CNN such that the accuracy and latency adapt to the requirements of a given scenario. The proposed system was tested on an UAS-UAS tracking scenario. It was demonstrated that the chasing UAS with the proposed closed loop perception subsystem was able to successfully track the target UAS more accurately than the baseline approach, with static CNNs, while consuming less energy. Furthermore, the adaptive latency of the proposed method lets the chasing UAS track the target UAS at higher velocities compared to baseline methods. © 2023, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.
KW  - Air navigation
KW  - Antennas
KW  - Convolutional neural networks
KW  - Real time systems
KW  - Stochastic systems
KW  - Unmanned aerial vehicles (UAV)
KW  - Closed-loop
KW  - Convolutional neural network
KW  - Energy
KW  - High-accuracy
KW  - Inverse relationship
KW  - Real-world
KW  - Resources consumption
KW  - System use
KW  - Unmanned aerial systems
KW  - World dynamics
KW  - Inverse problems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Di, J.
AU  - Feng, F.
AU  - Yang, Y.
AU  - Zhang, W.
TI  - UAV Image Object Detection Based on Improved YOLOv5s
PY  - 2023
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.3011970
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180127829&doi=10.1117%2f12.3011970&partnerID=40&md5=824c55fc8ca280bf892fd9b8df24b369
AB  - As we all know, the objects in the images taken by unmanned aerial vehicle (UAV) are relatively small, while our naked eyes are able to extract the information almost instantly, even from far away, image resolution and computational resources limitations make detecting smaller objects a genuinely challenging task for machines. We propose an algorithm based on YOLOv5s with small computational resources and high accuracy, so as to be applied to edge detection devices such as unmanned aerial vehicles. By simplifying the depth of the feature extraction network and adjusting the size of the feature map of the detection head, the target in the image taken by UAV can be accurately identified. In the end, we reduced the number of parameters by 70% at the expense of a little accuracy, while improving accuracy by 15.25%, or 5.2 percentage points, over the baseline. © 2023 SPIE. All rights reserved.
KW  - Edge device object detection
KW  - Small object detection
KW  - UAV image
KW  - Aircraft detection
KW  - Antennas
KW  - Edge detection
KW  - Feature extraction
KW  - Image enhancement
KW  - Object detection
KW  - Object recognition
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Computational resources
KW  - Edge device object detection
KW  - Image object detection
KW  - Naked-eye
KW  - Objects detection
KW  - Resource limitations
KW  - Small object detection
KW  - Unmanned aerial vehicle image
KW  - Vehicle images
KW  - Image resolution
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, Y.
AU  - Huang, L.
AU  - Zhang, Z.
AU  - Nie, B.
AU  - Zhang, F.
TI  - Analysis of Scale Sensitivity of Ship Detection in an Anchor-Free Deep Learning Framework
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12010038
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145845750&doi=10.3390%2felectronics12010038&partnerID=40&md5=133aaf58e1ec9fc9ac0936ad899282c4
AB  - Ship detection is an important task in sea surveillance. In the past decade, deep learning-based methods have been proposed for ship detection from images and videos. Convolutional features are observed to be very effective in representing ship objects. However, the scales of convolution often lead to different capacities of feature representation. It is unclear how the scale influences the performance of deep learning methods in ship detection. To this end, this paper studies the scale sensitivity of ship detection in an anchor-free deep learning framework. Specifically, we employ the classical CenterNet as the base and analyze the influence of the size, the depth, and the fusion strategy of convolution features on multi-scale ship target detection. Experiments show that, for small targets, the features obtained from the top-down path fusion can improve the detection performance more significantly than that from the bottom-up path fusion; on the contrary, the bottom-up path fusion achieves better detection performance on larger targets. © 2022 by the authors.
KW  - convolutional neural network
KW  - multi-scale features
KW  - object detection
KW  - scale sensitivity
KW  - ship detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Q.
AU  - Ye, X.-M.
AU  - Feng, W.-B.
TI  - Vehicle detection in foggy weather combining millimeter wave rada and machine vision
PY  - 2023
T2  - Chinese Journal of Liquid Crystals and Displays
DO  - 10.37188/CJLCD.2022-0412
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176120584&doi=10.37188%2fCJLCD.2022-0412&partnerID=40&md5=a243d57817d8afea4946bb4d0430daf2
AB  - Vehicle detection is very vital to the assisted driving system. Due to the serious degradation of the foggy road scene，the vehicle information in the image is not obvious，resulting in missed detection and false detection problems in vehicle detection. Aiming at the above problems，a vehicle detection method in foggy weather combining millimeter-wave radar and machine vision is proposed. First，the dark channel dehazing algorithm is used to preprocess the image to improve the salience of vehicle information in the image under foggy conditions. Then，the knowledge distillation is used to improve the YOLOv5s algorithm， and the knowledge distillation is introduced into the feature extraction network of YOLOv5s，which is used in the target positioning and classification stages to calculate the distillation loss and backpropagate the loss to train a small network model to improve the detection speed while ensuring the accuracy of visual detection. Finally，the distance matching algorithm based on the search of potential target detection areas is used to compare the visual detection results and the millimeter-wave radar detection results decision-making fusion. Based on the type and distance of the detected target，the interference information and erroneous information is filtered out，and the targets with high confidence after fusion in millimeter-wave radar detection and visual detection is retained. Thereby，the accuracy of vehicle detection is improved. The experimental results show that the method has the highest detection accuracy rate of 92. 8% and the recall rate of 90. 7% in foggy weather，which can realize the detection of vehicles in foggy weather. © 2023, Science Press. All rights reserved.
KW  - decision level fusion
KW  - defogging
KW  - distance matching
KW  - knowledge distillation
KW  - millimeter wave radar
KW  - vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Hu, R.
AU  - Ting, R.
TI  - An Object Detection Method for Remote Sensing Images Based on DA-YOLO
PY  - 2023
T2  - Lecture Notes in Electrical Engineering
DO  - 10.1007/978-981-99-0923-0_13
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152616921&doi=10.1007%2f978-981-99-0923-0_13&partnerID=40&md5=c6332991577d219be684cbeda04bb087
AB  - Aiming at the difficulty of small-scale objects in high-resolution remote sensing images, this paper proposes a new detector DA-YOLO (dilation and attention YOLO) to locate objects quickly and accurately. Firstly, during the data preprocessing, the remote sensing images processed by “quadruple cropping” to adjust the original image size and enlarge the number of data instance. Then, the CSPDarknet53 backbone network is optimized: the dilated separable convolution (DSC) module is applied to enlarge the receptive range of feature maps without losing the resolution of feature maps. Then, the convolutional block attention module (CBAM) is introduced for feature enhancement, and finally, the last four stages of feature maps are used instead of three stages to obtain more contour details of small-scale objects. Extensive experiments show that DA-YOLO has good performance in DOTA, with a 2.36% increase in mAP compared to the original YOLOv4 without a significant decrease in detection speed. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
KW  - Attention mechanism
KW  - Dilated separable convolution
KW  - Multi-scale
KW  - Object detection
KW  - Remote sensing image
KW  - Convolution
KW  - Object recognition
KW  - Remote sensing
KW  - Attention mechanisms
KW  - Dilated separable convolution
KW  - Feature map
KW  - High-resolution remote sensing images
KW  - Image-based
KW  - Multi-scales
KW  - Object detection method
KW  - Objects detection
KW  - Remote sensing images
KW  - Small scale
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, M.
AU  - Gao, F.
AU  - Yang, W.
AU  - Zhang, H.
TI  - Wildlife Object Detection Method Applying Segmentation Gradient Flow and Feature Dimensionality Reduction
PY  - 2023
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics12020377
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146810122&doi=10.3390%2felectronics12020377&partnerID=40&md5=ad289244ca243c7cbb56eadac28ccd13
AB  - This work suggests an enhanced natural environment animal detection algorithm based on YOLOv5s to address the issues of low detection accuracy and sluggish detection speed when automatically detecting and classifying large animals in natural environments. To increase the detection speed of the model, the algorithm first enhances the SPP by switching the parallel connection of the original maximum pooling layer for a series connection. It then expands the model’s receptive field using the dataset from this paper to enhance the feature fusion network by stacking the feature pyramid network structure as a whole; secondly, it introduces the GSConv module, which combines standard convolution, depth-separable convolution, and hybrid channels to reduce network parameters and computation, making the model lightweight and easier to deploy to endpoints. At the same time, GS bottleneck is used to replace the Bottleneck module in C3, which divides the input feature map into two channels and assigns different weights to them. The two channels are combined and connected in accordance with the number of channels, which enhances the model’s ability to express non-linear functions and resolves the gradient disappearance issue. Wildlife images are obtained from the OpenImages public dataset and real-life shots. The experimental results show that the improved YOLOv5s algorithm proposed in this paper reduces the computational effort of the model compared to the original algorithm, while also providing an improvement in both detection accuracy and speed, and it can be well applied to the real-time detection of animals in natural environments. © 2023 by the authors.
KW  - animal recognition
KW  - feature fusion networks
KW  - GSConv
KW  - segmentation gradient flow
KW  - YOLOv5s
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fei, X.
AU  - Han, Y.
AU  - Wong, S.V.
TI  - An Overview of and Prospects for Research on Energy Savings in Wheel Loaders
PY  - 2023
T2  - Automotive Experiences
DO  - 10.31603/ae.8759
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159388951&doi=10.31603%2fae.8759&partnerID=40&md5=1510040731d3fd73b5b33ba4891baf5b
AB  - Wheel loaders consume a large amount of energy, and research on energy savings in wheel loaders has been carried out for decades. This paper introduces several types of wheel loaders and compares their structures. The research progress of the energy savings of three different forms of wheel loaders is reviewed, including a diesel engine wheel loader, a hybrid wheel loader, and an electric wheel loader. In particular, the energy-saving control methods of an electric wheel loader in the working cycle are analyzed, as construction machinery electrification is an emerging trend. Based on the analysis of the driving features and the working process of a wheel loader, energy-saving control methods are introduced including the resistance reduction method, optimized control strategies, intelligent control, and unmanned WL research. Comparing various energy-saving research methods and the advantages of electric wheel loaders, the pure electric wheel loaders are advised to be researched at present and in the future. Controlling the torque distribution of the front and rear motors of electric wheel loaders and assistant drive control are proposed to be significant research prospects for energy savings in wheel loaders usage. © 2023, Universitas Muhammadiyah Magelang. All rights reserved.
KW  - Control strategy
KW  - Electric wheel loader
KW  - Energy savings
KW  - Wheel loader
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Yao, Y.
AU  - Zhu, L.J.
AU  - Shi, W.
TI  - DICE: Dynamic In-Situ Control for Edge-Based Applications
PY  - 2023
T2  - Proceedings - 2023 IEEE/ACM Symposium on Edge Computing, SEC 2023
DO  - 10.1145/3583740.3626817
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186117195&doi=10.1145%2f3583740.3626817&partnerID=40&md5=776f13598162c9cc5e23a1bc1960fd1d
AB  - This paper focuses on addressing computational constraints and energy limitations prevalent in edge-based applications through an innovative approach, dynamic in-situ control for edge-based applications (DICE). DICE capitalizes on the burgeoning trend in vehicle sensor technologies, such as camera, Radar, and LiDAR, which are becoming increasingly powerful and capable of performing pre-processing computations. DICE introduces a concept of 'downstream offloading', which distinguishes it from traditional offloading approaches that typically offload computational tasks from edge devices to more powerful Edge Servers. In contrast, DICE offloads part of the computational tasks from the Edge Server to the sensor itself, thereby optimizing data processing at the source and reducing the volume of data transmission required. This approach not only addresses the latency bottleneck frequently encountered in energy-intensive neural networks but also enhances the efficiency of data processing by selectively filtering out non-critical frames based on event-triggering mechanisms. DICE leverages the unique strengths of portable devices such as smartwatches and smartphones, even with their inherent computational and power limitations. The framework consists of an adaptive control layer for dynamic task allocation and an application layer designed to deploy quantized models on System on Chips (SoCs) like TinyML, thereby improving the efficiency of AI-driven applications while conservatively utilizing energy. This system proposes a sustainable, energy-efficient pathway for future edge-based applications. © 2023 ACM.
KW  - dynamic control
KW  - edge collaboration
KW  - smart sensor
KW  - Adaptive control systems
KW  - Computation offloading
KW  - Computing power
KW  - Data handling
KW  - Optical radar
KW  - System-on-chip
KW  - Computational constraints
KW  - Computational task
KW  - Dynamic controls
KW  - Edge collaboration
KW  - Edge server
KW  - Edge-based
KW  - Energy
KW  - Energy limitations
KW  - In-situ control
KW  - Innovative approaches
KW  - Energy efficiency
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Mungekar, K.
AU  - Marakarkandy, B.
AU  - Kelkar, S.
AU  - Gupta, P.
TI  - Design of an Aqua Drone for Automated Trash Collection from Swimming Pools Using a Deep Learning Framework
PY  - 2023
T2  - Lecture Notes in Networks and Systems
DO  - 10.1007/978-981-19-9225-4_41
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150965019&doi=10.1007%2f978-981-19-9225-4_41&partnerID=40&md5=b51873472d27a2666a57736c2886e152
AB  - Water conservation is of prime importance to sustain life. An ample amount of water is used in swimming pools. Material contamination of water in a swimming pool occurs from various sources, viz., leaves of trees and plants from the surrounding area, plastic bottles, wearables for protection of eyes, ears, and hair left by people who swim in the pools. Removal of this trash is a challenging task. The existing trash collector boats being huge in size are primarily designed for cleaning rivers and seas; moreover, manual intervention is needed for its operation. These boats are not suitable to be used in swimming pools. This paper presents an automatic robotic trash boat for floating trash detection, collection, and finally accumulation of the floating trash using a conveyor machine. The system is portable, user-friendly, environmentally pleasant, and facilitates remote control operation. The system uses a deep learning model based on a modified YOLO framework. The prototype has superior performance compared to existing systems with respect to accuracy and time. Performance indicators, viz., accuracy, error rate, precision, recall F1 score and the quadratic weighted kappa were used to evaluate the prototype. The final trained network obtained 92% accuracy, 0.0167 error rate, 0.23 logloss, 92% precision, 92% recall, 92% F1 score, and 0.9665 kappa quadratic weighted value. The system would help in cleaning swimming pools thus preventing contamination of water due to this water replenishment cycle can be reduced. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
KW  - Aqua drone
KW  - CNN
KW  - Trash collection
KW  - Boats
KW  - Deep learning
KW  - Drones
KW  - Plants (botany)
KW  - Remote control
KW  - Swimming pools
KW  - Water pollution
KW  - Aqua drone
KW  - Error rate
KW  - F1 scores
KW  - Learning frameworks
KW  - Manual intervention
KW  - Material contamination
KW  - Remote control operations
KW  - System use
KW  - Trash collection
KW  - User friendly
KW  - Lakes
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Talsma, D.
TI  - The Psychology of Cognition: An Introduction to Cognitive Neuroscience
PY  - 2023
T2  - The Psychology of Cognition: An Introduction to Cognitive Neuroscience
DO  - 10.4324/9781003319344
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171005735&doi=10.4324%2f9781003319344&partnerID=40&md5=c3a9edade2faaedf89d1b4da318de92d
AB  - This comprehensive, cutting-edge textbook offers a layered approach to the study of cognitive neuroscience and psychology. It embraces multiple exciting and influential theoretical approaches such as embodied cognition and predictive coding, and explaining new topics such as motor cognition, cognitive control, consciousness, and social cognition. Durk Talsma offers foundational knowledge which he expands and enhances with coverage of complex topics, explaining their interrelatedness and presenting them together with classic experiments and approaches in a historic context. Providing broad coverage of world-class international research this richly illustrated textbook covers key topics including: Action control and cognitive control Consciousness and attention Perception Multisensory processing and perception-action integration Motivation and reward processing Emotion and cognition Learning and memory Language processing Reasoning Numerical cognition and categorisation Judgement, decision making, and problem solving Social cognition Applied cognitive psychology With pedagogical features that include highlights of relevant methods and historical notes to spark student interest, this essential text will be invaluable reading for all students of cognitive psychology and cognitive neuroscience. © 2023 Durk Talsma.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xu, X.
AU  - Pan, H.
AU  - Wang, H.
AU  - Cao, Y.
TI  - Object Detection Algorithm for Railway Scenes Based on Infrared and RGB Image Fusion
PY  - 2023
T2  - Proceedings - 2023 International Conference on Pattern Recognition, Machine Vision and Intelligent Algorithms, PRMVIA 2023
DO  - 10.1109/PRMVIA58252.2023.00015
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163865123&doi=10.1109%2fPRMVIA58252.2023.00015&partnerID=40&md5=bdf3ff5f8119e28e4c4191b5143d195e
AB  - The driver-assistance system tends to fuse multi-modal sensor data, for instance, the infrared and RGB sensors, to detect intrusion objects to enhance driving safety. However, the semantic misalignment dilemma and the spectral imb-alance between infrared and RGB images make it hard to exp-loit the advantages of multi-sensors in the end-to-end learning system. To solve these problems, we employ the widely used affine transformation on our railway dataset to solve the se-mantic-misalignment issue, in addition, we propose a fusion module, DMF, to fuse the well-aligned features, which can bri-dge the domain gap among different sensors. To this end, we propose an efficient railway invasive object detection network, YOLOv5s-DMF. Compared with the state-of-the-art metho-ds, the YOLOv5s-DMF substantially reduces the MR by 14.23% by employing the well-established decouple head. And our YOLOv5s-DMF further increases the mAP@0.5 by 5.7% and the mAP@0.5:0.95by4.1%.  © 2023 IEEE.
KW  - component
KW  - deep learning
KW  - multi-modal fusion
KW  - object detection
KW  - Alignment
KW  - Automobile drivers
KW  - Deep learning
KW  - Image fusion
KW  - Learning systems
KW  - Object recognition
KW  - Railroads
KW  - Semantics
KW  - Component
KW  - Deep learning
KW  - Driver-assistance systems
KW  - Multi-modal fusion
KW  - Multimodal sensor
KW  - Object detection algorithms
KW  - Objects detection
KW  - RGB images
KW  - Scene-based
KW  - Sensors data
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Boulch, A.
AU  - Sautier, C.
AU  - Michele, B.
AU  - Puy, G.
AU  - Marlet, R.
TI  - ALSO: Automotive Lidar Self-Supervision by Occupancy Estimation
PY  - 2023
T2  - Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
DO  - 10.1109/CVPR52729.2023.01293
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171390587&doi=10.1109%2fCVPR52729.2023.01293&partnerID=40&md5=42a231526d2891e361120f22aaae1629
AB  - We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information, that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches. The code is available at github.com/valeoai/ALSO © 2023 IEEE.
KW  - Self-supervised or unsupervised representation learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CHAP
TI  - Compilation of references
PY  - 2022
T2  - Principles and Applications of Socio-Cognitive and Affective Computing
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161594795&partnerID=40&md5=6edd375a425d893f6cbf417d574d7d9a
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yi, J.
AU  - Chen, G.
AU  - Ru, Q.
AU  - Li, M.
TI  - Lightweight semantic segmentation for digital workshop objects
ST  - 数字化车间目标轻量级语义分割
PY  - 2023
T2  - Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS
DO  - 10.13196/j.cims.2023.03.021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163179956&doi=10.13196%2fj.cims.2023.03.021&partnerID=40&md5=ef49c521a752c2a55dbbee1f0fabd633
AB  - To meet the real-time demand of manufacturing in the digital workshop, a lightweight semantic segmentation network named Multi Pyramid Pooling Segmentation Network (MPPSNet) that satisfied both accuracy and real-time was proposed, which realized the semantic segmentation for the goal of digital workshop. The improved MobileNetv2 was used as the encoder of semantic segmentation network in MPPSNet, which effectively reduced the amount of network parameters and improved the real-time performance of the overall network operation; the multi-pyramid pooling network was used as the decoder of the segmentation network, which could integrate multiple layers of feature information and improve the accuracy of the network. Tests found that the semantic segmentation effect of MPPSNet was better than that of FCN8 and BiSeNet in VOC20 12data set. In the self-building object semantic segmentation data set of the digital workshop, the Mean Intersection over Unions ( MIoU) of segmenting human, machine tool, and mobile robot of workshop objects reached 7 1.8% in MPPSNet and the parameter amount of the entire network was 2.55M, which could meet the accurate and real-time requirements of segmenting workshop objects. © 2023 CIMS. All rights reserved.
KW  - digital workshop
KW  - lightweight
KW  - MobileNetv2
KW  - real-time
KW  - semantic segmentation
KW  - Machine tools
KW  - Semantic Segmentation
KW  - Digital workshop
KW  - Lightweight
KW  - Lightweight semantics
KW  - Mobilenetv2
KW  - Network operations
KW  - Network parameters
KW  - Overall networks
KW  - Real time performance
KW  - Real- time
KW  - Semantic segmentation
KW  - Semantics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Zong, C.
AU  - Meng, K.
AU  - Sun, J.
AU  - Zhou, Q.
TI  - Real Time Object Recognition Based on YOLO Model
PY  - 2023
T2  - 2023 3rd International Conference on Electronic Information Engineering and Computer Science, EIECS 2023
DO  - 10.1109/EIECS59936.2023.10435392
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187003667&doi=10.1109%2fEIECS59936.2023.10435392&partnerID=40&md5=1c20584431ef0de396853fc199dc8f01
AB  - Real-time object recognition is a fundamental task in computer vision with many applications. This paper presents a comprehensive survey of the evolution of the You Only Look Once (YOLO) object detection models, spanning from YOLOv1 to YOLOv8. Each iteration of the YOLO model is examined in detail, elucidating architectural advancements and innovations that have propelled real-time object recognition performance. The survey delves into the strengths and limitations of each YOLO version, highlighting their respective contributions to the field. Additionally, two prominent practical implementations of YOLO models are elucidated, exemplifying the models' efficacy in complex scenarios. The first case study explores YOLO's role in enabling real-time object detection for autonomous driving systems, enhancing safety and situational awareness. The second case study investigates the integration of YOLO models in unmanned aerial vehicles, showcasing their utility in aerial surveillance and reconnaissance. By providing an in-depth exploration of YOLO models and their evolution, this paper equips researchers and practitioners with a comprehensive understanding of real-time object recognition techniques. Furthermore, the analysis of practical applications underscores the tangible impact of YOLO models on cutting-edge technologies. Looking forward, this survey sets the stage for future advancements in real-time object recognition, addressing challenges and opportunities for refining performance in dynamic and complex environments.  © 2023 IEEE.
KW  - autonomous driving
KW  - computer vision
KW  - real-time object recognition
KW  - survey
KW  - unmanned aerial vehicles
KW  - YOLO models
KW  - Antennas
KW  - Autonomous vehicles
KW  - Iterative methods
KW  - Object detection
KW  - Object recognition
KW  - Real time systems
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Autonomous driving
KW  - Case-studies
KW  - Detection models
KW  - Objects detection
KW  - Performance
KW  - Real- time
KW  - Real-time object recognition
KW  - Unmanned aerial vehicle
KW  - You only look once model
KW  - Computer vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Shi, H.
AU  - Zhao, D.
TI  - License Plate Recognition System Based on Improved YOLOv5 and GRU
PY  - 2023
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3240439
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148333620&doi=10.1109%2fACCESS.2023.3240439&partnerID=40&md5=f1bab0371813ef2fd1a3756389fa0bf0
AB  - Aiming at the problem that the traditional license plate recognition method lacking of accuracy and speed, an end-to-end deep learning model for license plate location and recognition in natural scenarios was proposed. First, we added an improved channel attention mechanism to the down-sampling process of the You only look once(YOLOv5). Additionally, a location information is added in the ones to minimize the information loss from sampling, which can improve the feature extraction ability of the model. Then we reduce the number of parameters on the input side and set only one class in the YOLO layer, which improves the efficiency and accuracy of the detector for locating license plates. Finally, Gated recurrent units(GRU) + Connectionist temporal classification(CTC) was used to build the recognition network to complete the character segmentation-free recognition task of the license plate, significantly shortened the training time and improved the convergence speed and recognition accuracy of the network. The experimental results show that the average recognition precision of the license plate recognition model proposed in this paper reaches 98.98%, which is significantly better than the traditional recognition algorithm, and the recognition effect is good in complex environment with good stability and robustness.  © 2013 IEEE.
KW  - Deep learning
KW  - GRU
KW  - license plate recognition
KW  - target detection
KW  - YOLOv5
KW  - Deep learning
KW  - Learning systems
KW  - License plates (automobile)
KW  - Optical character recognition
KW  - Deep learning
KW  - End to end
KW  - Gated recurrent unit
KW  - Learning models
KW  - License plate location
KW  - License plate recognition systems
KW  - Licenses plate recognition
KW  - Recognition methods
KW  - Targets detection
KW  - YOLOv5
KW  - Location
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Y.
AU  - Zhu, G.
AU  - Shi, T.
AU  - Zhang, K.
AU  - Yan, J.
TI  - Small Object Detection in Remote Sensing Images Based on Feature Fusion and Attention
ST  - 基 于 特 征 融 合 与 注 意 力 的 遥 感 图 像 小 目 标 检 测
PY  - 2022
T2  - Guangxue Xuebao/Acta Optica Sinica
DO  - 10.3788/AOS202242.2415001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144635190&doi=10.3788%2fAOS202242.2415001&partnerID=40&md5=a4e59b16d3ddba261697c57f4da16d4c
AB  - To deal with issues such as less feature information and difficult positioning raised by small object detection in remote sensing images, this paper proposes a remote sensing image small-target detection algorithm FFAM-YOLO (Feature Fusion and Attention Mechanism YOLO) based on feature fusion and attention mechanism. Firstly, in terms of inadequate effective information in backbone network feature extraction and weak information representation in feature maps, the algorithm constructs a feature enhancement module (FEM) to fuse multiple receptive field features in lower-level feature maps and improve the network′s ability in extracting object features. Secondly, with low-level and high-level feature maps obtained by the backbone network, the algorithm′s low-level and high-level feature fusion structures are rebuilt, and a feature fusion module (FFM) is implemented to enhance the feature information of small targets. Thirdly, small object features are accurately captured by cascade attention mechanism (ESM) consisting of enhanced-efficient channel attention (E-ECA) and spatial attention module (SAM). Finally, the small object is detected in the output dualbranch feature maps, and results are delivered. The experimental results show that with the USOD (Unicorn Small Object Dataset), based on the constructed remote sensing images, the proposed algorithm achieves a precision of 91. 9% and a recall of 83. 5%, with an average precision AP of 89% for intersection ratio threshold (IoU) between the prediction box and the ground truth box of 0. 5 and an AP of 32. 6% for IoU of 0. 5∶ 0. 95, respectively, and the detection rate reaches 120 frame/s. The algorithm is with robustness and real-time performance. © 2022 Chinese Optical Society. All rights reserved.
KW  - attention mechanism
KW  - feature enhancement
KW  - feature fusion
KW  - machine vision
KW  - remote sensing image
KW  - small object detection
KW  - Feature extraction
KW  - Image enhancement
KW  - Image fusion
KW  - Object detection
KW  - Object recognition
KW  - Remote sensing
KW  - Attention mechanisms
KW  - Feature enhancement
KW  - Feature information
KW  - Feature map
KW  - Features fusions
KW  - Fusion mechanism
KW  - Machine-vision
KW  - Remote sensing images
KW  - Small object detection
KW  - Small objects
KW  - Computer vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - El Mazgualdi, C.
AU  - Masrour, T.
AU  - Barka, N.
AU  - El Hassani, I.
TI  - A Learning-Based Decision Tool towards Smart Energy Optimization in the Manufacturing Process
PY  - 2022
T2  - Systems
DO  - 10.3390/systems10050180
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140607359&doi=10.3390%2fsystems10050180&partnerID=40&md5=1017e5db1ed847347dba0d41a18b9564
AB  - We developed a self-optimizing decision system that dynamically minimizes the overall energy consumption of an industrial process. Our model is based on a deep reinforcement learning (DRL) framework, adopting three reinforcement learning methods, namely: deep Q-network (DQN), proximal policy optimization (PPO), and advantage actor–critic (A2C) algorithms, combined with a self-predicting random forest model. This smart decision system is a physics-informed DRL that sets the key industrial input parameters to optimize energy consumption while ensuring the product quality based on desired output parameters. The system is self-improving and can increase its performances without further human assistance. We applied the approach to the process of heating tempered glass. Indeed, the identification and control of tempered glass parameters is a challenging task requiring expertise. In addition, optimizing energy consumption while dealing with this issue is of great value-added. The evaluation of the decision system under the three configurations has been performed and consequently, outcomes and conclusions have been explained in this paper. Our intelligent decision system provides an optimized set of parameters for the heating process within the acceptance limits while minimizing overall energy consumption. This work provides the necessary foundations to address energy optimization issues related to process parameterization from theory to practice and providing real industrial application; further research opens a new horizon towards intelligent and sustainable manufacturing. © 2022 by the authors.
KW  - autonomous process control
KW  - deep reinforcement learning
KW  - dual-optimization problem
KW  - energy self-optimization
KW  - glass tempering process
KW  - Industry 4.0
KW  - intelligent manufacturing
KW  - process parameters self-configuration
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, G.
AU  - Li, J.
AU  - Yan, S.
AU  - Liu, R.
TI  - A Novel Small Target Detection Strategy: Location Feature Extraction in the Case of Self-Knowledge Distillation
PY  - 2023
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app13063683
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151551267&doi=10.3390%2fapp13063683&partnerID=40&md5=12f8b0a2b5963c1237b7e74044ae3709
AB  - Small target detection has always been a hot and difficult point in the field of target detection. The existing detection network has a good effect on conventional targets but a poor effect on small target detection. The main challenge is that small targets have few pixels and are widely distributed in the image, so it is difficult to extract effective features, especially in the deeper neural network. A novel plug-in to extract location features of the small target in the deep network was proposed. Because the deep network has a larger receptive field and richer global information, it is easier to establish global spatial context mapping. The plug-in named location feature extraction establishes the spatial context mapping in the deep network to obtain the global information of scattered small targets in the deep feature map. Additionally, the attention mechanism can be used to strengthen attention to the spatial information. The comprehensive effect of the above two can be utilized to realize location feature extraction in the deep network. In order to improve the generalization of the network, a new self-distillation algorithm was designed for pre-training that could work under self-supervision. The experiment was conducted on the public datasets (Pascal VOC and Printed Circuit Board Defect dataset) and the self-made dedicated small target detection dataset, respectively. According to the diagnosis of the false-positive error distribution, the location error was significantly reduced, which proved the effectiveness of the plug-in proposed for location feature extraction. The mAP results can prove that the detection effect of the network applying the location feature extraction strategy is much better than the original network. © 2023 by the authors.
KW  - attention mechanism
KW  - location feature extraction
KW  - self-knowledge distillation
KW  - small target detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, Y.
AU  - Zhang, S.
AU  - Xin, D.
AU  - Chen, D.
TI  - A Small Target Pedestrian Detection Model Based on Autonomous Driving
PY  - 2023
T2  - Journal of Advanced Transportation
DO  - 10.1155/2023/5349965
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149415458&doi=10.1155%2f2023%2f5349965&partnerID=40&md5=1fb258d4a28bc40284127cb58e191634
AB  - Since small-target pedestrians account for a small proportion of pixels in images and lack texture features, the feature information of small-target pedestrians is often ignored in the feature extraction process, leading to reduced accuracy and poor robustness. To improve the accuracy of small-target pedestrian detection and the anti-interference ability of the model, a small-target pedestrian detection model that fuses residual networks and feature pyramids is proposed. First, a residual block with a discard layer is constructed to replace the standard residual block in the residual network structure to reduce the complexity of the model computation process and solve the problems of gradient disappearance and explosion in the deep network. Then, feature selection and feature alignment modules are added to the lateral connection part of the feature pyramid to enhance important pedestrian features in the input image, and the multiscale feature fusion capability of the model is enhanced for small-target pedestrians, thereby improving the detection accuracy of small-target pedestrians and solving the problems of feature misalignment and ignored multiscale features in the feature pyramid network. Finally, a cascaded autofocus query module is proposed to increase the inference speed of the feature pyramid network through focusing and querying, thus improving the performance and efficiency of small-target pedestrian detection. The experimental results show that the proposed model achieves better detection results than previous models.  © 2023 Yang Zhang et al.
KW  - Autonomous vehicles
KW  - Feature Selection
KW  - Textures
KW  - Autonomous driving
KW  - Feature information
KW  - Feature pyramid
KW  - Model-based OPC
KW  - Multi-scale features
KW  - Pedestrian detection
KW  - Pedestrian detection models
KW  - Pyramid network
KW  - Small targets
KW  - Texture features
KW  - Image enhancement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bomantara, Y.A.
AU  - Mustafa, H.
AU  - Bartholomeus, H.
AU  - Kooistra, L.
TI  - Detection of Artificial Seed-like Objects from UAV Imagery
PY  - 2023
T2  - Remote Sensing
DO  - 10.3390/rs15061637
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151475481&doi=10.3390%2frs15061637&partnerID=40&md5=175da9d210ca1e74e9df522988b6f26f
AB  - In the last two decades, unmanned aerial vehicle (UAV) technology has been widely utilized as an aerial survey method. Recently, a unique system of self-deployable and biodegradable microrobots akin to winged achene seeds was introduced to monitor environmental parameters in the air above the soil interface, which requires geo-localization. This research focuses on detecting these artificial seed-like objects from UAV RGB images in real-time scenarios, employing the object detection algorithm YOLO (You Only Look Once). Three environmental parameters, namely, daylight condition, background type, and flying altitude, were investigated to encompass varying data acquisition situations and their influence on detection accuracy. Artificial seeds were detected using four variants of the YOLO version 5 (YOLOv5) algorithm, which were compared in terms of accuracy and speed. The most accurate model variant was used in combination with slice-aided hyper inference (SAHI) on full resolution images to evaluate the model’s performance. It was found that the YOLOv5n variant had the highest accuracy and fastest inference speed. After model training, the best conditions for detecting artificial seed-like objects were found at a flight altitude of 4 m, on an overcast day, and against a concrete background, obtaining accuracies of 0.91, 0.90, and 0.99, respectively. YOLOv5n outperformed the other models by achieving a mAP0.5 score of 84.6% on the validation set and 83.2% on the test set. This study can be used as a baseline for detecting seed-like objects under the tested conditions in future studies. © 2023 by the authors.
KW  - background type
KW  - deep learning
KW  - flying height
KW  - light conditions
KW  - object detection
KW  - unmanned aerial vehicles
KW  - Aircraft detection
KW  - Antennas
KW  - Data acquisition
KW  - Deep learning
KW  - Object recognition
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Artificial seeds
KW  - Background type
KW  - Condition
KW  - Deep learning
KW  - Environmental parameter
KW  - Flying heights
KW  - Light conditions
KW  - Objects detection
KW  - Unmanned aerial vehicle
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hasan, M.
AU  - Hanawa, J.
AU  - Goto, R.
AU  - Suzuki, R.
AU  - Fukuda, H.
AU  - Kuno, Y.
AU  - Kobayashi, Y.
TI  - LiDAR-based detection, tracking, and property estimation: A contemporary review
PY  - 2022
T2  - Neurocomputing
DO  - 10.1016/j.neucom.2022.07.087
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136156443&doi=10.1016%2fj.neucom.2022.07.087&partnerID=40&md5=a1a7a7fdaba21c60be60d6a4b4d441f5
AB  - Object detection, Person tracking, and Person property estimation (PPE) are identical innovation areas trying to improve their accuracy in different parameters to fit various real applications. For many years, so much research has been done in these fields. Many scientists also used many more techniques and algorithms. But most of the innovations were deeply based on image-based analysis, where cameras were the critical components of data acquisition. Over the years, new technologies arrived, and different types of research are happening. Rather than cameras, some other sensors, like infrared, depth cameras, and very recently LiDAR sensors, are used to estimate person properties, track them, as well as to detect them. Especially, height, age, gender, region, etc., parameters can be measured as person property. Eventually, 3D object detection by LiDAR will be a state-of-the-art research field with the advent of autonomous driving initiations. We studied many articles and found enthusiastic outcomes with these sensor setups to understand contemporary technology and its efficacy. We categorized these research articles into video camera-based studies and other sensor-based studies. So many surveys have been done on video-based analysis, even with deep learning techniques. Another sensor-based research is very recent, and we do not get enough study on it. We thought to summarize these studies in a survey article, especially LiDAR-based analysis. This article covered most of the recent possible sensor-based studies of detection, person tracking and property estimation except cameras (all, RGB, RGB-D, etc.) based learning. © 2022 Elsevier B.V.
KW  - LiDAR Sensor
KW  - Person Property Estimation
KW  - Person Recognition
KW  - Person Tracking
KW  - Sensor-based Sensing
KW  - Data acquisition
KW  - Deep learning
KW  - Object recognition
KW  - Optical radar
KW  - Surveys
KW  - Video cameras
KW  - Detection estimation
KW  - LiDAR sensor
KW  - Objects detection
KW  - Person property estimation
KW  - Person recognition
KW  - Person tracking
KW  - Property
KW  - Property estimation
KW  - Real applications
KW  - Sensor-based sensing
KW  - analytic method
KW  - Article
KW  - automated pattern recognition
KW  - deep learning
KW  - human
KW  - image analysis
KW  - three-dimensional imaging
KW  - videorecording
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xia, Z.
AU  - Shi, Y.
TI  - Small object in the dark light scene using dual-branch channel and spatial
PY  - 2023
T2  - Journal of Electronic Imaging
DO  - 10.1117/1.JEI.32.2.023037
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159405390&doi=10.1117%2f1.JEI.32.2.023037&partnerID=40&md5=58fce970634d30a99211445b0ab872e5
AB  - Object detection is an enabling technology of computer vision for locating instances and categorizing classes of objects in images or videos. It has made enormous strides with significant growing of deep learning research over the last decade. However, there are severe challenges on small object detection in complex scenes as they appear with low resolution and have not enough contrast from the background information. This disturbance may cause missed detection and detection accuracy decline for small objects. We propose a small object detection network in the dark light scene based on improved YOLOv5. The network takes YOLOv5 as the baseline and incorporates a channel and spatial dual-branch backbone module, which enhances the details by fusing the features of the two branches. We also introduce a densely linked feature fusion network before detection layers with receptive field block. The fusion network integrates deep features with shallow ones across different scales. A data augmentation module is used to enhance the brightness and limit the contrast for small object detection in the dark light scene. Experiments are carried out on the Dark Face and darkened Vis Drone dataset. The results show that the evaluation index of the proposed method is better than that of the comparison methods. From the detected images, it is obvious that the undetected frame of the small object in the dark light scene decreases and the detection accuracy improves. All of the results show that our model has better performance than some existing methods for small object detection in the dark light scene.  © 2023 SPIE and IS&T.
KW  - attention mechanism
KW  - dark light scene
KW  - data augmentation
KW  - small object
KW  - Deep learning
KW  - Image enhancement
KW  - Network layers
KW  - Object recognition
KW  - Attention mechanisms
KW  - Complex scenes
KW  - Dark light scene
KW  - Data augmentation
KW  - Detection accuracy
KW  - Enabling technologies
KW  - Lower resolution
KW  - Objects detection
KW  - Small object detection
KW  - Small objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Rakhmonov, A.A.U.
AU  - Subramanian, B.
AU  - Kim, T.
AU  - Kim, J.
TI  - Airy YOLOv5 for Disabled Sign Detection
PY  - 2023
T2  - International Conference on Ubiquitous and Future Networks, ICUFN
DO  - 10.1109/ICUFN57995.2023.10200853
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169295016&doi=10.1109%2fICUFN57995.2023.10200853&partnerID=40&md5=ade4c5a58df6740d603ca13b2a9e18e3
AB  - Designated parking spaces for individuals with disabilities are only meant to be used by vehicles with proper handicapped signage. Real-time monitoring is necessary to ensure that only authorized vehicles are parked in these spaces and to prevent unauthorized vehicles from using them. First, this research proposes to replace the backbone of a baseline YOLOv5 model which has 9 blocks with 6 EfficientNet blocks with less parameters but still have a higher accuracy in detecting disabled signs among other signages on the windshield of cars. Second, to compensate for the loss of blocks we have included an attention mechanism before detection part in our architecture which allows us to focus on the important regions needed for the task. Additionally, we propose to use a better optimizer AdamW to prevent overfitting. Based on these improvements, we have created a new object detector named Airy YOLOv5. To evaluate the effectiveness of our proposed method, a dataset containing images of cars with disabled signage on their windshields will be gathered and labeled. Experiments using this dataset show that our model achieves a better F1 score of 0.67 with 5 percent less parameters compared to the baseline model.  © 2023 IEEE.
KW  - depthwise separable convolution
KW  - disabled signage
KW  - small object detection
KW  - supervised learning
KW  - Machine learning
KW  - Object recognition
KW  - Traffic signs
KW  - Windshields
KW  - Attention mechanisms
KW  - Depthwise separable convolution
KW  - Disabled signage
KW  - High-accuracy
KW  - Optimizers
KW  - Overfitting
KW  - Parking spaces
KW  - Real time monitoring
KW  - Sign detection
KW  - Small object detection
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Musunuri, Y.R.
AU  - Kwon, O.-S.
AU  - Kung, S.-Y.
TI  - SRODNet: Object Detection Network Based on Super Resolution for Autonomous Vehicles
PY  - 2022
T2  - Remote Sensing
DO  - 10.3390/rs14246270
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144622410&doi=10.3390%2frs14246270&partnerID=40&md5=6ccfa3680334d84c9b14bd507b2322c1
AB  - Object detection methods have been applied in several aerial and traffic surveillance applications. However, object detection accuracy decreases in low-resolution (LR) images owing to feature loss. To address this problem, we propose a single network, SRODNet, that incorporates both super-resolution (SR) and object detection (OD). First, a modified residual block (MRB) is proposed in the SR to recover the feature information of LR images, and this network was jointly optimized with YOLOv5 to benefit from hierarchical features for small object detection. Moreover, the proposed model focuses on minimizing the computational cost of network optimization. We evaluated the proposed model using standard datasets such as VEDAI-VISIBLE, VEDAI-IR, DOTA, and Korean highway traffic (KoHT), both quantitatively and qualitatively. The experimental results show that the proposed method improves the accuracy of vehicular detection better than other conventional methods. © 2022 by the authors.
KW  - autonomous vehicles
KW  - modified residual block
KW  - object detection network
KW  - remote sensing data
KW  - super-resolution
KW  - Antennas
KW  - Autonomous vehicles
KW  - Feature extraction
KW  - Object recognition
KW  - Optical resolving power
KW  - Remote sensing
KW  - Autonomous Vehicles
KW  - Detection networks
KW  - Low resolution images
KW  - Modified residual block
KW  - Network-based
KW  - Object detection method
KW  - Object detection network
KW  - Objects detection
KW  - Remote sensing data
KW  - Superresolution
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Lin, Z.-R.
AU  - Wang, H.-C.
TI  - Efficient Optimization and Compression of Autonomous Vehicle Target Detection Models
PY  - 2023
T2  - Proceedings of the 3rd IEEE International Conference on Social Sciences and Intelligence Management, SSIM 2023
DO  - 10.1109/SSIM59263.2023.10469478
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190376060&doi=10.1109%2fSSIM59263.2023.10469478&partnerID=40&md5=075cf29fa848bff438fe5d54a130e9a1
AB  - Target detection is a crucial element of computer vision and often requires complex models and extensive computing resources. This inherent complexity poses significant challenges for use especially in the demanding field of autonomous vehicles. Thus, for the urgent need to reduce resource consumption while preserving the accuracy of target detection, we optimized the YOLO model by utilizing depthwise separable convolution. The model reduced resource usage without compromising detection performance. Additionally, quantitative compression techniques were incorporated to decrease the model size for easy deployment. The optimization strategy better-tailored target detection solutions to the specific requirements of autonomous vehicle applications.  © 2023 IEEE.
KW  - autonomous vehicles
KW  - model optimization
KW  - resource efficiency
KW  - YOLOv3-tiny
KW  - Autonomous Vehicles
KW  - Complex model
KW  - Computing resource
KW  - Detection models
KW  - Efficient optimisation
KW  - Model optimization
KW  - Resource efficiencies
KW  - Targets detection
KW  - Vehicle targets
KW  - YOLOv3-tiny
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wei, J.
AU  - Wang, Q.
AU  - Zhao, Z.
TI  - Interactive Transformer for Small Object Detection
PY  - 2023
T2  - Computers, Materials and Continua
DO  - 10.32604/cmc.2023.044284
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179132222&doi=10.32604%2fcmc.2023.044284&partnerID=40&md5=67edca15d0000e1f47a52a2fe87c8ef5
AB  - The detection of large-scale objects has achieved high accuracy, but due to the low peak signal to noise ratio (PSNR), fewer distinguishing features, and ease of being occluded by the surroundings, the detection of small objects, however, does not enjoy similar success. Endeavor to solve the problem, this paper proposes an attention mechanism based on cross-Key values. Based on the traditional transformer, this paper first improves the feature processing with the convolution module, effectively maintaining the local semantic context in the middle layer, and significantly reducing the number of parameters of the model. Then, to enhance the effectiveness of the attention mask, two Key values are calculated simultaneously along Query and Value by using the method of dual-branch parallel processing, which is used to strengthen the attention acquisition mode and improve the coupling of key information. Finally, focusing on the feature maps of different channels, the multi-head attention mechanism is applied to the channel attention mask to improve the feature utilization effect of the middle layer. By comparing three small object datasets, the plug-and-play interactive transformer (IT-transformer) module designed by us effectively improves the detection results of the baseline. © 2023 Tech Science Press. All rights reserved.
KW  - attention
KW  - plug-and-play
KW  - Small object detection
KW  - transformer
KW  - Object recognition
KW  - Semantics
KW  - Signal to noise ratio
KW  - Attention
KW  - Attention mechanisms
KW  - High-accuracy
KW  - Key values
KW  - Large-scale objects
KW  - Middle layer
KW  - Plug-and-play
KW  - Small object detection
KW  - Small objects
KW  - Transformer
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Tarun, R.
AU  - Priya Esther, B.
TI  - Traffic Anomaly Alert Model to Assist ADAS Feature based on Road Sign Detection in Edge Devices
PY  - 2023
T2  - 2023 4th International Conference on Electronics and Sustainable Communication Systems, ICESC 2023 - Proceedings
DO  - 10.1109/ICESC57686.2023.10193442
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168314654&doi=10.1109%2fICESC57686.2023.10193442&partnerID=40&md5=93dd2043dd3c65c48f14e65418556747
AB  - Gazing into the future of driver assistance systems such as Advance Driving Assistance System (ADAS), Radar, Automatic Emergency Braking (AEB), Lane-Keeping Assist (LKA), Traffic Jam Assist (TJA), and much more systems in affordable prices, which are to increase and improve the safety and security of the driver and the passengers, yet still they cause certain new problems due to some of the short comings of the current systems, when there is anomaly such as renovation work in the freeway. Therefore to overcome those situations, an system with low latency that must be able in to run in edge devices with less computational power and be highly precise, and affordable must be developed to alert the driver if the road has some anomaly when the vehicle is in ADAS mode. In this paper, a system that works in complimentary with ADAS will be able to detect road sign using the real-time capturing of he connected webcam and alert the driver using buzzer and red LED light to take control of the vehicle. To facilitate the road sign detection in edge devices with low latency, less size and high precision EfficientDet-Lite2 model architecture and TensorFlow Lite Model Maker is used to train the dataset, which uses transfer learning to reduce the amount of data for training, combining all that with Raspberry Pi 4 model b as the processor, the system is able to reliably detection and produce a output with up-to 90% average precision and low latency.  © 2023 IEEE.
KW  - Advanced driver-assistance systems
KW  - Deep Learning
KW  - edge device
KW  - Raspberry Pi
KW  - TensorFlow
KW  - Automobile drivers
KW  - Deep learning
KW  - Learning systems
KW  - Roads and streets
KW  - Traffic congestion
KW  - Traffic signs
KW  - Deep learning
KW  - Driving assistance systems
KW  - Edge device
KW  - Feature-based
KW  - Low latency
KW  - Raspberry pi
KW  - Road sign detection
KW  - System features
KW  - Tensorflow
KW  - Traffic anomalies
KW  - Advanced driver assistance systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Neves, F.S.
AU  - Claro, R.M.
AU  - Pinto, A.M.
TI  - End-to-End Detection of a Landing Platform for Offshore UAVs Based on a Multimodal Early Fusion Approach
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23052434
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149745238&doi=10.3390%2fs23052434&partnerID=40&md5=c2be59dfe67447284abecb218fc81b2d
AB  - A perception module is a vital component of a modern robotic system. Vision, radar, thermal, and LiDAR are the most common choices of sensors for environmental awareness. Relying on singular sources of information is prone to be affected by specific environmental conditions (e.g., visual cameras are affected by glary or dark environments). Thus, relying on different sensors is an essential step to introduce robustness against various environmental conditions. Hence, a perception system with sensor fusion capabilities produces the desired redundant and reliable awareness critical for real-world systems. This paper proposes a novel early fusion module that is reliable against individual cases of sensor failure when detecting an offshore maritime platform for UAV landing. The model explores the early fusion of a still unexplored combination of visual, infrared, and LiDAR modalities. The contribution is described by suggesting a simple methodology that intends to facilitate the training and inference of a lightweight state-of-the-art object detector. The early fusion based detector achieves solid detection recalls up to 99% for all cases of sensor failure and extreme weather conditions such as glary, dark, and foggy scenarios in fair real-time inference duration below 6 ms. © 2023 by the authors.
KW  - 3D LiDAR
KW  - computer vision
KW  - early-fusion
KW  - object detection
KW  - RGB camera
KW  - sensor fusion
KW  - thermal camera
KW  - Aircraft detection
KW  - Cameras
KW  - Object recognition
KW  - Offshore oil well production
KW  - Optical radar
KW  - Robot vision
KW  - Unmanned aerial vehicles (UAV)
KW  - 3d LiDAR
KW  - Early fusion
KW  - End to end
KW  - Landing platforms
KW  - Objects detection
KW  - Offshores
KW  - RGB cameras
KW  - Sensor fusion
KW  - Sensors' failures
KW  - Thermal camera
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chaturvedi, R.P.
AU  - Ghose, U.
TI  - A review of small object and movement detection based loss function and optimized technique
PY  - 2023
T2  - Journal of Intelligent Systems
DO  - 10.1515/jisys-2022-0324
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156095014&doi=10.1515%2fjisys-2022-0324&partnerID=40&md5=6170fefa93ca175f927ddcbcc580e171
AB  - The objective of this study is to supply an overview of research work based on video-based networks and tiny object identification. The identification of tiny items and video objects, as well as research on current technologies, are discussed first. The detection, loss function, and optimization techniques are classified and described in the form of a comparison table. These comparison tables are designed to help you identify differences in research utility, accuracy, and calculations. Finally, it highlights some future trends in video and small object detection (people, cars, animals, etc.), loss functions, and optimization techniques for solving new problems.  © 2023 the author(s), published by De Gruyter.
KW  - detection of small objects
KW  - detection of video objects
KW  - loss functions
KW  - optimization
KW  - Object recognition
KW  - Detection of small object
KW  - Detection of video object
KW  - Loss functions
KW  - Losses optimisation
KW  - Movement detection
KW  - Optimisations
KW  - Optimization techniques
KW  - Small object detection
KW  - Small objects
KW  - Video objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, H.
AU  - Todd, Z.
AU  - Bielski, N.
TI  - Equirectangular Image Data Detection, Segmentation and Classification of Varying Sized Traffic Signs: A Comparison of Deep Learning Methods
PY  - 2023
T2  - Sensors
DO  - 10.3390/s23073381
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152333384&doi=10.3390%2fs23073381&partnerID=40&md5=d488ef00d835b5207e9de2aadeb926d9
AB  - There are known limitations in mobile omnidirectional camera systems with an equirectangular projection in the wild, such as momentum-caused object distortion within images, partial occlusion and the effects of environmental settings. The localization, instance segmentation and classification of traffic signs from image data is of significant importance to applications such as Traffic Sign Detection and Recognition (TSDR) and Advanced Driver Assistance Systems (ADAS). Works show the efficacy of using state-of-the-art deep pixel-wise methods for this task yet rely on the input of classical landscape image data, automatic camera focus and collection in ideal weather settings, which does not accurately represent the application of technologies in the wild. We present a new processing pipeline for extracting objects within omnidirectional images in the wild, with included demonstration in a Traffic Sign Detection and Recognition (TDSR) system. We compare Mask RCNN, Cascade RCNN, and Hybrid Task Cascade (HTC) methods, while testing RsNeXt 101, Swin-S and HRNetV2p backbones, with transfer learning for localization and instance segmentation. The results from our multinomial classification experiment show that using our proposed pipeline, given that a traffic sign is detected, there is above a 95% chance that it is classified correctly between 12 classes despite the limitations mentioned. Our results on the projected images should provide a path to use omnidirectional images with image processing to enable the full surrounding awareness from one image source. © 2023 by the authors.
KW  - deep learning
KW  - omnidirectional camera imaging
KW  - small object detection
KW  - traffic sign detection
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Classification (of information)
KW  - Deep learning
KW  - Image classification
KW  - Image segmentation
KW  - Learning systems
KW  - Object detection
KW  - Pipelines
KW  - Video cameras
KW  - Data-detection
KW  - Deep learning
KW  - Image data
KW  - Localisation
KW  - Omnidirectional camera imaging
KW  - Omnidirectional cameras
KW  - Omnidirectional image
KW  - Small object detection
KW  - Traffic sign detection
KW  - Traffic sign detection and recognition
KW  - Traffic signs
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nigam, N.
AU  - Singh, D.P.
AU  - Choudhary, J.
TI  - A Review of Different Components of the Intelligent Traffic Management System (ITMS)
PY  - 2023
T2  - Symmetry
DO  - 10.3390/sym15030583
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151661781&doi=10.3390%2fsym15030583&partnerID=40&md5=cb5ab3853670a129ae4885e96619d632
AB  - Traffic congestion is a serious challenge in urban areas. So, to address this challenge, the intelligent traffic management system (ITMS) is used to manage traffic on road networks. Managing traffic helps to focus on environmental impacts as well as emergency situations. However, the ITMS system has many challenges in analyzing scenes of complex traffic. New technologies such as computer vision (CV) and artificial intelligence (AI) are being used to solve these challenges. As a result, these technologies have made a distinct identity in the surveillance industry, particularly when it comes to keeping a constant eye on traffic scenes. There are many vehicle attributes and existing approaches that are being used in the development of ITMS, along with imaging technologies. In this paper, we reviewed the ITMS-based components that describe existing imaging technologies and existing approaches on the basis of their need for developing ITMS. The first component describes the traffic scene and imaging technologies. The second component talks about vehicle attributes and their utilization in existing vehicle-based approaches. The third component explains the vehicle’s behavior on the basis of the second component’s outcome. The fourth component explains how traffic-related applications can assist in the management and monitoring of traffic flow, as well as in the reduction of congestion and the enhancement of road safety. The fifth component describes the different types of ITMS applications. The sixth component discusses the existing methods of traffic signal control systems (TSCSs). Aside from these components, we also discuss existing vehicle-related tools such as simulators that work to create realistic traffic scenes. In the last section named discussion, we discuss the future development of ITMS and draw some conclusions. The main objective of this paper is to discuss the possible solutions to different problems during the development of ITMS in one place, with the help of components that would play an important role for an ITMS developer to achieve the goal of developing efficient ITMS. © 2023 by the authors.
KW  - intelligent traffic management system (ITMS)
KW  - simulators
KW  - traffic signal control systems (TSCSs)
KW  - vehicle detection
KW  - vehicle tracking
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, K.
AU  - Zhou, T.
AU  - Zhang, Z.
AU  - Chen, T.
AU  - Chen, J.
TI  - PVF-DectNet: Multi-modal 3D detection network based on Perspective-Voxel fusion
PY  - 2023
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2023.105951
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147541158&doi=10.1016%2fj.engappai.2023.105951&partnerID=40&md5=c660354a3e5abf03a7c7022f2b327717
AB  - The detection of small objects such as pedestrians still poses challenges to the LiDAR-based 3D object detection due to the sparseness and disorder of point clouds. Conversely, images from cameras can provide rich semantic information, which makes these small-sized objects easy to be detected. To take use of the advantages of both devices to achieve better 3D object detection, research on the fusion of LiDAR and camera information is now being conducted. The existing fusion methods between point clouds and image are normally weighed more on the point clouds. Hence the semantic information of images is not fully utilized. We propose a new fusion method named PVFusion to try to fuse more image features. We first divide each point into a separate perspective voxel and project the voxel onto the image feature maps. Then the semantic feature of the perspective voxel is fused with the geometric feature of the point. A 3D object detection model (PVF-DectNet) is designed using PVFusion. During training we employ the ground truth paste (GT-Paste) data augmentation and solve the occlusion problem caused by newly added object. The KITTI validation set is used to validate the PVF-DectNet, which shows 3.6% AP improvement over the other feature fusion methods in pedestrian detection. On the KITTI test set, the PVF-DectNet outperforms the other multi-modal SOTA methods by 2.2% AP in pedestrian detection. And PVFusion shows better detection performance for sparse point clouds than PointFusion in both car and pedestrian categories. As for 32 beams LiDAR scene, there are 4.2% AP increment in moderate difficulty car category and 5.2% mAP improvement in pedestrian category. © 2023 Elsevier Ltd
KW  - 3D object detection
KW  - Deep learning
KW  - LiDAR-camera-based detector
KW  - Sensor fusion
KW  - Small objects
KW  - 3D modeling
KW  - Cameras
KW  - Deep learning
KW  - Image fusion
KW  - Object recognition
KW  - Optical radar
KW  - Semantics
KW  - 3D object
KW  - 3d object detection
KW  - Camera-based
KW  - Deep learning
KW  - LiDAR-camera-based detector
KW  - Multi-modal
KW  - Objects detection
KW  - Point-clouds
KW  - Sensor fusion
KW  - Small objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Gao, Z.
AU  - Wang, H.
AU  - Sun, Y.
AU  - Wang, F.
TI  - A Review of Object Detection Techniques for Automobile Assisted Driving9
PY  - 2023
T2  - Chinese Control Conference, CCC
DO  - 10.23919/CCC58697.2023.10240672
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175544203&doi=10.23919%2fCCC58697.2023.10240672&partnerID=40&md5=360b78b2bd7160f1befece655ed55204
AB  - In this paper, the development of current target detection technology and its advantages and disadvantages are described in the context of foreign literature, mainly focusing on the advantages and disadvantages of traditional visual detection algorithms and deep learning detection methods in image target detection algorithms are analyzed and compared. Radar target detection algorithms: traditional point cloud features and deep learning point cloud features target detection algorithms are analyzed and compared. Finally in the analysis of multi-sensor fusion algorithms. Random model fusion methods and artificial intelligence fusion methods are described, respectively. The characteristics and research status of each detection method and fusion method are analyzed together with the literature, and the future development trend of assisted driving target detection methods is prospected: detection methods are lightweight to meet in-vehicle needs, three or more sensors are fused to achieve the purpose of improving detection accuracy, and multi-sensor fusion is the future development trend for target detection of harsh road conditions and small and distant targets. © 2023 Technical Committee on Control Theory, Chinese Association of Automation.
KW  - aided driving
KW  - artificial intelligence
KW  - camera
KW  - data fusion
KW  - radar
KW  - target detection
KW  - Learning algorithms
KW  - Learning systems
KW  - Object detection
KW  - Signal detection
KW  - Tracking radar
KW  - 'current
KW  - Aided driving
KW  - Detection methods
KW  - Development trends
KW  - Fusion methods
KW  - Multi-sensor fusion
KW  - Objects detection
KW  - Point-clouds
KW  - Target detection algorithm
KW  - Targets detection
KW  - Deep learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Williams, K.C.
AU  - O'toole, M.D.
AU  - Peyton, A.J.
TI  - Scrap Metal Classification Using Magnetic Induction Spectroscopy and Machine Vision
PY  - 2023
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2023.3284930
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162630454&doi=10.1109%2fTIM.2023.3284930&partnerID=40&md5=518e473a56d75149401e64d117547ac3
AB  - The need to recover and recycle material toward building a circular economy is increasingly a global imperative. Nonferrous metals in particular are highly recyclable and can be extracted using processes such as eddy current separation. However, their further separation into recyclable groups based on metal or alloy continues to pose a challenge. Recently, we proposed a new technique to discriminate between nonferrous metals: magnetic induction spectroscopy (MIS) measures how a metal fragment scatters an excitation magnetic field over different frequencies. MIS is related to conductivity, which can be used to classify the fragment according to this property. In this article, we demonstrate for the first time the use of MIS with machine learning to classify nonferrous scrap metals drawn from commercial waste streams. Two approaches are explored: 1) MIS over a bandwidth from 3 to 90 kHz and 2) the combination of MIS with the physical color of the metal samples. We show that MIS alone can obtain purity and recovery rates >80% for most metal groups and waste streams, rising to >93% for stainless steel. The exception was the Zorba waste stream where the mix of aluminum alloys within the sample set led to poor conductivity contrasts. The introduction of color substantially improved results in this case, increasing purity and recovery rates by 20%-35% points. Of the machine-learning models tested, we found that random forest (RF), extra trees, and support vector machine (SVM) algorithms consistently achieved the highest performance. © 1963-2012 IEEE.
KW  - Classification algorithms
KW  - electromagnetic induction
KW  - machine vision
KW  - recycling
KW  - waste recovery
KW  - Computer vision
KW  - Copper
KW  - Electromagnetic induction
KW  - Learning algorithms
KW  - Learning systems
KW  - Magnetic separation
KW  - Recycling
KW  - Scrap metal reprocessing
KW  - Classification algorithm
KW  - Conductivity
KW  - Machine-vision
KW  - Non-ferrous metals
KW  - Recovery rate
KW  - Recyclables
KW  - Recycle materials
KW  - Recycling
KW  - Waste recoveries
KW  - Waste stream
KW  - Magnetic resonance imaging
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Lin, T.
AU  - Zhang, Y.
TI  - The application of improved Yolov5s in anomaly detection of circuit lines for small targets
PY  - 2023
T2  - ICIIBMS 2023 - 8th International Conference on Intelligent Informatics and Biomedical Sciences
DO  - 10.1109/ICIIBMS60103.2023.10347895
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182274727&doi=10.1109%2fICIIBMS60103.2023.10347895&partnerID=40&md5=7ca17ee1847200637c444e60b2b02163
AB  - The purpose of this study is to address the challenges and limitations of detecting small anomalies in circuit lines by modifying Yolov5s model. We optimized the model to improve its performance in detecting small abnormal targets. Taking the detection of exposed copper wires in meter wiring as an example, we conducted a series of experiments to validate the performance of the improved Yolov5s model. The experimental results showed that our improved model achieved a significant performance improvement on the self-made dataset for exposed copper wires in meter wiring. Compared to the baseline method, our improved model achieved a 4.5 percentage point increase in MAP [IoU=0.5:0.95], while maintaining accurate identification and precise localization of small abnormal targets. This provides a more reliable and efficient solution for circuit anomaly detection, with potential applications in the field. © 2023 IEEE.
KW  - anomaly detection
KW  - circuit lines
KW  - improved Yolov5s
KW  - small targets
KW  - Copper
KW  - Timing circuits
KW  - Wire
KW  - Anomaly detection
KW  - Baseline methods
KW  - Circuit line
KW  - Copper wires
KW  - Improved yolov5s
KW  - Localisation
KW  - Percentage points
KW  - Performance
KW  - Small targets
KW  - Anomaly detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, G.-Q.
AU  - Zhang, C.-Z.
AU  - Chen, M.-S.
AU  - Lin, Y.-C.
AU  - Tan, X.-H.
AU  - Liang, P.
AU  - Kang, Y.-X.
AU  - Zeng, W.-D.
AU  - Wang, Q.
TI  - Yolo-MSAPF: Multiscale Alignment Fusion with Parallel Feature Filtering Model for High Accuracy Weld Defect Detection
PY  - 2023
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2023.3302372
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167808991&doi=10.1109%2fTIM.2023.3302372&partnerID=40&md5=dd5010f30c486c6332fc8a61ef08e681
AB  - This work aims to improve the low accuracy caused by interference information during real-time weld surface detection. First, a weld surface dataset with 7580 pictures containing eight types of defects was established. Second, an improved detection model named Yolo-MSAPF was designed based on Yolo-v5 model and verified by the self-established database. Finally, a real-time detection system was built to analyze the performance of the detection model in an industrial environment. The design principle of the Yolo-MSAPF is to eliminate interference information but enhance necessary features in each scale as much as possible by multiscale alignment fusion (MSAF) with parallel features filtering (PFF) modules (i.e., MSAPF strategy). For the MSAF, not only the accuracy but the richness of fused features is guaranteed by aligning features at one level to fuse all other scales. After that, the fused features in each scale were individually filtered out in parallel spatial and channel in the PFF module. The results show that rate for the images with missing detection for eight types of defects sharply drops from 21.47% to 1.68% when the MSAPF strategy is used. Moreover, it is worth noting that the mAP@0.5 of the Yolo-MSAPF model reaches 95.3%, which is similar to the Yolo-v7 model, while the number of parameters is reduced by 30.1%, compared with the baseline model. Additionally, the great ability to screen out unqualified weld is also verified in the industrial environment. Soon, code and dataset will be available at https://github.com/Luckycat518/Yolo-MSAPF. © 1963-2012 IEEE.
KW  - Detection system
KW  - multiscale fusion
KW  - parallel enhanced attention
KW  - weld defect detection
KW  - Yolo-v5
KW  - Information filtering
KW  - Welds
KW  - Detection models
KW  - Detection system
KW  - Feature filtering
KW  - Industrial environments
KW  - Muli-scale fusion
KW  - Multi-scales
KW  - Parallel enhanced attention
KW  - Weld defects detections
KW  - Weld surfaces
KW  - Yolo-v5
KW  - Defects
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Marconato, E.
AU  - Bontempo, G.
AU  - Ficarra, E.
AU  - Calderara, S.
AU  - Passerini, A.
AU  - Teso, S.
TI  - Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal
PY  - 2023
T2  - Proceedings of Machine Learning Research
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174387101&partnerID=40&md5=66863975b5c8e72aabbe6c4875f67146
AB  - We introduce Neuro-Symbolic Continual Learning, where a model has to solve a sequence of neuro-symbolic tasks, that is, it has to map sub-symbolic inputs to high-level concepts and compute predictions by reasoning consistently with prior knowledge. Our key observation is that neuro-symbolic tasks, although different, often share concepts whose semantics remains stable over time. Traditional approaches fall short: existing continual strategies ignore knowledge altogether, while stock neuro-symbolic architectures suffer from catastrophic forgetting. We show that leveraging prior knowledge by combining neurosymbolic architectures with continual strategies does help avoid catastrophic forgetting, but also that doing so can yield models affected by reasoning shortcuts. These undermine the semantics of the acquired concepts, even when detailed prior knowledge is provided upfront and inference is exact, and in turn continual performance. To overcome these issues, we introduce COOL, a COncept-level cOntinual Learning strategy tailored for neuro-symbolic continual problems that acquires high-quality concepts and remembers them over time. Our experiments on three novel benchmarks highlights how COOL attains sustained high performance on neuro-symbolic continual learning tasks in which other strategies fail. © 2023 Proceedings of Machine Learning Research. All rights reserved.
KW  - Benchmarking
KW  - Knowledge management
KW  - Machine learning
KW  - Catastrophic forgetting
KW  - Concept levels
KW  - Continual learning
KW  - Knowledge reasoning
KW  - Learning strategy
KW  - Performance
KW  - Prior-knowledge
KW  - Sub-symbolic
KW  - Traditional approaches
KW  - Yield models
KW  - Semantics
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Wang, J.
AU  - Yuan, Y.
AU  - Luo, Z.
AU  - Xie, K.
AU  - Lin, D.
AU  - Iqbal, U.
AU  - Fidler, S.
AU  - Khamis, S.
TI  - Learning Human Dynamics in Autonomous Driving Scenarios
PY  - 2023
T2  - Proceedings of the IEEE International Conference on Computer Vision
DO  - 10.1109/ICCV51070.2023.01901
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181564134&doi=10.1109%2fICCV51070.2023.01901&partnerID=40&md5=0ea8ed361c8bcb9f1e8df7a5bb7e6dfb
AB  - Simulation has emerged as an indispensable tool for scaling and accelerating the development of self-driving systems. A critical aspect of this is simulating realistic and diverse human behavior and intent. In this work, we propose a holistic framework for learning physically plausible human dynamics from real driving scenarios, narrowing the gap between real and simulated human behavior in safety-critical applications. We show that state-of-the-art methods underperform in driving scenarios where video data is recorded from moving vehicles, and humans are frequently partially or fully occluded. Furthermore, existing methods often disregard the global scene where humans are situated, resulting in various motion artifacts like foot sliding, floating, or ground penetration. To address this challenge, we propose an approach that incorporates physics with a reinforcement learning-based motion controller to learn human dynamics for driving scenarios. Our framework can simulate physically plausible human dynamics that accurately match observed human motions and infill motions for occluded body parts, while improving the physical plausibility of the entire motion sequence. Experiments on the challenging Waymo Open Dataset show that our method outperforms state-of-the-art motion capture approaches significantly in recovering high-quality, physically plausible, and scene-aware human dynamics. © 2023 IEEE.
KW  - Automobile bodies
KW  - Autonomous vehicles
KW  - Dynamics
KW  - Reinforcement learning
KW  - Safety engineering
KW  - Autonomous driving
KW  - Driving systems
KW  - Holistic frameworks
KW  - Human behaviors
KW  - Human dynamics
KW  - Indispensable tools
KW  - Real drivings
KW  - Safety critical applications
KW  - Scalings
KW  - Self drivings
KW  - Behavioral research
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ma, C.
AU  - Fu, Y.
AU  - Wang, D.
AU  - Guo, R.
AU  - Zhao, X.
AU  - Fang, J.
TI  - YOLO-UAV: Object Detection Method of Unmanned Aerial Vehicle Imagery Based on Efficient Multi-Scale Feature Fusion
PY  - 2023
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3329713
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177566813&doi=10.1109%2fACCESS.2023.3329713&partnerID=40&md5=a3950013a20ef88f072be073ab8f0fc9
AB  - As Unmanned Aerial Vehicle (UAV) remote sensing technology progresses, the utilization of deep learning in UAV imagery object detection has become more prevalent. However, detecting small targets in complex backgrounds and distinguishing dense targets remains a major challenge. To address these issues and improve object detection efficiency, this study proposes an UAV imagery object detection method called YOLO-UAV by optimizing YOLOv5. YOLO-UAV first reconstructs the backbone and feature fusion networks by simplifying the network structure and reducing computational burden. The employment of a Dense-CSPDarknet53 backbone network, fashioned via the incorporation of dense connections, facilitates the extraction of latent image information through the recurrent utilization of features. In the Neck structure, an efficient feature fusion block with structural re-parameterization and ELAN strategies is integrated to effectively reduce interference from complex background noise while extracting more accurate and rich features. In addition, by proposing GS-Decoupled Head, this approach diminishes the parameter count of the decoupled head without compromising accuracy. It also separates classification tasks from regression tasks to lessen the influence of task disparities on prediction bias. To tackle the discrepancy between positive and negative samples in bounding box regression tasks, this study introduces a new loss function, Focal-ECIoU, capable of expediting network convergence and improve model positioning ability. Experimental findings from the public VisDrone2019 dataset indicate that YOLO-UAV outperforms other advanced object detection methods in comprehensive performance. Compared with the baseline model YOLOv5s, YOLO-UAV increased mAP0.5 from 35.1% to 46.7%, while mAP0.5:0.95 increased from 19.1% to 27.4%. For small-scale targets, APsmall increased from 10.2% to 17.3%. The experiment proves that YOLO-UAV performs well in improving object detection accuracy and has strong generalization ability, satisfying the practical requirements of UAV imagery object detection tasks.  © 2013 IEEE.
KW  - object detection
KW  - UAV imagery
KW  - VisDrone2019
KW  - YOLO-UAV
KW  - Aircraft detection
KW  - Antennas
KW  - Complex networks
KW  - Deep learning
KW  - Image enhancement
KW  - Object recognition
KW  - Remote sensing
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial vehicle
KW  - Complex background
KW  - Features fusions
KW  - Multi-scale features
KW  - Object detection method
KW  - Objects detection
KW  - Unmanned aerial vehicle imagery
KW  - Visdrone2019
KW  - YOLO-unmanned aerial vehicle
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Niu, X.
AU  - Wang, Q.
AU  - Liu, B.
AU  - Zhang, J.
TI  - An Automatic Chinaware Fragments Reassembly Method Framework Based on Linear Feature of Fracture Surface Contour
PY  - 2022
T2  - Journal on Computing and Cultural Heritage
DO  - 10.1145/3569091
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156103357&doi=10.1145%2f3569091&partnerID=40&md5=6401d8f522a773c40410ee933c87c7ab
AB  - For Chinaware fragments, it is difficult to assemble them directly without considering the wonderful patterns painted on them. Given the simplicity of the Chinaware designs, each object contains similar textures and patterns. Compared to the oddly diverse appliance modeling, the difference between fragments of different Chinaware is small. The fragments are small and the fracture is flat, and there are many potential matching objects. For the Chinaware fragments' reassembly, most of the work is still done manually. There is little available fully automatic reassembly work, and these approaches are for the reassembly of a single Chinaware. But what reality demands is the reassembly of the multi-Chinaware. Therefore, this article proposes an automatic method, whose strategy is analogous to the manual assembly, to accomplish this complex task. First, segment the contours of fracture surfaces of the fragments by the geometric feature of fracture edge; then, the contours are matched using our proposed multi-scale linear feature descriptor; given the massive fragments of multiple objects, we use the inaccurate matching strategy to build a matching set for each fracture surface contour and perform contour matching in this set. In this article, the fracture surface is segmented using the 2D slope information on the fragment edge. The descriptor proposed in the article uses distance triangle towers and chained angles to describe the "undulations"of the fracture surface. Moreover, the article uses a strict absolute-advantage-principle to reject false matching. In addition, after the initial reconstruction, we will iteratively adjust fragments according to the gap between the fragments to achieve better multi-fragment matching results. In this article, 18 porcelains, a total of 103 pieces, have been tested. Experiments are also carried out for special cases, including fragment reassembly with missing fragments and fragment reassembly with redundant fragments. The experimental results showed the effectiveness of method. However, the types of experimental data we currently have are relatively simple, and there is no way to reassemble the fragments in minutes. We hope to enable faster reassembly of more fragments in the future.  © 2022 Association for Computing Machinery.
KW  - contour matching
KW  - contour segmentation
KW  - Fragment reassembly
KW  - linear feature descriptor
KW  - Contour measurement
KW  - Fracture
KW  - Textures
KW  - Contour matching
KW  - Contour segmentation
KW  - Feature descriptors
KW  - Fracture surfaces
KW  - Fragment reassembly
KW  - Linear feature
KW  - Linear feature descriptor
KW  - Matchings
KW  - Reassembly
KW  - Surface contour
KW  - Iterative methods
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yi, W.
AU  - Wang, B.
TI  - Research on Underwater Small Target Detection Algorithm Based on Improved YOLOv7
PY  - 2023
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3290903
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163568086&doi=10.1109%2fACCESS.2023.3290903&partnerID=40&md5=a0c9946cc7d1109c7237762d61e275c3
AB  - Target detection research has always been difficult when it comes to small target detection in underwater situations. To address the issues of a high miss detection rate and poor underwater scene recognition in underwater small target detection tasks, an improved underwater small target detection technique utilizing YOLOv7 is proposed. To achieve the accuracy rate while considering the high detection speed, the YOLOv7 network is used as the basic network. The network concentrates more crucial feature information of small targets to increase detection accuracy while reducing model complexity by merging the SENet attention mechanism, enhancing the FPN network topology, and incorporating the EIoU loss function. Through simulation tests, the mAP, P, and R metrics are confirmed on the test set and contrasted with other conventional target detection techniques. The outcomes demonstrate that the enhanced algorithm outperforms competing networks and successfully raises detection accuracy on the test set.  © 2013 IEEE.
KW  - attention mechanism
KW  - EIoU
KW  - FPN
KW  - underwater small target detection
KW  - YOLOv7
KW  - Attention mechanisms
KW  - Detection accuracy
KW  - EIoU
KW  - FPN
KW  - Small target detection
KW  - Target detection algorithm
KW  - Targets detection
KW  - Test sets
KW  - Underwater small target detection
KW  - YOLOv7
KW  - Network topology
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kumar, S.
AU  - Sharma, S.C.
AU  - Kumar, R.
TI  - Wireless Sensor Network Based Real-Time Pedestrian Detection and Classification for Intelligent Transportation System
PY  - 2023
T2  - International Journal of Mathematical, Engineering and Management Sciences
DO  - 10.33889/IJMEMS.2023.8.1.012
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147885102&doi=10.33889%2fIJMEMS.2023.8.1.012&partnerID=40&md5=a1863e8541dfb70952c4793c342dde41
AB  - Pedestrian safety has become a critical consideration in developing society especially road traffic, an intelligent transportation need of the hour is the solution left. India tops the world with 11% of global road accidents. With this data, we have moved in the direction of computer vision applications for efficient and accurate pedestrian detection for intelligent transportation systems (ITS). The important application of this research is robot development, traffic management and control, unmanned vehicle driving (UVD), intelligent monitoring and surveillance system, and automatic pedestrian detection system. Much research has focused on pedestrian detection, but sustainable solution-driven research must still be required to overcome road accidents. We have proposed a wireless sensor network-based pedestrian detection system that classifies the real-time set of pedestrian activity and samples the reciprocally received signal strength (RSS) from the sensor node. We applied a histogram of oriented gradient (HOG) descriptor algorithm K-nearest neighbor, decision tree and linear support vector machine to measure the performance and prediction of the target. Also, these algorithms have performed a comparative analysis under different aspects. The linear support vector machine algorithm was trained with 481 samples. The performance achieves the accuracy of 98.90%and has accomplished superior results with a maximum precision of 0.99, recall of 0.98, and F-score of 0.95 with 2% error rate. The model's prediction indicates that it can be used in the intelligent transportation system. Finally, the limitation and the challenges discussed to provide an outlook for future research direction to perform effective pedestrian detection. © 2023 International Journal of Mathematical, Engineering and Management Sciences. All rights reserved.
KW  - Computer vision
KW  - Intelligent transportation system
KW  - Machine learning
KW  - Pedestrian detection
KW  - Unmanned vehicle driving
KW  - Accidents
KW  - Computer vision
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Learning systems
KW  - Motor transportation
KW  - Nearest neighbor search
KW  - Pedestrian safety
KW  - Roads and streets
KW  - Sensor nodes
KW  - Support vector machines
KW  - Intelligent transportation systems
KW  - Linear Support Vector Machines
KW  - Machine-learning
KW  - Network-based
KW  - Pedestrian detection
KW  - Pedestrian detection and classifications
KW  - Pedestrian detection system
KW  - Performance
KW  - Real- time
KW  - Unmanned vehicle driving
KW  - Decision trees
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Natarajan, B.
AU  - Elakkiya, R.
AU  - Bhuvaneswari, R.
AU  - Saleem, K.
AU  - Chaudhary, D.
AU  - Samsudeen, S.H.
TI  - Creating Alert Messages Based on Wild Animal Activity Detection Using Hybrid Deep Neural Networks
PY  - 2023
T2  - IEEE Access
DO  - 10.1109/ACCESS.2023.3289586
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163548013&doi=10.1109%2fACCESS.2023.3289586&partnerID=40&md5=822d42b1bf2e2b22efaab77346e36dda
AB  - The issue of animal attacks is increasingly concerning for rural populations and forestry workers. To track the movement of wild animals, surveillance cameras and drones are often employed. However, an efficient model is required to detect the animal type, monitor its locomotion and provide its location information. Alert messages can then be sent to ensure the safety of people and foresters. While computer vision and machine learning-based approaches are frequently used for animal detection, they are often expensive and complex, making it difficult to achieve satisfactory results. This paper presents a Hybrid Visual Geometry Group (VGG)-19+ Bidirectional Long Short-Term Memory (Bi-LSTM) network to detect animals and generate alerts based on their activity. These alerts are sent to the local forest office as a Short Message Service (SMS) to allow for immediate response. The proposed model exhibits great improvements in model performance, with an average classification accuracy of 98%, a mean Average Precision (mAP) of 77.2%, and a Frame Per Second (FPS) of 170. The model was tested both qualitatively and quantitatively using 40,000 images from three different benchmark datasets with 25 classes and achieved a mean accuracy and precision of above 98%. This model is a reliable solution for providing accurate animal-based information and protecting human lives.  © 2013 IEEE.
KW  - activity recognition
KW  - alert system
KW  - Animal detection
KW  - Bi-LSTM
KW  - convolutional neural network
KW  - VGG-Net
KW  - video surveillance
KW  - wild animal monitoring
KW  - Alarm systems
KW  - Animals
KW  - Cameras
KW  - Convolution
KW  - Deep neural networks
KW  - Feature extraction
KW  - Activity recognition
KW  - Alert systems
KW  - Animal detection
KW  - Bidirectional long short-term memory
KW  - Convolutional neural network
KW  - Deep learning
KW  - Features extraction
KW  - Task analysis
KW  - Video surveillance
KW  - Visual geometry group-net
KW  - Wild animal monitoring
KW  - Wild animals
KW  - Long short-term memory
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huangfu, P.
AU  - Dang, L.
TI  - A multi-scale pyramid feature fusion-based object detection method for remote sensing images
PY  - 2023
T2  - International Journal of Remote Sensing
DO  - 10.1080/01431161.2023.2288947
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179549519&doi=10.1080%2f01431161.2023.2288947&partnerID=40&md5=21284fa43ced3b490a73963afb354f1d
AB  - Object detection is a basic and challenging task in remote sensing image analysis that has received extensive attention in recent years. Feature fusion is one of the key steps in object detection. Most existing methods of feature fusion first complete the preliminary fusion of feature maps of different scales through ‘add’ or ‘concat’ operations, followed by using a single-scale convolution to further improve the fusion effect. However, due to the fact that multi-level features exhibit multi-scale representations, the fusion effect of existing methods is limited. To improve the efficiency of feature fusion, we propose a multi-scale pyramid feature fusion network, which performs multi-scale learning through multi-scale convolution kernels to complete multi-level feature fusion more effectively. Then we propose a lightweight decoupled head, which alleviates the conflict between the classification task and the localization task. We conducted experiments on the dataset of object detection in aerial images (DOTA) dataset and the HRSC2016 dataset to verify our proposed methods. The results show that the performance of our proposed methods is better than other existing methods, with an mAP of 73.3%, 67.6%, 65.0%, and 96.7% on the DOTA1.0, DOTA1.5, DOTA2.0, and HRSC2016 datasets, respectively. Meanwhile, the parameter quantity of the proposed model is 10.3 M, and the inference time is 5.1 ms, which meets the requirement of lightweight and ensures the timeliness of detection. © 2023 Informa UK Limited, trading as Taylor & Francis Group.
KW  - feature fusion
KW  - neural network
KW  - Object detection
KW  - remote sensing image
KW  - YOLO
KW  - Antennas
KW  - Convolution
KW  - Feature extraction
KW  - Image fusion
KW  - Object recognition
KW  - Remote sensing
KW  - Features fusions
KW  - Image-analysis
KW  - Multi-Scale pyramids
KW  - Multi-scales
KW  - Multilevels
KW  - Neural-networks
KW  - Object detection method
KW  - Objects detection
KW  - Remote sensing images
KW  - YOLO
KW  - artificial neural network
KW  - detection method
KW  - image resolution
KW  - remote sensing
KW  - satellite data
KW  - satellite imagery
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ando, S.
AU  - Kindo, T.
TI  - Direct Imaging of Stabilized Optical Flow and Possible Anomalies from Moving Vehicle
PY  - 2022
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2022.3199203
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137539903&doi=10.1109%2fTITS.2022.3199203&partnerID=40&md5=99a7191579e5c4cc0bb3c473719805b2
AB  - Machine perception of dynamic scenes becomes more and more important for autonomous vehicles and vision-based driver-assistance systems. Even with other 3D ranging devices, dense, detailed and instantaneous detection of optical flow is essential for early distinguishing small moving objects in the 3D environment from the moving vehicle. To overcome the limited performance in the immediacy, resolution, accuracy and acuity of existing methods, we provide an optical flow detection scheme based on a three-phase correlation image sensor (3PCIS) that is capable of Fourier-coefficient imaging combined with an exact and direct algorithm derived from the weighted integral method of identifying the differential equation model from a short-duration observation. To utilize inherent performances of the detection scheme by removing the large and rapid disturbances induced by the rotational fluctuations of the platform, we introduce a software operation of gaze in which the image coordinates are fixed on and smoothly pursue a forward stable object so that the optical flow field is relative to the moving coordinate system. In it, the gaze subsystem continuously provides angular velocity and pose between the camera and gaze target, while the imaging subsystem instantaneously obtains two optical flow distributions by cancelling the ego-rotation components and then removing the outwardly diverging components derived mainly from stationary 3D environments. Possible anomalies captured in each frame instantaneously provide candidates of hazardous objects that should be tracked and further investigated. We examine the performance of optical flow stabilization and anomaly detection using image sequences of monocular 3PCIS mounted on a moving vehicle on town roads and a highway.  © 2000-2011 IEEE.
KW  - autonomous vehicle
KW  - correlation image sensor
KW  - ego-motion
KW  - gaze
KW  - Optical flow
KW  - weighted integral method
KW  - Cameras
KW  - Differential equations
KW  - Fourier analysis
KW  - Image sensors
KW  - Optical correlation
KW  - Stereo image processing
KW  - Three dimensional computer graphics
KW  - Three dimensional displays
KW  - 3-D environments
KW  - Autonomous Vehicles
KW  - Correlation image sensor
KW  - Ego-motion
KW  - Gaze
KW  - Moving vehicles
KW  - Performance
KW  - Road
KW  - Three-dimensional display
KW  - Weighted integral method
KW  - Optical flows
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Zhao, Z.
AU  - Sun, W.
AU  - Cui, Q.
TI  - 3D Object Detection with Attention: Shell-Based Modeling
PY  - 2023
T2  - Computer Systems Science and Engineering
DO  - 10.32604/csse.2023.034230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147452198&doi=10.32604%2fcsse.2023.034230&partnerID=40&md5=66e822355bf0fa67992e6e9ce2b75d8e
AB  - LIDAR point cloud-based 3D object detection aims to sense the surrounding environment by anchoring objects with the Bounding Box (BBox). However, under the three-dimensional space of autonomous driving scenes, the previous object detection methods, due to the pre-processing of the original LIDAR point cloud into voxels or pillars, lose the coordinate information of the original point cloud, slow detection speed, and gain inaccurate bounding box positioning. To address the issues above, this study proposes a new two-stage network structure to extract point cloud features directly by PointNet++, which effectively preserves the original point cloud coordinate information. To improve the detection accuracy, a shell-based modeling method is proposed. It roughly determines which spherical shell the coordinates belong to. Then, the results are refined to ground truth, thereby narrowing the localization range and improving the detection accuracy. To improve the recall of 3D object detection with bounding boxes, this paper designs a self-attention module for 3D object detection with a skip connection structure. Some of these features are highlighted by weighting them on the feature dimensions. After training, it makes the feature weights that are favorable for object detection get larger. Thus, the extracted features are more adapted to the object detection task. Extensive comparison experiments and ablation experiments conducted on the KITTI dataset verify the effectiveness of our proposed method in improving recall and precision. © 2023 CRL Publishing. All rights reserved.
KW  - 3D object detection
KW  - autonomous driving
KW  - point cloud
KW  - self-attention mechanism
KW  - shell-based modeling
KW  - 3D modeling
KW  - Autonomous vehicles
KW  - Object recognition
KW  - Optical radar
KW  - Shells (structures)
KW  - 3D object
KW  - 3d object detection
KW  - Attention mechanisms
KW  - Autonomous driving
KW  - Based modelling
KW  - Bounding-box
KW  - Objects detection
KW  - Point-clouds
KW  - Self-attention mechanism
KW  - Shell-based modeling
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Huang, X.
TI  - Moving object detection in low-luminance images
PY  - 2023
T2  - Visual Computer
DO  - 10.1007/s00371-021-02320-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118855217&doi=10.1007%2fs00371-021-02320-1&partnerID=40&md5=8055c71b1f9989f30f0d4fc45d639a8b
AB  - Moving object detection in low-luminance Images is one of the most fundamental and difficult issues in machine vision. Therefore, in this paper, deep self-adaptive network (DSA-Net) is proposed to effectively detect moving objects in low-luminance scenes. Particularly, (1) three mechanisms are developed in this joint learning framework: graph-based unsupervised feature selection, feature representations ranking, and multiple-way feature interaction. (2) Both anti-occlusion and multi-object handling module are explored simultaneously in the unified DSA-Net model. (3) A weakly fine-tuning strategy is presented, including the easiness and group curriculum term. It leverages helpful prior-knowledge to guide the learner to select confident training samples. The experimental results show that DSA-Net outperforms the state-of-the-art methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
KW  - Deep learning
KW  - Low luminance
KW  - Moving object detection
KW  - Deep learning
KW  - Feature extraction
KW  - Graphic methods
KW  - Luminance
KW  - Object detection
KW  - Adaptive networks
KW  - Deep learning
KW  - Graph-based
KW  - Joint learning
KW  - Learning frameworks
KW  - Low luminance
KW  - Luminance images
KW  - Machine-vision
KW  - Moving objects
KW  - Moving-object detection
KW  - Object recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Patel, S.
TI  - Marigold Flower Blooming Stage Detection in Complex Scene Environment using Faster RCNN with Data Augmentation
PY  - 2023
T2  - International Journal of Advanced Computer Science and Applications
DO  - 10.14569/IJACSA.2023.0140379
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151795869&doi=10.14569%2fIJACSA.2023.0140379&partnerID=40&md5=2acba3381ad3d25a8fd85331d8189c03
AB  - In recent years, flower growing has developed into a lucrative agricultural sector that provides employment and business opportunities for small and marginal growers in both urban and rural locations in India. One of the most often cultivated flowers for landscaping design is the Marigold flower. It is also widely used to create garlands for ceremonial and social occasions using loose flowers. Understanding the appropriate stage of harvesting for each plant species is essential to ensuring the quality of the flowers after they have been picked. It has been demonstrated that human assessors consistently used a category scoring system to evaluate various flowering stages. Deep learning and convolutional neural networks have the potential to revolutionize agriculture by enabling efficient analysis of large-scale data. In order to address the problem of Marigold flower stages detection and classification in complex real-time field scenarios, this study proposes a fine-tuned Faster RCNN with ResNet50 network coupled with data augmentation. Faster RCNN is a popular deep learning framework for object detection that uses a region proposal network to efficiently identify object locations and features in an image. The Marigold flower dataset was collected from three different Marigold fields in the Anand District of Gujarat State, India. The collection includes of photos that were taken outdoors in natural light at various heights, angles, and distances. We have developed and fine-tuned a Faster RCNN detection and classification model to be particularly sensitive to Marigold flowers, and we have compared the generated method's performance to that of other cutting-edge models to determine its accuracy and effectiveness © 2023, International Journal of Advanced Computer Science and Applications.All Rights Reserved.
KW  - convolutional neural networks
KW  - Deep learning
KW  - marigold flower blooming stage detection
KW  - object detection
KW  - Complex networks
KW  - Convolution
KW  - Convolutional neural networks
KW  - Cultivation
KW  - Deep learning
KW  - Object recognition
KW  - Agricultural sector
KW  - Business opportunities
KW  - Complex scenes
KW  - Convolutional neural network
KW  - Data augmentation
KW  - Deep learning
KW  - Employment opportunities
KW  - Marigold flower blooming stage detection
KW  - Marigold flowers
KW  - Objects detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zou, T.
AU  - Chen, G.
AU  - Li, Z.
AU  - He, W.
AU  - Qu, S.
AU  - Gu, S.
AU  - Knoll, A.
TI  - KAM-Net: Keypoint-Aware and Keypoint-Matching Network for Vehicle Detection from 2-D Point Cloud
PY  - 2022
T2  - IEEE Transactions on Artificial Intelligence
DO  - 10.1109/TAI.2021.3112945
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132955393&doi=10.1109%2fTAI.2021.3112945&partnerID=40&md5=1aa3a6a3aa0eed0f9056f0511cddba22
AB  - Two-dimesional (2-D) LiDAR is an efficient alternative sensor for vehicle detection, which is one of the most critical tasks in autonomous driving. Compared to the fully developed 3-D LiDAR vehicle detection, 2-D LiDAR vehicle detection has much room to improve. Most existing state-of-the-art works represent 2-D point clouds as pseudo-images and then perform detection with traditional object detectors on 2-D images. However, they ignore the sparse representation and geometric information of vehicles in the 2-D cloud points. To address these issues, in this article, we present a novel keypoint-aware and keypoint-matching network termed as KAM-Net, which focuses on better detecting the vehicles by explicitly capturing and extracting the sparse information of L-shape in 2-D LiDAR point clouds. The whole framework consists of two stages - namely, keypoint-aware stage and keypoint-matching stage. The keypoint-aware stage utilizes the heatmap and edge extraction module to simultaneously predict the position of L-shaped keypoints and inflection offset of L-shaped endpoints. The keypoint-matching stage is followed to group the keypoints and produce the oriented bounding boxes with axis by utilizing the endpoint-matching and L-shaped-matching methods. Further, we conduct extensive experiments on a recently released public dataset to evaluate the effectiveness of our approach. The results show that our KAM-Net achieves a new state-of-the-art performance. The source code is available at https://github.com/ispc-lab/KAM-Net.  © 2020 IEEE.
KW  - Artificial intelligence algorithmic design and analysis
KW  - artificial intelligence in transportation
KW  - deep learning
KW  - supervised learning
KW  - Computer vision
KW  - Deep learning
KW  - Optical radar
KW  - Vehicles
KW  - Algorithmic analysis
KW  - Algorithmic design
KW  - Artificial intelligence algorithmic design and analyse
KW  - Artificial intelligence in transportation
KW  - Deep learning
KW  - Design and analysis
KW  - Key point matching
KW  - Keypoints
KW  - Point-clouds
KW  - Vehicles detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Woodruff, J.
AU  - Armengol-Estapé, J.
AU  - Ainsworth, S.
AU  - O'Boyle, M.F.P.
TI  - Bind the gap: compiling real software to hardware FFT accelerators
PY  - 2022
T2  - Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)
DO  - 10.1145/3519939.3523439
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132244166&doi=10.1145%2f3519939.3523439&partnerID=40&md5=1a1ef4eacec0ee237eaba1a0861e0abd
AB  - Specialized hardware accelerators continue to be a source of performance improvement. However, such specialization comes at a programming price. The fundamental issue is that of a mismatch between the diversity of user code and the functionality of fixed hardware, limiting its wider uptake. Here we focus on a particular set of accelerators: those for Fast Fourier Transforms. We present FACC (Fourier ACcelerator Compiler), a novel approach to automatically map legacy code to Fourier Transform accelerators. It automatically generates drop-in replacement adapters using Input-Output (IO)-based program synthesis that bridge the gap between user code and accelerators. We apply FACC to unmodified GitHub C programs of varying complexity and compare against two existing approaches. We target FACC to a high-performance library, FFTW, and two hardware accelerators, the NXP PowerQuad and the Analog Devices FFTA, and demonstrate mean speedups of 9x, 17x and 27x respectively © 2022 ACM.
KW  - FFT
KW  - hardware accelerator
KW  - program synthesis
KW  - Acceleration
KW  - C (programming language)
KW  - Program compilers
KW  - Fourier
KW  - Hardware accelerators
KW  - Input-output
KW  - Legacy code
KW  - Performance
KW  - Program synthesis
KW  - Real softwares
KW  - Specialisation
KW  - Specialized hardware
KW  - User codes
KW  - Fast Fourier transforms
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nigar, N.
AU  - Muhammad Faisal, H.
AU  - Kashif Shahzad, M.
AU  - Islam, S.
AU  - Oki, O.
TI  - An Offline Image Auditing System for Legacy Meter Reading Systems in Developing Countries: A Machine Learning Approach
PY  - 2022
T2  - Journal of Electrical and Computer Engineering
DO  - 10.1155/2022/4543530
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143389980&doi=10.1155%2f2022%2f4543530&partnerID=40&md5=9a264e6deb818fd753f985a21afc4abd
AB  - The developing countries are challenged with overbilling and underbilling, due to manual meter reading, which results in consumer dissatisfaction and loss of revenue. The existing automated meter reading (AMR) solutions are expensive; hence, sample-based manual snap auditing systems are introduced to control such meter reading inaccuracies. In these systems, the meter reader, besides reading, also collects meter images, which are used to manually audit the meter's accuracy. Although such systems are inexpensive, they are limited in their ability to be sustainable and ensure 100% accurate meter readings. In this paper, a novel offline optical character recognition (OCR) system-based Snap Audit system is proposed and tested for its efficient and real-time 100% accurate meter reading capabilities. The experimental results on 5,000 real-world instances show that the proposed approach processes an image in 0.05 seconds with 94% accuracy. Moreover, the developed approach is evaluated with four state-of-the-art algorithms: region convolution neural network (RCNN), nanonets, Fast-OCR, and PyTesseract. The results provide evidence that our new system design along with novel approach is more robust and efficient as compared to existing algorithms by 43.6%. © 2022 Natasha Nigar et al.
KW  - Deep learning
KW  - Legacy systems
KW  - Optical character recognition
KW  - Audit systems
KW  - Auditing systems
KW  - Automated meter readings
KW  - Machine learning approaches
KW  - Meter accuracy
KW  - Meter readers
KW  - Meter reading systems
KW  - Meter readings
KW  - Offline
KW  - Optical character recognition system
KW  - Developing countries
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yuan, Z.
AU  - Song, X.
AU  - Bai, L.
AU  - Wang, Z.
AU  - Ouyang, W.
TI  - Temporal-Channel Transformer for 3D Lidar-Based Video Object Detection for Autonomous Driving
PY  - 2022
T2  - IEEE Transactions on Circuits and Systems for Video Technology
DO  - 10.1109/TCSVT.2021.3082763
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107203510&doi=10.1109%2fTCSVT.2021.3082763&partnerID=40&md5=75ca7bf2a42273e2bc9e9a49e0429c83
AB  - The strong demand of autonomous driving in the industry has led to vigorous interest in 3D object detection and resulted in many excellent 3D object detection algorithms. However, the vast majority of algorithms only model single-frame data, ignoring the temporal clue in video sequence. In this work, we propose a new transformer, called Temporal-Channel Transformer (TCTR), to model the temporal-channel domain and spatial-wise relationships for video object detecting from Lidar data. As the special design of this transformer, the information encoded in the encoder is different from that in the decoder. The encoder encodes temporal-channel information of multiple frames while the decoder decodes the spatial-wise information for the current frame in a voxel-wise manner. Specifically, the temporal-channel encoder of the transformer is designed to encode the information of different channels and frames by utilizing the correlation among features from different channels and frames. On the other hand, the spatial decoder of the transformer decodes the information for each location of the current frame. Before conducting the object detection with detection head, a gate mechanism is further deployed for re-calibrating the features of current frame, which filters out the object-irrelevant information by repetitively refining the representation of target frame along with the up-sampling process. Experimental results reveal that TCTR achieves the state-of-the-art performance in grid voxel-based 3D object detection on the nuScenes benchmark. © 1991-2012 IEEE.
KW  - 3D object detection
KW  - Lidar-based video
KW  - temporal-channel attention
KW  - transformer
KW  - Autonomous vehicles
KW  - Benchmarking
KW  - Channel coding
KW  - Decoding
KW  - Encoding (symbols)
KW  - Motion compensation
KW  - Object recognition
KW  - Optical radar
KW  - Signal encoding
KW  - Autonomous driving
KW  - Channel encoder
KW  - Channel information
KW  - Multiple-frame
KW  - Single frames
KW  - State-of-the-art performance
KW  - Video object detections
KW  - Video sequences
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Humayun, M.
AU  - Ashfaq, F.
AU  - Jhanjhi, N.Z.
AU  - Alsadun, M.K.
TI  - Traffic Management: Multi-Scale Vehicle Detection in Varying Weather Conditions Using YOLOv4 and Spatial Pyramid Pooling Network
PY  - 2022
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics11172748
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137845925&doi=10.3390%2felectronics11172748&partnerID=40&md5=0a3f4dfd9f582cbb805ce6b258a93d6d
AB  - Detecting and counting on road vehicles is a key task in intelligent transport management and surveillance systems. The applicability lies both in urban and highway traffic monitoring and control, particularly in difficult weather and traffic conditions. In the past, the task has been performed through data acquired from sensors and conventional image processing toolbox. However, with the advent of emerging deep learning based smart computer vision systems the task has become computationally efficient and reliable. The data acquired from road mounted surveillance cameras can be used to train models which can detect and track on road vehicles for smart traffic analysis and handling problems such as traffic congestion particularly in harsh weather conditions where there are poor visibility issues because of low illumination and blurring. Different vehicle detection algorithms focusing the same issue deal only with on or two specific conditions. In this research, we address detecting vehicles in a scene in multiple weather scenarios including haze, dust and sandstorms, snowy and rainy weather both in day and nighttime. The proposed architecture uses CSPDarknet53 as baseline architecture modified with spatial pyramid pooling (SPP-NET) layer and reduced Batch Normalization layers. We also augment the DAWN Dataset with different techniques including Hue, Saturation, Exposure, Brightness, Darkness, Blur and Noise. This not only increases the size of the dataset but also make the detection more challenging. The model obtained mean average precision of 81% during training and detected smallest vehicle present in the image. © 2022 by the authors.
KW  - artificial intelligence
KW  - deep learning
KW  - intelligent traffic monitoring
KW  - traffic surveillance
KW  - urban and highway traffic analysis
KW  - vehicle detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tuteja, S.
AU  - Poddar, S.
AU  - Agrawal, D.
AU  - Karar, V.
TI  - PredictV: A Vehicle Prediction Scheme to Circumvent Occluded Frames
PY  - 2022
T2  - IEEE Access
DO  - 10.1109/ACCESS.2022.3151973
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124850165&doi=10.1109%2fACCESS.2022.3151973&partnerID=40&md5=3e1f4668c7a29b53815296d9bace2446
AB  - There are many methods to overcome traffic congestion on the roads, but occlusion is still there in most methods, so it is a dire need of the time that researchers have to look into this matter. In the rural and urban areas, heavy congestion on the roads has become the leading cause of occlusion. The PredictV method works on the prediction principle based on existing values and is proved one of the naval approaches for this problem. This scheme uses blob detection for the first frame detection and predicts other vehicles based on percentage increment in the different parameters to identify the vehicles. For improving the quality of the sample, data cleaning has been included in this work. With the help of this approach, congestion has been avoided, and this method prefers to use predicted points to detect vehicles on the frame. The suggested approach is implemented via a MATLAB simulator. It is tested on a large dataset which includes 7152 frames of 6 different videos from the Urban Tracker and KoPer dataset. In total, there are 46876 vehicles present on the frame at first, and the existing methods have detected only 58% and 73% of vehicles, whereas the detection rate is 82% with this suggested approach. The occlusion rate is only 17% on average, which has been reduced via this proposed approach which was 24% and 43% earlier than the existing one. The performance of this suggested method has been tested based on performance and results, which are pretty impressive that is almost 99.74%. In short, the outcome of this prediction technique has been improved now, and the effectiveness has been observed too.  © 2013 IEEE.
KW  - occlusion
KW  - prediction
KW  - Traffic monitoring
KW  - vehicle detection
KW  - Forecasting
KW  - MATLAB
KW  - Traffic congestion
KW  - Vehicles
KW  - Blob detection
KW  - Features extraction
KW  - Occlusion
KW  - Performance
KW  - Prediction algorithms
KW  - Prediction schemes
KW  - Rural and urban
KW  - Traffic monitoring
KW  - Urban areas
KW  - Vehicles detection
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kitayama, A.
AU  - Kuriyama, A.
AU  - Nagaishi, H.
AU  - Kuroda, H.
TI  - High-density implementation techniques for long-range radar using horn and lens antennas
PY  - 2021
T2  - IEICE Transactions on Electronics
DO  - 10.1587/transele.2021MMP0006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116395288&doi=10.1587%2ftransele.2021MMP0006&partnerID=40&md5=90149dad623806d29bc55a2ed33b60c1
AB  - Long-range radars (LRRs) for higher level autonomous driving (AD) will require more antennas than simple driving assistance. The point at issue here is 50–60% of the LRR module area is used for antennas. To miniaturize LRR modules, we use horn and lens antenna with highly efficient gain. In this paper, we propose two high-density implementation techniques for radio-frequency (RF) front-end using horn and lens antennas. In the first technique, the gap between antennas was eliminated by taking advantage of the high isolation performance of horn and lens antennas. In the second technique, the RF front-end including microstrip-lines, monolithic microwave integrated circuits, and peripheral parts is placed in the valley area of each horn. We fabricated a prototype LRR operating at 77 GHz with only one printed circuit board (PCB). To detect vehicles horizontally and vertically, this LRR has a minimum antenna configuration of one Tx antenna and four Rx antennas placed in 2×2 array, and 30 mm thickness. Evaluation results revealed that vehicles could be detected up to 320 m away and that the horizontal and vertical angle error was less than +/− 0.2 degrees, which is equivalent to the vehicle width over 280 m. Thus, horn and lens antennas implemented using the proposed techniques are very suitable for higher level AD LRRs. © 2021 Institute of Electronics, Information and Communication, Engineers, IEICE. All rights reserved.
KW  - 77 GHz long-range radar
KW  - Antenna isolation
KW  - Compact implementation
KW  - Direction of arrival estimation
KW  - Horn and lens antenna
KW  - Antenna arrays
KW  - Direction of arrival
KW  - Horn antennas
KW  - Printed circuit boards
KW  - Radar
KW  - Radar antennas
KW  - Vehicles
KW  - 77 GHz long-range radar
KW  - Antenna isolation
KW  - Autonomous driving
KW  - Compact implementation
KW  - Direction of arrival estimation
KW  - Driving assistance
KW  - Implementation techniques
KW  - Long-range radars
KW  - Radio frequency front end
KW  - Simple++
KW  - Lens antennas
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - An, Y.
AU  - Liu, W.
AU  - Cui, Y.
AU  - Wang, J.
AU  - Li, X.
AU  - Hu, H.
TI  - Multilevel Ground Segmentation for 3-D Point Clouds of Outdoor Scenes Based on Shape Analysis
PY  - 2022
T2  - IEEE Transactions on Instrumentation and Measurement
DO  - 10.1109/TIM.2022.3154835
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125351377&doi=10.1109%2fTIM.2022.3154835&partnerID=40&md5=dd8284aace139eea1857c409e2f671ac
AB  - Ground segmentation of 3-D point clouds acquired by laser sensors plays a crucial role in many applications, such as environment perception, scene understanding, and environment modeling. This article proposes a novel multilevel framework of the ground segmentation for 3-D point clouds of outdoor scenes based on shape analysis. The local shape of the 3-D point cloud of an outdoor scene is captured by principal component analysis. Then, the 3-D point cloud is classified into scattered points, linear points, and surface points. The unit normal vectors of the surface points are calculated and mapped into a unit ball. On the normal ball, the normal vectors are clustered, which segments the surface points into some surface regions correspondingly. Each surface region is further divided into several surface fragments according to point positions. The surface fragment that meets the ground conditions is regarded as a part of the initial ground. Finally, the ground is obtained by using the 2-D Gaussian process regression. The proposed method explores both local shapes and multilevel structures of outdoor scenes and constructs a probabilistic ground model in order to improve the accuracy and adaptivity of ground segmentation. Experiment results demonstrate that the proposed method has good performance. © 2022 IEEE.
KW  - Ground segmentation
KW  - laser rangefinder (LRF)
KW  - normal clustering
KW  - point cloud
KW  - shape analysis
KW  - Principal component analysis
KW  - Robotics
KW  - Three dimensional computer graphics
KW  - 3D point cloud
KW  - Ground segmentation
KW  - Laser range finders
KW  - Multilevels
KW  - Normal Clustering
KW  - Outdoor scenes
KW  - Point-clouds
KW  - Scene-based
KW  - Shape-analysis
KW  - Surface points
KW  - Range finders
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, G.
AU  - Wang, F.
AU  - Qu, S.
AU  - Chen, K.
AU  - Yu, J.
AU  - Liu, X.
AU  - Xiong, L.
AU  - Knoll, A.
TI  - Pseudo-Image and Sparse Points: Vehicle Detection with 2D LiDAR Revisited by Deep Learning-Based Methods
PY  - 2021
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2020.3007631
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120441579&doi=10.1109%2fTITS.2020.3007631&partnerID=40&md5=d3a8a43d738cc1b7b32b75c5bc2e5eb2
AB  - Detecting and locating surrounding vehicles robustly and efficiently are essential capabilities for autonomous vehicles. Existing solutions often rely on vision-based methods or 3D LiDAR-based methods. These methods are either too expensive in both sensor pricing (3D LiDAR) and computation (camera and 3D LiDAR) or less robust in resisting harsh environment changes (camera). In this work, we revisit the LiDAR based approaches for vehicle detection with a less expensive 2D LiDAR by utilizing modern deep learning approaches. We aim at filling in the gap as few previous works conclude an efficient and robust vehicle detection solution in a deep learning way in 2D. To this end, we propose a learning based method with the input of pseudo-images, named Cascade Pyramid Region Proposal Convolution Neural Network (Cascade Pyramid RCNN), and a hybrid learning method with the input of sparse points, named Hybrid Resnet Lite. Experiments are conducted with our newly 2D LiDAR vehicle dataset recorded in complex traffic environments. Results demonstrate that the Cascade Pyramid RCNN outperforms state-of-the-art methods in accuracy while the proposed Hybrid Resnet Lite provides superior performance of the speed and lightweight model by hybridizing learning based and non-learning based modules. As few previous works conclude an efficient and robust vehicle detection solution with 2D LiDAR, our research fills in this gap and illustrates that even with limited sensing source from a 2D LiDAR, detecting obstacles like vehicles efficiently and robustly is still achievable.  © 2000-2011 IEEE.
KW  - 2D LiDAR
KW  - autonomous driving
KW  - deep learning
KW  - intelligent transportation system
KW  - Vehicle detection
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Optical radar
KW  - 2d LiDAR
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Convolution neural network
KW  - Deep learning
KW  - Harsh environment
KW  - Intelligent transportation systems
KW  - Learning-based methods
KW  - Vehicles detection
KW  - Vision-based methods
KW  - Cameras
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kalita, R.
AU  - Talukdar, A.K.
AU  - Kumar Sarma, K.
TI  - Real-Time Human Detection with Thermal Camera Feed using YOLOv3
PY  - 2020
T2  - 2020 IEEE 17th India Council International Conference, INDICON 2020
DO  - 10.1109/INDICON49873.2020.9342089
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101540968&doi=10.1109%2fINDICON49873.2020.9342089&partnerID=40&md5=1c69894f78f5ca235daec86ea5532c9e
AB  - Human detection is needed for various applications such as advanced driver assistance systems and autonomous driving, security and surveillance etc. Thermal imaging is often adopted for night time because of its capability of capturing the energy emitted from human body where visible light camera fails. In this paper, we employ YOLOv3 for an accurate real-time human detection using thermal images. We modified the network parameters according to the characteristics of the human, making this method more suitable for detecting human. Subset of Korea Advanced Institute of Science and Technology (KAIST) multispectral dataset consisting of 47650 thermal images is used for training and testing of YOLOv3. During experimentation, it is observed that humans are detected at 17 millisecond which is much faster than a local machine detection. Our test result also shows improved performance of the detector with thermal image with average precision of 95.5% and miss rate of 4.7%. © 2020 IEEE.
KW  - Deep learning
KW  - Real-time
KW  - Thermal image
KW  - YOLOv3
KW  - Automobile drivers
KW  - Cameras
KW  - Image enhancement
KW  - Infrared imaging
KW  - Security systems
KW  - Statistical tests
KW  - Autonomous driving
KW  - Human detection
KW  - Korea advanced institute of science and technologies
KW  - Network parameters
KW  - Security and surveillances
KW  - Thermal camera
KW  - Thermal images
KW  - Training and testing
KW  - Advanced driver assistance systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Li, B.
AU  - Song, Z.
AU  - Guo, X.
TI  - Monocular Camera Ranging based on Vehicle Attitude Estimation
PY  - 2022
T2  - Proceedings - 2022 10th International Conference on Information Systems and Computing Technology, ISCTech 2022
DO  - 10.1109/ISCTech58360.2022.00122
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161876145&doi=10.1109%2fISCTech58360.2022.00122&partnerID=40&md5=192e70dd905f9a4d66cd18cf987f65d0
AB  - Assisted and autonomous driving technologies are gaining popularity. A key component of autonomous driving technology is affected by the measurement of the distance in front of the vehicle. Distance prediction using monocular cameras is an efficient and low-cost ranging method. There are some limitations to existing monocular camera-based vehicle distance estimation methods. If known object features are used as the calculation standard, the calibration information is easily lost in real-world road conditions. In this paper, we proposed a method for estimating vehicle distances based on vehicle pose estimation and RGB images based on known vehicle metrics without using external references. This method estimates the distance of the vehicle by using the vehicle's characteristics as the calibration value. Good results are obtained after testing on the KITTI dataset. © 2022 IEEE.
KW  - autonomous driving
KW  - Mask-RCNN
KW  - measurement of distance
KW  - monocular cameras
KW  - YoloV5
KW  - Calibration
KW  - Cameras
KW  - Statistical tests
KW  - Assisted drivings
KW  - Attitude estimation
KW  - Autonomous driving
KW  - Efficient costs
KW  - Low-costs
KW  - Mask-RCNN
KW  - Measurement of distance
KW  - Measurements of
KW  - Monocular cameras
KW  - Yolov5
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, W.
AU  - Yuan, Q.
AU  - Chen, L.
AU  - Zheng, L.
AU  - Tang, X.
TI  - Human Target Detection Method Based on Fusion of Radar and Image Data
ST  - 基于雷达与图像数据融合的人体目标检测方法
PY  - 2021
T2  - Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing
DO  - 10.16337/j.1004-9037.2021.02.014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104522325&doi=10.16337%2fj.1004-9037.2021.02.014&partnerID=40&md5=4c676ac37df5d62d432ff09356934287
AB  - Three-dimensional (3-D) human target detection has important application value in intelligent security, robot, automatic driving and other fields. At present, the 3-D human target detection method based on radar and image data fusion mainly adopts two-stage network structure, which respectively completes the selection of candidate boundary boxes with high target probability and the target classification/regression of target candidate boxes. Although the preselection of target candidate bounding box enables the two-stage network structure to achieve higher detection accuracy and positioning accuracy, the complexity of the network structure leads to the limitation of the operation speed, which cannot be applied in scenarios with high real-time requirements. In order to solve the above problem, this paper studies a real-time detection method of 3-D human targets based on improved RetinaNet. The backbone network and feature pyramid network are combined for point cloud and image feature extraction, and the fused feature anchors are input into the functional network to output the 3-D boundary boxes and target category information. By using the one-stage network structure, the method directly regresses the category probability and position coordinates of the targets, solving the imbalance problem of positive and negative samples in the process of one-stage network training by introducing focal loss function. Experiments on KITTI dataset show that the proposed method outperforms the contrast algorithms in terms of average accuracy and time-consuming, and can effectively balance the accuracy and real-time performance of target detection. © 2021 by Journal of Data Acquisition and Processing.
KW  - 3-D human target detection
KW  - Deep learning
KW  - Focal loss function
KW  - Improved RetinaNet
KW  - Multi-sensor information fusion
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Changfeng, X.
AU  - Wang, C.
AU  - Yu, L.
AU  - Chao, L.
AU  - Xiufeng, Z.
TI  - A Review of Lane Line Detection Technology Based on Machine Vision
PY  - 2021
T2  - Lecture Notes in Electrical Engineering
DO  - 10.1007/978-981-33-6318-2_88
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101832713&doi=10.1007%2f978-981-33-6318-2_88&partnerID=40&md5=2ca542ee8af70e8b6ddcea7af99a84c3
AB  - In recent years, lane line detection is the first step of autopilot technology. In this paper, the development of lane line detection technology based on machine vision is studied, and lane line technology is subdivided. The development of image processing technology, region of interest extraction and lane line fitting in recent years is classified. At the same time, the traditional algorithm and deep learning algorithm are analyzed. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
KW  - Area of interest
KW  - Deep learning
KW  - Image pre-processing
KW  - Lane detection
KW  - Machine vision
KW  - Target detection
KW  - Deep learning
KW  - Image segmentation
KW  - Learning algorithms
KW  - Manufacture
KW  - Image processing technology
KW  - Line detection
KW  - Line fitting
KW  - On-machines
KW  - Region of interest
KW  - Computer vision
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zarei, N.
AU  - Moallem, P.
AU  - Shams, M.
TI  - Fast-Yolo-Rec: Incorporating Yolo-Base Detection and Recurrent-Base Prediction Networks for Fast Vehicle Detection in Consecutive Images
PY  - 2022
T2  - IEEE Access
DO  - 10.1109/ACCESS.2022.3221942
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142780442&doi=10.1109%2fACCESS.2022.3221942&partnerID=40&md5=ddeacc8c7e054b662642d57c147a3270
AB  - Despite significant advances and innovations in deep network-based vehicle detection methods, finding a balance between detector accuracy and speed remains a significant challenge. This study aims to present an algorithm that can manage the speed and accuracy of the detector in real-time vehicle detection while increasing detector speed with accuracy comparable to high-speed detectors. To this end, the Fast-Yolo-Rec algorithm is proposed. The proposed method includes a new Yolo-based detection network and LSTM-based position prediction networks. The proposed semantic attention mechanism in the spatial semantic attention module (SSAM) significantly impacts accuracy and speed on par with the most recent fast detectors. Recurrent position prediction networks, on the other hand, improve the detection speed by estimating the current vehicle position using vehicle position history. The vehicle trajectories are classified, and the LSTM network for the specified trajectory predicts the vehicle positions. The Fast-Yolo-Rec algorithm not only determines the position of the vehicle faster than high-speed detectors but also allows for the speed control of the detection network with acceptable accuracy. The evaluation results on a large Highway dataset show that the proposed scheme outperforms the baseline methods.  © 2013 IEEE.
KW  - attention mechanism
KW  - recurrent prediction network
KW  - Yolo-based detection network
KW  - Feature extraction
KW  - Long short-term memory
KW  - Semantic Web
KW  - Semantics
KW  - Speed
KW  - Vehicles
KW  - Attention mechanisms
KW  - Detection networks
KW  - Features extraction
KW  - Hardware
KW  - High speed detectors
KW  - Prediction algorithms
KW  - Recurrent prediction network
KW  - Vehicle position
KW  - Vehicles detection
KW  - Yolo-based detection network
KW  - Forecasting
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fang, Z.
AU  - Lin, T.
AU  - Li, Z.
AU  - Yao, Y.
AU  - Zhang, C.
AU  - Ma, R.
AU  - Chen, Q.
AU  - Fu, S.
AU  - Ren, H.
TI  - Automatic Walking Method of Construction Machinery Based on Binocular Camera Environment Perception
PY  - 2022
T2  - Micromachines
DO  - 10.3390/mi13050671
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129904666&doi=10.3390%2fmi13050671&partnerID=40&md5=df94aaa7f825dfed533b111da3fc3bf7
AB  - In this paper, we propose an end-to-end automatic walking system for construction machin-ery, which uses binocular cameras to capture images of construction machinery for environmental perception, detects target information in binocular images, estimates the relative distance between the current target and cameras, and predicts the real-time control signal of construction machinery. This system consists of two parts: the binocular recognition ranging model and the control model. Objects within 5 m can be quickly detected by the recognition ranging model, and at the same time, the distance of the object can be accurately ranged to ensure the full perception of the surrounding environment of the construction machinery. The distance information of the object, the feature information of the binocular image, and the control signal of the previous stage are sent to the control model; then, the prediction of the control signal of the construction machinery can be output in the next stage. In this way, the automatic walking experiment of the construction machinery in a specific scenario is completed, which proves that the model can control the machinery to complete the walking task smoothly and safely. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - binocular detection
KW  - construction machinery
KW  - end-to-end
KW  - ranging
KW  - unmanned driving
KW  - Binoculars
KW  - Construction equipment
KW  - Machinery
KW  - Binocular camera
KW  - Binocular detection
KW  - Construction machinery
KW  - Control model
KW  - Control signal
KW  - End to end
KW  - Environment perceptions
KW  - Methods of constructions
KW  - Unmanned drivings
KW  - Walking systems
KW  - Cameras
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, B.
AU  - Yang, X.
TI  - Small obstacles image detection and classification for driver assistance
PY  - 2022
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-022-12706-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127671179&doi=10.1007%2fs11042-022-12706-5&partnerID=40&md5=3ca669ab8525f4ff82cd792974a20d7c
AB  - Small obstacles can cause big accidents, even if the vehicle is equipped with an intelligent auxiliary system. In order to detect four kinds of small obstacles quickly and accurately, this paper proposes an optimized neural network algorithm based on YOLOv3. K-Means+ is used to determine the prior box and enhance the adaptability of the YOLO scale. For the data samples imbalance, loss function of YOLO is improved to increase the precision of the prediction box. In addition, a special classification and counting algorithm is proposed to get results quickly and visually. The experimental results show that the our method can classify and locate four kinds of small obstacles more accurately and faster. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Classification
KW  - Intelligent auxiliary system
KW  - Object detection
KW  - Small obstacle
KW  - YOLO
KW  - Abstracting
KW  - Accidents
KW  - Automobile drivers
KW  - Auxiliary equipment
KW  - Image classification
KW  - K-means clustering
KW  - Auxiliary systems
KW  - Data sample
KW  - Driver assistance
KW  - Image detection
KW  - Images classification
KW  - Intelligent auxiliary system
KW  - K-means
KW  - Neural networks algorithms
KW  - Small obstacle
KW  - YOLO
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yu, X.-J.
AU  - Huai, Y.-H.
AU  - Yao, Z.-W.
AU  - Sun, Z.-C.
AU  - Yu, A.
TI  - Key technologies in autonomous vehicle for engineering
ST  - 工程车辆无人驾驶关键技术
PY  - 2021
T2  - Jilin Daxue Xuebao (Gongxueban)/Journal of Jilin University (Engineering and Technology Edition)
DO  - 10.13229/j.cnki.jdxbgxb20210038
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110116601&doi=10.13229%2fj.cnki.jdxbgxb20210038&partnerID=40&md5=a6effdd74249d21f71119e366e2910dd
AB  - As the society emphasizes on the life safety of operators and the standard of machinery performance requirements for construction, engineering vehicles are developing in the direction of autonomy, efficiency and reliability. In order to realize the automatic transfer and operation of unmanned engineering vehicles, this paper systematically summarizes the relevant technologies at home and abroad, and analyzes the research progress of key technologies of unmanned engineering vehicles in detail in terms of environment perception, motion planning, engineering operation and condition monitoring, etc. It points out that the technologies of unstructured environment identification, path planning and trajectory tracking of vehicles with variable body structure and automated operation still need to be broken through, and proposes the adoption of mechanism/structure optimization design, advanced communication means, machine learning and digital twin, etc., which is conducive to promoting the development of key technologies of unmanned engineering vehicles. © 2021 Editorial Board of Journal of Jilin University (Engineering and Technology Edition). All right reserved.
KW  - Digital twin
KW  - Engineering vehicles
KW  - Environment perception
KW  - Motion planning
KW  - Unmanned driving
KW  - Automatic vehicle identification
KW  - Automobile frames
KW  - Autonomous vehicles
KW  - Condition monitoring
KW  - Digital twin
KW  - Machinery
KW  - Motion tracking
KW  - Automated operations
KW  - Efficiency and reliability
KW  - Engineering operation
KW  - Engineering vehicles
KW  - Environment perceptions
KW  - Machinery performance
KW  - Trajectory tracking
KW  - Unstructured environments
KW  - Engineering education
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liu, C.K.
AU  - Negrut, D.
TI  - The Role of Physics-Based Simulators in Robotics
PY  - 2021
T2  - Annual Review of Control, Robotics, and Autonomous Systems
DO  - 10.1146/annurev-control-072220-093055
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102671355&doi=10.1146%2fannurev-control-072220-093055&partnerID=40&md5=d18b0aa57e78a3736bd05f64e7eace45
AB  - Physics-based simulation provides an accelerated and safe avenue for developing, verifying, and testing robotic control algorithms and prototype designs. In the quest to leverage machine learning for developing AI-enabled robots, physics-based simulation can generate a wealth of labeled training data in a short amount of time. Physics-based simulation also creates an ideal proving ground for developing intelligent robots that can both learn from their mistakes and be verifiable. This article provides an overview of the use of simulation in robotics, emphasizing how robots (with sensing and actuation components), the environment they operate in, and the humans they interact with are simulated in practice. It concludes with an overview of existing tools for simulation in robotics and a short discussion of aspects that limit the role that simulation plays today in intelligent robot design. © 2021 by Annual Reviews. All rights reserve.
KW  - robot simulation
KW  - testing through simulation
KW  - virtual prototyping of robots
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Piao, C.
AU  - Ding, X.
AU  - He, J.
AU  - Jang, S.
AU  - Liu, M.
TI  - Implementation of Image Transmission Based on Vehicle-to-Vehicle Communication
PY  - 2022
T2  - Journal of Information Processing Systems
DO  - 10.3745/JIPS.03.0176
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129917939&doi=10.3745%2fJIPS.03.0176&partnerID=40&md5=690ec68146822613ccfa1637beaaaac1
AB  - Weak over-the-horizon perception and blind spot are the main problems in intelligent connected vehicles (ICVs). In this paper, a V2V image transmission-based road condition warning method is proposed to solve them. The encoded road emergency images which are collected by the ICV are transmitted to the on-board unit (OBU) through Ethernet. The OBU broadcasts the fragmented image information including location and clock of the vehicle to other OBUs. To satisfy the channel quality of the V2X communication in different times, the optimal fragment length is selected by the OBU to process the image information. Then, according to the position and clock information of the remote vehicles, OBU of the receiver selects valid messages to decode the image information which will help the receiver to extend the perceptual field. The experimental results show that our method has an average packet loss rate of 0.5%. The transmission delay is about 51.59 ms in low-speed driving scenarios, which can provide drivers with timely and reliable warnings of the road conditions. © 2022. KIPS
KW  - Internet of vehicles
KW  - Real-time image transmission
KW  - Road condition warning
KW  - V2x
KW  - Clocks
KW  - Vehicle to Everything
KW  - Vehicle to vehicle communications
KW  - Vehicle transmissions
KW  - Blind spots
KW  - Image information
KW  - Internet of vehicle
KW  - On-board units
KW  - Over the horizons
KW  - Real-time image transmissions
KW  - Road condition
KW  - Road condition warning
KW  - V2x
KW  - Vehicle-to-vehicle communication
KW  - Roads and streets
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bangalore Ramaiah, N.K.
AU  - Kundu, S.K.
TI  - Stereo Vision-Based Road Debris Detection System for Advanced Driver Assistance Systems
PY  - 2021
T2  - SAE International Journal of Transportation Safety
DO  - 10.4271/09-10-01-0003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122004994&doi=10.4271%2f09-10-01-0003&partnerID=40&md5=d1fdd5686f5549543f57a8ae3b5e65c3
AB  - Reliable detection of obstacles around an autonomous vehicle is essential to avoid potential collision and ensure safe driving. However, a vast majority of existing systems are mainly focused on detecting large obstacles such as vehicles, pedestrians, and so on. Detection of small obstacles such as road debris, which pose a serious potential threat are often overlooked. In this article, a novel stereo vision-based road debris detection algorithm is proposed that detects debris on the road surfaces and estimates their height accurately. Moreover, a collision warning system that could warn the driver of an imminent crash by using 3D information of detected debris has been studied. A novel feature-based classifier that uses a combination of strong and weak features has been developed for the proposed algorithm, which identifies debris from selected candidates and calculates its height. 3D information of detected debris and vehicle's speed are used in the collision warning system to warn the driver to safely maneuver the vehicle. The performance of the proposed algorithm has been evaluated by implementing it on a passenger vehicle. Experimental results confirm that the proposed algorithm can successfully detect debris of ≥5 cm height for up to a 22 m distance with an accuracy of 90%. Moreover, the debris detection algorithm runs at 20 Hz in a commercially available stereo camera making it suitable for real-time applications in commercial vehicles. © 2022 SAE International.
KW  - Classification
KW  - Collision Warning System
KW  - Debris detection
KW  - Feature extraction
KW  - Stereo vision
KW  - Accidents
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Classification (of information)
KW  - Commercial vehicles
KW  - Debris
KW  - Roads and streets
KW  - Signal detection
KW  - Stereo image processing
KW  - Stereo vision
KW  - 3D information
KW  - Autonomous Vehicles
KW  - Collision warning system
KW  - Debris detection
KW  - Detection algorithm
KW  - Detection system
KW  - Features extraction
KW  - Reliable detection
KW  - Safe driving
KW  - Vision based
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ponnaganti, V.
AU  - Moh, M.
AU  - Moh, T.-S.
TI  - Utilizing CNNs for Object Detection with LiDAR Data for Autonomous Driving
PY  - 2021
T2  - Proceedings of the 2021 15th International Conference on Ubiquitous Information Management and Communication, IMCOM 2021
DO  - 10.1109/IMCOM51814.2021.9377361
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103740940&doi=10.1109%2fIMCOM51814.2021.9377361&partnerID=40&md5=eedc851bc446f20e5289d88f3c902356
AB  - This project evaluates the feasibility of utilizing popular Convolutional Neural Networks (CNNs) to detect objects present in LiDAR (Light Detection And Ranging) data, and the resulting neural network's performance. This work aims to further existing experimentation using raw LiDAR data that is analyzed and represented in a two-dimensional frame. Using this method, hundreds of frames were generated to create a dataset that was used for neural network training and validation on an existing CNN architecture. The LiDAR dataset was used to train YOLOv3, a popular CNN model, to detect vehicles. This research aims to test a smaller version of the network, YOLOv3-tiny, to measure the change in accuracy between using YOLOv3 and YOLOv3-tiny on the LiDAR dataset. The results are then compared to the loss typically found when going from YOLOv3 to YOLOV3-tiny on camera-based images. In prior experimentation, a preprocessing method was also introduced to attempt to isolate target objects in the frame. The method will be evaluated in this paper to measure its effect on the final accuracy metric of the network. Lastly, the runtime performance of these networks will be evaluated on two embedded platforms to understand if the frame rate that the networks perform on is usable for real-world applications, based on the frame rate the sensor is capable of outputting and the inference speed of the network on the embedded platforms. © 2021 IEEE.
KW  - Artificial Intelligence
KW  - Convolutional Neural Network
KW  - LiDAR
KW  - Object Detection
KW  - Convolutional neural networks
KW  - Information management
KW  - Lithium compounds
KW  - Object detection
KW  - Statistical tests
KW  - Autonomous driving
KW  - Embedded platforms
KW  - LIDAR (light detection and ranging)
KW  - Neural network training
KW  - Pre-processing method
KW  - Raw lidar data
KW  - Run-time performance
KW  - Target object
KW  - Optical radar
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zuzanna, K.
AU  - Tomasz, U.
AU  - Michal, G.
AU  - Robert, P.
TI  - How High-Tech Solutions Support the Fight Against IUU and Ghost Fishing: A Review of Innovative Approaches, Methods, and Trends
PY  - 2022
T2  - IEEE Access
DO  - 10.1109/ACCESS.2022.3212384
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139839520&doi=10.1109%2fACCESS.2022.3212384&partnerID=40&md5=fddf04845e10db2bb5d465bb88596f79
AB  - Illegal, Unreported, and Unregulated fishing is a major threat to human food supply and marine ecosystem health. Not only is it a cause of significant economic loss but also its effects have serious long-term environmental implications, such as overfishing and ocean pollution. The beginning of the fight against this problem dates since the early 2000s. From that time, a number of approaches and methods have been developed and reported. A key role in this topic is played by machine learning algorithms which exploit data provided by classical and high-tech sensors, devices and systems such as for example: CCTV, on-board cameras placed on autonomous vehicles, Global Positioning Systems, radars, Automatic Identification Systems, Vessel Monitoring Systems, or Coastal Surveillance Systems. The main objective of this paper is to provide the reader with knowledge about the scale of this phenomenon, methods to tackle the issue, and the current state of research on the subject. This has been achieved through a review of existing approaches that deal with these harmful phenomena by using dedicated artificial intelligence and machine learning tools, as well as the accompanying equipment and devices. In addition, flaws and gaps in current methods, and future directions are disscussed.  © 2013 IEEE.
KW  - Artificial intelligence
KW  - autonomous systems
KW  - boat tracking
KW  - ghost fishing
KW  - intelligent vehicles
KW  - IUU
KW  - machine learning
KW  - Artificial intelligence
KW  - Automation
KW  - Ecosystems
KW  - Fisheries
KW  - Food supply
KW  - Learning algorithms
KW  - Learning systems
KW  - Losses
KW  - Marine pollution
KW  - Security systems
KW  - 'current
KW  - Autonomous system
KW  - Boat tracking
KW  - Ghost fishing
KW  - High tech
KW  - Innovative approaches
KW  - Innovative method
KW  - Innovative trends
KW  - IUU
KW  - Machine-learning
KW  - Health risks
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Panahi, M.
AU  - Khosravi, K.
AU  - Ahmad, S.
AU  - Panahi, S.
AU  - Heddam, S.
AU  - Melesse, A.M.
AU  - Omidvar, E.
AU  - Lee, C.-W.
TI  - Cumulative infiltration and infiltration rate prediction using optimized deep learning algorithms: A study in Western Iran
PY  - 2021
T2  - Journal of Hydrology: Regional Studies
DO  - 10.1016/j.ejrh.2021.100825
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105350635&doi=10.1016%2fj.ejrh.2021.100825&partnerID=40&md5=8ad0039c00eb039b19cfbf33a3d951f5
AB  - Study region: Sixteen different sites from two provinces (Lorestan and Illam) in the western part of Iran were considered for the field data measurement of cumulative infiltration, infiltration rate, and other effective variables that affect infiltration process. Study focus: Soil infiltration is recognized as a fundamental process of the hydrologic cycle affecting surface runoff, soil erosion, and groundwater recharge. Hence, accurate prediction of the infiltration process is one of the most important tasks in hydrological science. As direct measurement is difficult and costly, and empirical models are inaccurate, the current study proposed a standalone, and optimized deep learning algorithm of a convolutional neural network (CNN) using gray wolf optimization (GWO), a genetic algorithm (GA), and an independent component analysis (ICA) for cumulative infiltration and infiltration rate prediction. First, 154 raw datasets were collected including the time of measuring; sand, clay, and silt percent; bulk density; soil moisture percent; infiltration rate; and cumulative infiltration using field survey. Next, 70 % of the dataset were used for model building and the remaining 30 % was used for model validation. Then, based on the correlation coefficient between input variables and outputs, different input combinations were constructed. Finally, the prediction power of each developed algorithm was evaluated using different visually-based (scatter plot, box plot and Taylor diagram) and quantitatively-based [root mean square error (RMSE), mean absolute error (MAE), the Nash-Sutcliffe efficiency (NSE), and percentage of bias (PBIAS)] metrics. New Hydrological Insights for the Region: Finding revealed that the time of measurement is more important for cumulative infiltration, while soil characteristics (i.e. silt content) are more significant in infiltration rate prediction. This shows that in the study area, silt parameter, which is the dominant constituent parameter, can control infiltration process more effectively. Effectiveness of the variables in the present study, in the order of importance are time, silt, clay, moisture content, sand, and bulk density. This can be related to the fact that most of study area is rangeland and thus, overgrazing leads to compaction of the silt soil that can lead to a slow infiltration process. Soil moisture content and bulk density are not highly effective in our study because these two factors do not significantly change across the study area. Findings demonstrated that the optimum input variable combination, is the one in which all input variables are considered. The results illustrated that CNN algorithms have a very high performance, while a metaheuristic algorithm enhanced the performance of a standalone CNN algorithm (from 7% to 28 %). The results also showed that a CNN-GWO algorithm outperformed the other algorithms, followed by CNN-ICA, CNN-GA, and CNN for both cumulative infiltration and infiltration rate prediction. All developed algorithms underestimated cumulative infiltration, while overestimating infiltration rates. © 2021 The Authors
KW  - CNN
KW  - Cumulative infiltration
KW  - Deep learning
KW  - Infiltration rate
KW  - Iran
KW  - Metaheuristic
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cao, J.
AU  - Pang, Y.
AU  - Xie, J.
AU  - Khan, F.S.
AU  - Shao, L.
TI  - From Handcrafted to Deep Features for Pedestrian Detection: A Survey
PY  - 2022
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
DO  - 10.1109/TPAMI.2021.3076733
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105102179&doi=10.1109%2fTPAMI.2021.3076733&partnerID=40&md5=56ba6774facccab7d4b7c71957740c74
AB  - Pedestrian detection is an important but challenging problem in computer vision, especially in human-centric tasks. Over the past decade, significant improvement has been witnessed with the help of handcrafted features and deep features. Here we present a comprehensive survey on recent advances in pedestrian detection. First, we provide a detailed review of single-spectral pedestrian detection that includes handcrafted features based methods and deep features based approaches. For handcrafted features based methods, we present an extensive review of approaches and find that handcrafted features with large freedom degrees in shape and space have better performance. In the case of deep features based approaches, we split them into pure CNN based methods and those employing both handcrafted and CNN based features. We give the statistical analysis and tendency of these methods, where feature enhanced, part-Aware, and post-processing methods have attracted main attention. In addition to single-spectral pedestrian detection, we also review multi-spectral pedestrian detection, which provides more robust features for illumination variance. Furthermore, we introduce some related datasets and evaluation metrics, and a deep experimental analysis. We conclude this survey by emphasizing open problems that need to be addressed and highlighting various future directions. Researchers can track an up-To-date list at https://github.com/JialeCao001/PedSurvey. © 1979-2012 IEEE.
KW  - deep features based methods
KW  - handcrafted features based methods
KW  - multi-spectral pedestrian detection
KW  - Pedestrian detection
KW  - Algorithms
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Lighting
KW  - Neural Networks, Computer
KW  - Pedestrians
KW  - Surveys
KW  - Evaluation metrics
KW  - Experimental analysis
KW  - Freedom degrees
KW  - Human-centric
KW  - Multi-spectral
KW  - Pedestrian detection
KW  - Postprocessing methods
KW  - algorithm
KW  - human
KW  - illumination
KW  - image processing
KW  - pedestrian
KW  - procedures
KW  - Feature extraction
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Guoqiang, C.
AU  - Huailong, Y.
AU  - Zhuangzhuang, M.
TI  - Vehicle and Pedestrian Detection Based on Multi-Level Feature Fusion in Autonomous Driving
PY  - 2021
T2  - Recent Advances in Computer Science and Communications
DO  - 10.2174/2666255813666200304123323
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122536990&doi=10.2174%2f2666255813666200304123323&partnerID=40&md5=46316be0ae326b338e329af9325d300c
AB  - Aims: The factors including light, weather, dynamic objects, seasonal effects and struc-tures bring great challenges for the autonomous driving algorithm in the real world. Autonomous vehicles can detect different object obstacles in complex scenes to ensure safe driving. Background: The ability to detect vehicles and pedestrians is critical to the safe driving of autonomous vehicles. Automated vehicle vision systems must handle extremely wide and challenging sce-narios. Objective: The goal of the work is to design a robust detector to detect vehicles and pedestrians. The main contribution is that the Multi-level Feature Fusion Block (MFFB) and the Detector Cascade Block (DCB) are designed. The multi-level feature fusion and multi-step prediction are used which greatly improve the detection object precision. Methods: The paper proposes a vehicle and pedestrian object detector, which is an end-to-end deep convolutional neural network. The key parts of the paper are to design the Multi-level Feature Fusion Block (MFFB) and Detector Cascade Block (DCB). The former combines inherent multi-level features by combining contextual information with useful multi-level features that combine high resolution but low semantics and low resolution but high semantic features. The latter uses multi-step prediction, cascades a series of detectors, and combines predictions of multiple feature maps to handle objects of different sizes. Results: The experiments on the RobotCar dataset and the KITTI dataset show that our algorithm can achieve high precision results through real-time detection. The algorithm achieves 84.61% mAP on the RobotCar dataset and is evaluated on the well-known KITTI benchmark dataset, achieving 81.54% mAP. In particular, the detection accuracy of a single-category vehicle reaches 90.02%. Conclusion: The experimental results show that the proposed algorithm has a good trade-off between detection accuracy and detection speed, which is beyond the current state-of-the-art Refine-Det algorithm. The 2D object detector is proposed in the paper, which can solve the problem of vehicle and pedestrian detection and improve the accuracy, robustness and generalization ability in autonomous driving. © 2021 Bentham Science Publishers.
KW  - Algorithm
KW  - Autonomous driving
KW  - Convolutional neural network
KW  - Deep learning
KW  - Multi-level feature fusion
KW  - Vehicle and pedestrian detection
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Economic and social effects
KW  - Feature extraction
KW  - Forecasting
KW  - Object detection
KW  - Pedestrian safety
KW  - Semantics
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Convolutional neural network
KW  - Deep learning
KW  - Features fusions
KW  - Multi-level feature fusion
KW  - Multilevels
KW  - Pedestrian detection
KW  - Safe driving
KW  - Vehicles detection
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rao, Z.
AU  - He, M.
AU  - Dai, Y.
AU  - Shen, Z.
TI  - Patch attention network with generative adversarial model for semi-supervised binocular disparity prediction
PY  - 2022
T2  - Visual Computer
DO  - 10.1007/s00371-020-02001-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096025370&doi=10.1007%2fs00371-020-02001-5&partnerID=40&md5=25dc8d65ef9882cdddb71b3c3c46aa7f
AB  - In this paper, we address the challenging points of binocular disparity estimation: (1) unsatisfactory results in the occluded region when utilizing warping function in unsupervised learning; (2) inefficiency in running time and the number of parameters as adopting a lot of 3D convolutions in the feature matching module. To solve these drawbacks, we propose a patch attention network for semi-supervised stereo matching learning. First, we employ a channel-attention mechanism to aggregate the cost volume by selecting its different surfaces for reducing a large number of 3D convolution, called the patch attention network (PA-Net). Second, we use our proposed PA-Net as a generator and then combine it, traditional unsupervised learning loss, and the adversarial learning model to construct a semi-supervised learning framework for improving performance in the occluded areas. We have trained our PA-Net in supervised learning, semi-supervised learning, and unsupervised learning manners. Extensive experiments show that (1) our semi-supervised learning framework can overcome the drawbacks of unsupervised learning and significantly improve the performance in the ill-posed region by using only a few or inaccurate ground truths; (2) our PA-Net can outperform other state-of-the-art approaches in supervised learning and use fewer parameters. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
KW  - Binocular disparity estimation
KW  - Generative adversarial model
KW  - Patch attention mechanism
KW  - Semi-supervised learning
KW  - Binoculars
KW  - Convolution
KW  - Unsupervised learning
KW  - Adversarial learning
KW  - Attention mechanisms
KW  - Binocular disparity
KW  - Feature matching
KW  - Improving performance
KW  - Semi-supervised
KW  - State-of-the-art approach
KW  - Warping function
KW  - Semi-supervised learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Carranza-García, M.
AU  - Lara-Benítez, P.
AU  - García-Gutiérrez, J.
AU  - Riquelme, J.C.
TI  - Enhancing object detection for autonomous driving by optimizing anchor generation and addressing class imbalance
PY  - 2021
T2  - Neurocomputing
DO  - 10.1016/j.neucom.2021.04.001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104482306&doi=10.1016%2fj.neucom.2021.04.001&partnerID=40&md5=30661692e9442238a28a7633d5c8ea49
AB  - Object detection has been one of the most active topics in computer vision for the past years. Recent works have mainly focused on pushing the state-of-the-art in the general-purpose COCO benchmark. However, the use of such detection frameworks in specific applications such as autonomous driving is yet an area to be addressed. This study presents an enhanced 2D object detector based on Faster R-CNN that is better suited for the context of autonomous vehicles. Two main aspects are improved: the anchor generation procedure and the performance drop in minority classes. The default uniform anchor configuration is not suitable in this scenario due to the perspective projection of the vehicle cameras. Therefore, we propose a perspective-aware methodology that divides the image into key regions via clustering and uses evolutionary algorithms to optimize the base anchors for each of them. Furthermore, we add a module that enhances the precision of the second-stage header network by including the spatial information of the candidate regions proposed in the first stage. We also explore different re-weighting strategies to address the foreground-foreground class imbalance, showing that the use of a reduced version of focal loss can significantly improve the detection of difficult and underrepresented objects in two-stage detectors. Finally, we design an ensemble model to combine the strengths of the different learning strategies. Our proposal is evaluated with the Waymo Open Dataset, which is the most extensive and diverse up to date. The results demonstrate an average accuracy improvement of 6.13% mAP when using the best single model, and of 9.69% mAP with the ensemble. The proposed modifications over the Faster R-CNN do not increase computational cost and can easily be extended to optimize other anchor-based detection frameworks. © 2021 Elsevier B.V.
KW  - Anchor optimization
KW  - Autonomous vehicles
KW  - Class imbalance
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Object detection
KW  - Autonomous vehicles
KW  - Clustering algorithms
KW  - Deep neural networks
KW  - Object recognition
KW  - 2D objects
KW  - Anchor optimization
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Class imbalance
KW  - Convolutional neural network
KW  - Deep learning
KW  - Detection framework
KW  - Objects detection
KW  - State of the art
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ahlberg, C.
AU  - León, M.
AU  - Ekstrand, F.
AU  - Ekström, M.
TI  - The genetic algorithm census transform: evaluation of census windows of different size and level of sparseness through hardware in-the-loop training
PY  - 2021
T2  - Journal of Real-Time Image Processing
DO  - 10.1007/s11554-020-00993-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087560320&doi=10.1007%2fs11554-020-00993-w&partnerID=40&md5=1b489719934554744bb86110077cdf96
AB  - Stereo correspondence is a well-established research topic and has spawned categories of algorithms combining several processing steps and strategies. One core part to stereo correspondence is to determine matching cost between the two images, or patches from the two images. Over the years several different cost metrics have been proposed, one being the Census Transform (CT). The CT is well proven for its robust matching, especially along object boundaries, with respect to outliers and radiometric differences. The CT also comes at a low computational cost and is suitable for hardware implementation. Two key developments to the CT are non-centric and sparse comparison schemas, to increase matching performance and/or save computational resources. Recent CT algorithms share both traits but are handcrafted, bounded with respect to symmetry, edge lengths and defined for a specific window size. To overcome this, a Genetic Algorithm (GA) was applied to the CT, proposing the Genetic Algorithm Census Transform (GACT), to automatically derive comparison schemas from example data. In this paper, FPGA-based hardware acceleration of GACT, has enabled evaluation of census windows of different size and shape, by significantly reducing processing time associated with training. The experiments show that lateral GACT windows produce better matching accuracy and require less resources when compared to square windows. © 2020, The Author(s).
KW  - Census transform
KW  - FPGA
KW  - Genetic algorithm
KW  - Matching cost metric
KW  - Real time
KW  - SoC
KW  - Stereo correspondence
KW  - VHDL
KW  - Computerized tomography
KW  - Genetic algorithms
KW  - Hardware-in-the-loop simulation
KW  - Image segmentation
KW  - Surveys
KW  - Computational costs
KW  - Computational resources
KW  - Hard-ware-in-the-loop
KW  - Hardware acceleration
KW  - Hardware implementations
KW  - Matching performance
KW  - Object boundaries
KW  - Stereo correspondences
KW  - Stereo image processing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, J.
AU  - Qian, J.
AU  - Li, Y.
AU  - Liu, Z.
AU  - Chen, Y.
AU  - Chen, J.
TI  - Automatic Extraction of Power Lines from Aerial Images of Unmanned Aerial Vehicles
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22176431
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137566186&doi=10.3390%2fs22176431&partnerID=40&md5=70d59c7974fa958ec81eb5e52ab5cf3d
AB  - Automatic power line extraction from aerial images of unmanned aerial vehicles is one of the key technologies of power line inspection. However, the faint power line targets and complex image backgrounds make the extraction of power lines a greater challenge. In this paper, a new power line extraction method is proposed, which has two innovative points. Innovation point one, based on the introduction of the Mask RCNN network algorithm, proposes a block extraction strategy to realize the preliminary extraction of power lines with the idea of “part first and then the whole”. This strategy globally reduces the anchor frame size, increases the proportion of power lines in the feature map, and reduces the accuracy degradation caused by the original negative anchor frames being misclassified as positive anchor frames. Innovation point two, the proposed connected domain group fitting algorithm solves the problem of broken and mis-extracted power lines even after the initial extraction and solves the problem of incomplete extraction of power lines by background texture interference. Through experiments on 60 images covering different complex image backgrounds, the performance of the proposed method far exceeds that of commonly used methods such as LSD, Yolact++, and Mask RCNN. DSCPL, TPR, precision, and accuracy are as high as 73.95, 81.75, 69.28, and 99.15, respectively, while FDR is only 30.72. The experimental results show that the proposed algorithm has good performance and can accomplish the task of power line extraction under complex image backgrounds. The algorithm in this paper solves the main problems of power line extraction and proves the feasibility of the algorithm in other scenarios. In the future, the dataset will be expanded to improve the performance of the algorithm in different scenarios. © 2022 by the authors.
KW  - aerial images
KW  - connected domain grouping
KW  - deep learning
KW  - Mask RCNN
KW  - power line segmentation
KW  - UAV
KW  - Antennas
KW  - Complex networks
KW  - Deep learning
KW  - Image segmentation
KW  - Textures
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial images
KW  - Complex image
KW  - Connected domain grouping
KW  - Connected domains
KW  - Deep learning
KW  - Line extraction
KW  - Line segmentation
KW  - Mask RCNN
KW  - Power line segmentation
KW  - Power lines
KW  - Extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, W.
AU  - Wang, S.
AU  - Guo, Y.
AU  - Zhao, Y.
TI  - Obstacle detection method of unmanned electric locomotive in coal mine based on YOLOv3-4L
PY  - 2022
T2  - Journal of Electronic Imaging
DO  - 10.1117/1.JEI.31.2.023032
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129819242&doi=10.1117%2f1.JEI.31.2.023032&partnerID=40&md5=460e3deb463673f820c9b3b6d343276e
AB  - It is one of the most critical technologies for unmanned electric locomotives to detect the obstacles in front of their operation quickly and accurately, which is of great significance for the safe operation of electric locomotives. Aiming at the problems of current computer vision detection methods, such as error warning, low detection accuracy, and slow detection speed, an obstacle intelligent detection method for unmanned electric locomotives based on an improved YOLOv3 (YOLOv3-4L) algorithm is proposed. The obstacle image data set of the electric locomotive running area is constructed to provide a testing environment for various obstacle detection algorithms. In the network structure, the darknet-53 feature extraction network is simplified, and the four-scale detection structure is formed by adding the shallow layer detection scale to the detection layer, which can improve the detection speed and accuracy of the algorithm for obstacles in front of the locomotive. Distance intersection over union loss function and Focal loss function are adopted to redesign the loss function of the target detector to further improve the detection accuracy of the algorithm. Traditional computer vision techniques such as perspective transformation, sliding window, and least square cubic polynomial are used to detect the track lines. By finding the area where the track was located and extending a certain distance to the outside of the track, the dangerous area of electric locomotive running is obtained. The improved YOLOv3 algorithm is utilized to detect obstacles, and only the types and positions of obstacles coincident with dangerous areas are output. The experimental results show that the traditional computer vision techniques such as perspective transformation, sliding window, and least square cubic polynomial can detect not only straight track but also curved track, which makes up for the shortcomings of the Hough transforms in detecting curved tracks. Compared with the original YOLOv3 algorithm, the YOLOv3-4L algorithm improves the mean average precision by 5.1%, and the detection speed increases by 7 fps. YOLOv3-4L detection model has high detection accuracy and speed, which can meet the actual working conditions and provide technical reference for unmanned driving of electric locomotives in underground coal mines.  © 2022 SPIE and IST.
KW  - computer vision
KW  - dangerous area
KW  - feature extraction network
KW  - feature scale
KW  - loss function
KW  - Computer vision
KW  - Engines
KW  - Extraction
KW  - Hough transforms
KW  - Statistical tests
KW  - Dangerous area
KW  - Detection accuracy
KW  - Detection methods
KW  - Detection speed
KW  - Feature extraction network
KW  - Feature scale
KW  - Features extraction
KW  - Loss functions
KW  - Obstacles detection
KW  - Traditional computers
KW  - Feature extraction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Perumal, P.S.
AU  - Sujasree, M.
AU  - Chavhan, S.
AU  - Gupta, D.
AU  - Mukthineni, V.
AU  - Shimgekar, S.R.
AU  - Khanna, A.
AU  - Fortino, G.
TI  - An insight into crash avoidance and overtaking advice systems for Autonomous Vehicles: A review, challenges and solutions
PY  - 2021
T2  - Engineering Applications of Artificial Intelligence
DO  - 10.1016/j.engappai.2021.104406
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111931318&doi=10.1016%2fj.engappai.2021.104406&partnerID=40&md5=7023d1048354b81d2cd60718cabe443c
AB  - Emergence of communication technologies made the automotive industries across the globe to embrace Advanced Driver Assistance Systems (ADAS) by considerable investments to ensure accident-free travel, reduction of pollution, fuel conservation. ADAS achieves its goals by integrating complex subsystems such as obstacle avoidance, overtaking advice, lane changing assistance, planning shortest routes, parking assistance, automatic gear shifting, etc., using the emerging technologies. This article emphasizes the road safety aspect of the ADAS by exploring Crash Avoidance and Overtaking Advice (CAOA) subsystems. Existing studies have a noticeable lack of connectivity between various aspects of CAOA subsystems. This review deeply explores and connects CAOA subsystems like road geometries, road debris, obstacle avoidance algorithms powered by Artificial Intelligence (AI), overtaking advice systems, perception challenges of human drivers in various light and weather conditions, driver inattention and misjudgments, vehicle blind-spots, vehicle parameter analysis, performance of vision sensors, in-vehicle computers, driver–vehicle interactions, Vehicle to Infrastructure (V2I) technologies. This article emphasizes the three primary performance metrics of the ADAS, namely accuracy, response time and robustness. Finally, this article discusses a typical functional architecture and gaps identified in existing studies. This article is structured to assist like-minded researchers, who work on CAOA systems for road safety. © 2021 Elsevier Ltd
KW  - ADAS
KW  - Communication network
KW  - Connected autonomous vehicles
KW  - Crash avoidance
KW  - Deep learning models
KW  - Driver assistance
KW  - In-vehicle computers
KW  - Primary vision sensors
KW  - V2X
KW  - Accident prevention
KW  - Accidents
KW  - Advanced driver assistance systems
KW  - Automotive industry
KW  - Autonomous vehicles
KW  - Collision avoidance
KW  - Deep learning
KW  - Investments
KW  - Motor transportation
KW  - Road vehicles
KW  - Roads and streets
KW  - Vehicle to vehicle communications
KW  - Advanced driver assistances
KW  - Communications networks
KW  - Connected autonomous vehicle
KW  - Crash avoidance
KW  - Deep learning model
KW  - Driver assistance
KW  - Driver-assistance systems
KW  - In-vehicle computer
KW  - Primary vision sensor
KW  - V2X
KW  - Automobile drivers
M3  - Short survey
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Rakotoniaina, Z.A.T.
AU  - Chelbi, N.E.
AU  - Gingras, D.
AU  - Faulconnier, F.
TI  - LiDAR and Radar Sensor Fusion for Detection and Tracking of Dynamic Objects in Autonomous Vehicles using Probabilistic Occupancy Grid, YOLO and DeepSORT
PY  - 2022
T2  - IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC
DO  - 10.1109/ITSC55140.2022.9922271
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141838120&doi=10.1109%2fITSC55140.2022.9922271&partnerID=40&md5=d5a897942b92229b28270bf1d1fc521d
AB  - Detection and tracking of on-road moving objects are crucial for vehicle perception. Data Fusion of perception sensors like cameras, lidar, and radar, allows the automotive perception to achieve a certain level of robustness and performance. The efficiency of vehicle perception determines its accuracy to take better decisions related to road safety. In this article, we propose two approaches for the detection and tracking of on-road moving objects by combining information from lidar and radar sensors. In this work, we propose to use a probabilistic occupancy grid for sensor fusion of radar and lidar point cloud, and a modified YOLO DeepSORT approach to detect, classify, and track the moving objects. Using both traditional and deep learning approaches, data fusion allows us to improve the robustness of the automotive perception pipeline. We finally used the nuScenes dataset to evaluate and compare the accuracy of our approaches with state-of-the-art trackers and detectors. We achieved an AMOTA score of 68.3% and a mAP score of 68.7% for the tracking and detection tasks respectively.  © 2022 IEEE.
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Intelligent vehicle highway systems
KW  - Motor transportation
KW  - Object detection
KW  - Roads and streets
KW  - Sensor data fusion
KW  - Tracking radar
KW  - Automotives
KW  - Autonomous Vehicles
KW  - Detection and tracking
KW  - Dynamic objects
KW  - Moving objects
KW  - Occupancy grids
KW  - Performance
KW  - Probabilistics
KW  - Radar sensors
KW  - Sensor fusion
KW  - Optical radar
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sellat, Q.
AU  - Bisoy, S.
AU  - Priyadarshini, R.
AU  - Vidyarthi, A.
AU  - Kautish, S.
AU  - Barik, R.K.
TI  - Intelligent Semantic Segmentation for Self-Driving Vehicles Using Deep Learning
PY  - 2022
T2  - Computational Intelligence and Neuroscience
DO  - 10.1155/2022/6390260
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123815278&doi=10.1155%2f2022%2f6390260&partnerID=40&md5=b00f9cb65a26237a4e377623f7a14ed7
AB  - Understanding the situation is a critical component of any self-driving system. Accurate real-time visual signal processing to create pixelwise classed pictures, also known as semantic segmentation, is critical for scenario comprehension and subsequent acceptance of this new technology. Due to the intricate interaction between pixels in each frame of the received camera data, such efficiency in terms of processing time and accuracy could not be achieved prior to recent advances in deep learning algorithms. We present an effective approach for semantic segmentation for self-driving automobiles in this study. We combine deep learning architectures like convolutional neural networks and autoencoders, as well as cutting-edge approaches like feature pyramid networks and bottleneck residual blocks, to develop our model. The CamVid dataset, which has undergone considerable data augmentation, is utilised to train and test our model. To validate the suggested model, we compare the acquired findings to various baseline models reported in the literature.  © 2022 Qusay Sellat et al.
KW  - Autonomous Vehicles
KW  - Deep Learning
KW  - Image Processing, Computer-Assisted
KW  - Neural Networks, Computer
KW  - Semantics
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Learning algorithms
KW  - Semantics
KW  - Statistical tests
KW  - Critical component
KW  - Driving systems
KW  - Effective approaches
KW  - Learning architectures
KW  - Processing accuracies
KW  - Processing time
KW  - Real- time
KW  - Self drivings
KW  - Semantic segmentation
KW  - Visual signal processing
KW  - image processing
KW  - semantics
KW  - Semantic Segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Cheng, Y.
AU  - Zhu, J.
AU  - Jiang, M.
AU  - Fu, J.
AU  - Pang, C.
AU  - Wang, P.
AU  - Sankaran, K.
AU  - Onabola, O.
AU  - Liu, Y.
AU  - Liu, D.
AU  - Bengio, Y.
TI  - FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters
PY  - 2021
T2  - Proceedings of the IEEE International Conference on Computer Vision
DO  - 10.1109/ICCV48922.2021.01077
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124784920&doi=10.1109%2fICCV48922.2021.01077&partnerID=40&md5=de7ab02048c2142350b280214555d22f
AB  - Marine debris is severely threatening the marine lives and causing sustained pollution to the whole ecosystem. To prevent the wastes from getting into the ocean, it is helpful to clean up the floating wastes in inland waters using the autonomous cleaning devices like unmanned surface vehicles. The cleaning efficiency relies on a high-accurate and robust object detection system. However, the small size of the target, the strong light reflection over water surface, and the reflection of other objects on bank-side all bring challenges to the vision-based object detection system. To promote the practical application for autonomous floating wastes cleaning, we present FloW†, the first dataset for floating waste detection in inland water areas. The dataset consists of an image sub-dataset FloW-Img and a multimodal sub-dataset FloW-RI which contains synchronized millimeter wave radar data and images. Accurate annotations for images and radar data are provided, supporting floating waste detection strategies based on image, radar data, and the fusion of two sensors. We perform several baseline experiments on our dataset, including vision-based and radar-based detection methods. The results show that, the detection accuracy is relatively low and floating waste detection still remains a challenging task. © 2021 IEEE
KW  - Cleaning
KW  - Light reflection
KW  - Marine pollution
KW  - Millimeter waves
KW  - Object detection
KW  - Oil spills
KW  - Tracking radar
KW  - Autonomous cleaning
KW  - Cleaning devices
KW  - Cleaning efficiency
KW  - Inland waters
KW  - Marine debris
KW  - Marine life
KW  - Object detection systems
KW  - Radar data
KW  - Robust object detection
KW  - Vision based
KW  - Object recognition
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Luo, Q.
AU  - Wang, J.
AU  - Gao, M.
AU  - He, Z.
AU  - Yang, Y.
AU  - Zhou, H.
TI  - Multiple Mechanisms to Strengthen the Ability of YOLOv5s for Real-Time Identification of Vehicle Type
PY  - 2022
T2  - Electronics (Switzerland)
DO  - 10.3390/electronics11162586
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137393119&doi=10.3390%2felectronics11162586&partnerID=40&md5=dc23cffcd250ee53855a6f44f2d226c1
AB  - Identifying the type of vehicle on the road is a challenging task, especially in the natural environment with all its complexities, such that the traditional architecture for object detection requires an excessively large amount of computation. Such lightweight networks as MobileNet are fast but cannot satisfy the performance-related requirements of this task. Improving the detection-related performance of small networks is, thus, an outstanding challenge. In this paper, we use YOLOv5s as the backbone network to propose a large-scale convolutional fusion module called the ghost cross-stage partial network (G_CSP), which can integrate large-scale information from different feature maps to identify vehicles on the road. We use the convolutional triplet attention network (C_TA) module to extract attention-based information from different dimensions. We also optimize the original spatial pyramid pooling fast (SPPF) module and use the dilated convolution to increase the capability of the network to extract information. The optimized module is called the DSPPF. The results of extensive experiments on the bdd100K, VOC2012 + 2007, and VOC2019 datasets showed that the improved YOLOv5s network performs well and can be used on mobile devices in real time. © 2022 by the authors.
KW  - C_TA
KW  - DSPPF
KW  - G_CSP
KW  - object detection
KW  - vehicle type detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cao, X.
AU  - Zhang, Z.
AU  - Sun, Y.
AU  - Wang, P.
AU  - Xu, S.
AU  - Liu, F.
AU  - Wang, C.
AU  - Peng, F.
AU  - Mu, S.
AU  - Liu, W.
AU  - Yang, Y.
TI  - The review of image processing and edge computing for intelligent transportation system
ST  - 面向智慧交通的图像处理与边缘计算
PY  - 2022
T2  - Journal of Image and Graphics
DO  - 10.11834/jig.211266
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131901853&doi=10.11834%2fjig.211266&partnerID=40&md5=3c437780aa6042e458dd79b4c30d110e
AB  - Current intelligent transportation system (ITS) issue is challenged to improve conventional transportation engineering solutions nowadays. ITS has integrated advanced information and communication technologies (ICTs) into the holistic transportation system for a safe, efficient, comfortable and environment friendly transport ecological construction. A variety of ITS applications and services have emerged in the intelligent traffic management system, autonomous driving system, and cooperative vehicle-infrastructure system (CVIS). To realise the intelligentisation of the traffic management, the intelligent traffic management system is mainly supported by optimised smart traffic facilities. Autonomous driving is mainly based on vehicle intelligence, relying on the cooperation of visual perception, radar perception, positioning system, on-board computing and artificial intelligence. The CVIS depends on the collaborative work of intelligent vehicles, roadside equipment and cloud platforms in support of the internet of vehicles (IOV) to fully implement dynamic real-time information interaction between vehicles and related traffic issues to achieve the vehicle-road collaborative management and traffic safety. Image processing is one of the core technologies that can support the deployment of a large number of ITS applications. It is based on computer algorithms application to extract useful visual sensor data derived information, including image enhancement and restoration, feature extraction and classification, as well as the semantic and instance segmentation. Image enhancement and restoration refers to improve the visual performance of the image and render more suitable image for human or machine analysis to deal with the image quality reduction issues of the challenging traffic system scenarios. Image feature extraction and classification technology is the core of object detection for accurate images or videos derived traffic objects locating and identifying, which is the basis for solving more complex higher-level tasks such as segmentation, scene understanding, object tracking, image description, event detection and activity recognition. To achieve higher-precision environmental perception in ITS, the semantic and instance segmentation realises pixel-level image classification based on scene semantic information in terms of the provision of different labels for instances of the same category. These image processing technologies can illustrate crucial references and information to enhance the capabilities of the perception, recognition, object detection, tracking, and path planning modules for more ITS applications. The multifaceted scenarios integration provides important technical support for intelligent traffic management, autonomous driving and the CVIS. In addition, the wide deployment of sensing devices places tremendous demands on data transmission and processing. As the widely recognised data processing technology, the centralised cloud computing is challenged to meet the real-time requirements of most massive data applications in ITS, which leads to uncertainty and barriers in the transmission process. Differentiated from the centralised cloud computing, the multi-access edge computing (MEC) technology deploys sensing, computation, and storage resources close to the network edge and provide a low-latency response-based platform for ITS, high bandwidth, and real-time applications and services access to network information. Our research reviews the current development status and typical applications of ITS. We focus on current image processing and MEC technologies for ITS. Future research direction of ITS and its related image processing and MEC technologies are predicted. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.
KW  - Autonomous driving
KW  - Cooperative vehicle-infrastructure system(CVIS)
KW  - Deep learning
KW  - Edge computing
KW  - Image processing
KW  - Intelligent transportation system(ITS)
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Junaid, M.
AU  - Szalay, Z.
AU  - Török, Á.
TI  - Evaluation of non-classical decision-making methods in self driving cars: Pedestrian detection testing on cluster of images with different luminance conditions
PY  - 2021
T2  - Energies
DO  - 10.3390/en14217172
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118554181&doi=10.3390%2fen14217172&partnerID=40&md5=a94bac53c5cc2906715977ec2669aa4f
AB  - Self-driving cars, i.e., fully automated cars, will spread in the upcoming two decades, according to the representatives of automotive industries; owing to technological breakthroughs in the fourth industrial revolution, as the introduction of deep learning has completely changed the concept of automation. There is considerable research being conducted regarding object detection systems, for instance, lane, pedestrian, or signal detection. This paper specifically focuses on pedestrian detection while the car is moving on the road, where speed and environmental conditions affect visibility. To explore the environmental conditions, a pedestrian custom dataset based on Common Object in Context (COCO) is used. The images are manipulated with the inverse gamma correction method, in which pixel values are changed to make a sequence of bright and dark images. The gamma correction method is directly related to luminance intensity. This paper presents a flexible, simple detection system called Mask R-CNN, which works on top of the Faster R-CNN (Region Based Convolutional Neural Network) model. Mask R-CNN uses one extra feature instance segmentation in addition to two available features in the Faster R-CNN, called object recognition. The performance of the Mask R-CNN models is checked by using different Convolu-tional Neural Network (CNN) models as a backbone. This approach might help future work, especially when dealing with different lighting conditions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Illumination
KW  - Instance segmentation
KW  - Inverse gamma correction
KW  - Mask R-CNN
KW  - Pedestrian custom dataset
KW  - Transfer learning
KW  - Automotive industry
KW  - Convolution
KW  - Convolutional neural networks
KW  - Decision making
KW  - Deep learning
KW  - Inverse problems
KW  - Luminance
KW  - Object detection
KW  - Object recognition
KW  - Statistical tests
KW  - Convolutional neural network
KW  - Correction method
KW  - Environmental conditions
KW  - Inverse gamma correction
KW  - Mask region based convolutional neural network
KW  - Neural network model
KW  - Pedestrian custom dataset
KW  - Pedestrian detection
KW  - Region-based
KW  - Transfer learning
KW  - Instance Segmentation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Blachut, K.
AU  - Danilowicz, M.
AU  - Szolc, H.
AU  - Wasala, M.
AU  - Kryjak, T.
AU  - Komorkiewicz, M.
TI  - Automotive Perception System Evaluation with Reference Data from a UAV’s Camera Using ArUco Markers and DCNN
PY  - 2022
T2  - Journal of Signal Processing Systems
DO  - 10.1007/s11265-021-01734-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123914056&doi=10.1007%2fs11265-021-01734-3&partnerID=40&md5=be00ba7ba8217131d116c6ecfe04e333
AB  - Testing and evaluation of an automotive perception system is a complicated task which requires special equipment and infrastructure. To compute key performance indicators and compare the results with real-world situation, some additional sensors and manual data labelling are often required. In this article, we propose a different approach, which is based on a UAV equipped with a 4K camera flying above a test track. Two computer vision methods are used to precisely determine the positions of the objects around the car – one based on ArUco markers and the other on a DCNN (we provide the algorithms used on GitHub). The detections are then correlated with the perception system readings. For the static and dynamic experiments, the differences between various systems are mostly below 0.5 m. The results of the experiments performed indicate that this approach could be an interesting alternative to existing evaluation solutions. © 2021, The Author(s).
KW  - ADAS
KW  - ArUco markers
KW  - Automatic labelling
KW  - Automotive
KW  - DCNN
KW  - Drone
KW  - Evaluation
KW  - LiDAR
KW  - Perception systems
KW  - Testing
KW  - UAV
KW  - Benchmarking
KW  - Drones
KW  - ADAS
KW  - Aruco marker
KW  - Automatic labelling
KW  - Automotives
KW  - DCNN
KW  - Drone
KW  - Evaluation
KW  - LiDAR
KW  - Perception systems
KW  - System evaluation
KW  - Cameras
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kumawat, H.
AU  - Mukhopadhyay, S.
TI  - Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection
PY  - 2022
T2  - Proceedings of the International Joint Conference on Neural Networks
DO  - 10.1109/IJCNN55064.2022.9892184
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140709378&doi=10.1109%2fIJCNN55064.2022.9892184&partnerID=40&md5=53fc48cb51881e32b0330c35e1a90adf
AB  - An autonomous system's perception engine must provide an accurate understanding of the environment for it to make decisions. Deep learning based object detection networks experience degradation in the performance and robustness for small and far away objects due to a reduction in object's feature map as we move to higher layers of the network. In this work, we propose a novel radar-guided spatial attention for RGB images to improve the perception quality of autonomous vehicles operating in a dynamic environment. In particular, our method improves the perception of small and long range objects, which are often not detected by the object detectors in RGB mode. The proposed method consists of two RGB object detectors, namely the Primary detector and a lightweight Secondary detector. The primary detector takes a full RGB image and generates primary detections. Next, the radar proposal framework creates regions of interest (ROIs) for object proposals by projecting the radar point cloud onto the 2D RGB image. These ROIs are cropped and fed to the secondary detector to generate secondary detections which are then fused with the primary detections via non-maximum suppression. This method helps in recovering the small objects by preserving the object's spatial features through an increase in their receptive field. We evaluate our fusion method on the challenging nuScenes dataset and show that our fusion method with SSD-lite as primary and secondary detector improves the baseline primary yolov3 detector's recall by 14 % while requiring three times fewer computational resources. © 2022 IEEE.
KW  - Autonomous Systems
KW  - Deep Learning
KW  - Object Detection
KW  - Radar
KW  - RGB Camera
KW  - Sensor Fusion
KW  - Behavioral research
KW  - Deep learning
KW  - Image enhancement
KW  - Network layers
KW  - Object recognition
KW  - Radar imaging
KW  - Tracking radar
KW  - Autonomous system
KW  - Deep learning
KW  - Fusion methods
KW  - Object detectors
KW  - Objects detection
KW  - Region-of-interest
KW  - Regions of interest
KW  - RGB cameras
KW  - RGB images
KW  - Sensor fusion
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xue, J.
AU  - Cheng, F.
AU  - Li, Y.
AU  - Song, Y.
AU  - Mao, T.
TI  - Detection of Farmland Obstacles Based on an Improved YOLOv5s Algorithm by Using CIoU and Anchor Box Scale Clustering
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22051790
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125045718&doi=10.3390%2fs22051790&partnerID=40&md5=5a7f947fcbf1efeb6857e85d5bdfe02f
AB  - It is necessary to detect multi‐type farmland obstacles in real time and accurately for un-manned agricultural vehicles. An improved YOLOv5s algorithm based on the K‐Means clustering algorithm and CIoU Loss function was proposed to improve detection precision and speed up real-time detection. The K‐Means clustering algorithm was used in order to generate anchor box scales to accelerate the convergence speed of model training. The CIoU Loss function, combining the three geometric measures of overlap area, center distance and aspect ratio, was adopted to reduce the occurrence of missed and false detection and improve detection precision. The experimental results showed that the inference time of a single image was reduced by 75% with the improved YOLOv5s algorithm; compared with that of the Faster R‐CNN algorithm, real‐time performance was effec-tively improved. Furthermore, the mAP value of the improved algorithm was increased by 5.80% compared with that of the original YOLOv5s, which indicates that using the CIoU Loss function had an obvious effect on reducing the missed detection and false detection of the original YOLOv5s. Moreover, the detection of small target obstacles of the improved algorithm was better than that of the Faster R‐CNN. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Deep learning
KW  - Farmland obstacles
KW  - Target detection
KW  - YOLOv5s
KW  - Algorithms
KW  - Cluster Analysis
KW  - Farms
KW  - Aspect ratio
KW  - Clustering algorithms
KW  - Deep learning
KW  - Image enhancement
KW  - Inference engines
KW  - Anchor-box
KW  - Deep learning
KW  - Detection precision
KW  - False detections
KW  - Farmland obstacle
KW  - K-means clustering algorithms
KW  - Loss functions
KW  - Missed detections
KW  - Targets detection
KW  - YOLOv5
KW  - agricultural land
KW  - algorithm
KW  - cluster analysis
KW  - Farms
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, Y.
AU  - Liu, L.
AU  - Zhao, H.
AU  - López-Benítez, M.
AU  - Yu, L.
AU  - Yue, Y.
TI  - Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22114208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131703088&doi=10.3390%2fs22114208&partnerID=40&md5=8e2f428493ac9745a882311b903d038b
AB  - With recent developments, the performance of automotive radar has improved significantly. The next generation of 4D radar can achieve imaging capability in the form of high-resolution point clouds. In this context, we believe that the era of deep learning for radar perception has arrived. However, studies on radar deep learning are spread across different tasks, and a holistic overview is lacking. This review paper attempts to provide a big picture of the deep radar perception stack, including signal processing, datasets, labelling, data augmentation, and downstream tasks such as depth and velocity estimation, object detection, and sensor fusion. For these tasks, we focus on explaining how the network structure is adapted to radar domain knowledge. In particular, we summarise three overlooked challenges in deep radar perception, including multi-path effects, uncertainty problems, and adverse weather effects, and present some attempts to solve them. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - automotive radars
KW  - autonomous driving
KW  - deep learning
KW  - multi-sensor fusion
KW  - object detection
KW  - radar signal processing
KW  - Perception
KW  - Radar
KW  - Signal Processing, Computer-Assisted
KW  - Weather
KW  - Automotive radar
KW  - Deep learning
KW  - Object detection
KW  - Object recognition
KW  - Sensor data fusion
KW  - Tracking radar
KW  - Automotive radar
KW  - Autonomous driving
KW  - Deep learning
KW  - High resolution point clouds
KW  - Imaging capabilities
KW  - Labelings
KW  - Multi-sensor fusion
KW  - Performance
KW  - Review papers
KW  - Signal-processing
KW  - perception
KW  - signal processing
KW  - telecommunication
KW  - weather
KW  - Autonomous vehicles
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Juyal, A.
AU  - Sharma, S.
AU  - Matta, P.
TI  - Multiclass Objects Localization Using Deep Learning Technique in Autonomous Vehicle
PY  - 2022
T2  - 6th IEEE International Conference on Computational System and Information Technology for Sustainable Solutions, CSITSS 2022
DO  - 10.1109/CSITSS57437.2022.10026411
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147846523&doi=10.1109%2fCSITSS57437.2022.10026411&partnerID=40&md5=9b7ef12216f83254c5e5089ac98b61c5
AB  - In a conventional automobile, the vehicle is controlled by a human brain. The eyes, which gather visual data from surrounding traffic, are the most important human sense. This visual input is then sent to the brain through the eyes. The eyes then communicate this visual information to the brain, which is in charge of operating the car by applying brakes, accelerating, or turning left or right. Autonomous vehicles or self-driving cars, on the other hand, rely on sensors, LiDAR, RADAR, and cameras to gather traffic data. This data from multiple sensors and cameras must be processed for the car to travel smoothly and securely. Intelligent software that can simulate a human brain, classify, and localize numerous sorts of traffic objects in real- time is required for autonomous vehicles. The image is the major input of an autonomous vehicle, and the program must detect the object from the input image. Object localization, or predicting the true position of a recognized object in an image, is a critical problem. The software can only predict the next step or decide what distance to keep between the car and other traffic objects, as well as whether to brake and accelerate, after exact object localization. We are proposing an object localization method using YOLOv5 as an object detector and LiDAR to calculate the distance. We created a custom dataset of 6000 images of five different classes and achieved 89.2% mAP value and at a.95 IoU threshold, our model achieved 52.1 mAP. We have also compared several conceivable computer vision challenges and real-time deep learning models in object localization in this work.  © 2022 IEEE.
KW  - autonomous
KW  - Deep learning
KW  - Faster RCNN
KW  - Object localization
KW  - Self-driving car
KW  - SSD
KW  - YOLOv5
KW  - Brain
KW  - Brakes
KW  - Cameras
KW  - Deep learning
KW  - Learning systems
KW  - Object detection
KW  - Object recognition
KW  - Optical radar
KW  - Autonomous
KW  - Autonomous Vehicles
KW  - Deep learning
KW  - Fast RCNN
KW  - Human brain
KW  - Object localization
KW  - Real- time
KW  - SSD
KW  - Traffic objects
KW  - YOLOv5
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, Z.
AU  - Sun, J.
AU  - Yu, J.
AU  - Liu, K.
AU  - Duan, J.
AU  - Chen, L.
AU  - Chen, C.L.P.
TI  - An Image-Based Benchmark Dataset and a Novel Object Detector for Water Surface Object Detection
PY  - 2021
T2  - Frontiers in Neurorobotics
DO  - 10.3389/fnbot.2021.723336
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116895167&doi=10.3389%2ffnbot.2021.723336&partnerID=40&md5=217ac554f4b1baa515fdc31370995f18
AB  - Water surface object detection is one of the most significant tasks in autonomous driving and water surface vision applications. To date, existing public large-scale datasets collected from websites do not focus on specific scenarios. As a characteristic of these datasets, the quantity of the images and instances is also still at a low level. To accelerate the development of water surface autonomous driving, this paper proposes a large-scale, high-quality annotated benchmark dataset, named Water Surface Object Detection Dataset (WSODD), to benchmark different water surface object detection algorithms. The proposed dataset consists of 7,467 water surface images in different water environments, climate conditions, and shooting times. In addition, the dataset comprises a total of 14 common object categories and 21,911 instances. Simultaneously, more specific scenarios are focused on in WSODD. In order to find a straightforward architecture to provide good performance on WSODD, a new object detector, named CRB-Net, is proposed to serve as a baseline. In experiments, CRB-Net was compared with 16 state-of-the-art object detection methods and outperformed all of them in terms of detection precision. In this paper, we further discuss the effect of the dataset diversity (e.g., instance size, lighting conditions), training set size, and dataset details (e.g., method of categorization). Cross-dataset validation shows that WSODD significantly outperforms other relevant datasets and that the adaptability of CRB-Net is excellent. © Copyright © 2021 Zhou, Sun, Yu, Liu, Duan, Chen and Chen.
KW  - baseline
KW  - cross-dataset validation
KW  - dataset
KW  - detector
KW  - surface object detection
KW  - Autonomous vehicles
KW  - Benchmarking
KW  - Large dataset
KW  - Object recognition
KW  - water
KW  - Autonomous driving
KW  - Baseline
KW  - Benchmark datasets
KW  - Cross-dataset validation
KW  - Dataset
KW  - Image-based
KW  - Object detectors
KW  - Objects detection
KW  - Surface object detection
KW  - Water surface
KW  - algorithm
KW  - aquatic environment
KW  - Article
KW  - benchmarking
KW  - climate
KW  - data base
KW  - illumination
KW  - imaging
KW  - measurement precision
KW  - validation process
KW  - water surface object detection dataset
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Cheng, Y.
AU  - Xu, H.
AU  - Liu, Y.
TI  - Robust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar
PY  - 2021
T2  - Proceedings of the IEEE International Conference on Computer Vision
DO  - 10.1109/ICCV48922.2021.01498
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127081552&doi=10.1109%2fICCV48922.2021.01498&partnerID=40&md5=a2bc207bd921cf0bf005d261e2817458
AB  - In recent years, unmanned surface vehicles (USVs) have been experiencing growth in various applications. With the expansion of USVs' application scenes from the typical marine areas to inland waters, new challenges arise for the object detection task, which is an essential part of the perception system of USVs. In our work, we focus on a relatively unexplored task for USVs in inland waters: small object detection on water surfaces, which is of vital importance for safe autonomous navigation and USVs' certain missions such as floating waste cleaning. Considering the limitations of vision-based object detection, we propose a novel radar-vision fusion based method for robust small object detection on water surfaces. By using a novel representation format of millimeter wave radar point clouds and applying a deep-level multi-scale fusion of RGB images and radar data, the proposed method can efficiently utilize the characteristics of radar data and improve the accuracy and robustness for small object detection on water surfaces. We test the method on the real-world floating bottle dataset that we collected and released. The result shows that, our method improves the average detection accuracy significantly compared to the vision-based methods and achieves state-of-the-art performance. Besides, the proposed method performs robustly when single sensor degrades. © 2021 IEEE
KW  - Bottles
KW  - Image enhancement
KW  - Millimeter waves
KW  - Object recognition
KW  - Radar imaging
KW  - Statistical tests
KW  - Synthetic aperture radar
KW  - Unmanned surface vehicles
KW  - Autonomous navigation
KW  - Inland waters
KW  - Marine areas
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - Perception systems
KW  - Radar data
KW  - Small object detection
KW  - Vehicle applications
KW  - Water surface
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Dazlee, N.M.A.A.
AU  - Khalil, S.A.
AU  - Abdul-Rahman, S.
AU  - Mutalib, S.
TI  - Object Detection for Autonomous Vehicles with Sensor-based Technology Using YOLO
PY  - 2022
T2  - International Journal of Intelligent Systems and Applications in Engineering
DO  - 10.18201/ijisae.2022.276
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128167980&doi=10.18201%2fijisae.2022.276&partnerID=40&md5=15b2021d6550f4a80f9474611ec8897b
AB  - The year 2020 has been a tough year with the global pandemic situation, and the utmost priority is to live in a clean, green, and safe environment. One of the areas that the governments are emphasizing for the readiness of our ecosystem is autonomous and contactless environments in adapting to the new norm. Thus, Autonomous Vehicle (AV) is a promising technology to bring forward. One of the critical aspects of Autonomous Navigation is object detection. Most AV use multiple sensors to detect objects, such as cameras, radar and Light Detection and Ranging sensor (LiDAR). Nowadays, the LiDAR sensor is widely implemented due to the ability to detect objects in the form of pulsed lasers, benefiting in low-light object detection. However, even with advanced technology, poor programming can affect the performance of object detection system. Thus, the study explores the state-of-the-art of You Only Look Once (YOLO) algorithms namely Tiny-YOLO and Complex-YOLO for object detection on KITTI dataset. Their performances were compared based on accuracy, precision, and recall metrics. The results showed that the Complex-YOLO has better performance as the mean average precision is higher than the Tiny-YOLO model when tested with equal parameters. © 2022, Ismail Saritas. All rights reserved.
KW  - Autonomous Vehicle
KW  - KITTI
KW  - LiDAR
KW  - Object Detection
KW  - Sensor
KW  - YOLO
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nex, F.
AU  - Armenakis, C.
AU  - Cramer, M.
AU  - Cucci, D.A.
AU  - Gerke, M.
AU  - Honkavaara, E.
AU  - Kukko, A.
AU  - Persello, C.
AU  - Skaloud, J.
TI  - UAV in the advent of the twenties: Where we stand and what is next
PY  - 2022
T2  - ISPRS Journal of Photogrammetry and Remote Sensing
DO  - 10.1016/j.isprsjprs.2021.12.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122819494&doi=10.1016%2fj.isprsjprs.2021.12.006&partnerID=40&md5=7741605ac38cb3758131594caf76dabe
AB  - The use of Unmanned Aerial Vehicles (UAVs) has surged in the last two decades, making them popular instruments for a wide range of applications, and leading to a remarkable number of scientific contributions in geoscience, remote sensing and engineering. However, the development of best practices for high quality of UAV mapping are often overlooked representing a drawback for their wider adoption. UAV solutions then require an inter-disciplinary research, integrating different expertise and combining several hardware and software components on the same platform. Despite the high number of peer-reviewed papers on UAVs, little attention has been given to the interaction between research topics from different domains (such as robotics and computer vision) that impact the use of UAV in remote sensing. The aim of this paper is to (i) review best practices for the use of UAVs for remote sensing and mapping applications and (ii) report on current trends - including adjacent domains - for UAV use and discuss their future impact in photogrammetry and remote sensing. Hardware developments, navigation and acquisition strategies, and emerging solutions for data processing in innovative applications are considered in this analysis. As the number and the heterogeneity of debated topics are large, the paper is organized according to very specific questions considered most relevant by the authors. © 2021 The Authors
KW  - Data processing
KW  - Deep learning
KW  - Hyperspectral
KW  - LiDAR
KW  - Navigation
KW  - Photogrammetry
KW  - Remote sensing
KW  - Sensors
KW  - UAV
KW  - Antennas
KW  - Computer hardware
KW  - Computer vision
KW  - Data handling
KW  - Deep learning
KW  - Mapping
KW  - Photogrammetry
KW  - Remote sensing
KW  - Robotics
KW  - Robots
KW  - Best practices
KW  - Deep learning
KW  - Geosciences
KW  - High quality
KW  - HyperSpectral
KW  - Inter-disciplinary researches
KW  - LiDAR
KW  - Remote engineering
KW  - Remote-sensing
KW  - Scientific contributions
KW  - best management practice
KW  - data processing
KW  - hardware
KW  - instrumentation
KW  - interdisciplinary approach
KW  - photogrammetry
KW  - remote sensing
KW  - software
KW  - unmanned vehicle
KW  - Unmanned aerial vehicles (UAV)
M3  - Short survey
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Roszyk, K.
AU  - Nowicki, M.R.
AU  - Skrzypczyński, P.
TI  - Adopting the YOLOv4 Architecture for Low-Latency Multispectral Pedestrian Detection in Autonomous Driving
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22031082
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123549403&doi=10.3390%2fs22031082&partnerID=40&md5=38a2e662d5e7593952c8accf016e56a0
AB  - Detecting pedestrians in autonomous driving is a safety-critical task, and the decision to avoid a a person has to be made with minimal latency. Multispectral approaches that combine RGB and thermal images are researched extensively, as they make it possible to gain robustness under varying illumination and weather conditions. State-of-the-art solutions employing deep neural networks offer high accuracy of pedestrian detection. However, the literature is short of works that evaluate multispectral pedestrian detection with respect to its feasibility in obstacle avoidance scenarios, taking into account the motion of the vehicle. Therefore, we investigated the real-time neural network detector architecture You Only Look Once, the latest version (YOLOv4), and demonstrate that this detector can be adapted to multispectral pedestrian detection. It can achieve accuracy on par with the state-of-the-art while being highly computationally efficient, thereby supporting low-latency decision making. The results achieved on the KAIST dataset were evaluated from the perspective of automotive applications, where low latency and a low number of false negatives are critical parameters. The middle fusion approach to YOLOv4 in its Tiny variant achieved the best accuracy to computational efficiency trade-off among the evaluated architectures. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Deep learning
KW  - Multispectral fusion
KW  - Pedestrian detection
KW  - Real-time
KW  - You Only Look Once
KW  - Computational efficiency
KW  - Decision making
KW  - Deep neural networks
KW  - Economic and social effects
KW  - Network architecture
KW  - Pedestrian safety
KW  - Autonomous driving
KW  - Critical tasks
KW  - Deep learning
KW  - Low latency
KW  - Multi-spectral
KW  - Multispectral fusion
KW  - Pedestrian detection
KW  - Real- time
KW  - State of the art
KW  - You only look once
KW  - Autonomous vehicles
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, G.
AU  - Wang, F.
AU  - Li, W.
AU  - Hong, L.
AU  - Conradt, J.
AU  - Chen, J.
AU  - Zhang, Z.
AU  - Lu, Y.
AU  - Knoll, A.
TI  - NeuroIV: Neuromorphic Vision Meets Intelligent Vehicle Towards Safe Driving with a New Database and Baseline Evaluations
PY  - 2022
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2020.3022921
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104099888&doi=10.1109%2fTITS.2020.3022921&partnerID=40&md5=6a6079237a2d6affe1441f8743cd729f
AB  - Neuromorphic vision sensors such as the Dynamic and Active-pixel Vision Sensor (DAVIS) using silicon retina are inspired by biological vision, they generate streams of asynchronous events to indicate local log-intensity brightness changes. Their properties of high temporal resolution, low-bandwidth, lightweight computation, and low-latency make them a good fit for many applications of motion perception in the intelligent vehicle. However, as a younger and smaller research field compared to classical computer vision, neuromorphic vision is rarely connected with the intelligent vehicle. For this purpose, we present three novel datasets recorded with DAVIS sensors and depth sensor for the distracted driving research and focus on driver drowsiness detection, driver gaze-zone recognition, and driver hand-gesture recognition. To facilitate the comparison with classical computer vision, we record the RGB, depth and infrared data with a depth sensor simultaneously. The total volume of this dataset has 27360 samples. To unlock the potential of neuromorphic vision on the intelligent vehicle, we utilize three popular event-encoding methods to convert asynchronous event slices to event-frames and adapt state-of-the-art convolutional architectures to extensively evaluate their performances on this dataset. Together with qualitative and quantitative results, this work provides a new database and baseline evaluations named NeuroIV in cross-cutting areas of neuromorphic vision and intelligent vehicle.  © 2000-2011 IEEE.
KW  - advanced driver assistance system
KW  - database and baseline evaluations
KW  - deep learning
KW  - distracted driving
KW  - event encoding
KW  - Neuromorphic vision
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Computer vision
KW  - Database systems
KW  - Deep learning
KW  - Encoding (symbols)
KW  - Vehicles
KW  - Active pixel
KW  - Asynchronous event
KW  - Database and baseline evaluation
KW  - Deep learning
KW  - Depth sensors
KW  - Distracted driving
KW  - Event encoding
KW  - Neuromorphic visions
KW  - Safe driving
KW  - Vision sensors
KW  - Signal encoding
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Luo, H.
AU  - Wang, P.
AU  - Chen, H.
AU  - Xu, M.
TI  - Object Detection Method Based on Shallow Feature Fusion and Semantic Information Enhancement
PY  - 2021
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2021.3103612
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116513288&doi=10.1109%2fJSEN.2021.3103612&partnerID=40&md5=aa93ad0cc9e74800905167163a725996
AB  - In recent years, object detection algorithm based on deep learning has made great progress, but the detection effect is not ideal for small objects detection. Some methods use high-resolution features or enhance shallow features to improve the detection accuracy of small objects. However, using high-resolution features for detection needs higher computational cost, and enhancing shallow features by propagating semantic information from high-level into low-level may bring information aliasing. To address this issue, we propose a novel object detection method based on shallow feature fusion and semantic information enhancement (FFSI). The high-level semantic information is injected into low-level features to guide the enhancement of specific detail information. In order to reduce the information aliasing in shallow features and enhance the receptive field of shallow features, we design two parallel modules: context information enhancement module (CIE) and receptive field enhancement module (RFE). CIE highlights the location of objects by establishing the relationship between local and global context information. RFE enhances the receptive field of shallow features by using dilated convolution to adapt to object detection of different scales, especially small objects. The proposed model is evaluated extensively on PASCAL VOC, and COCO datasets. The experimental results demonstrate that the proposed FFSI model has competitive performance. More importantly, this study reveals that FFSI outperforms the state-of-the-art methods in detecting small objects.  © 2001-2012 IEEE.
KW  - context information enhancement
KW  - Object detection
KW  - receptive field enhancement
KW  - shallow feature enhancement
KW  - Deep learning
KW  - Feature extraction
KW  - Object recognition
KW  - Semantics
KW  - Context information
KW  - Context information enhancement
KW  - Feature enhancement
KW  - Features fusions
KW  - Field enhancement
KW  - Receptive field enhancement
KW  - Receptive fields
KW  - Semantics Information
KW  - Shallow feature enhancement
KW  - Small objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Castells-Rufas, D.
AU  - Ngo, V.
AU  - Borrego-Carazo, J.
AU  - Codina, M.
AU  - Sanchez, C.
AU  - Gil, D.
AU  - Carrabina, J.
TI  - A Survey of FPGA-Based Vision Systems for Autonomous Cars
PY  - 2022
T2  - IEEE Access
DO  - 10.1109/ACCESS.2022.3230282
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144769258&doi=10.1109%2fACCESS.2022.3230282&partnerID=40&md5=4ad4b5917f90b53518d2ab4c034d3690
AB  - On the road to making self-driving cars a reality, academic and industrial researchers are working hard to continue to increase safety while meeting technical and regulatory constraints Understanding the surrounding environment is a fundamental task in self-driving cars. It requires combining complex computer vision algorithms. Although state-of-the-art algorithms achieve good accuracy, their implementations often require powerful computing platforms with high power consumption. In some cases, the processing speed does not meet real-time constraints. FPGA platforms are often used to implement a category of latency-critical algorithms that demand maximum performance and energy efficiency. Since self-driving car computer vision functions fall into this category, one could expect to see a wide adoption of FPGAs in autonomous cars. In this paper, we survey the computer vision FPGA-based works from the literature targeting automotive applications over the last decade. Based on the survey, we identify the strengths and weaknesses of FPGAs in this domain and future research opportunities and challenges.  © 2013 IEEE.
KW  - Autonomous automobile
KW  - computer vision
KW  - field programmable gate arrays
KW  - reconfigurable architectures
KW  - Accident prevention
KW  - Computer vision
KW  - Electric power utilization
KW  - Energy efficiency
KW  - Feature extraction
KW  - Field programmable gate arrays (FPGA)
KW  - Image classification
KW  - Object detection
KW  - Object recognition
KW  - Autonomous car
KW  - Computer vision algorithms
KW  - Features extraction
KW  - Field programmable gate array
KW  - Field programmables
KW  - Images classification
KW  - Objects detection
KW  - Programmable gate array
KW  - Surrounding environment
KW  - Vision systems
KW  - Reconfigurable architectures
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Han, X.
TI  - Modified Cascade RCNN Based on Contextual Information for Vehicle Detection
PY  - 2021
T2  - Sensing and Imaging
DO  - 10.1007/s11220-021-00342-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104096133&doi=10.1007%2fs11220-021-00342-6&partnerID=40&md5=86203ad48ff9f29b7640f8686464a393
AB  - In the process of traditional vehicle detection, there are some problems such as the fault detection and missing detection for small objects and shielded objects. Therefore, we propose a modified Cascade region-based convolutional neural network (RCNN) based on contextual information for vehicle detection. Firstly, the feature pyramid is improved to integrate the shallow information into the deep network layer by layer to enhance the features of small objects and occlusion objects. In here, we introduce the predictive optimization module and combine the context information of the region of interest (ROI), which makes the feature information have stronger robustness. Meanwhile, the multi-scale and multi-stage prediction is realized through the multi-threshold prediction network of internal cascade. Under the premise that the network parameters are basically unchanged, the accuracy rate is improved. Secondly, the multi-branch dilated convolution is introduced to reduce the feature loss during the down-sampling process. Finally, the region of interest and context information are fused to enhance the object feature expression. Experimental results show that the new Cascade RCNN method can better detect small and shielded vehicles compared with other state-of-the-art vehicle detection methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Cascade RCNN
KW  - Contextual information
KW  - Feature pyramid
KW  - ROI
KW  - Vehicle detection
KW  - Convolution
KW  - Convolutional neural networks
KW  - Fault detection
KW  - Image segmentation
KW  - Network layers
KW  - Vehicles
KW  - Context information
KW  - Contextual information
KW  - Feature expression
KW  - Feature information
KW  - Network parameters
KW  - Optimization module
KW  - Region of interest
KW  - The region of interest (ROI)
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Kim, S.
AU  - Park, S.-Y.
TI  - Expandable spherical projection and feature fusion methods for object detection from fisheye images
PY  - 2021
T2  - Proceedings of MVA 2021 - 17th International Conference on Machine Vision Applications
DO  - 10.23919/MVA51890.2021.9511379
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113987286&doi=10.23919%2fMVA51890.2021.9511379&partnerID=40&md5=4cf295a7f08ca8065344b527840f73ae
AB  - One of the key requirements for enhanced autonomous driving systems is accurate detection of the objects from a wide range of view. Large-angle images from a fisheye lens camera can be an effective solution for automotive applications. However, it comes with the cost of strong radial distortions. In particular, the fisheye camera has a photographic effect of exaggerating the size of objects in central regions of the image, while making objects near the marginal area appear smaller. Therefore, we propose the Expandable Spherical Projection that expands center or margin regions to produce straight edges of de-warped objects with less unwanted background in the bounding boxes. In addition to this, we analyze the influence of multi-scale feature fusion in a real-time object detector, which learns to extract more meaningful information for small objects. We present three different types of concatenated YOLOv3-SPP architectures. Moreover, we demonstrate the effectiveness of our proposed projection and feature-fusion using multiple fisheye lens datasets, which shows up to 4.7% AP improvement compared to fisheye images and baseline model.  © 2021 MVA Organization.
KW  - Cameras
KW  - Computer vision
KW  - Feature extraction
KW  - Image enhancement
KW  - Image fusion
KW  - Automotive applications
KW  - Autonomous driving
KW  - Effective solution
KW  - Feature fusion method
KW  - Multi-scale features
KW  - Object detectors
KW  - Radial distortions
KW  - Spherical projection
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sharma, V.K.
AU  - Mir, R.N.
TI  - A comprehensive and systematic look up into deep learning based object detection techniques: A review
PY  - 2020
T2  - Computer Science Review
DO  - 10.1016/j.cosrev.2020.100301
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097498691&doi=10.1016%2fj.cosrev.2020.100301&partnerID=40&md5=3fe81e0533dc670e9746d057afde6cd7
AB  - Object detection can be regarded as one of the most fundamental and challenging visual recognition task in computer vision and it has received great attention over the past few decades. Object detection techniques find their application in almost all the spheres of life, most prominent ones being surveillance, autonomous driving, pedestrian detection and so on. The primary focus of visual object detection is to detect objects belonging to certain class targets with absolute localization in a realistic scene or an input image and also to assign each detected instance of an object a predefined class label. Owing to rapid development of deep neural networks, the performance of object detectors has rapidly improved and as a result of this deep learning based detection techniques have been actively studied over the past several years. In this paper we provide a comprehensive survey of latest advances in deep learning based visual object detection. Firstly we have reviewed a large body of recent works in literature and using that we have analyzed traditional and current object detectors. Afterwards and primarily we provide a rigorous overview of backbone architectures for object detection followed by a systematic cover up of current learning strategies. Some popular datasets and metrics used for object detection are analyzed as well. Finally we discuss applications of object detection and provide several future directions to facilitate future research for visual object detection with deep learning. © 2020 Elsevier Inc.
KW  - Classification
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Localization
KW  - Object detection
KW  - Segmentation
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Learning systems
KW  - Object recognition
KW  - Transfer learning
KW  - 'current
KW  - Autonomous driving
KW  - Convolutional neural network
KW  - Deep learning
KW  - Localisation
KW  - Object detectors
KW  - Objects detection
KW  - Segmentation
KW  - Visual objects
KW  - Visual recognition
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fursa, I.
AU  - Fandi, E.
AU  - Musat, V.
AU  - Culley, J.
AU  - Gil, E.
AU  - Teeti, I.
AU  - Bilous, L.
AU  - Vander Sluis, I.
AU  - Rast, A.
AU  - Bradley, A.
TI  - Worsening Perception: Real-Time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions
PY  - 2022
T2  - SAE International Journal of Connected and Automated Vehicles
DO  - 10.4271/12-05-01-0008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124040624&doi=10.4271%2f12-05-01-0008&partnerID=40&md5=b97dee0adbf4bfcf9ea06ddaa54053f1
AB  - Autonomous vehicles (AVs) rely heavily upon their perception subsystems to "see"the environment in which they operate. Unfortunately, the effect of variable weather conditions presents a significant challenge to object detection algorithms, and thus, it is imperative to test the vehicle extensively in all conditions which it may experience. However, the development of robust AV subsystems requires repeatable, controlled testing - while real weather is unpredictable and cannot be scheduled. Real-world testing in adverse conditions is an expensive and time-consuming task, often requiring access to specialist facilities. Simulation is commonly relied upon as a substitute, with increasingly visually realistic representations of the real world being developed. In the context of the complete AV control pipeline, subsystems downstream of perception need to be tested with accurate recreations of the perception system output, rather than focusing on subjective visual realism of the input - whether in simulation or the real world. This study develops the untapped potential of a lightweight weather augmentation method in an autonomous racing vehicle - focusing not on visual accuracy but rather the effect upon perception subsystem performance in real time. With minimal adjustment, the prototype developed in this study can replicate the effects of water droplets on the camera lens and fading light conditions. This approach introduces a latency of less than 8 ms using computer hardware well suited to being carried in the vehicle - rendering it ideal for real-time implementation that can be run during experiments in simulation and augmented reality testing in the real world. © 2022 SAE International.
KW  - Augmented reality testing
KW  - Autonomous driving
KW  - Autonomous racing
KW  - Autonomous vehicle simulation
KW  - Sensor modelling
KW  - Weather augmentation
KW  - Autonomous vehicles
KW  - Computer hardware
KW  - Meteorology
KW  - Object detection
KW  - Real time control
KW  - Vehicle performance
KW  - Augmented reality testing
KW  - Autonomous driving
KW  - Autonomous racing
KW  - Autonomous vehicle simulation
KW  - Autonomous Vehicles
KW  - Real- time
KW  - Real-world
KW  - Sensors models
KW  - Vehicle simulation
KW  - Weather augmentation
KW  - Augmented reality
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lin, L.
AU  - Gong, S.
AU  - Peeta, S.
AU  - Wu, X.
TI  - Long short-term memory-based human-driven vehicle longitudinal trajectory prediction in a connected and autonomous vehicle environment
PY  - 2021
T2  - Transportation Research Record
DO  - 10.1177/0361198121993471
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108263300&doi=10.1177%2f0361198121993471&partnerID=40&md5=f7ef46e61da4aa33025c6d5fd97a586e
AB  - The advent of connected and autonomous vehicles (CAVs) will change driving behavior and travel environment, and provide opportunities for safer, smoother, and smarter road transportation. During the transition from the current human-driven vehicles (HDVs) to a fully CAV traffic environment, the road traffic will consist of a ‘‘mixed’’ traffic flow of HDVs and CAVs. Equipped with multiple sensors and vehicle-to-vehicle communications, a CAV can track surrounding HDVs and receive trajectory data of other CAVs in communication range. These trajectory data can be leveraged with recent advances in deep learning methods to potentially predict the trajectories of a target HDV. Based on these predictions, CAVs can react to circumvent or mitigate traffic flow oscillations and accidents. This study develops attention-based long short-term memory (LSTM) models for HDV longitudinal trajectory prediction in a mixed flow environment. The model and a few other LSTM variants are tested on the Next Generation Simulation US 101 dataset with different CAV market penetration rates (MPRs). Results illustrate that LSTM models that utilize historical trajectories from surrounding CAVs perform much better than those that ignore information even when the MPR is as low as 0.2. The attention-based LSTM models can provide more accurate multi-step longitudinal trajectory predictions. Further, grid-level average attention weight analysis is conducted and the CAVs with higher impact on the target HDV’s future trajectories are identified. © National Academy of Sciences: Transportation Research Board 2021.
KW  - Autonomous vehicles
KW  - Brain
KW  - Forecasting
KW  - Learning systems
KW  - Roads and streets
KW  - Trajectories
KW  - Vehicle to vehicle communications
KW  - 'current
KW  - Autonomous Vehicles
KW  - Driving behaviour
KW  - Market penetration
KW  - Memory modeling
KW  - Penetration rates
KW  - Road transportation
KW  - Trajectories datum
KW  - Trajectory prediction
KW  - Vehicle traffic
KW  - Long short-term memory
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Alabdulkreem, E.
AU  - Alzahrani, J.S.
AU  - Nemri, N.
AU  - Alharbi, O.
AU  - Mohamed, A.
AU  - Marzouk, R.
AU  - Hilal, A.M.
TI  - Computational Intelligence with Wild Horse Optimization Based Object Recognition and Classification Model for Autonomous Driving Systems
PY  - 2022
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app12126249
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132916314&doi=10.3390%2fapp12126249&partnerID=40&md5=49ae7876db005b3de0b3388fa0276a74
AB  - Presently, autonomous systems have gained considerable attention in several fields such as transportation, healthcare, autonomous driving, logistics, etc. It is highly needed to ensure the safe operations of the autonomous system before launching it to the general public. Since the design of a completely autonomous system is a challenging process, perception and decision-making act as vital parts. The effective detection of objects on the road under varying scenarios can considerably enhance the safety of autonomous driving. The recently developed computational intelligence (CI) and deep learning models help to effectively design the object detection algorithms for environment perception depending upon the camera system that exists in the autonomous driving systems. With this motivation, this study designed a novel computational intelligence with a wild horse optimization-based object recognition and classification (CIWHO-ORC) model for autonomous driving systems. The proposed CIWHO-ORC technique intends to effectively identify the presence of multiple static and dynamic objects such as vehicles, pedestrians, signboards, etc. Additionally, the CIWHO-ORC technique involves the design of a krill herd (KH) algorithm with a multi-scale Faster RCNN model for the detection of objects. In addition, a wild horse optimizer (WHO) with an online sequential ridge regression (OSRR) model was applied for the classification of recognized objects. The experimental analysis of the CIWHO-ORC technique is validated using benchmark datasets, and the obtained results demonstrate the promising outcome of the CIWHO-ORC technique in terms of several measures. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - autonomous systems
KW  - classification
KW  - computational intelligence
KW  - decision support
KW  - deep learning
KW  - object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Machkour, Z.
AU  - Ortiz-Arroyo, D.
AU  - Durdevic, P.
TI  - Classical and Deep Learning based Visual Servoing Systems: a Survey on State of the Art
PY  - 2022
T2  - Journal of Intelligent and Robotic Systems: Theory and Applications
DO  - 10.1007/s10846-021-01540-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121693466&doi=10.1007%2fs10846-021-01540-w&partnerID=40&md5=ed2d40ee7ab4c15b47376a4d8ac940f1
AB  - Computer vision, together with bayesian estimation algorithms, sensors, and actuators, are used in robotics to solve a variety of critical tasks such as localization, obstacle avoidance, and navigation. Classical approaches in visual servoing systems relied on extracting features from images to control robot movements. Now, state of the art computer vision systems use deep neural networks in tasks such as object recognition, detection, segmentation, and tracking. These networks and specialized controllers play a predominant role in the design and implementation of modern visual servoing systems due to their accuracy, flexibility, and adaptability. Recent research in direct systems for visual servoing has created robotic systems capable of relying only on the information contained in the whole image. Furthermore, end-to-end systems learn the control laws during training, eliminating entirely the controller. This paper presents a comprehensive survey on the state of the art in visual servoing systems, discussing the latest classical methods not included in other surveys but emphasizing the new approaches based on deep neural networks and their applications in a broad variety of applications within robotics. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.
KW  - Computer vision
KW  - Deep neural networks
KW  - Robotics
KW  - Visual servoing
KW  - Bayesian networks
KW  - Computer vision
KW  - Controllers
KW  - Object detection
KW  - Object recognition
KW  - Robotics
KW  - Robots
KW  - Surveys
KW  - Visual servoing
KW  - Bayesian estimations
KW  - Critical tasks
KW  - Estimation algorithm
KW  - Localisation
KW  - Obstacles avoidance
KW  - On state
KW  - Sensors and actuators
KW  - Servoing systems
KW  - State of the art
KW  - Visual-servoing
KW  - Deep neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Cervera-Uribe, A.A.
AU  - Méndez-Monroy, P.E.
TI  - U19-Net: a deep learning approach for obstacle detection in self-driving cars
PY  - 2022
T2  - Soft Computing
DO  - 10.1007/s00500-022-06980-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127645556&doi=10.1007%2fs00500-022-06980-6&partnerID=40&md5=8879530c341e179e8eec3bc529cb8678
AB  - Development of self-driving cars aims to drive safely from one point to another in a coordinated system where an on-board system should react and possibly alert drivers about the driving environments and possible collisions that may arise between drivers and obstacles. There are many deep learning approaches available for obstacle detection especially convolutional neural networks (CNNs) with improvement accuracy, and encoder–decoder networks are CNNs with a current attraction for researchers mainly because these models provide better results than classical statistical models for image segmentation and object classification tasks. This work proposes U19-Net an encoder–decoder deep model that explores the deep layers of a VGG19 model as an encoder following a symmetrical approach with an U-Net decoder designed for pixel-wise classifications. The U19-Net has end-to-end learning successfully effectiveness for the vehicle and pedestrian detection within the open-source Udacity dataset showing an IoU score of 87.08 and 78.18%, respectively. The proposed U19-Net is compared with five recent CNN networks using the AP metric, obtaining near results (less than 5%) for the faster R-CNN, one of the most commonly used networks for object detection. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Object detection
KW  - Self-driving cars
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Convolution
KW  - Convolutional neural networks
KW  - Decoding
KW  - Deep learning
KW  - Image enhancement
KW  - Image segmentation
KW  - Object recognition
KW  - Obstacle detectors
KW  - Signal encoding
KW  - 'current
KW  - Convolutional neural network
KW  - Coordinated system
KW  - Deep learning
KW  - Driving environment
KW  - Encoder-decoder
KW  - Learning approach
KW  - Obstacles detection
KW  - On-board systems
KW  - Statistic modeling
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liang, T.
AU  - Bao, H.
AU  - Pan, W.
AU  - Pan, F.
TI  - ALODAD: An Anchor-Free Lightweight Object Detector for Autonomous Driving
PY  - 2022
T2  - IEEE Access
DO  - 10.1109/ACCESS.2022.3166923
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128324587&doi=10.1109%2fACCESS.2022.3166923&partnerID=40&md5=915b546b35faf0e1b66d190874894799
AB  - Vision-based object detection is an essential component of autonomous driving. Because vehicles typically have limited on-board computing resources, a small-sized detection model is required. Simultaneously, high object detection accuracy and real-time inference detection speeds are required to ensure safety while driving. In this paper, an anchor-free lightweight object detector for autonomous driving called ALODAD is proposed. ALODAD incorporates an attention scheme into the lightweight neural network GhostNet and builds an anchor-free detection framework to achieve lower computational costs and provide parameters with high detection accuracy. Specifically, the lightweight backbone neural network integrates a convolutional block attention model that analyzes the valuable features from traffic scene images to generate an accurate bounding box, and then constructs feature pyramids for multi-scale object detection. The proposed method adds an intersection over union (IoU) branch to the decoupled detector to rank the vast number of candidate detections accurately. To increase the data diversity, data augmentation was used during training. Extensive experiments based on benchmarks demonstrate that the proposed method offers improved performance compared to the baseline. The proposed method can achieve an increased detection accuracy while meeting the real-time requirements of autonomous driving. The proposed method was compared with the YOLOv5 and RetinaNet models and 98.7% and 94.5% were obtained for the average precision metrics AP50 and AP75, respectively, on the BCTSDB dataset.  © 2013 IEEE.
KW  - Autonomous driving
KW  - deep learning
KW  - lightweight
KW  - object detection
KW  - Benchmarking
KW  - Deep learning
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Anchor-free
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Computational modelling
KW  - Deep learning
KW  - Features extraction
KW  - Lightweight
KW  - Location awareness
KW  - Neural-networks
KW  - Objects detection
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lee, K.-F.
AU  - Chen, X.-Z.
AU  - Yu, C.-W.
AU  - Chin, K.-Y.
AU  - Wang, Y.-C.
AU  - Hsiao, C.-Y.
AU  - Chen, Y.-L.
TI  - An Intelligent Driving Assistance System Based on Lightweight Deep Learning Models
PY  - 2022
T2  - IEEE Access
DO  - 10.1109/ACCESS.2022.3213328
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139833241&doi=10.1109%2fACCESS.2022.3213328&partnerID=40&md5=46525957a2f98f9e913c9b4f7c5b97ba
AB  - An intelligent driver assistance system is developed in this study, which is able to remind the drivers to turn on the head lights or wipers through situation recognition method when driving at night or on rainy days. Furthermore, the object detection results from multiple perspective views are integrated, and the surrounding object detection results are produced for collision avoidance. The system is able to alarm the drivers based on the lightweight deep learning model and the distance estimation method when surrounding vehicles are too close. Experimental results show that the proposed methods and the chosen lightweight model in our proposed system obtain reliable performance and sufficient computational efficiency under limited computing resource. In conclude, our proposed system obtains high probability to be adopted for the development of advanced driver assistance systems (ADAS). The proposed system can not only assist the driver in determining the vision ahead, but also provide an instant overview of the vehicle's surrounding conditions to enhance driving safety.  © 2013 IEEE.
KW  - Advanced driver assistance systems
KW  - distance estimation method
KW  - lightweight deep learning model
KW  - situation recognition method
KW  - vehicle detection method
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Computational efficiency
KW  - Deep learning
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Vehicles
KW  - Deep learning
KW  - Detection methods
KW  - Distance estimation
KW  - Distance estimation method
KW  - Estimation methods
KW  - Features extraction
KW  - Learning models
KW  - Lightweight deep learning model
KW  - Objects detection
KW  - Recognition methods
KW  - Situation recognition
KW  - Situation recognition method
KW  - Vehicle detection method
KW  - Vehicles detection
KW  - Computer architecture
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, Y.
AU  - Chen, S.
AU  - Zhao, J.
AU  - Yao, R.
AU  - Xue, Y.
AU  - Saddik, A.E.
TI  - CLT-Det: Correlation Learning Based on Transformer for Detecting Dense Objects in Remote Sensing Images
PY  - 2022
T2  - IEEE Transactions on Geoscience and Remote Sensing
DO  - 10.1109/TGRS.2022.3204770
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137869636&doi=10.1109%2fTGRS.2022.3204770&partnerID=40&md5=4f3b3490f71d746fed30fc95b6d0728a
AB  - Challenges still exist in the task of object detection in remote sensing images with densely distributed objects due to large variation in scale and neglect of the relative position and correlation. To address these issues, a correlation learning detector based on transformer (CLT-Det) is proposed for detecting dense objects in remote sensing images. A transformer attention module (TAM) is designed to improve the densely packed objects' model representation ability by learning pixelwise attention with a transformer. To alleviate the semantic gap caused by the variations in scale, a feature refinement module (FRM) is proposed by improving the multiscale feature pyramid. A correlation transformer module (CTM) is proposed to extract correlation information and it encodes position information of dense objects' features on the classification branch for fully using the position information and correlation among objects. Extensive experiments compared with several state-of-art methods on two challenging remote sensing datasets, namely, dataset for object detection in aerial images (DOTA) and HRSC2016, demonstrate that the proposed CLT-Det achieves promising and competitive performance.  © 1980-2012 IEEE.
KW  - Correlation learning
KW  - deep learning
KW  - object detection
KW  - remote sensing images
KW  - vision transformer (ViT)
KW  - Classification (of information)
KW  - Deep learning
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
KW  - Remote sensing
KW  - Correlation
KW  - Correlation learning
KW  - Deep learning
KW  - Features extraction
KW  - Objects detection
KW  - Remote sensing images
KW  - Remote-sensing
KW  - Transformer
KW  - Vision transformer
KW  - algorithm
KW  - correlation
KW  - detection method
KW  - image analysis
KW  - satellite imagery
KW  - Semantics
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Alo, U.R.
AU  - Nkwo, F.O.
AU  - Nweke, H.F.
AU  - Achi, I.I.
AU  - Okemiri, H.A.
TI  - Non-pharmaceutical interventions against covid-19 pandemic: Review of contact tracing and social distancing technologies, protocols, apps, security and open research directions
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22010280
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121964814&doi=10.3390%2fs22010280&partnerID=40&md5=25f45575a247690b5fc2987cae899def
AB  - The COVID-19 Pandemic has punched a devastating blow on the majority of the world’s population. Millions of people have been infected while hundreds of thousands have died of the disease throwing many families into mourning and other psychological torments. It has also crip-pled the economy of many countries of the world leading to job losses, high inflation, and dwindling Gross Domestic Product (GDP). The duo of social distancing and contact tracing are the major tech-nological-based non-pharmaceutical public health intervention strategies adopted for combating the dreaded disease. These technologies have been deployed by different countries around the world to achieve effective and efficient means of maintaining appropriate distance and tracking the transmission pattern of the diseases or identifying those at high risk of infecting others. This paper aims to synthesize the research efforts on contact tracing and social distancing to minimize the spread of COVID-19. The paper critically and comprehensively reviews contact tracing technolo-gies, protocols, and mobile applications (apps) that were recently developed and deployed against the coronavirus disease. Furthermore, the paper discusses social distancing technologies, appropriate methods to maintain distances, regulations, isolation/quarantine, and interaction strategies. In addition, the paper highlights different security/privacy vulnerabilities identified in contact tracing and social distancing technologies and solutions against these vulnerabilities. We also x-rayed the strengths and weaknesses of the various technologies concerning their application in contact tracing and social distancing. Finally, the paper proposed insightful recommendations and open research directions in contact tracing and social distancing that could assist researchers, developers, and gov-ernments in implementing new technological methods to combat the menace of COVID-19. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Artificial intelligence
KW  - Contact tracing
KW  - COVID-19
KW  - Internet of things
KW  - Review
KW  - Sensor technologies
KW  - Social distancing
KW  - Contact Tracing
KW  - COVID-19
KW  - Humans
KW  - Pandemics
KW  - Physical Distancing
KW  - SARS-CoV-2
KW  - Coronavirus
KW  - Employment
KW  - Application security
KW  - Contact tracing
KW  - COVID-19
KW  - Gross domestic products
KW  - Health interventions
KW  - Intervention strategy
KW  - Job loss
KW  - Non-pharmaceutical interventions
KW  - Sensor technologies
KW  - Social distancing
KW  - contact examination
KW  - human
KW  - pandemic
KW  - Internet of things
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xu, X.
AU  - Chen, X.
AU  - Wu, B.
AU  - Wang, Z.
AU  - Zhen, J.
TI  - Exploiting high-fidelity kinematic information from port surveillance videos via a YOLO-based framework
PY  - 2022
T2  - Ocean and Coastal Management
DO  - 10.1016/j.ocecoaman.2022.106117
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126681274&doi=10.1016%2fj.ocecoaman.2022.106117&partnerID=40&md5=ab6305aab6ac72ed28595bfa9acfc97a
AB  - Port surveillance videos provide rich and intuitive temporal and spatial information and motion information, which is conducive to path planning, obstacle avoidance and subsequent risk prediction of automated guided vehicle (AGV) in automated container terminal (ACT). Extracting high-fidelity kinematic traffic relevant data from port surveillance videos become an active yet important research task in the ACT community. To that aim, the study proposes a deep-learning based framework to exploit spatial-temporal information from port videos with steps of object detection, object moving displacement mapping and speed estimation. First, the proposed framework detects both AGVs and people in the video with you only look once (YOLO) based model (i.e., YOLO V5 model). Second, we map the AGV (and people) moving distance from videos into physical world with the help of pinhole imaging rule. Third, we estimate AGV and people moving speed in the video with the help of linear regression model. The experiment results suggested that the proposed framework obtains satisfied performance considering that the average object moving displacement error is 0.19 m and the speed error is 0.08 m/s. The research findings help ACT management departments and regulations with high-fidelity traffic relevant data to further port safety and management efficiency. © 2022 Elsevier Ltd
KW  - Automated container terminal
KW  - Automated guided vehicle
KW  - Kinematic information exploitation
KW  - Linear regression model
KW  - YOLO detection Model
KW  - Automatic guided vehicles
KW  - Automation
KW  - Containers
KW  - Kinematics
KW  - Monitoring
KW  - Motion planning
KW  - Neural networks
KW  - Port terminals
KW  - Security systems
KW  - Automated container terminals
KW  - Automated guided vehicles
KW  - Detection models
KW  - High-fidelity
KW  - Information exploitation
KW  - Kinematic information
KW  - Kinematic information exploitation
KW  - Linear regression modelling
KW  - Surveillance video
KW  - You only look once detection model
KW  - container terminal
KW  - displacement
KW  - exploitation
KW  - kinematics
KW  - mapping method
KW  - videography
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Lopes, A.
AU  - Souza, R.
AU  - Pedrini, H.
TI  - A survey on RGB-D datasets
PY  - 2022
T2  - Computer Vision and Image Understanding
DO  - 10.1016/j.cviu.2022.103489
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134589068&doi=10.1016%2fj.cviu.2022.103489&partnerID=40&md5=5df8a4118f61fea898fba903c26ebd6e
AB  - RGB-D data is essential for solving many problems in computer vision. Hundreds of public RGB-D datasets containing various scenes, such as indoor, outdoor, aerial, driving, and medical, have been proposed. These datasets are useful for different applications and are fundamental for addressing classic computer vision tasks, such as monocular depth estimation. This paper reviewed and categorized image datasets that include depth information. We gathered 231 datasets that contain accessible data and grouped them into three categories: scene/objects, body, and medical. We also provided an overview of the different types of sensors, depth applications, and we examined trends and future directions of the usage and creation of datasets containing depth data, and how they can be applied to investigate the development of generalizable machine learning models in the monocular depth estimation field. © 2022 The Author(s)
KW  - Computer vision
KW  - Depth datasets
KW  - Monocular depth estimation
KW  - RGB-D data
KW  - Antennas
KW  - Depth dataset
KW  - Depth Estimation
KW  - Depth information
KW  - Image datasets
KW  - Indoor/outdoor
KW  - Machine learning models
KW  - Monocular depth estimation
KW  - RGB-D data
KW  - Scene object
KW  - Three categories
KW  - Computer vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xie, Z.
AU  - Tsuzaki, M.
AU  - Lu, H.
AU  - Serikawa, S.
TI  - Improved Point-Voxel Region Convolutional Neural Network for Small Object Detection
PY  - 2022
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.2657106
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146716974&doi=10.1117%2f12.2657106&partnerID=40&md5=03ca81ba5745b7c9d3b8db04ad07c53e
AB  - With the widespread use of LiDAR sensors, 3D object detection through 3D point cloud data processing has become a research target in robotics and autonomous driving. However, the disorder and sparsity of point cloud data are the problems in traditional point cloud data processing. It is challenging to detect objects using a large amount of point cloud data. Conventional 3D object detectors have mainly grid-based methods and point-based methods. PV-RCNN proposed a framework that combines voxel-based and point-based techniques, and object features are extracted using 3D voxel CNNs. However, the resolution reduction caused by the CNN affects the localization of objects. This study aims to improve the detection accuracy of more minor things by feeding not only a single output of the voxel CNN but also multiple outputs, including high-resolution outputs, to the RPN. We came out with a new network that introduces the Multi-Scale Region Proposal Network to reduce the effect of resolution degradation. Our network has better recognition accuracy for small objects like bicycles than the original PV-RCNN. In extensive experiments, we demonstrate that our model has a 5% improvement for small things, such as cyclists training on the KITTI dataset. © 2022 SPIE.
KW  - 3D detection
KW  - Autonomous Driving
KW  - Machine Learning
KW  - Convolutional neural networks
KW  - Data handling
KW  - Machine learning
KW  - Object detection
KW  - Object recognition
KW  - Optical radar
KW  - 3d detection
KW  - 3D object
KW  - 3D point cloud
KW  - Autonomous driving
KW  - Cloud data processing
KW  - Convolutional neural network
KW  - Machine-learning
KW  - Objects detection
KW  - Point cloud data
KW  - Small object detection
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Marwala, T.
TI  - Rational Machines and Artificial Intelligence
PY  - 2021
T2  - Rational Machines and Artificial Intelligence
DO  - 10.1016/B978-0-12-820676-8.09990-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127631243&doi=10.1016%2fB978-0-12-820676-8.09990-7&partnerID=40&md5=88fc1007e099eca581b16217ef3a0b7f
AB  - Intelligent machines are populating our social, economic and political spaces. These intelligent machines are powered by Artificial Intelligence technologies such as deep learning. They are used in decision making. One element of decision making is the issue of rationality. Regulations such as the General Data Protection Regulation (GDPR) require that decisions that are made by these intelligent machines are explainable. Rational Machines and Artificial Intelligence proposes that explainable decisions are good but the explanation must be rational to prevent these decisions from being challenged. Noted author Tshilidzi Marwala studies the concept of machine rationality and compares this to the rationality bounds prescribed by Nobel Laureate Herbert Simon and rationality bounds derived from the work of Nobel Laureates Richard Thaler and Daniel Kahneman. Rational Machines and Artificial Intelligence describes why machine rationality is flexibly bounded due to advances in technology. This effectively means that optimally designed machines are more rational than human beings. Readers will also learn whether machine rationality can be quantified and identify how this can be achieved. Furthermore, the author discusses whether machine rationality is subjective. Finally, the author examines whether a population of intelligent machines collectively make more rational decisions than individual machines. Examples in biomedical engineering, social sciences and the financial sectors are used to illustrate these concepts. © 2021 Elsevier Inc. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Jing, Z.
AU  - Ji, Z.
AU  - Wang, L.
AU  - Zhou, G.
AU  - Gao, Q.
AU  - Zhao, W.
AU  - Dai, S.
TI  - Lane Detection Based on Two-Stage Noise Features Filtering and Clustering
PY  - 2022
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2022.3187997
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135800123&doi=10.1109%2fJSEN.2022.3187997&partnerID=40&md5=f6ce48b121d5de79f604e3ecd99a4ff3
AB  - Lane detection is essential for autonomous vehicles, and vision-based lane detection is widely used in the field of intelligent driving cars because of its low cost. Aiming to solve the problem of false detection caused by surrounding vehicles during lane detection, a new multi-lane detection method with strong anti-interference ability is proposed. The main contribution of this work is the utilization of object detection neural network which is applied to remove vehicles from road images as much as possible. Compared with the traditional feature-based detection algorithms, this method can preserve lane line features more accurately and effectively. Firstly, binarization of global optimal threshold method is utilized to extract the basic features of lanes. In order to further remove non-lane line noises and extract lane features as clear as possible, YOLOv4-tiny object detection network is introduced to detect and filter vehicles on the road. Then the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm with denoising function is applied to cluster the different lane features. Finally, an improved RANSAC (Random Sample Consensus) method is used to perform quadratic curve fitting on the clustered features. The experimental results on dataset show that consumed time of a single frame image is only 0.119 second and the precision of the proposed method can reach 94.4%. Compared with other detection algorithms, the proposed algorithm has good real-time performance, stability and robustness in a variety of complex scenarios.  © 2001-2012 IEEE.
KW  - clustering
KW  - DBSCAN
KW  - Lane detection
KW  - YOLOv4-tiny
KW  - Clustering algorithms
KW  - Curve fitting
KW  - Feature extraction
KW  - Object recognition
KW  - Roads and streets
KW  - Signal detection
KW  - Vehicles
KW  - Autonomous Vehicles
KW  - Clusterings
KW  - Density-based spatial clustering of applications with noise
KW  - Detection algorithm
KW  - Feature clustering
KW  - Feature filtering
KW  - Lane detection
KW  - Noise features
KW  - Objects detection
KW  - YOLOv4-tiny
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Callegaro, D.
AU  - Levorato, M.
AU  - Restuccia, F.
TI  - SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection
PY  - 2022
T2  - Proceedings - 2022 IEEE 23rd International Symposium on a World of Wireless, Mobile and Multimedia Networks, WoWMoM 2022
DO  - 10.1109/WoWMoM54355.2022.00034
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137075043&doi=10.1109%2fWoWMoM54355.2022.00034&partnerID=40&md5=82ec1dc7ab8f05c8c13308241fc81107
AB  - Mobile devices such as drones and autonomous vehicles increasingly rely on object detection (OD) through deep neural networks (DNNs) to perform critical tasks such as navigation, target-tracking and surveillance, just to name a few. Due to their high complexity, the execution of these DNNs requires excessive time and energy. Low-complexity object tracking (OT) is thus used along with OD, where the latter is periodically applied to generate "fresh"references for tracking. However, the frames processed with OD incur large delays, which does not comply with real-time applications requirements. Offloading OD to edge servers can mitigate this issue, but existing work focuses on the optimization of the offloading process in systems where the wireless channel has a very large capacity. Herein, we consider systems with constrained and erratic channel capacity, and establish parallel OT (at the mobile device) and OD (at the edge server) processes that are resilient to large OD latency. We propose Katch-Up, a novel tracking mechanism that improves the system resilience to excessive OD delay. We show that this technique greatly improves the quality of the reference available to tracking, and boosts performance up to 33%. However, while Katch-Up significantly improves performance, it also increases the computing load of the mobile device. Hence, we design SmartDet, a low-complexity controller based on deep reinforcement learning (DRL) that learns to achieve the right trade-off between resource utilization and OD performance. SmartDet takes as input highly-heterogeneous context-related information related to the current video content and the current network conditions to optimize frequency and type of OD offloading, as well as Katch-Up utilization. We extensively evaluate SmartDet on a real-world testbed composed by a JetSon Nano as mobile device and a GTX 980 Ti as edge server, connected through a Wi-Fi link, to collect several network-related traces, as well as energy measurements. We consider a state-of-the-art video dataset (ILSVRC 2015 - VID) and state-of-the-art OD models (EfficientDet 0, 2 and 4). Experimental results show that SmartDet achieves an optimal balance between tracking performance - mean Average Recall (mAR) and resource usage. With respect to a baseline with full Katch-Up usage and maximum channel usage, we still increase mAR by 4% while using 50% less of the channel and 30% power resources associated with Katch-Up. With respect to a fixed strategy using minimal resources, we increase mAR by 20% while using Katch-Up on 1/3 of the frames. © 2022 IEEE.
KW  - deep reinforcement learning
KW  - Edge Computing
KW  - object detection
KW  - object tracking
KW  - Aircraft detection
KW  - Complex networks
KW  - Deep neural networks
KW  - Economic and social effects
KW  - Large dataset
KW  - Mobile edge computing
KW  - Mobile telecommunication systems
KW  - Object detection
KW  - Object recognition
KW  - Wi-Fi
KW  - 'current
KW  - Context-Aware
KW  - Deep reinforcement learning
KW  - Edge computing
KW  - Edge server
KW  - Lower complexity
KW  - Object Tracking
KW  - Objects detection
KW  - Reinforcement learnings
KW  - State of the art
KW  - Reinforcement learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhou, X.
AU  - Zhang, Z.
AU  - Lu, Y.
AU  - Wang, Q.
AU  - Wang, K.
TI  - SAR image recognition based on improved R-FCN
ST  - 基于改进R-FCN的SAR图像识别
PY  - 2022
T2  - Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics
DO  - 10.12305/j.issn.1001-506X.2022.04.17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128320702&doi=10.12305%2fj.issn.1001-506X.2022.04.17&partnerID=40&md5=65dc2222b5ecab18d0c542e2888d1eba
AB  - With remarkable achievements in target recognition, deep learning provides new ideas for improving the accuracy and speed of target recognition in synthetic aperture radar (SAR) images. In this paper, region-based fully convolutional networks (R-FCN) are applied to SAR image target recognition, and good results have been achieved. In order to solve the problem of small data set and high data similarity, the R-FCN model based on transfer learning is proposed for target recognition in SAR images. The faster region convolutional neural networks (Faster R-CNN) and the R-FCN models are trained and optimized, and the experimental results are compared with the improved R-FCN model based on transfer learning proposed in this paper. The results show that the proposed method has better recognition effects and faster recognition speed for SAR images. © 2022, Editorial Office of Systems Engineering and Electronics. All right reserved.
KW  - Fully convolutional network (FCN)
KW  - Machine vision
KW  - Migration study
KW  - Synthetic aperture radar(SAR)
KW  - Target recognition
KW  - Automatic target recognition
KW  - Computer vision
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Image enhancement
KW  - Radar imaging
KW  - Radar target recognition
KW  - Convolutional networks
KW  - Fully convolutional network
KW  - Machine-vision
KW  - Migration study
KW  - Model-based OPC
KW  - Network models
KW  - Region-based
KW  - Synthetic aperture radar images
KW  - Target recognition
KW  - Transfer learning
KW  - Synthetic aperture radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, J.
AU  - Wang, C.
AU  - Jiang, B.
AU  - Song, H.
AU  - Meng, Q.
TI  - Visual Perception Enabled Industry Intelligence: State of the Art, Challenges and Prospects
PY  - 2021
T2  - IEEE Transactions on Industrial Informatics
DO  - 10.1109/TII.2020.2998818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090965411&doi=10.1109%2fTII.2020.2998818&partnerID=40&md5=c0dcda3047d66f4f81070515f701bc7c
AB  - Visual perception refers to the process of organizing, identifying, and interpreting visual information in environmental awareness and understanding. With the rapid progress of multimedia acquisition technology, research on visual perception has been a hot topic in the academical field and industrial applications. Especially after the introduction of artificial intelligence theory, intelligent visual perception has been widely used to promote the development of industrial production towards intelligence. In this article, we review the previous research and application of visual perception in different industrial fields such as product surface defect detection, intelligent agricultural production, intelligent driving, image synthesis, and event reconstruction. The applications basically cover most of the intelligent visual perception processing technologies. Through this survey, it will provide a comprehensive reference for research on this direction. Finally, this article also summarizes the current challenges of visual perception and predicts its future development trends.  © 2005-2012 IEEE.
KW  - Artificial intelligence
KW  - industrial application
KW  - visual perception
KW  - Agricultural robots
KW  - Agriculture
KW  - Artificial intelligence
KW  - Industrial research
KW  - Surface defects
KW  - Technology transfer
KW  - Agricultural productions
KW  - Development trends
KW  - Environmental awareness
KW  - Event reconstruction
KW  - Industrial production
KW  - Processing technologies
KW  - Research and application
KW  - Visual information
KW  - Vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Juyal, A.
AU  - Sharma, S.
AU  - Matta, P.
TI  - Deep Learning Methods for Object Detection in Autonomous Vehicles
PY  - 2021
T2  - Proceedings of the 5th International Conference on Trends in Electronics and Informatics, ICOEI 2021
DO  - 10.1109/ICOEI51242.2021.9452932
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113452357&doi=10.1109%2fICOEI51242.2021.9452932&partnerID=40&md5=ba401e15ce31a9ae5c0cd0ef294ee788
AB  - The automotive industry and researchers have recently shown an interest in autonomous vehicles. In an autonomous vehicle, different technologies may be used. Radio Detection and Ranging Technology (RADAR), Light Image Detection and Ranging Technology (LiDAR) and computer vision are commonly recognized techniques. Computer vision is a method of extracting significant features from a digital image that allows the computer to perceive the features of objects and interpret the image. In recent research, it has been found that methods of deep learning can detect objects in real time. In the current review, approaches focused on CNN are discussed and their performances are compared.  © 2021 IEEE.
KW  - autonomous vehicle
KW  - CNN
KW  - Fast R-CNN
KW  - Faster R-CNN
KW  - R-CNN
KW  - SSD
KW  - YOLO
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Learning systems
KW  - Object detection
KW  - Optical radar
KW  - Digital image
KW  - Image detection
KW  - Learning methods
KW  - Radio detection and ranging technologies
KW  - Real time
KW  - Recent researches
KW  - Deep learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kegyes, T.
AU  - Süle, Z.
AU  - Abonyi, J.
TI  - The Applicability of Reinforcement Learning Methods in the Development of Industry 4.0 Applications
PY  - 2021
T2  - Complexity
DO  - 10.1155/2021/7179374
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121602568&doi=10.1155%2f2021%2f7179374&partnerID=40&md5=48dd6dda6a10c61da94827ca3c4a5c2f
AB  - Reinforcement learning (RL) methods can successfully solve complex optimization problems. Our article gives a systematic overview of major types of RL methods, their applications at the field of Industry 4.0 solutions, and it provides methodological guidelines to determine the right approach that can be fitted better to the different problems, and moreover, it can be a point of reference for R&D projects and further researches.  © 2021 Tamás Kegyes et al.
KW  - Reinforcement learning
KW  - Complex optimization problems
KW  - Methodological guidelines
KW  - Reinforcement learning method
KW  - Industry 4.0
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Khosravian, A.
AU  - Amirkhani, A.
AU  - Masih-Tehrani, M.
TI  - Enhancing the robustness of the convolutional neural networks for traffic sign detection
PY  - 2022
T2  - Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering
DO  - 10.1177/09544070211042961
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113667503&doi=10.1177%2f09544070211042961&partnerID=40&md5=9cda32a282365cc1bc935be27ebb88b5
AB  - The detection of traffic signs in clean and noise-free images has been investigated by numerous researchers; however, very few of these works have focused on noisy environments. While in the real world, for different reasons (e.g. the speed and acceleration of a vehicle and the roughness around it), the input images of the convolutional neural networks (CNNs) could be extremely noisy. Contrary to other research works, in this paper, we investigate the robustness of the deep learning models against the synthetically modeled noises in the detection of small objects. To this end, the state-of-the-art architectures of Faster-RCNN Resnet101, R-FCN Resnet101, and Faster-RCNN Inception Resnet V2 are trained by means of the Tsinghua-Tencent 100K database, and the performances of the trained models on noisy data are evaluated. After verifying the robustness of these models, different training scenarios (1 – Modeling various climatic conditions, 2 – Style randomization, and 3 – Augmix augmentation) are used to enhance the model robustness. The findings indicate that these scenarios result in up to 13.09%, 12%, and 13.61% gains in the mentioned three networks by means of the mPC metric. They also result in 11.74%, 8.89%, and 7.27% gains in the rPC metric, demonstrating that improvement in robustness does not lead to performance drop on the clean data. © IMechE 2021.
KW  - convolutional neural network
KW  - image distortions
KW  - model robustness
KW  - small object detection
KW  - Traffic sign
KW  - Convolution
KW  - Deep learning
KW  - Object detection
KW  - Traffic signs
KW  - Climatic conditions
KW  - Learning models
KW  - Model robustness
KW  - Noisy environment
KW  - State of the art
KW  - Three networks
KW  - Traffic sign detection
KW  - Training scenario
KW  - Convolutional neural networks
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CHAP
AU  - Shankar, U.
TI  - Machine and Deep Learning Algorithms and Applications Uday Shankar Shanthamallu
PY  - 2021
T2  - Synthesis Lectures on Signal Processing
DO  - 10.2200/S01135ED1V01Y202109SPR022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122127802&doi=10.2200%2fS01135ED1V01Y202109SPR022&partnerID=40&md5=397cfc3609142ed547504d9ac41f5b56
AB  - This book introduces basic machine learning concepts and applications for a broad audience that includes students, faculty, and industry practitioners. We begin by describing how machine learning provides capabilities to computers and embedded systems to learn from data. A typical machine learning algorithm involves training, and generally the performance of a machine learning model improves with more training data. Deep learning is a sub-area of machine learning that involves extensive use of layers of artificial neural networks typically trained on massive amounts of data. Machine and deep learning methods are often used in contemporary data science tasks to address the growing data sets and detect, cluster, and classify data patterns. Although machine learning commercial interest has grown relatively recently, the roots of machine learning go back to decades ago. We note that nearly all organizations, including industry, government, defense, and health, are using machine learning to address a variety of needs and applications.The machine learning paradigms presented can be broadly divided into the following three categories: supervised learning, unsupervised learning, and semi-supervised learning. Supervised learning algorithms focus on learning a mapping function, and they are trained with supervision on labeled data. Supervised learning is further sub-divided into classification and regression algorithms. Unsupervised learning typically does not have access to ground truth, and often the goal is to learn or uncover the hidden pattern in the data. Through semi-supervised learning, one can effectively utilize a large volume of unlabeled data and a limited amount of labeled data to improve machine learning model performances. Deep learning and neural networks are also covered in this book. Deep neural networks have attracted a lot of interest during the last ten years due to the availability of graphics processing units (GPU) computational power, big data, and new software platforms. They have strong capabilities in terms of learning complex mapping functions for different types of data. We organize the book as follows. The book starts by introducing concepts in supervised, unsupervised, and semi-supervised learning. Several algorithms and their inner workings are presented within these three categories. We then continue with a brief introduction to artificial neural network algorithms and their properties. In addition, we cover an array of applications and provide extensive bibliography. The book ends with a summary of the key machine learning concepts. Table of Contents:Preface / Acknowledgments / Introduction to Machine Learning / Supervised Learning / Unsupervised Learning / Semi-Supervised Learning / Neural Networks and Deep Learning / Machine and Deep Learning Applications / Conclusion and Future Directions / Bibliography / Authors' Biographies  Copyright © 2021 by Morgan & Claypool.
KW  - artificial intelligence
KW  - big data
KW  - deep learning
KW  - Internet of things
KW  - machine learning
KW  - neural networks
KW  - signal processing
KW  - supervised learning
KW  - unsupervised learning
KW  - Big data
KW  - Classification (of information)
KW  - Computer graphics
KW  - Deep neural networks
KW  - Embedded systems
KW  - Graphics processing unit
KW  - Learning algorithms
KW  - Mapping
KW  - Multilayer neural networks
KW  - Program processors
KW  - Signal processing
KW  - Supervised learning
KW  - Deep learning
KW  - Embedded-system
KW  - Labeled data
KW  - Learn+
KW  - Machine learning algorithms
KW  - Machine learning models
KW  - Mapping functions
KW  - Neural-networks
KW  - Signal-processing
KW  - Three categories
KW  - Internet of things
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Choi, K.
AU  - Oh, B.-S.
AU  - Yu, S.
TI  - Memory access minimization for mean-shift tracking in mobile devices
PY  - 2021
T2  - Multimedia Tools and Applications
DO  - 10.1007/s11042-020-09364-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088828559&doi=10.1007%2fs11042-020-09364-w&partnerID=40&md5=341c589bcad54222361ca67ea384ae46
AB  - Due to the development of artificial intelligence and computer vision technology, many autonomous drones have been studied. However, computer vision technology requires high performance CPU due to its high complexity, and battery consumption is so high that drones are constrained to fly for a long time. Therefore, low-power mobile devices require tracking algorithms that minimize battery consumption. In this paper, we propose a mean-shift based tracking algorithm that minimizes memory access to reduce battery consumption. To accomplish this, we minimize the number of memory accesses by using an algorithm that divides the direction of the mean-shift vector into eight, and calculates the sum of the density maps only for the new area without calculating the sum of the density maps for the already calculated area. It is possible to increase the calculation efficiency by lowering the memory access cost. Experimental results show that the proposed method is more efficient than the existing method. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
KW  - Mean shift
KW  - Mobile device
KW  - Object tracking
KW  - Artificial intelligence
KW  - Computer vision
KW  - Drones
KW  - Electric batteries
KW  - Tracking (position)
KW  - Battery consumption
KW  - Calculation efficiency
KW  - Computer vision technology
KW  - High complexity
KW  - Mean shift tracking
KW  - Mean shift vector
KW  - Memory access
KW  - Tracking algorithm
KW  - Memory architecture
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kwak, D.-H.
AU  - Son, G.-J.
AU  - Park, M.-K.
AU  - Kim, Y.-D.
TI  - Rapid foreign object detection system on seaweed using vnir hyperspectral imaging
PY  - 2021
T2  - Sensors
DO  - 10.3390/s21165279
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111777131&doi=10.3390%2fs21165279&partnerID=40&md5=66a586e80cce9c1dceb31661af537a33
AB  - The consumption of seaweed is increasing year by year worldwide. Therefore, the foreign object inspection of seaweed is becoming increasingly important. Seaweed is mixed with various materials such as laver and sargassum fusiforme. So it has various colors even in the same seaweed. In addition, the surface is uneven and greasy, causing diffuse reflections frequently. For these reasons, it is difficult to detect foreign objects in seaweed, so the accuracy of conventional foreign object detectors used in real manufacturing sites is less than 80%. Supporting real‐time inspection should also be considered when inspecting foreign objects. Since seaweed requires mass production, rapid inspection is essential. However, hyperspectral imaging techniques are generally not suitable for high‐speed inspection. In this study, we overcome this limitation by using dimensionality reduction and using simplified operations. For accuracy improvement, the proposed algorithm is carried out in 2 stages. Firstly, the subtraction method is used to clearly distinguish seaweed and conveyor belts, and also detect some relatively easy to detect foreign objects. Secondly, a standardization inspection is performed based on the result of the subtraction method. During this process, the proposed scheme adopts simplified and burdenless calculations such as subtraction, division, and one‐by‐one matching, which achieves both accuracy and low latency performance. In the experiment to evaluate the performance, 60 normal seaweeds and 60 seaweeds containing foreign objects were used, and the accuracy of the proposed algorithm is 95%. Finally, by implementing the proposed algorithm as a foreign object detection platform, it was confirmed that real‐time operation in rapid inspection was possible, and the possibility of deployment in real manufacturing sites was confirmed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Foreign object detection
KW  - Hyperspectral imaging
KW  - Seaweed
KW  - Signal processing
KW  - Spectroscopy
KW  - Visible and near‐infrared
KW  - Algorithms
KW  - Foreign Bodies
KW  - Hyperspectral Imaging
KW  - Seaweed
KW  - Vegetables
KW  - Belt conveyors
KW  - Dimensionality reduction
KW  - Hyperspectral imaging
KW  - Inspection
KW  - Object recognition
KW  - Seaweed
KW  - Spectroscopy
KW  - Accuracy Improvement
KW  - Conveyor belts
KW  - Diffuse reflection
KW  - Foreign object
KW  - Manufacturing sites
KW  - Mass production
KW  - Sargassum fusiforme
KW  - Subtraction method
KW  - algorithm
KW  - foreign body
KW  - seaweed
KW  - vegetable
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, J.
AU  - Zhang, L.
AU  - Liu, T.
AU  - Wang, Y.
TI  - YOLSO: You Only Look Small Object
PY  - 2021
T2  - Journal of Visual Communication and Image Representation
DO  - 10.1016/j.jvcir.2021.103348
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117947649&doi=10.1016%2fj.jvcir.2021.103348&partnerID=40&md5=a3b3c1c0e7af637130e7cd0c657d2674
AB  - Small object detection is challenging and far from satisfactory. Most general object detectors suffer from two critical issues with small objects: (1) Feature extractor based on classification network cannot express the characteristics of small objects reasonably due to insufficient appearance information of targets and a large amount of background interference around them. (2) The detector requires a much higher location accuracy for small objects than for general objects. This paper proposes an effective and efficient small object detector YOLSO to address the above problems. For feature representation, we analyze the drawbacks in previous backbones and present a Half-Space Shortcut(HSSC) module to build a background-aware backbone. Furthermore, a coarse-to-fine Feature Pyramid Enhancement(FPE) module is introduced for layer-wise aggregation at a granular level to enhance the semantic discriminability. For loss function, we propose an exponential L1 loss to promote the convergence of regression, and a focal IOU loss to focus on prime samples with high classification confidence and high IOU. Both of them significantly improves the location accuracy of small objects. The proposed YOLSO sets state-of-the-art results on two typical small object datasets, MOCOD and VeDAI, at a speed of over 200 FPS. In the meantime, it also outperforms the baseline YOLOv3 by a wide margin on the common COCO dataset. © 2021 Elsevier Inc.
KW  - Accurate location
KW  - Background-aware
KW  - Granular feature aggregation
KW  - High speed
KW  - Small object detection
KW  - Classification (of information)
KW  - Feature extraction
KW  - Geometry
KW  - Location
KW  - Object recognition
KW  - Semantics
KW  - Accurate location
KW  - Background-aware
KW  - Detector suffers
KW  - Feature aggregation
KW  - Granular feature aggregation
KW  - High Speed
KW  - Location accuracy
KW  - Object detectors
KW  - Small object detection
KW  - Small objects
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Bhowmick, A.
AU  - Saharia, S.
AU  - Hazarika, S.M.
TI  - Non-parametric scene parsing: Label transfer methods and datasets
PY  - 2022
T2  - Computer Vision and Image Understanding
DO  - 10.1016/j.cviu.2022.103418
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128685236&doi=10.1016%2fj.cviu.2022.103418&partnerID=40&md5=85e79a08d2a0f11e4dd45421b07c41a4
AB  - Scene parsing is the problem of densely labeling every pixel in an image with a meaningful class label. Driven by powerful methods, remarkable progress has been achieved in scene parsing over a short period of time. With growing data, non-parametric scene parsing or label transfer approach has emerged as an exciting and rapidly growing research area within Computer Vision. This paper constitutes a first survey examining label transfer methods through the lens of non-parametric, data-driven philosophy. We provide insights on non-parametric system design and its working stages, i.e. algorithmic components such as scene retrieval, scene correspondence, contextual smoothing, etc. We propose a synthetic categorization of all the major existing methods, discuss the necessary background, the design choices, followed by an overview of the shortcomings and challenges for a better understanding of label transfer. In addition, we introduce the existing standard benchmark datasets, the evaluation metrics, and the comparisons of model-based and data-driven methods. Finally, we provide our recommendations and discuss the current challenges and promising research directions in the field. © 2022 Elsevier Inc.
KW  - Label transfer
KW  - Non-parametric
KW  - Scene parsing
KW  - Class labels
KW  - Data driven
KW  - Label transfer
KW  - Labelings
KW  - Nonparametrics
KW  - Research areas
KW  - Scene parsing
KW  - Short periods
KW  - Through the lens
KW  - Transfer method
KW  - Search engines
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Camara, F.
AU  - Bellotto, N.
AU  - Cosar, S.
AU  - Nathanael, D.
AU  - Althoff, M.
AU  - Wu, J.
AU  - Ruenz, J.
AU  - Dietrich, A.
AU  - Fox, C.W.
TI  - Pedestrian Models for Autonomous Driving Part I: Low-Level Models, from Sensing to Tracking
PY  - 2021
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2020.3006768
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116912007&doi=10.1109%2fTITS.2020.3006768&partnerID=40&md5=ef52a5dd15686c425173ebb9bfe59ae4
AB  - Autonomous vehicles (AVs) must share space with pedestrians, both in carriageway cases such as cars at pedestrian crossings and off-carriageway cases such as delivery vehicles navigating through crowds on pedestrianized high-streets. Unlike static obstacles, pedestrians are active agents with complex, interactive motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behavior as well as detecting and tracking them. This narrative review article is Part I of a pair, together surveying the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low-level image detection to high-level psychology models, from the perspective of an AV designer. This self-contained Part I covers the lower levels of this stack, from sensing, through detection and recognition, up to tracking of pedestrians. Technologies at these levels are found to be mature and available as foundations for use in high-level systems, such as behavior modelling, prediction and interaction control.  © 2000-2011 IEEE.
KW  - autonomous vehicles
KW  - datasets
KW  - detection
KW  - eHMI
KW  - game-theoretic models
KW  - microscopic and macroscopic behavior models
KW  - pedestrian interaction
KW  - pedestrians
KW  - Review
KW  - sensing
KW  - signaling models
KW  - survey
KW  - tracking
KW  - trajectory prediction
KW  - Autonomous vehicles
KW  - Game theory
KW  - Pedestrian safety
KW  - Autonomous Vehicles
KW  - Behaviour models
KW  - Dataset
KW  - Detection
KW  - EHMI
KW  - Game-theoretic model
KW  - Macroscopic behaviors
KW  - Microscopic and macroscopic behavior model
KW  - Microscopic behavior
KW  - Pedestrian
KW  - Pedestrian interaction
KW  - Signaling model
KW  - Tracking
KW  - Trajectory prediction
KW  - Surveys
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Fernandes, D.
AU  - Silva, A.
AU  - Névoa, R.
AU  - Simões, C.
AU  - Gonzalez, D.
AU  - Guevara, M.
AU  - Novais, P.
AU  - Monteiro, J.
AU  - Melo-Pinto, P.
TI  - Point-cloud based 3D object detection and classification methods for self-driving applications: A survey and taxonomy
PY  - 2021
T2  - Information Fusion
DO  - 10.1016/j.inffus.2020.11.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097174674&doi=10.1016%2fj.inffus.2020.11.002&partnerID=40&md5=2edc0aee0430be2aefa746c09cdd7ade
AB  - Autonomous vehicles are becoming central for the future of mobility, supported by advances in deep learning techniques. The performance of aself-driving system is highly dependent on the quality of the perception task. Developments in sensor technologies have led to an increased availability of 3D scanners such as LiDAR, allowing for a more accurate representation of the vehicle's surroundings, leading to safer systems. The rapid development and consequent rise of research studies around self-driving systems since early 2010, resulted in a tremendous increase in the number and novelty of object detection methods. After the first wave of works that essentially tried to expand known techniques from object detection in images, more recently there has been a notable development in newer and more adapted to LiDAR data works. This paper addresses the existing literature on object detection using LiDAR data within the scope of self-driving and brings a systematic way for analysing it. Unlike general object detection surveys, we will focus on point-cloud data, which presents specific challenges, notably its high-dimensional and sparse nature. This work introduces a common object detection pipeline and taxonomy to facilitate a thorough comparison between different techniques and, departing from it, this work will critically examine the representation of data (critical for complexity reduction), feature extraction and finally the object detection models. A comparison between performance results of the different models is included, alongside with some future research challenges. © 2020 Elsevier B.V.
KW  - 3D object detection models
KW  - Autonomous vehicles
KW  - Computer vision
KW  - Deep learning
KW  - LiDAR
KW  - Perception
KW  - Deep learning
KW  - Feature extraction
KW  - Image classification
KW  - Object recognition
KW  - Optical radar
KW  - Surveys
KW  - Taxonomies
KW  - Classification methods
KW  - Complexity reduction
KW  - High-dimensional
KW  - Learning techniques
KW  - Object detection method
KW  - Point cloud data
KW  - Research challenges
KW  - Sensor technologies
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - You, S.
AU  - Ji, Y.
AU  - Liu, S.
AU  - Mei, C.
AU  - Yao, X.
AU  - Feng, Y.
TI  - A Thermal Infrared Pedestrian-Detection Method for Edge Computing Devices
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22176710
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137570075&doi=10.3390%2fs22176710&partnerID=40&md5=edff602f4c4b7748cc69f8fbb43f60a8
AB  - The thermal imaging pedestrian-detection system has excellent performance in different lighting scenarios, but there are problems regarding weak texture, object occlusion, and small objects. Meanwhile, large high-performance models have higher latency on edge devices with limited computing power. To solve the above problems, in this paper, we propose a real-time thermal imaging pedestrian-detection method for edge computing devices. Firstly, we utilize multi-scale mosaic data augmentation to enhance the diversity and texture of objects, which alleviates the impact of complex environments. Then, the parameter-free attention mechanism is introduced into the network to enhance features, which barely increases the computing cost of the network. Finally, we accelerate multi-channel video detection through quantization and multi-threading techniques on edge computing devices. Additionally, we create a high-quality thermal infrared dataset to facilitate the research. The comparative experiments on the self-built dataset, YDTIP, and three public datasets, with other methods show that our method also has certain advantages. © 2022 by the authors.
KW  - attention mechanism
KW  - data augmentation
KW  - pedestrian detection
KW  - real-time
KW  - thermal infrared images
KW  - Infrared imaging
KW  - Infrared radiation
KW  - Textures
KW  - Attention mechanisms
KW  - Computing devices
KW  - Data augmentation
KW  - Detection methods
KW  - Edge computing
KW  - Pedestrian detection
KW  - Real- time
KW  - Thermal infrared images
KW  - Thermal-imaging
KW  - Thermal-infrared
KW  - article
KW  - attention
KW  - controlled study
KW  - human
KW  - pedestrian
KW  - quantization
KW  - thermography
KW  - Edge computing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Billah, A.Md.
AU  - Faruque, I.A.
TI  - Bioinspired visuomotor feedback in a multiagent group/swarm context
PY  - 2021
T2  - IEEE Transactions on Robotics
DO  - 10.1109/TRO.2020.3033703
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098758879&doi=10.1109%2fTRO.2020.3033703&partnerID=40&md5=3bb3ce1bb925cb309fd672beafae52b1
AB  - Practical aerial swarm applications are limited by the need to provide fast, robust feedback. To consider how biological agents incorporate visual feedback in multiagent environments, this study extends a well-developed model of bioinspired visual feedback for individual agent to feedback in multiagent dynamic environments. A multiagent visual model is developed and analyzed for an individual agent operating in a group of other agents. The theoretical model indicates a stable equilibrium trajectory within the neighboring agents. When a minority of agents conducting bioinspired visual feedback are introduced into a planar swarm simulation, results show robustness of agents to perturbations from reference condition. The bioinspired feedback rule is implemented experimentally using ground microbots for two cases identified by a theoretical observability condition. The experimental performance of the single agent in an established swarm is consistent with the theoretical stability and observability predictions. The results indicate insect visuomotor response can help participate in established swarms, and indicate the importance of optic flow input in maintaining group motion. © 2020 IEEE.
KW  - Aerospace control
KW  - Autonomous vehicle navigation
KW  - Biologically-inspired robots
KW  - Feedback
KW  - Multi-agent systems
KW  - Optical control
KW  - Swarms
KW  - Visual servoing
KW  - Agricultural robots
KW  - Antennas
KW  - Multi agent systems
KW  - Observability
KW  - Visual communication
KW  - Biological agents
KW  - Developed model
KW  - Dynamic environments
KW  - Individual agent
KW  - Multi-agent environment
KW  - Reference condition
KW  - SWARM simulation
KW  - Theoretical modeling
KW  - Feedback
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Mirugwe, A.
AU  - Nyirenda, J.
AU  - Dufourq, E.
TI  - Automating Bird Detection Based on Webcam Captured Images using Deep Learning
PY  - 2022
T2  - EPiC Series in Computing
DO  - 10.29007/9fr5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135805420&doi=10.29007%2f9fr5&partnerID=40&md5=caa3fd2bfcc53761a656ea008cdf028a
AB  - One of the most challenging problems faced by ecologists and other biological researchers today is to analyze the massive amounts of data being collected by advanced monitoring systems like camera traps, wireless sensor networks, high-frequency radio trackers, global positioning systems, and satellite tracking systems being used today. It has become expensive, laborious, and time-consuming to analyze this huge data using manual and traditional statistical techniques. Recent developments in the deep learning field are showing promising results towards automating the analysis of these extremely large datasets. The primary objective of this study was to test the capabilities of the state-of-the-art deep learning architectures to detect birds in the webcam captured images. A total of 10592 images were collected for this study from the Cornell Lab of Ornithology live stream feeds situated in six unique locations in United States, Ecuador, New Zealand, and Panama. To achieve the main objective of the study, we studied and evaluated two convolutional neural network object detection meta-architectures, single-shot detector (SSD) and Faster R-CNN in combination with MobileNet-V2, ResNet50, ResNet101, ResNet152, and Inception ResNet-V2 feature extractors. Through transfer learning, all the models were initialized using weights pre-trained on the MS COCO (Microsoft Common Objects in Context) dataset provided by TensorFlow 2 object detection API. The Faster R-CNN model coupled with ResNet152 outperformed all other models with a mean average precision of 92.3%. However, the SSD model with the MobileNet-V2 feature extraction network achieved the lowest inference time (110ms) and the smallest memory capacity (30.5MB) compared to its counterparts. The outstanding results achieved in this study confirm that deep learning-based algorithms are capable of detecting birds of different sizes in different environments and the best model could potentially help ecologists in monitoring and identifying birds from other species. © 2022, EasyChair. All rights reserved.
KW  - Faster R-CNN
KW  - ResNets Deep Learning
KW  - Single-Shot Detector
KW  - Transfer Learning
KW  - Birds
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Feature extraction
KW  - Large dataset
KW  - Multilayer neural networks
KW  - Network architecture
KW  - Transfer learning
KW  - Wireless sensor networks
KW  - Advanced monitoring
KW  - Bird detection
KW  - Fast R-CNN
KW  - Monitoring system
KW  - Objects detection
KW  - Resnet deep learning
KW  - Single-shot
KW  - Single-shot detector
KW  - Transfer learning
KW  - WebCams
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Vierhauser, M.
AU  - Bayley, S.
AU  - Wyngaard, J.
AU  - Xiong, W.
AU  - Cheng, J.
AU  - Huseman, J.
AU  - Lutz, R.
AU  - Cleland-Huang, J.
TI  - Interlocking Safety Cases for Unmanned Autonomous Systems in Shared Airspaces
PY  - 2021
T2  - IEEE Transactions on Software Engineering
DO  - 10.1109/TSE.2019.2907595
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106064142&doi=10.1109%2fTSE.2019.2907595&partnerID=40&md5=cc7a8990a00f1123df18a86b3475479e
AB  - The growing adoption of unmanned aerial vehicles (UAVs) for tasks such as eCommerce, aerial surveillance, and environmental monitoring introduces the need for new safety mechanisms in an increasingly cluttered airspace. In our work we thus emphasize safety issues that emerge at the intersection of infrastructures responsible for controlling the airspace, and the diverse UAVs operating in their space. We build on safety assurance cases (SAC) - a state-of-the-art solution for reasoning about safety - and propose a novel approach based on interlocking SACs. The infrastructure safety case (ISAC) specifies assumptions upon UAV behavior, while each UAV demonstrates compliance to the ISAC by presenting its own (pluggable) safety case (pSAC) which connects to the ISAC through a set of interlock points. To collect information on each UAV we enforce a 'trust but monitor' policy, supported by runtime monitoring and an underlying reputation model. We evaluate our approach in three ways: first by developing ISACs for two UAV infrastructures, second by running simulations to evaluate end-to-end effectiveness, and finally via an outdoor field-study with physical UAVs. The results show that interlocking SACs can be effective for identifying, specifying, and monitoring safety-related constraints upon UAVs flying in a controlled airspace.  © 1976-2012 IEEE.
KW  - monitoring
KW  - safety assurance cases
KW  - UAV
KW  - unmanned autonomous systems
KW  - Antennas
KW  - Safety engineering
KW  - Aerial surveillance
KW  - Autonomous systems
KW  - Environmental Monitoring
KW  - Reputation modeling
KW  - Running simulations
KW  - Runtime Monitoring
KW  - Safety mechanisms
KW  - State of the art
KW  - Unmanned aerial vehicles (UAV)
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ristea, N.-C.
AU  - Anghel, A.
AU  - Ionescu, R.T.
TI  - Estimating the Magnitude and Phase of Automotive Radar Signals under Multiple Interference Sources with Fully Convolutional Networks
PY  - 2021
T2  - IEEE Access
DO  - 10.1109/ACCESS.2021.3128151
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534731&doi=10.1109%2fACCESS.2021.3128151&partnerID=40&md5=2f70ad174cf6416c9404b6aba762b54d
AB  - Radar sensors are gradually becoming a wide-spread equipment for road vehicles, playing a crucial role in autonomous driving and road safety. The broad adoption of radar sensors increases the chance of interference among sensors from different vehicles, generating corrupted range profiles and range-Doppler maps. In order to extract distance and velocity of multiple targets from range-Doppler maps, the interference affecting each range profile needs to be mitigated. In this paper, we propose a fully convolutional neural network for automotive radar interference mitigation. In order to train our network in a real-world scenario, we introduce a new data set of realistic automotive radar signals with multiple targets and multiple interferers. To our knowledge, we are the first to apply weight pruning in the automotive radar domain, obtaining superior results compared to the widely-used dropout. While most previous works successfully estimated the magnitude of automotive radar signals, we propose a deep learning model that can accurately estimate the phase. For instance, our novel approach reduces the phase estimation error with respect to the commonly-adopted zeroing technique by half, from 12.55 degrees to 6.58 degrees. Considering the lack of databases for automotive radar interference mitigation, we release as open source our large-scale data set that closely replicates the real-world automotive scenario for multiple interference cases, allowing others to objectively compare their future work in this domain. Our data set is available for download at: http://github.com/ristea/arim-v2.  © 2013 IEEE.
KW  - automotive radar
KW  - Autonomous driving
KW  - deep learning
KW  - fully convolutional networks
KW  - interference mitigation
KW  - phase estimation
KW  - Automotive radar
KW  - Autonomous vehicles
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Motor transportation
KW  - Radar interference
KW  - Radar measurement
KW  - Roads and streets
KW  - Automotive radar
KW  - Autonomous driving
KW  - Convolutional networks
KW  - Deep learning
KW  - Fully convolutional network
KW  - Interference mitigation
KW  - Phase-estimation
KW  - Radar sensors
KW  - Radar signals
KW  - Range-profiles
KW  - Convolution
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Taghizadeh, M.
AU  - Chalechale, A.
TI  - A comprehensive and systematic review on classical and deep learning based region proposal algorithms
PY  - 2022
T2  - Expert Systems with Applications
DO  - 10.1016/j.eswa.2021.116105
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118832919&doi=10.1016%2fj.eswa.2021.116105&partnerID=40&md5=7dd9eb4bbf0a1a5bd0157f4b1ecec30f
AB  - Development of region proposal algorithms has rapidly become one of the most critical research areas over recent years. The perfect accuracy of region-based recognition techniques has led to the use of proposal algorithms as an imperative core in various recognition problems. The main purpose of these algorithms is to extract effective regions of an image with an appropriate number that will reduce the search space and increase detection accuracy. The early development of these algorithms was based on a set of hand-crafted features. Recently, with advances in deep learning techniques, they have been widely and successfully applied to the region proposals. This paper reviews region proposal algorithms, theory, and evaluation metrics and also addresses the existing challenges. In addition, we present a classification for generating proposals, including classical and advanced methods based on hand-crafted features and deep learning, respectively. Both categories are described in details, and an extensive review of recent works is presented. The proposal improvement methods, including ranking algorithms, are also described. In total, more than 60 different algorithms have been studied and classified, and we also point out several applications based on region proposals. © 2021 Elsevier Ltd
KW  - Deep learning
KW  - Ranking
KW  - Region proposal
KW  - Region proposal network
KW  - Segmentation
KW  - Image segmentation
KW  - Critical researches
KW  - Deep learning
KW  - Proposal algorithm
KW  - Ranking
KW  - Region proposal
KW  - Region proposal network
KW  - Region-based
KW  - Research areas
KW  - Segmentation
KW  - Systematic Review
KW  - Deep learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Reyes‐muñoz, A.
AU  - Guerrero‐ibáñez, J.
TI  - Vulnerable Road Users and Connected Autonomous Vehicles Interaction: A Survey
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22124614
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132119538&doi=10.3390%2fs22124614&partnerID=40&md5=549b0d5f8003ccf9a108204cd61a3be6
AB  - There is a group of users within the vehicular traffic ecosystem known as Vulnerable Road Users (VRUs). VRUs include pedestrians, cyclists, motorcyclists, among others. On the other hand, connected autonomous vehicles (CAVs) are a set of technologies that combines, on the one hand, communication technologies to stay always ubiquitous connected, and on the other hand, automated technologies to assist or replace the human driver during the driving process. Autonomous vehicles are being visualized as a viable alternative to solve road accidents providing a general safe environment for all the users on the road specifically to the most vulnerable. One of the problems facing autonomous vehicles is to generate mechanisms that facilitate their integration not only within the mobility environment, but also into the road society in a safe and efficient way. In this paper, we analyze and discuss how this integration can take place, reviewing the work that has been developed in recent years in each of the stages of the vehicle‐human interaction, analyzing the challenges of vulnerable users and proposing solutions that contribute to solving these challenges. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - automated vehicles
KW  - connected vehicles
KW  - deep learning
KW  - machine learning
KW  - pedestrians
KW  - Accidents, Traffic
KW  - Autonomous Vehicles
KW  - Bicycling
KW  - Ecosystem
KW  - Humans
KW  - Pedestrians
KW  - Accidents
KW  - Deep learning
KW  - Motor transportation
KW  - Pedestrian safety
KW  - Roads and streets
KW  - Surveys
KW  - Vehicle to vehicle communications
KW  - Automated technology
KW  - Automated vehicles
KW  - Autonomous Vehicles
KW  - Communicationtechnology
KW  - Connected vehicle
KW  - Deep learning
KW  - Machine-learning
KW  - Pedestrian
KW  - Road users
KW  - Vehicle interactions
KW  - cycling
KW  - ecosystem
KW  - human
KW  - pedestrian
KW  - prevention and control
KW  - traffic accident
KW  - Autonomous vehicles
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Song, Y.
AU  - Xie, Z.
AU  - Wang, X.
AU  - Zou, Y.
TI  - MS-YOLO: Object Detection Based on YOLOv5 Optimized Fusion Millimeter-Wave Radar and Machine Vision
PY  - 2022
T2  - IEEE Sensors Journal
DO  - 10.1109/JSEN.2022.3167251
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128676237&doi=10.1109%2fJSEN.2022.3167251&partnerID=40&md5=2956271dac5e77a63786f3278c3786c8
AB  - Millimeter-wave radar and machine vision are both important means for intelligent vehicles to perceive the surrounding environment. Aiming at the problem of multi-sensor fusion, this paper proposes the object detection method of millimeter-wave radar and vision fusion. Radar and camera complement each other, and radar data fusion in machine vision network can effectively reduce the rate of missed detection under insufficient light conditions, and improve the accuracy of remote small object detection. The radar information is processed by mapping transformation neural network to obtain the mask map, so that radar information and visual information in the same scale. A multi-data source deep learning object detection network (MS-YOLO) based on millimeter-wave radar and vision fusion was proposed. Homemade datasets were used for training and testing. This maximized the use of sensor information and improved the detection accuracy under the premise of ensuring the detection speed. Compared with the original YOLOv5 (the fifth version of the You Only Look Once) network, the results show that the MS-YOLO network meets the accuracy requirements better. Among the models, the large model of MS-YOLO has the highest accuracy with an mAP reaching 0.888. The small model of MS-YOLO has good accuracy and speed, and the mAP reaches 0.841 while maintaining a high frame rate of 65 fps.  © 2001-2012 IEEE.
KW  - deep learning
KW  - MS-YOLO
KW  - multi-sensor fusion
KW  - object detection
KW  - Computer vision
KW  - Deep learning
KW  - Millimeter waves
KW  - Object recognition
KW  - Radar equipment
KW  - Sensor data fusion
KW  - Tracking radar
KW  - Deep learning
KW  - Intelligent sensors
KW  - Machine-vision
KW  - Millimeter-wave radar
KW  - Millimetre-wave radar
KW  - MS-YOLO
KW  - Multi-sensor fusion
KW  - Radar detection
KW  - Sensor phenomenon and characterizations
KW  - Wave machine
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Akshatha, K.R.
AU  - Karunakar, A.K.
AU  - Satish Shenoy, B.
TI  - Human Classification in Aerial Images Using Convolutional Neural Networks
PY  - 2022
T2  - Smart Innovation, Systems and Technologies
DO  - 10.1007/978-981-16-7996-4_39
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125288277&doi=10.1007%2f978-981-16-7996-4_39&partnerID=40&md5=8e6548230c18e100a4fb1ee7ba14e4b7
AB  - Automatic detection of people in aerial images has potential applications in traffic monitoring, surveillance, human behavior analysis, etc. However, developing an algorithm for detection of human locations in aerial images is challenging because of the small target size, cluttered background, and varying appearance of humans. Deep learning-based object detections frameworks internally use the standard convolutional neural network (CNN) based classifiers for feature extraction and classification. Though these pre-trained classifiers perform image classification tasks with very good accuracy, they are computationally complex and hence require huge computation time. In this work, we custom-designed CNN-based classifiers to perform the human classification in aerial images and compared the performance with the standard VGG-16 based human classifier. Custom-designed classifier with fewer number of layers achieved a reduced computation time while maintaining good accuracy. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.
KW  - Aerial images
KW  - Convolutional neural network
KW  - Human classification
KW  - Antennas
KW  - Behavioral research
KW  - Classification (of information)
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Feature extraction
KW  - Image classification
KW  - Object detection
KW  - Aerial images
KW  - Automatic Detection
KW  - Computation time
KW  - Convolutional neural network
KW  - Human behavior analysis
KW  - Human classification
KW  - Network-based
KW  - Small targets
KW  - Target size
KW  - Traffic monitoring
KW  - Convolution
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Markert, T.
AU  - Matich, S.
AU  - Neykov, D.
AU  - Muenig, M.
AU  - Theissler, A.
AU  - Atzmueller, M.
TI  - Visual Detection of Tiny and Transparent Objects for Autonomous Robotic Pick-and-Place Operations
PY  - 2022
T2  - IEEE International Conference on Emerging Technologies and Factory Automation, ETFA
DO  - 10.1109/ETFA52439.2022.9921555
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141395344&doi=10.1109%2fETFA52439.2022.9921555&partnerID=40&md5=024a91785ef7e5759328a7774e96cf35
AB  - For the manufacturing of miniature force/torque sensors, extreme accuracy is required due to the tiny size of the strain gauges inside the sensors (2×2.5 mm). The current method of manually assembling them by hand is difficult, time-intensive, and error-prone. To improve this, a system to pick up the tiny objects from a plate and place them on elementary cells is being devised using a 6-axis robot arm with custom end-effector and a camera with magnification lens. This paper focuses on the perception module by evaluating methods for detecting tiny and transparent objects and obtaining spatial information from 2D images. Additionally, it considers aspects of the camera-to-robot calibration process, which are necessary to transfer the accuracy of image recognition into the real world. An approach using image segmentation and blob detection is taken, precluding the need for machine learning models. This is possible due to the superb image quality achieved by the sufficiently advanced camera and lighting setup. As a conclusion, we propose a perception module, which is capable of pinpointing strain gauge positions within ±0.1 mm and can also recognize different types of components based on physical dimensions. Our end-to-end approach for automatic pick-and-place operations integrates the perception module, camera-to-robot calibration, and a last-minute correction routine, which ultimately leads to an overall positioning accuracy of ± 0.3 mm.  © 2022 IEEE.
KW  - factory automation
KW  - machine vision
KW  - micro assembly
KW  - pick-and-place systems
KW  - transparent object detection
KW  - Computer vision
KW  - End effectors
KW  - Factory automation
KW  - Image recognition
KW  - Image segmentation
KW  - Intelligent robots
KW  - Object detection
KW  - Strain gages
KW  - Machine-vision
KW  - Microassemblies
KW  - Objects detection
KW  - Pick and place
KW  - Pick-and-place system
KW  - Robot calibration
KW  - Strain-gages
KW  - Transparent object detection
KW  - Transparent objects
KW  - Visual detection
KW  - Cameras
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Patel, H.
AU  - Prajapati, K.
AU  - Sarvaiya, A.
AU  - Upla, K.
AU  - Raja, K.
AU  - Ramachandra, R.
AU  - Busch, C.
TI  - Depthwise Convolution For Compact Object Detector In nighttime Images
PY  - 2022
T2  - IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops
DO  - 10.1109/CVPRW56347.2022.00053
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137754353&doi=10.1109%2fCVPRW56347.2022.00053&partnerID=40&md5=01af5f1aeb1506933b8e4399609f7afd
AB  - Despite thermal imaging primarily used for nighttime surveillance, uniform temperature of object and background makes it difficult to acquire details in the scene being observed and thereby object detection. Further, thermal images collected over long distances degrade the spatial resolution of the acquired objects and so do the moving objects leading to noisy features. We present a computationally efficient object detection approach using Depthwise Deep Convolutional Neural Network (DDCNN) for detecting and classifying objects in nighttime images under low resolution. The Depthwise Convolution (DC) employed in the proposed approach minimises the network's computational complexity resulting in the lowest number of training parameters (i.e., 3M) as compared to the other existing state-of-the-art methods such as FRCNN (52M), SSD (24M) and YOLO-v3 (61M) parameters. Further, by introducing novel Tversky and Intersection over Union (IoU) loss functions into the compact architectural design, we improve nighttime object detection accuracy. The validity of the proposed model is assessed on numerous datasets such as FLIR, KAIST, MS, and our internal dataset having multiple objects in each image. The experimental results from the proposed method indicate both quantitative and qualitative improvements over the recent state-of-the-art methods for nighttime imaging. The proposed approach achieves a mean Average Precision (mAP) of 52.39% and a highest individual object detection accuracy of 72.70% accuracy for cars in nigh-time situations suggesting applications in real-time use cases. © 2022 IEEE.
KW  - Computer vision
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Infrared imaging
KW  - Object recognition
KW  - Compact objects
KW  - Detection accuracy
KW  - Moving objects
KW  - Object detectors
KW  - Objects detection
KW  - Spatial resolution
KW  - State-of-the-art methods
KW  - Thermal images
KW  - Thermal-imaging
KW  - Uniform temperature
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Xiao, W.
AU  - Liu, M.
AU  - Chen, X.
TI  - Research Status and Development Trend of Underground Intelligent Load-Haul-Dump Vehicle—A Comprehensive Review
PY  - 2022
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app12189290
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138600675&doi=10.3390%2fapp12189290&partnerID=40&md5=9d249a626542228e7b583999c7d7d426
AB  - The underground intelligent load-haul-dump vehicle (LHD) is a product of the deep integration of traditional LHD with information network technology, automatic controlling and artificial intelligence technology. It gathers the functions of environmental perception, autonomous driving and fault diagnosis in one machine and exhibits higher safety and greater efficiency than traditional LHD. Hence, it is a particularly important piece of underground mining equipment for building green, safe and smart mines. Taking the studies about intelligent LHD collected by CNKI and WOS databases from 1980 to 2022 as a sample data source, employing Citespace visual analysis software for key feature extraction from the documents, statistical analysis was conducted to clarify the current research progress and the frontier topics of the intelligent LHD academia in the past 40 years, in relation to the future development trends. The development history and application status of underground intelligent LHD was expounded in this article, summarizing the research status at home and abroad from four aspects: ore heap perception and modeling technology, trajectory planning method of bucket shoveling, autonomous navigation technology, real-time monitoring and intelligent fault diagnosis technology. The demerits and merits of the technologies were reviewed as well, with future developing and researching trends of the underground intelligent LHD concluded. © 2022 by the authors.
KW  - autonomous navigation
KW  - CiteSpace
KW  - fault diagnosis
KW  - heap perception
KW  - real-time monitoring
KW  - trajectory planning
KW  - underground intelligent LHD
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jabłoński, P.
AU  - Iwaniec, J.
AU  - Zabierowski, W.
TI  - Comparison of Pedestrian Detectors for LiDAR Sensor Trained on Custom Synthetic, Real and Mixed Datasets
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22187014
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138405256&doi=10.3390%2fs22187014&partnerID=40&md5=032ec2535e663829b5803c9d33805eaa
AB  - Deep learning algorithms for object detection used in autonomous vehicles require a huge amount of labeled data. Data collecting and labeling is time consuming and, most importantly, in most cases useful only for a single specific sensor application. Therefore, in the course of the research which is presented in this paper, the LiDAR pedestrian detection algorithm was trained on synthetically generated data and mixed (real and synthetic) datasets. The road environment was simulated with the application of the 3D rendering Carla engine, while the data for analysis were obtained from the LiDAR sensor model. In the proposed approach, the data generated by the simulator are automatically labeled, reshaped into range images and used as training data for a deep learning algorithm. Real data from Waymo open dataset are used to validate the performance of detectors trained on synthetic, real and mixed datasets. YOLOv4 neural network architecture is used for pedestrian detection from the LiDAR data. The goal of this paper is to verify if the synthetically generated data can improve the detector’s performance. Presented results prove that the YOLOv4 model trained on a custom mixed dataset achieved an increase in precision and recall of a few percent, giving an F1-score of 0.84. © 2022 by the authors.
KW  - ADAS
KW  - artificial data
KW  - Carla simulator
KW  - deep learning
KW  - LiDAR
KW  - object detection
KW  - Waymo open dataset
KW  - YOLOv4
KW  - Algorithms
KW  - Humans
KW  - Neural Networks, Computer
KW  - Pedestrians
KW  - Deep learning
KW  - Learning algorithms
KW  - Network architecture
KW  - Object recognition
KW  - Optical radar
KW  - Three dimensional computer graphics
KW  - ADAS
KW  - Artificial data
KW  - Carlum simulator
KW  - Deep learning
KW  - LiDAR
KW  - Objects detection
KW  - Pedestrian detection
KW  - Performance
KW  - Waymo open dataset
KW  - YOLOv4
KW  - algorithm
KW  - human
KW  - pedestrian
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Bratulescu, R.-A.
AU  - Vatasoiu, R.-I.
AU  - Suciu, G.
AU  - Mitroi, S.-A.
AU  - Vochin, M.-C.
AU  - Sachian, M.-A.
TI  - Object Detection in Autonomous Vehicles
PY  - 2022
T2  - International Symposium on Wireless Personal Multimedia Communications, WPMC
DO  - 10.1109/WPMC55625.2022.10014804
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147040169&doi=10.1109%2fWPMC55625.2022.10014804&partnerID=40&md5=d58b79aab344608ed7a621e1b105b236
AB  - In the coming years, autonomous driving will be the primary focus of the automobile industry. The great majority of accidents are caused by human mistakes, and autonomous cars can help to lower this number significantly, thus improving road safety. Object identification plays a critical part in autonomous vehicle driving, and deep learning techniques are used to implement it. YOLO is one of the most common methods for recognizing and identifying things that emerge on the road. Its popularity has developed as a result of its superior performance in terms of speed, high accuracy, and learning capabilities when compared to other object recognition approaches such as Retina-Net, fast R- CNN, and Single-Shot MultiBox Detection (SSD).  © 2022 IEEE.
KW  - autonomous vehicles
KW  - object detection
KW  - R-CNN
KW  - Retina-Net
KW  - SSD
KW  - YOLO
KW  - Accident prevention
KW  - Accidents
KW  - Automotive industry
KW  - Autonomous vehicles
KW  - Deep learning
KW  - Learning systems
KW  - Motor transportation
KW  - Object recognition
KW  - Roads and streets
KW  - Autonomous car
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Objects detection
KW  - R-CNN
KW  - Retina-net
KW  - Road safety
KW  - Single-shot
KW  - Single-shot multibox detection
KW  - YOLO
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Liang, T.
AU  - Bao, H.
AU  - Pan, W.
AU  - Fan, X.
AU  - Li, H.
TI  - DetectFormer: Category-Assisted Transformer for Traffic Scene Object Detection
PY  - 2022
T2  - Sensors
DO  - 10.3390/s22134833
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132843302&doi=10.3390%2fs22134833&partnerID=40&md5=b4aff40dfc69308200543c2d5f2a433c
AB  - Object detection plays a vital role in autonomous driving systems, and the accurate detection of surrounding objects can ensure the safe driving of vehicles. This paper proposes a category-assisted transformer object detector called DetectFormer for autonomous driving. The proposed object detector can achieve better accuracy compared with the baseline. Specifically, ClassDecoder is assisted by proposal categories and global information from the Global Extract Encoder (GEE) to improve the category sensitivity and detection performance. This fits the distribution of object categories in specific scene backgrounds and the connection between objects and the image context. Data augmentation is used to improve robustness and attention mechanism added in backbone network to extract channel-wise spatial features and direction information. The results obtained by benchmark experiment reveal that the proposed method can achieve higher real-time detection performance in traffic scenes compared with RetinaNet and FCOS. The proposed method achieved a detection performance of 97.6% and 91.4% in AP50 and AP75 on the BCTSDB dataset, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - autonomous driving
KW  - deep learning
KW  - object detection
KW  - transformer
KW  - Autonomous vehicles
KW  - Benchmarking
KW  - Deep learning
KW  - Object recognition
KW  - Autonomous driving
KW  - Deep learning
KW  - Detection performance
KW  - Driving systems
KW  - Object detectors
KW  - Objects detection
KW  - Safe driving
KW  - Scene object
KW  - Traffic scene
KW  - Transformer
KW  - article
KW  - attention
KW  - deep learning
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Nafea, M.M.
AU  - Tan, S.Y.
AU  - Jubair, M.A.
AU  - Abd, M.T.
TI  - A Review of Lightweight Object Detection Algorithms for Mobile Augmented Reality
PY  - 2022
T2  - International Journal of Advanced Computer Science and Applications
DO  - 10.14569/IJACSA.2022.0131162
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143865177&doi=10.14569%2fIJACSA.2022.0131162&partnerID=40&md5=056fa56818219188fbc74ec37285978a
AB  - Augmented Reality (AR) has led to several technologies being at the forefront of innovation and change in every sector and industry. Accelerated advances in Computer Vision (CV), AR, and object detection refined the process of analyzing and comprehending the environment. Object detection has recently drawn a lot of attention as one of the most fundamental and difficult computer vision topics. The traditional object detection techniques are fully computer-based and typically need massive Graphics Processing Unit (GPU) power, while they aren't usually real-time. However, an AR application required real-time superimposed digital data to enable users to improve their field of view. This paper provides a comprehensive review of most of the recent lightweight object detection algorithms that are suitable to be used in AR applications. Four sources including Web of Science, Scopus, IEEE Xplore, and ScienceDirect were included in this review study. A total of ten papers were discussed and analyzed from four perspectives: accuracy, speed, small object detection, and model size. Several interesting challenges are discussed as recommendations for future work in the object detection field. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.
KW  - Augmented reality (ar)
KW  - Computer vision (cv)
KW  - Non-graphics processing unit (non-gpu)
KW  - Object detection
KW  - Real time
KW  - Augmented reality
KW  - Computer graphics equipment
KW  - Computer vision
KW  - Object detection
KW  - Object recognition
KW  - Program processors
KW  - Signal detection
KW  - Augmented reality
KW  - Augmented reality applications
KW  - Computer vision
KW  - Digital datas
KW  - Mobile augmented reality
KW  - Non-graphic processing unit
KW  - Object detection algorithms
KW  - Objects detection
KW  - Power
KW  - Real- time
KW  - Graphics processing unit
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rampriya, R.S.
AU  - Suganya, R.
AU  - Nathan, S.
AU  - Perumal, P.S.
TI  - A Comparative Assessment of Deep Neural Network Models for Detecting Obstacles in the Real Time Aerial Railway Track Images
PY  - 2022
T2  - Applied Artificial Intelligence
DO  - 10.1080/08839514.2021.2018184
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122411221&doi=10.1080%2f08839514.2021.2018184&partnerID=40&md5=af1ccac851d028d146458200a19c9b90
AB  - Obstacles on the railway track leading to derailment accidents that cause significant damages to the railway in terms of killed and injuries over the years. Count of accident is increasing day by day due to its causes such as boulders on track, trees falling on the gauge, etc. Monitoring these events has been possible with humans working in railways. But when it comes to the real-time scenario, it turns to fatal work and requires more workers, particularly in a dangerous area. Also, this manual monitoring is not adequate to halt derailment accidents. In this perspective, railroad obstacle detection from aerial images has been growing as a trending research topic under artificial intelligence. Also, this mandates the assessment of familiar and latest deep neural network models such as CenterNet Hourglass, EfficientDet, Faster RCNN, SSD Mobile Net, SSD ResNet, and YOLO that detects the violator of accidents with the aid of our own developed Rail Obstacle Detection Dataset (RODD). These detectors were implemented on real-time aerial railway track images captured by Unmanned Aerial Vehicle (UAV) in India. Initially, the input images in the collected datasets were undergone to data preprocessing after that; the above mentioned deep neural models were trained individually. After that, the experiment is analyzed based on training, time, and performance metrics. At last, the results are visualized, evaluated, and compared; hence based on the performance, some effective deep neural network models have identified for detecting obstacles. The result shows that SSD Mobile Net and Faster RCNN can be used for railroad obstacle detection even in the different lighting conditions in railway with the accuracy of 96.75% and 84.75%, respectively. © 2022 The Author(s). Published with license by Taylor & Francis Group, LLC.
KW  - Aircraft detection
KW  - Antennas
KW  - Deep neural networks
KW  - Derailments
KW  - Neural network models
KW  - Obstacle detectors
KW  - Railroad tracks
KW  - Railroad transportation
KW  - Rails
KW  - Unmanned aerial vehicles (UAV)
KW  - Aerial images
KW  - Comparative assessment
KW  - Dangerous area
KW  - Manual monitoring
KW  - Mobile nets
KW  - Neural network model
KW  - Obstacles detection
KW  - Railway track
KW  - Real- time
KW  - Workers'
KW  - Railroads
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Carranza-García, M.
AU  - Galán-Sales, F.J.
AU  - Luna-Romera, J.M.
AU  - Riquelme, J.C.
TI  - Object detection using depth completion and camera-LiDAR fusion for autonomous driving
PY  - 2022
T2  - Integrated Computer-Aided Engineering
DO  - 10.3233/ICA-220681
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133274406&doi=10.3233%2fICA-220681&partnerID=40&md5=f5ea75aaa5f2aa9ab57e445accd3fd2b
AB  - Autonomous vehicles are equipped with complimentary sensors to perceive the environment accurately. Deep learning models have proven to be the most effective approach for computer vision problems. Therefore, in autonomous driving, it is essential to design reliable networks to fuse data from different sensors. In this work, we develop a novel data fusion architecture using camera and LiDAR data for object detection in autonomous driving. Given the sparsity of LiDAR data, developing multi-modal fusion models is a challenging task. Our proposal integrates an efficient LiDAR sparse-to-dense completion network into the pipeline of object detection models, achieving a more robust performance at different times of the day. The Waymo Open Dataset has been used for the experimental study, which is the most diverse detection benchmark in terms of weather and lighting conditions. The depth completion network is trained with the KITTI depth dataset, and transfer learning is used to obtain dense maps on Waymo. With the enhanced LiDAR data and the camera images, we explore early and middle fusion approaches using popular object detection models. The proposed data fusion network provides a significant improvement compared to single-modal detection at all times of the day, and outperforms previous approaches that upsample depth maps with classical image processing algorithms. Our multi-modal and multi-source approach achieves a 1.5, 7.5, and 2.1 mean AP increase at day, night, and dawn/dusk, respectively, using four different object detection meta-architectures.  © 2022 - IOS Press. All rights reserved.
KW  - Autonomous driving
KW  - data fusion
KW  - deep learning
KW  - object detection
KW  - transfer learning
KW  - Autonomous vehicles
KW  - Cameras
KW  - Convolutional neural networks
KW  - Data fusion
KW  - Deep learning
KW  - Image enhancement
KW  - Network architecture
KW  - Object recognition
KW  - Optical radar
KW  - Transfer learning
KW  - Autonomous driving
KW  - Autonomous Vehicles
KW  - Computer vision problems
KW  - Deep learning
KW  - Detection models
KW  - Effective approaches
KW  - Learning models
KW  - Objects detection
KW  - Reliable Networks
KW  - Transfer learning
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CHAP
AU  - Nikolenko, S.I.
TI  - Synthetic data for deep learning
PY  - 2021
T2  - Springer Optimization and Its Applications
DO  - 10.1007/978-3-030-75178-4_1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111984929&doi=10.1007%2f978-3-030-75178-4_1&partnerID=40&md5=d9bc097816634c367422da230a697d49
M3  - Book chapter
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, J.
AU  - Chai, W.
AU  - Venkatachalapathy, A.
AU  - Tan, K.L.
AU  - Haghighat, A.
AU  - Velipasalar, S.
AU  - Adu-Gyamfi, Y.
AU  - Sharma, A.
TI  - A Survey on Driver Behavior Analysis From In-Vehicle Cameras
PY  - 2022
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2021.3126231
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136201736&doi=10.1109%2fTITS.2021.3126231&partnerID=40&md5=0d5924b070155c24cef0ed878023fafb
AB  - Distracted or drowsy driving is unsafe driving behavior responsible for thousands of crashes every year. Studying driver behavior has challenges associated with observing drivers in their natural environment. The naturalistic driving study (NDS) has become the most sought-after approach, since it eliminates the bias of a controlled setup, allowing researchers to understand drivers' behavior in real-world scenarios. Video recordings collected in NDS research are incredibly insightful in identifying driver errors. Computer vision techniques have been used to autonomously analyze video data and classify drivers' behavior. While computer vision scientists focus on image analytics, NDS researchers are interested in the factors impacting driver behavior. This survey paper makes a concerted effort to serve both communities by comprehensively reviewing studies, describing their data collection, computer vision techniques implemented, and performance in classifying driver behavior. The scope is limited to studies employing at least one camera observing the driver inside a vehicle. Based on their objective, papers have been classified as detecting low-level (e.g. head orientation) or high-level (e.g. distraction detection) driver information. Papers have been further classified based on the datasets they employ. In addition to twelve public datasets, many private datasets have also been identified, and their data collection design is discussed to highlight any impact on model performance. Across each task, algorithms employed and their performance are discussed to establish a baseline. A comparison of different frameworks for NDS video data analytics throws light on the existing gaps in the state-of-the-art that can be addressed by future computer vision research.  © 2000-2011 IEEE.
KW  - Driver behavior analysis
KW  - Driver distraction
KW  - Drowsiness
KW  - Face detection
KW  - Gaze
KW  - Head orientation
KW  - Lane change
KW  - Survey
KW  - Behavioral research
KW  - Cameras
KW  - Computer vision
KW  - Data acquisition
KW  - Data Analytics
KW  - Face recognition
KW  - Video recording
KW  - Computer vision techniques
KW  - Driver behaviour analysis
KW  - Driver distractions
KW  - Driver's behavior
KW  - Drowsiness
KW  - Faces detection
KW  - Gaze
KW  - Head orientation
KW  - Lane change
KW  - Naturalistic driving studies
KW  - Surveys
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rosenfeld, A.
TI  - Image Analysis and Computer Vision: 1996
PY  - 1997
T2  - Computer Vision and Image Understanding
DO  - 10.1006/cviu.1997.0602
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031116899&doi=10.1006%2fcviu.1997.0602&partnerID=40&md5=6f45c82cc2a82cdcfe61f88e4bb6caa5
AB  - This paper presents a bibliography of nearly 2150 references related to computer vision and image analysis, arranged by subject matter. The topics covered include computational techniques; feature detection and segmentation; image and scene analysis; two-dimensional shape; pattern; color and texture; matching and stereo; three-dimensional recovery and analysis; three-dimensional shape; and motion. A few references are also given on related topics, including geometry and graphics, compression and processing, sensors and optics, visual perception, neural networks, artificial intelligence and pattern recognition, as well as on applications. © 1997 Academic Press.
KW  - Color image processing
KW  - Computational geometry
KW  - Computational methods
KW  - Feature extraction
KW  - Image analysis
KW  - Image compression
KW  - Image quality
KW  - Image reconstruction
KW  - Image segmentation
KW  - Neural networks
KW  - Stereo vision
KW  - Three dimensional computer graphics
KW  - Feature detection
KW  - Computer vision
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Liu, H.
AU  - Li, X.
TI  - Target tracking for mobile robot platforms via object matching and background anti-matching
PY  - 2010
T2  - Robotics and Autonomous Systems
DO  - 10.1016/j.robot.2010.08.002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957275313&doi=10.1016%2fj.robot.2010.08.002&partnerID=40&md5=5b281f3f06c648c457481629e89e93a9
AB  - This study proposes a novel method for target tracking based on the combination of object matching and background anti-matching which take account of both the global property of covariance matching and local property of mean shift tracking synthetically. In the background anti-matching phrase, a certain number of background regions are extracted based on the feature of color orientation codes via an entropy filter, and the covariance matrix is adapted to match these regions to get the global motion of the background; further, the object matching is carried out by a mean-shift tracking algorithm. The proposed method is evaluated in various datasets in comparison with their counterpart algorithms; experimental results sufficiently demonstrate the effectiveness of the method proposed in this study. © 2010 Elsevier B.V. All rights reserved.
KW  - Background anti-matching
KW  - Color orientation codes
KW  - Covariance matching
KW  - Entropy filter
KW  - Motion compensation
KW  - Target tracking
KW  - Color
KW  - Covariance matrix
KW  - Entropy
KW  - Motion compensation
KW  - Target tracking
KW  - Tracking (position)
KW  - Background anti-matching
KW  - Background region
KW  - Covariance matching
KW  - Data sets
KW  - Global motion
KW  - Global properties
KW  - Local property
KW  - Mean shift
KW  - Mean shift tracking
KW  - Mobile robot platforms
KW  - Novel methods
KW  - Object matching
KW  - Orientation codes
KW  - Tracking algorithm
KW  - Color matching
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Kolb, A.
AU  - Barth, E.
AU  - Koch, R.
AU  - Larsen, R.
TI  - Time-of-flight cameras in computer graphics
PY  - 2010
T2  - Computer Graphics Forum
DO  - 10.1111/j.1467-8659.2009.01583.x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649089694&doi=10.1111%2fj.1467-8659.2009.01583.x&partnerID=40&md5=2fca592955ca398331ef69e33e16dadc
AB  - A growing number of applications depend on accurate and fast 3D scene analysis. Examples are model and lightfield acquisition, collision prevention, mixed reality and gesture recognition. The estimation of a range map by image analysis or laser scan techniques is still a time-consuming and expensive part of such systems. A lower-priced, fast and robust alternative for distance measurements are time-of-flight (ToF) cameras. Recently, significant advances have been made in producing low-cost and compact ToF devices, which have the potential to revolutionize many fields of research, including computer graphics, computer vision and human machine interaction (HMI). These technologies are starting to have an impact on research and commercial applications. The upcoming generation of ToF sensors, however, will be even more powerful and will have the potential to become 'ubiquitous real-time geometry devices' for gaming, web-conferencing, and numerous other applications. This paper gives an account of recent developments in ToF technology and discusses the current state of the integration of this technology into various graphics-related applications. © 2010 The Eurographics Association and Blackwell Publishing Ltd.
KW  - 3D time-of-flight cameras
KW  - Depth keying
KW  - Light fields
KW  - Range images
KW  - Scene analysis
KW  - Sensor fusion
KW  - Tracking
KW  - User interaction
KW  - Cameras
KW  - Computer graphics
KW  - Computer vision
KW  - Image analysis
KW  - Surface discharges
KW  - Tracking (position)
KW  - Virtual reality
KW  - Depth keying
KW  - Light fields
KW  - Range images
KW  - Scene analysis
KW  - Sensor fusion
KW  - Time-of-flight cameras
KW  - User interaction
KW  - Human computer interaction
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Cultrera, L.
AU  - Seidenari, L.
AU  - Becattini, F.
AU  - Pala, P.
AU  - Del Bimbo, A.
TI  - Explaining autonomous driving by learning end-to-end visual attention
PY  - 2020
T2  - IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops
DO  - 10.1109/CVPRW50498.2020.00178
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090162507&doi=10.1109%2fCVPRW50498.2020.00178&partnerID=40&md5=1e1ccc6606878c17f4e94a4a4595ae76
AB  - Current deep learning based autonomous driving approaches yield impressive results also leading to inproduction deployment in certain controlled scenarios. One of the most popular and fascinating approaches relies on learning vehicle controls directly from data perceived by sensors. This end-to-end learning paradigm can be applied both in classical supervised settings and using reinforcement learning. Nonetheless the main drawback of this approach as also in other learning problems is the lack of ex- plainability. Indeed, a deep network will act as a black-box outputting predictions depending on previously seen driving patterns without giving any feedback on why such decisions were taken.While to obtain optimal performance it is not critical to obtain explainable outputs from a learned agent, especially in such a safety critical field, it is of paramount importance to understand how the network behaves. This is particularly relevant to interpret failures of such systems.In this work we propose to train an imitation learning based agent equipped with an attention model. The attention model allows us to understand what part of the image has been deemed most important. Interestingly, the use of attention also leads to superior performance in a standard benchmark using the CARLA driving simulator. © 2020 IEEE.
KW  - Autonomous vehicles
KW  - Behavioral research
KW  - Benchmarking
KW  - Computer vision
KW  - Control system synthesis
KW  - Reinforcement learning
KW  - Safety engineering
KW  - Attention model
KW  - Autonomous driving
KW  - Driving simulator
KW  - Imitation learning
KW  - Learning paradigms
KW  - Learning problem
KW  - Optimal performance
KW  - Visual Attention
KW  - Deep learning
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Anderson, R.
TI  - Security Engineering: A Guide to Building Dependable Distributed Systems, Third Edition
PY  - 2020
T2  - Security Engineering: A Guide to Building Dependable Distributed Systems, Third Edition
DO  - 10.1002/9781119644682
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179271512&doi=10.1002%2f9781119644682&partnerID=40&md5=027098bf23c3a2131e3be49dae73f138
AB  - In Security Engineering: A Guide to Building Dependable Distributed Systems, Third Edition Cambridge University professor Ross Anderson updates his classic textbook and teaches readers how to design, implement, and test systems to withstand both error and attack. This book became a best-seller in 2001 and helped establish the discipline of security engineering. By the second edition in 2008, underground dark markets had let the bad guys specialize and scale up; attacks were increasingly on users rather than on technology. The book repeated its success by showing how security engineers can focus on usability. Now the third edition brings it up to date for 2020. As people now go online from phones more than laptops, most servers are in the cloud, online advertising drives the Internet and social networks have taken over much human interaction, many patterns of crime and abuse are the same, but the methods have evolved. Ross Anderson explores what security engineering means in 2020, including: How the basic elements of cryptography, protocols, and access control translate to the new world of phones, cloud services, social media and the Internet of Things Who the attackers are - from nation states and business competitors through criminal gangs to stalkers and playground bullies, What they do - from phishing and carding through SIM swapping and software exploits to DDoS and fake news, Security psychology, from privacy through ease-of-use to deception, The economics of security and dependability - why companies build vulnerable systems and governments look the other way, How dozens of industries went online - well or badly, How to manage security and safety engineering in a world of agile development - from reliability engineering to DevSecOps, The third edition of Security Engineering ends with a grand challenge: sustainable security. As we build ever more software and connectivity into safety-critical durable goods like cars and medical devices, how do we design systems we can maintain and defend for decades? Or will everything in the world need monthly software upgrades, and become unsafe once they stop?. © 2020 by Ross Anderson Published by John Wiley & Sons, Inc.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Liu, S.
AU  - Geng, K.
AU  - Yin, G.
AU  - Wu, C.
AU  - Ye, J.
TI  - Small objects detection with multi-layer laser radar based on projection dimensionality reduction
PY  - 2019
T2  - Chinese Control Conference, CCC
DO  - 10.23919/ChiCC.2019.8866188
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074436587&doi=10.23919%2fChiCC.2019.8866188&partnerID=40&md5=5dcb058b04f6b20bef9c86b725be908a
AB  - Small objects on the road is extremely dangerous obstacles especially when driving at high speed. Detection of such small obstacles is crucial to the safety of autonomous car user. Therefore, a novel of small objects detection algorithm by using multi-layer laser radar is proposed in this paper. Firstly, the road edge point-clouds was extracted from the numerous raw point-clouds based on Hough Transform, and the drivable area and non-drivable area was separated by filtering the point-clouds outside the road edge. Secondly, projection-dimensionality reduction and Hough Transform was applied to recognize and filter the road point-clouds and the remaining point-clouds can be considered as the objects and some interference points. Then, the noise point-clouds was filtered by the outliers filtering algorithm. Finally, the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm was used to cluster the small objects. The proposed algorithm was tested on the platform of driverless formula one car and the experimental results show that the algorithm is efficient and robustness. © 2019 Technical Committee on Control Theory, Chinese Association of Automation.
KW  - DBSCAN
KW  - Hough transform
KW  - Laser radar
KW  - Projection dimensionality reduction
KW  - Small object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ertler, C.
AU  - Mislej, J.
AU  - Ollmann, T.
AU  - Porzi, L.
AU  - Neuhold, G.
AU  - Kuang, Y.
TI  - The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale
PY  - 2020
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
DO  - 10.1007/978-3-030-58592-1_5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097447491&doi=10.1007%2f978-3-030-58592-1_5&partnerID=40&md5=8d8dc56433cae56f6b3a6a3002e38c88
AB  - Traffic signs are essential map features for smart cities and navigation. To develop accurate and robust algorithms for traffic sign detection and classification, a large-scale and diverse benchmark dataset is required. In this paper, we introduce a new traffic sign dataset of 105K street-level images around the world covering 400 manually annotated traffic sign classes in diverse scenes, wide range of geographical locations, and varying weather and lighting conditions. The dataset includes 52K fully annotated images. Additionally, we show how to augment the dataset with 53K semi-supervised, partially annotated images. This is the largest and the most diverse traffic sign dataset consisting of images from all over the world with fine-grained annotations of traffic sign classes. We run extensive experiments to establish strong baselines for both detection and classification tasks. In addition, we verify that the diversity of this dataset enables effective transfer learning for existing large-scale benchmark datasets on traffic sign detection and classification. The dataset is freely available for academic research (www.mapillary.com/dataset/trafficsign). © 2020, Springer Nature Switzerland AG.
KW  - Classification (of information)
KW  - Computer vision
KW  - Large dataset
KW  - Transfer learning
KW  - Academic research
KW  - Benchmark datasets
KW  - Classification tasks
KW  - Geographical locations
KW  - Lighting conditions
KW  - Robust algorithm
KW  - Semi-supervised
KW  - Traffic sign detection
KW  - Traffic signs
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Yang, B.
AU  - Dong, Z.
TI  - Progress and perspective of point cloud intelligence
ST  - 点云智能研究进展与趋势
PY  - 2019
T2  - Cehui Xuebao/Acta Geodaetica et Cartographica Sinica
DO  - 10.11947/j.AGCS.2019.20190465
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077467563&doi=10.11947%2fj.AGCS.2019.20190465&partnerID=40&md5=6b3898db02f3866b77c143a1e7882e68
AB  - With the rapid development of the reality capture, such as laser scanning and oblique photogrammetry, point cloud has become the third important data source following vector maps and imagery, and also plays an increasingly important role in scientific research and engineering in the fields of earth science, spatial cognition, and smart city, and so on. However, how to acquire valid and accurate three-dimensional geospatial information from point clouds has become the scientific frontier and the urgent demand in the field of surveying and mapping as well as the geoscience applications. To address the challenges mentioned above, point cloud intelligence came into being. This paper summarizes the state-of-the art of point cloud intelligence in acquisition equipment, the intelligent processing, scientific research and the major engineering applications, focusing on its three important areas: the theoretical methods, the key techniques of intelligent processing and the major engineering applications. Finally, the promising development tendency of the point cloud intelligence is summarized. © 2019, Surveying and Mapping Press. All right reserved.
KW  - Deep learning
KW  - Point cloud big data
KW  - Point cloud intelligence
KW  - Semantic labeling
KW  - Structured modelling
KW  - Ubiquitous point cloud
KW  - Semantics
KW  - Acquisition equipments
KW  - Engineering applications
KW  - Geo-spatial informations
KW  - Geoscience applications
KW  - Intelligent processing
KW  - Point cloud
KW  - Scientific researches
KW  - Semantic labeling
KW  - computer simulation
KW  - data processing
KW  - Earth science
KW  - engineering geology
KW  - information management
KW  - laser method
KW  - machine learning
KW  - scanner
KW  - three-dimensional modeling
KW  - Deep learning
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Murthy, C.B.
AU  - Hashmi, M.F.
AU  - Bokde, N.D.
AU  - Geem, Z.W.
TI  - Investigations of object detection in images/videos using various deep learning techniques and embedded platforms-A comprehensive review
PY  - 2020
T2  - Applied Sciences (Switzerland)
DO  - 10.3390/app10093280
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085064580&doi=10.3390%2fapp10093280&partnerID=40&md5=c090334d20eb5ec8665ee0a4325ff775
AB  - In recent years there has been remarkable progress in one computer vision application area: object detection. One of the most challenging and fundamental problems in object detection is locating a specific object from the multiple objects present in a scene. Earlier traditional detection methods were used for detecting the objects with the introduction of convolutional neural networks. From 2012 onward, deep learning-based techniques were used for feature extraction, and that led to remarkable breakthroughs in this area. This paper shows a detailed survey on recent advancements and achievements in object detection using various deep learning techniques. Several topics have been included, such as Viola-Jones (VJ), histogram of oriented gradient (HOG), one-shot and two-shot detectors, benchmark datasets, evaluation metrics, speed-up techniques, and current state-of-art object detectors. Detailed discussions on some important applications in object detection areas, including pedestrian detection, crowd detection, and real-time object detection on Gpu-based embedded systems have been presented. At last, we conclude by identifying promising future directions. © 2020 by the authors.
KW  - Computer vision (CV)
KW  - Convolutional neural network (CNN)
KW  - Deep learning techniques
KW  - Graphics processing units (GPUs)
KW  - Object detection
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Adarsh, P.
AU  - Rathi, P.
AU  - Kumar, M.
TI  - YOLO v3-Tiny: Object Detection and Recognition using one stage improved model
PY  - 2020
T2  - 2020 6th International Conference on Advanced Computing and Communication Systems, ICACCS 2020
DO  - 10.1109/ICACCS48705.2020.9074315
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084648204&doi=10.1109%2fICACCS48705.2020.9074315&partnerID=40&md5=bd6a91de7af4e9515490ea817459f963
AB  - Object detection has seen many changes in algorithms to improve performance both on speed and accuracy. By the continuous effort of so many researchers, deep learning algorithms are growing rapidly with an improved object detection performance. Various popular applications like pedestrian detection, medical imaging, robotics, self-driving cars, face detection, etc. reduces the efforts of humans in many areas. Due to the vast field and various state-of-the-art algorithms, it is a tedious task to cover all at once. This paper presents the fundamental overview of object detection methods by including two classes of object detectors. In two stage detector covered algorithms are RCNN, Fast RCNN, and Faster RCNN, whereas in one stage detector YOLO v1, v2, v3, and SSD are covered. Two stage detectors focus more on accuracy, whereas the primary concern of one stage detectors is speed. We will explain an improved YOLO version called YOLO v3-Tiny, and then its comparison with previous methods for detection and recognition of object is described graphically. © 2020 IEEE.
KW  - Computer vision
KW  - Convolutional Neural Networks
KW  - Deep learning
KW  - Faster RCNN
KW  - image processing
KW  - Object detection
KW  - YOLO v3
KW  - YOLO v3-Tiny
KW  - Deep learning
KW  - Face recognition
KW  - Learning algorithms
KW  - Medical imaging
KW  - Medical robotics
KW  - Object recognition
KW  - Detection performance
KW  - Improve performance
KW  - Object detection and recognition
KW  - Object detection method
KW  - Object detectors
KW  - Pedestrian detection
KW  - Recognition of objects
KW  - State-of-the-art algorithms
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rumez, M.
AU  - Grimm, D.
AU  - Kriesten, R.
AU  - Sax, E.
TI  - An Overview of Automotive Service-Oriented Architectures and Implications for Security Countermeasures
PY  - 2020
T2  - IEEE Access
DO  - 10.1109/ACCESS.2020.3043070
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147696411&doi=10.1109%2fACCESS.2020.3043070&partnerID=40&md5=e5eb8dbc11975beb724c586f0a4eed95
AB  - New requirements from the customers’ and manufacturers’ point of view such as adding new software functions during the product life cycle require a transformed architecture design for future vehicles. The paradigm of signal-oriented communication established for many years will increasingly be replaced by service-oriented approaches in order to increase the update and upgrade capability. In this article, we provide an overview of current protocols and communication patterns for automotive architectures based on the service-oriented architecture (SOA) paradigm and compare them with signal-oriented approaches. Resulting challenges and opportunities of SOAs with respect to information security are outlined and discussed. For this purpose, we explain different security countermeasures and present a state of the section of automotive approaches in the fields of firewalls, Intrusion Detection Systems (IDSs) and Identity and Access Management (IAM). Our final discussion is based on an exemplary hybrid architecture (signal- and service-oriented) and examines the adaptation of existing security measures as well as their specific security features. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.
KW  - access control
KW  - Automotive SOA
KW  - connected vehicles
KW  - cybersecurity
KW  - firewall
KW  - intrusion detection system (IDS)
KW  - service-oriented architectures
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Marvin, S.
TI  - DICTIONARY OF SCIENTIFIC PRINCIPLES
PY  - 2012
T2  - Dictionary of Scientific Principles
DO  - 10.1002/9781118582121
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000058388&doi=10.1002%2f9781118582121&partnerID=40&md5=59ffb059270dbafc071dcf19243af00f
AB  - Dictionary of Scientific Principles presents a unique and timeless collection of (almost) all known rules or laws commonly called principles, identified throughout the history of scientific development, their definition, and use. Exploring a broad range of disciplines, the book first lists more than 2, 000 principles organized in a standard alphabetical order, then provides a list of subject headings for which related principles are identified. A staple addition to every library, the dictionary will also be of interest to scientists and general readers. © 2011 by John Wiley & Sons, Inc. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rosenfeld, A.
TI  - Image Analysis and Computer Vision: 1999
PY  - 2000
T2  - Computer Vision and Image Understanding
DO  - 10.1006/cviu.2000.0835
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000293529&doi=10.1006%2fcviu.2000.0835&partnerID=40&md5=419aca2a4754424115097d404e9ae5ce
AB  - This paper presents a bibliography of nearly 1700 references related to computer vision and image analysis, arranged by subject matter. The topics covered include computational techniques; feature detection and segmentation; image and scene analysis; two-dimensional shape; pattern; color and texture; matching and stereo; 2[Formula presented]-dimensional recovery and analysis; three-dimensional shape; and motion. A few references are also given on related topics, including geometry and graphics, compression and processing, sensors and optics, visual perception, neural networks, artificial intelligence and pattern recognition, as well as on applications. © 2000 Academic Press
KW  - Color matching
KW  - Computer vision
KW  - Image analysis
KW  - Image segmentation
KW  - Color and textures
KW  - Computational technique
KW  - Feature detection
KW  - Scene analysis
KW  - Subject matters
KW  - Three-dimensional shape
KW  - Visual perception
KW  - Stereo image processing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Rosenfeld, A.
TI  - Image analysis and computer vision: 1990
PY  - 1991
T2  - CVGIP: Image Understanding
DO  - 10.1016/1049-9660(91)90020-P
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025993254&doi=10.1016%2f1049-9660%2891%2990020-P&partnerID=40&md5=3444519eeda99360e60afac5cab41c66
AB  - This paper presents a bibliography of over 1600 references related to computer vision and image analysis, arranged by subject matter. The topics covered include architectures; computational techniques; feature detection, segmentation, and image analysis; matching, stereo, and time-varying imagery; shape and pattern; color and texture; and three-dimensional scene analysis. A few references are also given on related topics, such as computational geometry, computer graphics, image input/output and coding, image processing, optical processing, visual perception, neural nets, pattern recognition, and artificial intelligence. © 1991.
KW  - bibliography
KW  - computer vision
KW  - image analysis
M3  - Short survey
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Ye, T.
AU  - Zhang, Z.
AU  - Zhang, X.
AU  - Zhou, F.
TI  - Autonomous Railway Traffic Object Detection Using Feature-Enhanced Single-Shot Detector
PY  - 2020
T2  - IEEE Access
DO  - 10.1109/ACCESS.2020.3015251
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089944513&doi=10.1109%2fACCESS.2020.3015251&partnerID=40&md5=ad00d0417c425e3a6218703f00210d41
AB  - With the high growth rates of railway transportation, it is extremely important to detect railway obstacles ahead of the train to ensure safety. Manual and traditional feature-extraction methods have been utilized in this scenario. There are also deep learning-based railway object detection approaches. However, in the case of a complex railway scene, these object detection approaches are either inefficient or have insufficient accuracy, particularly for small objects. To address this issue, we propose a feature-enhanced single-shot detector (FE-SSD). The proposed method inherits a prior detection module of RON and a feature transfer block of FB-Net. It also employs a novel receptive field-enhancement module. Through the integration of these three modules, the feature discrimination and robustness are significantly enhanced. Experimental results for a railway traffic dataset built by our team indicated that the proposed approach is superior to other SSD-derived models, particularly for small-object detection, while achieving real-time performance close to that of the SSD. The proposed method achieved a mean average precision of 0.895 and a frame rate of 38 frames per second on a railway traffic dataset with an input size of 320×320 pixels. The experimental results indicate that the proposed method can be used for real-world railway object detection.  © 2013 IEEE.
KW  - Feature transfer block
KW  - prior detection module
KW  - railway object detection
KW  - receptive field-enhancement module
KW  - Deep learning
KW  - Feature extraction
KW  - Object recognition
KW  - Railroads
KW  - Detection approach
KW  - Detection modules
KW  - Feature discrimination
KW  - Feature extraction methods
KW  - Frames per seconds
KW  - Railway transportation
KW  - Real time performance
KW  - Small object detection
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Ranjan Kumar, H.S.
AU  - Bhat, D.
TI  - A novel method to recognize object in Images using Convolution Neural Networks
PY  - 2019
T2  - 2019 International Conference on Intelligent Computing and Control Systems, ICCS 2019
DO  - 10.1109/ICCS45141.2019.9065367
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084088134&doi=10.1109%2fICCS45141.2019.9065367&partnerID=40&md5=5dca8de0becf7ed3144c7cae915e0052
AB  - Detection and recognition of object is challenging task nowadays in the computer vision domain. To solve this problem, we have many techniques and algorithms that help us to detect objects in natural environment. Prime components of object detection are classification and feature extraction. In this paper we have used a method called Convolution Neural Network (CNN) a technique for training images for object recognition. Machine learning techniques are applied on computer vision domain. This research aims to propose a new design and a methodology to support a system in order to recognize the object and give us the pop-up message in case the object is considered as a weapon. Image processing techniques are used to develop the system. This experiment is conducted to access the following qualities, say, to verify whether the system is able to detect the object and to measure the accuracy of the proposed system. © 2019 IEEE.
KW  - Computer vision
KW  - Image processing
KW  - Inception v3
KW  - Machine learning technique
KW  - Object detection technique
KW  - Computer vision
KW  - Control systems
KW  - Convolution
KW  - Feature extraction
KW  - Intelligent computing
KW  - Learning systems
KW  - Object recognition
KW  - Convolution neural network
KW  - Image processing technique
KW  - Machine learning techniques
KW  - Natural environments
KW  - Recognition of objects
KW  - Training image
KW  - Object detection
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, G.
AU  - Xie, H.
AU  - Yan, W.
AU  - Chang, Y.
AU  - Qu, X.
TI  - Detection of Road Objects with Small Appearance in Images for Autonomous Driving in Various Traffic Situations Using a Deep Learning Based Approach
PY  - 2020
T2  - IEEE Access
DO  - 10.1109/ACCESS.2020.3036620
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097348264&doi=10.1109%2fACCESS.2020.3036620&partnerID=40&md5=6daaa4eb28b72ac2839a61bd0ae43102
AB  - Effectively detecting road objects in various environments would significantly improve driving safety for autonomous vehicles. However, small objects, low illumination, and blurred outline in images strongly limit the performance of current road object detection methods. To solve these problems, this paper proposed a novel deep learning anchor-free approach based on CenterNet. The atrous spatial pyramid pooling (ASPP) was used to extract features from multiple scales to improve the detection performance while not increasing the computational cost and the number of parameters. The space to depth algorithm was then adopted in our proposed approach to optimize the traditional downsampling process. A large-scale naturalistic driving dataset (BDD100K) was used to examined the effectiveness of our proposed approach. The experimental results show that our proposed approach can effectively improve the detection performance on small objects in various traffic situations. © 2013 IEEE.
KW  - advanced driver assistance system (ADAS)
KW  - Autonomous vehicle (AV)
KW  - deep learning
KW  - driving safety
KW  - object detection
KW  - Autonomous vehicles
KW  - Large dataset
KW  - Object detection
KW  - Roads and streets
KW  - Autonomous driving
KW  - Computational costs
KW  - Detection performance
KW  - Learning-based approach
KW  - Low illuminations
KW  - Object detection method
KW  - Spatial pyramids
KW  - Traffic situations
KW  - Deep learning
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Grauman, K.
AU  - Leibe, B.
TI  - Visual object recognition
PY  - 2011
T2  - Synthesis Lectures on Artificial Intelligence and Machine Learning
DO  - 10.2200/S00332ED1V01Y201103AIM011
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955445232&doi=10.2200%2fS00332ED1V01Y201103AIM011&partnerID=40&md5=572605e073b66d4aae4f94b74edc1425
AB  - The visual recognition problem is central to computer vision research. From robotics to information retrieval, many desired applications demand the ability to identify and localize categories, places, and objects. This tutorial overviews computer vision algorithms for visual object recognition and image classification. We introduce primary representations and learning approaches, with an emphasis on recent advances in the field. The target audience consists of researchers or students working in AI, robotics, or vision who would like to understand what methods and representations are available for these problems. This lecture summarizes what is and isn't possible to do reliably today, and overviews key concepts that could be employed in systems requiring visual categorization. Copyright © 2011 by Morgan & Claypool.
KW  - dealing with outliers in correspondences
KW  - detection and description of local invariant features
KW  - detection via sliding windows
KW  - efficient algorithms for matching local features
KW  - Generalized distance transform
KW  - global representations versus local descriptors
KW  - histograms of oriented gradients and rectangular features
KW  - Hough voting
KW  - methods to verify geometric consistency according to parameterized geometric transformations
KW  - part-based models
KW  - pyramid match kernels
KW  - RANSAC and the Generalized Hough transform
KW  - star graph models and fully connected constellations
KW  - the Deformable Part-based Model
KW  - the Implicit Shape Model
KW  - tree-based and hashing-based search algorithms
KW  - visual vocabularies and bags-of-words
KW  - window-based descriptors
KW  - Computer vision
KW  - Content based retrieval
KW  - Deformation
KW  - Geometry
KW  - Graphic methods
KW  - Hough transforms
KW  - Image matching
KW  - Information retrieval
KW  - Learning algorithms
KW  - Object recognition
KW  - Robotics
KW  - Stars
KW  - Trees (mathematics)
KW  - dealing with outliers in correspondences
KW  - detection and description of local invariant features
KW  - detection via sliding windows
KW  - efficient algorithms for matching local features
KW  - Generalized distance transform
KW  - global representations versus local descriptors
KW  - histograms of oriented gradients and rectangular features
KW  - Hough voting
KW  - methods to verify geometric consistency according to parameterized geometric transformations
KW  - part-based models
KW  - pyramid match kernels
KW  - RANSAC and the Generalized Hough transform
KW  - star graph models and fully connected constellations
KW  - the Deformable Part-based Model
KW  - the Implicit Shape Model
KW  - tree-based and hashing-based search algorithms
KW  - visual vocabularies and bags-of-words
KW  - window-based descriptors
KW  - Mathematical transformations
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Sotak, M.
TI  - Coarse alignment algorithm for ADIS16405
PY  - 2010
T2  - Przeglad Elektrotechniczny
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956034129&partnerID=40&md5=0f801159c0085af50bfefbe7b4245969
AB  - The paper presents the coarse alignment algorithm for the sensor ADIS16405. The ADIS16405 is the new six-degree of freedom inertial measurement unit which is produced by Analog Devices. The main goal of the paper is to describe determination of the initial attitude of the sensor with respect to the navigation frame as a referenced frame. In this work the attitude is represented by the Euler angles (the roll, pitch and yaw angles). Developed coarse alignment algorithm is implemented for the real-time integrated navigation system that is in a progress nowadays.
KW  - Alignment
KW  - Euler angles
KW  - Inertial measurement unit
KW  - Low-cost navigation
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Klinefelter, E.
AU  - Nanzer, J.A.
TI  - Interferometric microwave radar with a feedforward neural network for vehicle speed-over-ground estimation
PY  - 2020
T2  - IEEE Microwave and Wireless Components Letters
DO  - 10.1109/LMWC.2020.2966191
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081716685&doi=10.1109%2fLMWC.2020.2966191&partnerID=40&md5=3465a3086bee7b996caebff30bde4ff0
AB  - A novel approach to measuring the ground speed of autonomous vehicles using interferometric radar is presented. Using a microwave radar with interferometric processing and a feedforward neural network with local regression, high-accuracy velocity estimation is achieved with a downward-facing radar that, unlike forward-looking Doppler radars, can be protected from road debris and is, furthermore, unaffected by wheel slip and requires no external inputs, such as Global Navigation Satellite Systems (GNSS). A 16.9-GHz active interferometric array generates a grating lobe pattern and as the ground passes through the pattern, the range of frequencies over which the response is distributed increases proportionally with the ground velocity. We implement a feedforward neural network to estimate the velocity based on the interferometer's frequency response. The estimator achieved a root-mean-squared error of 0.138 (m/s), equivalent to that of the forward-looking Doppler radars. © 2001-2012 IEEE.
KW  - Automotive radar
KW  - autonomous vehicles (AVs)
KW  - ego-estimation
KW  - machine learning
KW  - neural networks
KW  - radar interferometry
KW  - velocity estimation
KW  - Automotive radar
KW  - Autonomous vehicles
KW  - Doppler radar
KW  - Feedforward neural networks
KW  - Frequency response
KW  - Global positioning system
KW  - Interferometry
KW  - Learning algorithms
KW  - Learning systems
KW  - Mean square error
KW  - Neural networks
KW  - Velocity
KW  - Global Navigation Satellite Systems
KW  - Interferometric array
KW  - Interferometric processing
KW  - Interferometric radars
KW  - Local regression
KW  - Radar interferometry
KW  - Root mean squared errors
KW  - Velocity estimation
KW  - Radar measurement
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Seikavandi, M.J.
AU  - Nasrollahi, K.
AU  - Moeslund, T.B.
TI  - Deep car detection by fusing grayscale image and weighted upsampled LiDAR depth
PY  - 2020
T2  - Proceedings of SPIE - The International Society for Optical Engineering
DO  - 10.1117/12.2586908
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107836656&doi=10.1117%2f12.2586908&partnerID=40&md5=8b2b97ff41c514e31cacaccd6064c6b6
AB  - Recent advances have shown sensor-fusion's vital role in accurate detection, especially for advanced driver assistance systems. We introduce a novel procedure for depth upsampling and sensor-fusion that together lead to an improved detection performance, compared to state-of-the-art results for detecting cars. Upsampling is generally based on combining data from an image to compensate for the low resolution of a LiDAR (Light Detector and Ranging). This paper, on the other hand, presents a framework to obtain dense depth map solely from a single LiDAR point cloud that makes it possible to use just one deep network for both LiDAR and image modalities. The produced full-depth map is added to the grayscale version of the image to produce a two-channel input for a deep neural network. The simple preprocessing structure is efficiently competent in filing cars' shapes, which helps the fusion framework to outperforms the state-of-the-art on the KITTI object detection for the Car class. © 2021 SPIE.
KW  - Autonomous Driving
KW  - Deep Learning
KW  - Depth Perception
KW  - LiDAR
KW  - Multimodal Fusion
KW  - Object Detection
KW  - Sensor Fusion
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Chemical detection
KW  - Computer vision
KW  - Deep neural networks
KW  - Object detection
KW  - Signal sampling
KW  - Dense depth map
KW  - Depth upsampling
KW  - Detection performance
KW  - Gray-scale images
KW  - Image modality
KW  - Lidar point clouds
KW  - Low resolution
KW  - State of the art
KW  - Optical radar
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Lu, Y.
AU  - Yang, M.
AU  - Wang, C.
AU  - Wang, B.
TI  - Pedestrian tracking based on laser and image data fusion
PY  - 2019
T2  - 2019 IEEE International Conference on Real-Time Computing and Robotics, RCAR 2019
DO  - 10.1109/RCAR47638.2019.9044002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089136353&doi=10.1109%2fRCAR47638.2019.9044002&partnerID=40&md5=13150505c1e4c293454c25160ee4858b
AB  - Pedestrian tracking in vehicle coordinates is not only a necessary part of the perception for autonomous cars but also a challenging task in computer vision. This paper presents a multi-sensor fusion model combining images and laser scanning data to track pedestrians in occupied grid maps. In our approach, the bounding boxes of pedestrians detected in images are used to generate regions of interest (ROI) in grids. A data association method based on multi-characteristic Mahalanobis distance (MMD) and sliding windows is proposed to establish correspondence between the detections in different frames. Then Sampling Importance Resampling Particle Filter (SIR PF) is used to update pedestrians' states with the fusion of images and grids. Experiments show our method has competitive efficiency and accuracy compared to conventional methods. Codes of this paper is released1,. © 2018 IEEE.
KW  - Agricultural robots
KW  - Importance sampling
KW  - Robotics
KW  - Sensor data fusion
KW  - Conventional methods
KW  - Laser scanning data
KW  - Mahalanobis distances
KW  - Multi characteristics
KW  - Multi-sensor fusion
KW  - Pedestrian tracking
KW  - Regions of interest
KW  - Sampling importance resampling
KW  - Image fusion
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Poncela, A.
AU  - Urdiales, C.
AU  - de Trazegnies, C.
AU  - Sandoval, F.
TI  - A new sonar-based landmark for localization in indoor environments
PY  - 2007
T2  - Soft Computing
DO  - 10.1007/s00500-006-0069-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750298587&doi=10.1007%2fs00500-006-0069-3&partnerID=40&md5=3356caf01af871c1ad7a74630d04c4d4
AB  - This paper presents a new sonar based landmark to represent significant places in an environment for localization purposes. This landmark is based on extracting the contour free of obstacles around the robot from a local evidence grid. This contour is represented by its curvature, calculated by a noise-resistant function which adapts to the natural scale of the contour at each point. Then, curvature is reduced to a short feature vector by using Principal Component Analysis. The landmark calculation method has been successfully tested in a medium scale real environment using a Pioneer robot with Polaroid sonar sensors. © Springer-Verlag 2006.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Mannan, M.S.
TI  - Lees' Loss Prevention in the Process Industries: Hazard Identification, Assessment and Control: Third Edition
PY  - 2004
T2  - Lees' Loss Prevention in the Process Industries: Hazard Identification, Assessment and Control: Third Edition
DO  - 10.1016/B978-0-7506-7555-0.X5081-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041120899&doi=10.1016%2fB978-0-7506-7555-0.X5081-6&partnerID=40&md5=0f3f24c9690607fd0823046f6958522e
AB  - Over the last three decades the process industries have grown very rapidly, with corresponding increases in the quantities of hazardous materials in process, storage or transport. Plants have become larger and are often situated in or close to densely populated areas. Increased hazard of loss of life or property is continually highlighted with incidents such as Flixborough, Bhopal, Chernobyl, Three Mile Island, the Phillips 66 incident, and Piper Alpha to name but a few.The field of Loss Prevention is, and continues to, be of supreme importance to countless companies, municipalities and governments around the world, because of the trend for processing plants to become larger and often be situated in or close to densely populated areas, thus increasing the hazard of loss of life or property. This book is a detailed guidebook to defending against these, and many other, hazards. It could without exaggeration be referred to as the "bible" for the process industries. This is THE standard reference work for chemical and process engineering safety professionals. For years, it has been the most complete collection of information on the theory, practice, design elements, equipment, regulations and laws covering the field of process safety. An entire library of alternative books (and cross-referencing systems) would be needed to replace or improve upon it, but everything of importance to safety professionals, engineers and managers can be found in this all-encompassing reference instead. Frank Lees' world renowned work has been fully revised and expanded by a team of leading chemical and process engineers working under the guidance of one of the worldâ??s chief experts in this field. Sam Mannan is professor of chemical engineering at Texas A and M University, and heads the Mary Kay Oâ??Connor Process Safety Center at Texas A and M. He received his MS and Ph.D. in chemical engineering from the University of Oklahoma, and joined the chemical engineering department at Texas A and M University as a professor in 1997. He has over 20 years of experience as an engineer, working both in industry and academiaNew detail is added to chapters on fire safety, engineering, explosion hazards, analysis and suppression, and new appendices feature more recent disasters. The many thousands of references have been updated along with standards and codes of practice issued by authorities in the US, UK/Europe and internationally. In addition to all this, more regulatory relevance and case studies have been included in this edition. Written in a clear and concise style, Loss Prevention in the Process Industries covers traditional areas of personal safety as well as the more technological aspects and thus provides balanced and in-depth coverage of the whole field of safety and loss prevention. © 2005 Elsevier Inc. All rights reserved.
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Zhang, J.
AU  - Zhong, Y.
AU  - Wang, M.
TI  - An efficient stereo matching based on fragment matching
PY  - 2019
T2  - Visual Computer
DO  - 10.1007/s00371-018-1491-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047263906&doi=10.1007%2fs00371-018-1491-0&partnerID=40&md5=44338ee7e1b1b799985c082a2c4f92cb
AB  - We propose a stereo matching method based on image fragments. Unlike traditional pixel-based stereos matching methods, we use edge information in the reference image to divide it into small fragments, and we then use the segments to find the best matching fragments in another reference image from the horizontal and vertical directions. We obtain two disparity maps, and using the match confidence value for each disparity map, we can produce a more accurate disparity map. Next, we calculate the exact disparity value for each pixel within the fragment. Finally, the disparity map is filled and smoothed to obtain the final disparity result. Experiments demonstrated that the proposed method has low computation complexity, high matching accuracy, and the disparity of object edge is clear, and it achieved good performance with the Middlebury and KITTI benchmark. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.
KW  - Image edge
KW  - Same color region median filter
KW  - Segment matching
KW  - Stereo matching
KW  - Benchmarking
KW  - Color matching
KW  - Image segmentation
KW  - Median filters
KW  - Pixels
KW  - Computation complexity
KW  - Confidence values
KW  - Image edge
KW  - Region median filter
KW  - Segment matching
KW  - Stereo matching
KW  - Stereo matching method
KW  - Vertical direction
KW  - Stereo image processing
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Chen, G.
AU  - Mao, Z.
AU  - Yi, H.
AU  - Li, X.
AU  - Bai, B.
AU  - Liu, M.
AU  - Zhou, H.
TI  - Pedestrian detection based on panoramic depth map transformed from 3d-lidar data
PY  - 2020
T2  - Periodica polytechnica Electrical engineering and computer science
DO  - 10.3311/PPee.14960
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091931096&doi=10.3311%2fPPee.14960&partnerID=40&md5=881b37cc5665f91ca1c75d62eb9b2b65
AB  - Object detection is a crucial task of autonomous driving. This paper addresses an effective algorithm for pedestrian detection of the panoramic depth map transformed from the 3D-LiDAR data. Firstly, the 3D point clouds are transformed into panoramic depth maps, and then the panoramic depth maps are enhanced. Secondly, the grounds of the 3D point clouds are removed. The remaining point clouds are clustered, filtered and projected onto the previously generated panoramic depth maps, and new panoramic depth maps are obtained. Finally, the new panoramic depth maps are jointed to generate depth maps with different sizes, which are used as input of the improved PVANET for pedestrian detection. The 2D image of the panoramic depth map applied to the proposed algorithm is transformed from 3D point cloud, effectively containing the panorama of the sensor, and is more suitable for the environment perception of autonomous driving. Compared with the detection algorithm based on RGB images, the proposed algorithm cannot be affected by light, and can maintain the normal average precision of pedestrian detection at night. In order to increase the robustness of detecting small objects like pedestrians, the network structure based on the original PVANET is modified in this paper. A new dataset is built by processing the 3D-LiDAR data and the model trained on the new dataset perform well. The experimental results show that the proposed algorithm achieves high accuracy and robustness in pedestrian detection under different illumination conditions. Furthermore, when trained on the new dataset, the model exhibits average precision improvements of 2.8-5.1 % over the original PVANET, making it more suitable for autonomous driving applications. © 2020 Budapest University of Technology and Economics. All rights reserved.
KW  - 3D-LiDAR data
KW  - Improved PVANET
KW  - Panoramic depth map
KW  - Small object detection
KW  - Autonomous vehicles
KW  - Object recognition
KW  - Optical radar
KW  - Autonomous driving
KW  - Detection algorithm
KW  - Effective algorithms
KW  - Environment perceptions
KW  - Illumination conditions
KW  - Network structures
KW  - Pedestrian detection
KW  - Precision improvement
KW  - Object detection
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Jiang, M.
AU  - Aoyama, T.
AU  - Takaki, T.
AU  - Ishii, I.
TI  - Pixel-level and robust vibration source sensing in high-frame-rate video analysis
PY  - 2016
T2  - Sensors (Switzerland)
DO  - 10.3390/s16111842
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994691341&doi=10.3390%2fs16111842&partnerID=40&md5=11270661961b238c9b73a675c4d6c0af
AB  - We investigate the effect of appearance variations on the detectability of vibration feature extraction with pixel-level digital filters for high-frame-rate videos. In particular, we consider robust vibrating object tracking, which is clearly different from conventional appearance-based object tracking with spatial pattern recognition in a high-quality image region of a certain size. For 512 _ 512 videos of a rotating fan located at different positions and orientations and captured at 2000 frames per second with different lens settings, we verify how many pixels are extracted as vibrating regions with pixel-level digital filters. The effectiveness of dynamics-based vibration features is demonstrated by examining the robustness against changes in aperture size and the focal condition of the camera lens, the apparent size and orientation of the object being tracked, and its rotational frequency, as well as complexities and movements of background scenes. Tracking experiments for a flying multicopter with rotating propellers are also described to verify the robustness of localization under complex imaging conditions in outside scenarios. © 2016 by the authors; licensee MDPI, Basel, Switzerland.
KW  - Drone tracking
KW  - High-frame-rate video
KW  - Object tracking
KW  - Pixel-level digital filters
KW  - Vibration source localization
KW  - Aircraft detection
KW  - Computer graphics
KW  - Digital filters
KW  - Feature extraction
KW  - Image processing
KW  - Pixels
KW  - Frames per seconds
KW  - High frame rate
KW  - High quality images
KW  - Imaging conditions
KW  - Object Tracking
KW  - Pixel level
KW  - Rotational frequency
KW  - Vibration sources
KW  - Vibration analysis
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Deruyttere, T.
AU  - Vandenhende, S.
AU  - Grujicic, D.
AU  - Liu, Y.
AU  - Van Gool, L.
AU  - Blaschko, M.
AU  - Tuytelaars, T.
AU  - Moens, M.-F.
TI  - Commands 4 Autonomous Vehicles (C4AV) Workshop Summary
PY  - 2020
T2  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
DO  - 10.1007/978-3-030-66096-3_1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101783520&doi=10.1007%2f978-3-030-66096-3_1&partnerID=40&md5=2781660150b366f388a0cd0758b8a5b2
AB  - The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the Commands for Autonomous Vehicles (C4AV) challenge based on the recent Talk2Car dataset. This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work. © 2020, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Visual languages
KW  - Freeforms
KW  - Natural language queries
KW  - Natural languages
KW  - Potential failures
KW  - Spoken languages
KW  - State of the art
KW  - Autonomous vehicles
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Tian, W.
AU  - Chen, L.
AU  - Zou, K.
AU  - Lauer, M.
TI  - Vehicle tracking at nighttime by kernelized experts with channel-wise and temporal reliability estimation
PY  - 2018
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2017.2771410
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038361372&doi=10.1109%2fTITS.2017.2771410&partnerID=40&md5=35b7bb8d1da9c741c38ebfa243891b7d
AB  - Despite the fact that in recent years, vision-based tracking approaches have made significant progress, the task of tracking vehicles at night still remains challenging. Visual information is strongly deteriorated or at least degraded due to poor illumination conditions. This reduces the perceptive ability of vision systems significantly and can even lead to target loss, resulting in false estimation and/or false prediction of object behavior. In this paper, we propose a novel online-learning method to track vehicles at night. Our method is based on the kernelized correlation filter and assembles different feature channels to kernelized experts. By estimating their reliabilities, we force the appearance model to focus on the most discriminative visual features to accomplish the classification. In addition, a temporal optimization step in conjunction with a memory model is used to remove outliers and keep the most reliable samples to train the tracker models. Experiments over various daytime and weather conditions show that our approach outperforms existing trackers at night and in case of bad weather while offering state-of-the-art performance in more favorable situations. As our tracker has only little computational cost, it is appropriate for use cases with real-time requirements like in automotive or industrial applications. © 2018 IEEE.
KW  - correlation filter
KW  - kernelized expert
KW  - Nighttime vehicle tracking
KW  - reliability estimation
KW  - weather robustness
KW  - Correlation methods
KW  - Flow visualization
KW  - Lighting
KW  - Personnel training
KW  - Reliability
KW  - Reliability analysis
KW  - Vehicles
KW  - Correlation filters
KW  - Illumination conditions
KW  - Image color analysis
KW  - kernelized expert
KW  - On-line learning methods
KW  - Real time requirement
KW  - Reliability estimation
KW  - State-of-the-art performance
KW  - Target tracking
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, L.
AU  - Cao, Z.
AU  - Cui, Z.
AU  - Cao, C.
AU  - Pi, Y.
TI  - Negative Latency Recognition Method for Fine-Grained Gestures Based on Terahertz Radar
PY  - 2020
T2  - IEEE Transactions on Geoscience and Remote Sensing
DO  - 10.1109/TGRS.2020.2985421
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095856026&doi=10.1109%2fTGRS.2020.2985421&partnerID=40&md5=b8478f023bf895a74b2ea77012253dbf
AB  - Noncontact gesture recognition is gradually being applied to emerging applications, such as smart cars and smart phones. Negative latency gesture recognition (recognition before a gesture is finished) is desirable due to the instantaneous feedback. However, it is difficult for existing methods to achieve a high precision and negative latency gesture recognition. A fragment can provide too few features to directly identify all gestures well. By observing a large number of existing gesture sets and people's daily operating habits, we found that some high frequency used gestures are similar. To the best of our knowledge, it is the first time to redivide the gestures into two subsets according to their movement physical states. We divided the gestures with different shapes or motion states into a parent-class subset, and further divided each pair of parent-class gestures to obtain a child-class subset. In order to achieve a better tradeoff between the high-precision and negative latency, an approach of motion pattern and behavior intention (MPBI) is proposed. Taking full advantage of the characteristics of each subset, MPBI includes two models. First, pattern model coarsely classify the parent-class gestures by a convolutional network, and then intention model further classifies child-class gestures according to their opposite motion direction. MPBI is evaluated on a 340-GHz terahertz radar. With the advantage of its accurate ranging, intention model can recognize child-class gestures directly without training. MPBI is evaluated on 12 gestures and achieves a recognition accuracy of 94.13%, which only needs a 0.033-s gesture fragment as an input sample.  © 1980-2012 IEEE.
KW  - Gesture recognition
KW  - high resolution range profile (HRRP)
KW  - real-time
KW  - terahertz radar
KW  - Convolutional neural networks
KW  - Radar
KW  - Set theory
KW  - Smartphones
KW  - Convolutional networks
KW  - Emerging applications
KW  - High frequency HF
KW  - Intention modeling
KW  - Motion direction
KW  - Recognition accuracy
KW  - Recognition methods
KW  - Terahertz radars
KW  - algorithm
KW  - data processing
KW  - model validation
KW  - radar
KW  - recognition
KW  - remote sensing
KW  - satellite data
KW  - Gesture recognition
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Smeulders, A.W.M.
AU  - Chu, D.M.
AU  - Cucchiara, R.
AU  - Calderara, S.
AU  - Dehghan, A.
AU  - Shah, M.
TI  - Visual tracking: An experimental survey
PY  - 2014
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
DO  - 10.1109/TPAMI.2013.230
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903121415&doi=10.1109%2fTPAMI.2013.230&partnerID=40&md5=f0acbfd2f9f028f4f3a2837b51e6c412
AB  - There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers. © 2014 IEEE.
KW  - Camera surveillance
KW  - Computer vision
KW  - Image processing
KW  - Object tracking
KW  - Tracking dataset
KW  - Tracking evaluation
KW  - Video understanding
KW  - Cameras
KW  - Computer vision
KW  - Image processing
KW  - Tracking (position)
KW  - Camera surveillance
KW  - Experimental survey
KW  - Illumination changes
KW  - Object Tracking
KW  - Realistic scenario
KW  - Tracking evaluation
KW  - Video fragments
KW  - Video understanding
KW  - Security systems
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Borrego-Carazo, J.
AU  - Castells-Rufas, D.
AU  - Biempica, E.
AU  - Carrabina, J.
TI  - Resource-Constrained Machine Learning for ADAS: A Systematic Review
PY  - 2020
T2  - IEEE Access
DO  - 10.1109/ACCESS.2020.2976513
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081676310&doi=10.1109%2fACCESS.2020.2976513&partnerID=40&md5=30daebe1e79874e3004a5ae8316936d5
AB  - The advent of machine learning (ML) methods for the industry has opened new possibilities in the automotive domain, especially for Advanced Driver Assistance Systems (ADAS). These methods mainly focus on specific problems ranging from traffic sign and light recognition to pedestrian detection. In most cases, the computational resources and power budget found in ADAS systems are constrained while most machine learning methods are computationally intensive. The usual solution consists in adapting the ML models to comply with the memory and real-time (RT) requirements for inference. Some models are easily adapted to resource-constrained hardware, such as Support Vector Machines, while others, like Neural Networks, need more complex processes to fit into the desired hardware. The ADAS hardware (HW platforms) are diverse, from complex MPSoC CPUs down to classical MCUs, DPSs and application-specific FPGAs and ASICs or specific GPU platforms (such as the NVIDIA families Tegra or Jetson). Therefore, there is a tradeoff between the complexity of the ML model implemented and the selected platform that impacts the performance metrics: function results, energy consumption and speed (latency and throughput). In this paper, a survey in the form of systematic review is conducted to analyze the scope of the published research works that embed ML models into resource-constrained implementations for ADAS applications and what are the achievements regarding the ML performance, energy and speed trade-off. © 2013 IEEE.
KW  - ADAS
KW  - automotive engineering
KW  - embedded software
KW  - FPGA
KW  - GPU
KW  - Machine learning
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Automotive engineering
KW  - Budget control
KW  - Complex networks
KW  - Economic and social effects
KW  - Embedded software
KW  - Energy utilization
KW  - Field programmable gate arrays (FPGA)
KW  - Graphics processing unit
KW  - Program processors
KW  - Support vector machines
KW  - System-on-chip
KW  - ADAS
KW  - Application specific
KW  - Automotive domains
KW  - Computational resources
KW  - Machine learning methods
KW  - Pedestrian detection
KW  - Performance metrics
KW  - Specific problems
KW  - Learning systems
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Firouzi, H.
AU  - Najjaran, H.
TI  - Robust decentralized multi-model adaptive template tracking
PY  - 2012
T2  - Pattern Recognition
DO  - 10.1016/j.patcog.2012.05.005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864277724&doi=10.1016%2fj.patcog.2012.05.005&partnerID=40&md5=835b922fe066d16fb7b75b4bb669be15
AB  - In this paper, a robust and efficient visual tracking method through the fusion of several distributed adaptive templates is proposed. It is assumed that the target object is initially localized either manually or by an object detector at the first frame. The object region is then partitioned into several non-overlapping subregions. The new location of each subregion is found by an EM 1-like gradient-based optimization algorithm. The proposed localization algorithm is capable of simultaneously optimizing several possible solutions in a probabilistic framework. Each possible solution is an initializing point for the optimization algorithm which improves the accuracy and reliability of the proposed gradient-based localization method to the local extrema. Moreover, each subregion is defined by two adaptive templates named immediate and delayed templates to solve the drift problem. 2 The immediate template is updated by short-term appearance changes whereas the delayed template models the long-term appearance variations. Therefore, the combination of short-term and long-term appearance modeling can solve the template tracking drift problem. At each tracking step, the new location of an object is estimated by fusing the tracking result of each subregion. This fusion method is based on the local and global properties of the object motion to increase the robustness of the proposed tracking method against outliers, shape variations, scale changes. The accuracy and robustness of the proposed tracking method is verified by several experimental results. The results also show the superior efficiency of the proposed method by comparing it to several state-of-the-art trackers as well as the manually labeled ground truth data. © 2012 Elsevier Ltd.
KW  - Decentralized object localization
KW  - EM algorithm
KW  - Mixture of Gaussian
KW  - Non-rigid object
KW  - Robust fusion
KW  - Pattern recognition
KW  - Software engineering
KW  - EM algorithms
KW  - Mixture of Gaussians
KW  - Non-rigid objects
KW  - Object localization
KW  - Robust fusion
KW  - Algorithms
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Wang, C.
AU  - Zhang, X.
AU  - Zang, X.
AU  - Liu, Y.
AU  - Ding, G.
AU  - Yin, W.
AU  - Zhao, J.
TI  - Feature sensing and robotic grasping of objects with uncertain information: A review
PY  - 2020
T2  - Sensors (Switzerland)
DO  - 10.3390/s20133707
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087464951&doi=10.3390%2fs20133707&partnerID=40&md5=6c016b46bddc7382309c39764285c70d
AB  - As there come to be more applications of intelligent robots, their task object is becoming more varied. However, it is still a challenge for a robot to handle unfamiliar objects. We review the recent work on the feature sensing and robotic grasping of objects with uncertain information. In particular, we focus on how the robot perceives the features of an object, so as to reduce the uncertainty of objects, and how the robot completes object grasping through the learning-based approach when the traditional approach fails. The uncertain information is classified into geometric information and physical information. Based on the type of uncertain information, the object is further classified into three categories, which are geometric-uncertain objects, physical-uncertain objects, and unknown objects. Furthermore, the approaches to the feature sensing and robotic grasping of these objects are presented based on the varied characteristics of each type of object. Finally, we summarize the reviewed approaches for uncertain objects and provide some interesting issues to be more investigated in the future. It is found that the object’s features, such as material and compactness, are difficult to be sensed, and the object grasping approach based on learning networks plays a more important role when the unknown degree of the task object increases. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
KW  - Feature sensing
KW  - Geometric uncertainty
KW  - Physical uncertainty
KW  - Robotic grasping
KW  - Uncertain objects
KW  - End effectors
KW  - Object recognition
KW  - Robotics
KW  - Geometric information
KW  - Learning network
KW  - Learning-based approach
KW  - Physical information
KW  - Robotic grasping
KW  - Three categories
KW  - Traditional approaches
KW  - Uncertain informations
KW  - learning
KW  - review
KW  - robotics
KW  - uncertainty
KW  - Intelligent robots
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Nissan, E.
TI  - Computer Applications for Handling Legal Evidence, Police Investigation and Case Argumentation
PY  - 2012
T2  - Law, Governance and Technology Series
DO  - 10.1007/978-90-481-8990-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145400546&doi=10.1007%2f978-90-481-8990-8&partnerID=40&md5=ec69acda7b39f0235302713548f42ab5
KW  - Analitic Tools
KW  - Applications of Computing
KW  - Argumentation
KW  - Argumentation Methods
KW  - Argumentation Tools
KW  - Artificial Intelligence & Law
KW  - Cadaver Dogs
KW  - Computer Forensics
KW  - Computer Techniques
KW  - Crime Analysis
KW  - Crime Detection
KW  - Criminal Investigation
KW  - Criminal Trials
KW  - DNA
KW  - Environmental Forensics
KW  - Evidence
KW  - Evidentiary Value
KW  - Exoneration
KW  - Face Processing
KW  - Facial Reconstruction
KW  - Fingerprints
KW  - Forensic Archeaology
KW  - Forensic Disciplines
KW  - Forensic Engineering
KW  - Forensic Geology
KW  - Forensic Palynology
KW  - Forensic Science
KW  - Forensic Testing
KW  - Gas Soil Surveying
KW  - Historical Perspective
KW  - Identification Methods
KW  - Identity Parades
KW  - Intelligence Analysts
KW  - Juridic Culture
KW  - Law Enforcement
KW  - Legal Evidence
KW  - Legal Narratives
KW  - Legal Professionals
KW  - Litigation
KW  - Modelling of Reasoning on Legal Evidence
KW  - Narrow Evidence Domains
KW  - Odorology
KW  - Police Intelligence
KW  - Polygraph Tests
KW  - Prosecution
KW  - Questioned Documents Evidence
KW  - Reasoning
KW  - Reasoning of Jurors
KW  - Scent Detection
KW  - Self-Incriminating Confessions
KW  - Training Police Officers
KW  - Wigmore Chart
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Xue, J.
AU  - Fang, J.
AU  - Li, T.
AU  - Zhang, B.
AU  - Zhang, P.
AU  - Ye, Z.
AU  - Dou, J.
TI  - BLVD: Building a large-scale 5D semantics benchmark for autonomous driving
PY  - 2019
T2  - Proceedings - IEEE International Conference on Robotics and Automation
DO  - 10.1109/ICRA.2019.8793523
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071462075&doi=10.1109%2fICRA.2019.8793523&partnerID=40&md5=a49212cd595b15661108ebf48edf5e12
AB  - In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/. © 2019 IEEE.
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Hou, Q.
AU  - Xing, J.
TI  - Multiobject detection algorithm based on adaptive default box mechanism
PY  - 2020
T2  - Complexity
DO  - 10.1155/2020/5763476
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091973208&doi=10.1155%2f2020%2f5763476&partnerID=40&md5=b42f1a781e5d07170a985369a4c6b578
AB  - Multiobject detection tasks in complex scenes have become an important research topic, which is the basis of other computer vision tasks. Considering the defects of the traditional single shot multibox detector (SSD) algorithm, such as poor small object detection effect, reliance on manual setting for default box generation, and insufficient semantic information of the low detection layer, the detection effect in complex scenes was not ideal. Aiming at the shortcomings of the SSD algorithm, an improved algorithm based on the adaptive default box mechanism (ADB) is proposed. The algorithm introduces the adaptive default box mechanism, which can improve the imbalance of positive and negative samples and avoid manually set default box super parameters. Experimental results show that, compared with the traditional SSD algorithm, the improved algorithm has a better detection effect and higher accuracy in complex scenes.  © 2020 Jinling Li et al.
KW  - Semantics
KW  - Signal detection
KW  - Complex scenes
KW  - Detection algorithm
KW  - Detection effect
KW  - Detection tasks
KW  - Improved * algorithm
KW  - Multiobject
KW  - Research topics
KW  - Semantics Information
KW  - Single-shot
KW  - Small object detection
KW  - Object detection
M3  - Retracted
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Hu, J.
AU  - Abubakar, S.
AU  - Liu, S.
AU  - Dai, X.
AU  - Yang, G.
AU  - Sha, H.
TI  - Near-infrared road-marking detection based on a modified faster regional convolutional neural network
PY  - 2019
T2  - Journal of Sensors
DO  - 10.1155/2019/7174602
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077800319&doi=10.1155%2f2019%2f7174602&partnerID=40&md5=646dcb9ac7163787c88e958aaeb6e4ea
AB  - Pedestrians, motorist, and cyclist remain the victims of poor vision and negligence of human drivers, especially in the night. Millions of people die or sustain physical injury yearly as a result of traffic accidents. Detection and recognition of road markings play a vital role in many applications such as traffic surveillance and autonomous driving. In this study, we have trained a nighttime road-marking detection model using NIR camera images. We have modified the VGG-16 base network of the state-of-the-art faster R-CNN algorithm by using a multilayer feature fusion technique. We have demonstrated another promising feature fusion technique of concatenating all the convolutional layers within a stage to extract image features. The modification boosts the overall detection performance of the model by utilizing the advantages of the shallow layers and the deep layers of the VGG-16 network. The training samples were augmented using random rotation and translation to enhance the heterogeneity of the detection algorithm. We have achieved a mean average precision (mAP) of 89.48% and 92.83% for the baseline faster R-CNN and our modified method, respectively. © 2019 Junping Hu et al.
KW  - Convolution
KW  - Highway markings
KW  - Infrared devices
KW  - Neural networks
KW  - Roads and streets
KW  - Autonomous driving
KW  - Convolutional neural network
KW  - Detection algorithm
KW  - Detection models
KW  - Detection performance
KW  - Physical injuries
KW  - State of the art
KW  - Traffic surveillance
KW  - Road and street markings
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Long, Q.
AU  - Xie, Q.
AU  - Mita, S.
AU  - Ishimaru, K.
AU  - Shirai, N.
TI  - Small object detection based on stereo vision
PY  - 2016
T2  - International Journal of Automotive Engineering
DO  - 10.20485/jsaeijae.7.1_9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963959026&doi=10.20485%2fjsaeijae.7.1_9&partnerID=40&md5=083c21ad38abb5e0924b143b3751d7c6
AB  - Small size objects which dimensions are around 0.15m are one of the major security risks to driving vehicles in the highway. Lidar and radar are hard to detect this kind of objects due to the sparsity of their detecting signal. Vision based methods are possible to solve this problem because camera can generate dense information. We propose a new method to detection small objects in the highway based on stereo vision. This method uses Multi-Path-Viterbi algorithm to obtain dense depth information of stereo images. Based on the depth information, road surface can be detected. Objects on road can be mapped to the 3D space to determine their size and location, then small objects dangerous to the host Vehicle can be recognized and located. © 2016 Society of Automotive Engineers of Japan, Inc.
KW  - Obstacle detection
KW  - Road environment recognition
KW  - Safety
KW  - Stereo matching
KW  - Vehicle
KW  - Accident prevention
KW  - Object detection
KW  - Obstacle detectors
KW  - Optical radar
KW  - Roads and streets
KW  - Stereo image processing
KW  - Vehicles
KW  - Viterbi algorithm
KW  - Depth information
KW  - Obstacle detection
KW  - Road environment
KW  - Road surfaces
KW  - Security risks
KW  - Small object detection
KW  - Stereo matching
KW  - Vision-based methods
KW  - Stereo vision
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Buch, N.
AU  - Velastin, S.A.
AU  - Orwell, J.
TI  - A review of computer vision techniques for the analysis of urban traffic
PY  - 2011
T2  - IEEE Transactions on Intelligent Transportation Systems
DO  - 10.1109/TITS.2011.2119372
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052346510&doi=10.1109%2fTITS.2011.2119372&partnerID=40&md5=a343a7dc439772bcca05cb6934bb9621
AB  - Automatic video analysis from urban surveillance cameras is a fast-emerging field based on computer vision techniques. We present here a comprehensive review of the state-of-the-art computer vision for traffic video with a critical analysis and an outlook to future research directions. This field is of increasing relevance for intelligent transport systems (ITSs). The decreasing hardware cost and, therefore, the increasing deployment of cameras have opened a wide application field for video analytics. Several monitoring objectives such as congestion, traffic rule violation, and vehicle interaction can be targeted using cameras that were typically originally installed for human operators. Systems for the detection and classification of vehicles on highways have successfully been using classical visual surveillance techniques such as background estimation and motion tracking for some time. The urban domain is more challenging with respect to traffic density, lower camera angles that lead to a high degree of occlusion, and the variety of road users. Methods from object categorization and 3-D modeling have inspired more advanced techniques to tackle these challenges. There is no commonly used data set or benchmark challenge, which makes the direct comparison of the proposed algorithms difficult. In addition, evaluation under challenging weather conditions (e.g., rain, fog, and darkness) would be desirable but is rarely performed. Future work should be directed toward robust combined detectors and classifiers for all road users, with a focus on realistic conditions during evaluation. © 2006 IEEE.
KW  - Closed-circuit television (CCTV)
KW  - intersection monitoring
KW  - road user counting
KW  - road users
KW  - traffic analysis
KW  - urban traffic
KW  - vehicle classification
KW  - vehicle detection
KW  - visual surveillance
KW  - Algorithms
KW  - Cameras
KW  - Computer vision
KW  - Detectors
KW  - Mathematical operators
KW  - Monitoring
KW  - Motor transportation
KW  - Roads and streets
KW  - Security systems
KW  - Television networks
KW  - Three dimensional
KW  - Vehicles
KW  - Wireless telecommunication systems
KW  - Closed-circuit television (CCTV)
KW  - Road users
KW  - traffic analysis
KW  - Urban traffic
KW  - vehicle classification
KW  - vehicle detection
KW  - Visual surveillance
KW  - Traffic congestion
M3  - Review
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Desanamukula, V.S.
AU  - Chilukuri, P.K.
AU  - Padala, P.
AU  - Padala, P.
AU  - Pvgd, P.R.
TI  - AMMDAS: Multi-modular generative masks processing architecture with adaptive wide field-of-view modeling strategy
PY  - 2020
T2  - IEEE Access
DO  - 10.1109/ACCESS.2020.3033537
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100327038&doi=10.1109%2fACCESS.2020.3033537&partnerID=40&md5=c4a310f6659a91dbabb9682ccb025700
AB  - The usage of transportation systems is inevitable; any assistance module which can catalyze the flow involved in transportation systems, parallelly improving the reliability of processes involved is a boon for day-to-day human lives. This paper introduces a novel, cost-effective, and highly responsive Post-active Driving Assistance System, which is "Adaptive-Mask-Modelling Driving Assistance System" with intuitive wide field-of-view modeling architecture. The proposed system is a vision-based approach, which processes a panoramic-front view (stitched from temporal synchronous left, right stereo camera feed) & simple monocular-rear view to generate robust & reliable proximity triggers along with co-relative navigation suggestions. The proposed system generates robust objects, adaptive field-of-view masks using FRCNN+Resnet-101_FPN, DSED neural-networks, and are later processed and mutually analyzed at respective stages to trigger proximity alerts and frame reliable navigation suggestions. The proposed DSED network is an Encoder-Decoder-Convolutional-Neural-Network to estimate lane-offset parameters which are responsible for adaptive modeling of field-of-view range (1570-2100) during live inference. Proposed stages, deep-neural-networks, and implemented algorithms, modules are state-of-the-art and achieved outstanding performance with minimal loss(L{p, t}, Lδ, LTotal) values during benchmarking analysis on our custom-built, KITTI, MS-COCO, Pascal-VOC, Make-3D datasets. The proposed assistance-system is tested on our custom-built, multiple public datasets to generalize its reliability and robustness under multiple wild conditions, input traffic scenarios & locations. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.
KW  - Adaptive field of view modeling
KW  - Automotive applications
KW  - Driving assistance systems
KW  - Lane detection and analysis
KW  - Object detection and tracking
KW  - Spatial auto-correlation
KW  - Air navigation
KW  - Benchmarking
KW  - Computer architecture
KW  - Convolutional neural networks
KW  - Cost effectiveness
KW  - Deep neural networks
KW  - Intelligent vehicle highway systems
KW  - Man machine systems
KW  - Stereo image processing
KW  - Stereo vision
KW  - Driving assistance systems
KW  - Model architecture
KW  - Processing architectures
KW  - Relative navigation
KW  - Reliability and robustness
KW  - Transportation system
KW  - Vision-based approaches
KW  - Wide field of view
KW  - Network architecture
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - JOUR
AU  - Zhu, Y.
AU  - Wang, H.
AU  - Qu, Z.
TI  - Detection method for slow and small target image based on multi-frame accumulation filter
PY  - 2017
T2  - Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics
DO  - 10.3969/j.issn.1001-506X.2017.08.05
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027706205&doi=10.3969%2fj.issn.1001-506X.2017.08.05&partnerID=40&md5=902623828b52685fbcdc7a1b8bc121f2
AB  - Aiming at the problem that slow and small target (SST) could not be detected continuously in the radar adjacent frames, resulting in radar loss of target, a detection method for SST based on image multi-frame accumulation filter is proposed. First, the interferences between the fixed clutter and the SST are removed by using the inter-frame difference technique. Then, the parameters of the minimum circumscribed rectangle, consisting of F3, F2 and F1, are introduced as the characteristic factors of the filter, and these parameters are extracted in the accumulation region by the image marking technique. Finally, the slow moving clutter is removed in the target by controlling the characteristic factor, which improves the detection performance of the radar to the SST. Simulation results verify the effectiveness of the proposed algorithm. © 2017, Editorial Office of Systems Engineering and Electronics. All right reserved.
KW  - Characte-ristic factor
KW  - Inter-frame difference technique
KW  - Multi-frame accumulation
KW  - Slow and small target (SST)
KW  - Bandpass filters
KW  - Clutter (information theory)
KW  - Radar
KW  - Radar imaging
KW  - Adjacent frames
KW  - Characte-ristic factor
KW  - Characteristic factors
KW  - Detection methods
KW  - Detection performance
KW  - Inter-frame differences
KW  - Multi-frame
KW  - Small targets
KW  - Tracking radar
M3  - Article
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Mannan, S.
TI  - Lees' Loss Prevention in the Process Industries: Hazard Identification, Assessment And Control: Fourth Edition
PY  - 2012
T2  - Lees' Loss Prevention in the Process Industries: Hazard Identification, Assessment and Control: Fourth Edition
DO  - 10.1016/C2009-0-24104-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041156174&doi=10.1016%2fC2009-0-24104-3&partnerID=40&md5=01960fdac03a4ca40047a41013ab9203
AB  - Safety in the process industries is critical for those who work with chemicals and hazardous substances or processes. The field of loss prevention is, and continues to be, of supreme importance to countless companies, municipalities and governments around the world, and Lees' is a detailed reference to defending against hazards. Recognized as the standard work for chemical and process engineering safety professionals, it provides the most complete collection of information on the theory, practice, design elements, equipment, regulations and laws covering the field of process safety. An entire library of alternative books (and cross-referencing systems) would be needed to replace or improve upon it, but everything of importance to safety professionals, engineers and managers can be found in this all-encompassing three volume reference instead. THE process safety encyclopedia, trusted worldwide for over 30 years. Now available in print and online, to aid searchability and portability. Over 3,600 print pages cover the full scope of process safety and loss prevention, compiling theory, practice, standards, legislation, case studies and lessons learned in one resource as opposed to multiple sources. © 2012 Elsevier Inc. All rights reserved.
KW  - Chemical equipment
KW  - Chemical hazards
KW  - Chemical substance
KW  - Engineering safety
KW  - Hazard Assessment
KW  - Hazard control
KW  - Hazard identification
KW  - Hazardous process
KW  - Hazardous substances
KW  - Process industries
KW  - Process safety
KW  - Safety professionals
KW  - Loss prevention
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - BOOK
AU  - Nissan, E.
TI  - Computer applications for handling legal evidence, police investigation and case argumentation
PY  - 2012
T2  - Computer Applications for Handling Legal Evidence, Police Investigation and Case Argumentation
DO  - 10.1007/978-90-481-8990-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862219121&doi=10.1007%2f978-90-481-8990-8&partnerID=40&md5=585596d82e0d9ff045f310ef26818e65
AB  - This book provides an overview of computer techniques and tools - especially from artificial intelligence (AI) - for handling legal evidence, police intelligence, crime analysis or detection, and forensic testing, with a sustained discussion of methods for the modelling of reasoning and forming an opinion about the evidence, methods for the modelling of argumentation, and computational approaches to dealing with legal, or any, narratives. By the 2000s, the modelling of reasoning on legal evidence has emerged as a significant area within the well-established field of AI & Law. An overview such as this one has never been attempted before. It offers a panoramic view of topics, techniques and tools. It is more than a survey, as topic after topic, the reader can get a closer view of approaches and techniques. One aim is to introduce practitioners of AI to the modelling legal evidence. Another aim is to introduce legal professionals, as well as the more technically oriented among law enforcement professionals, or researchers in police science, to information technology resources from which their own respective field stands to benefit. Computer scientists must not blunder into design choices resulting in tools objectionable for legal professionals, so it is important to be aware of ongoing controversies. A survey is provided of argumentation tools or methods for reasoning about the evidence. Another class of tools considered here is intended to assist in organisational aspects of managing of the evidence. Moreover, tools appropriate for crime detection, intelligence, and investigation include tools based on link analysis and data mining. Concepts and techniques are introduced, along with case studies. So are areas in the forensic sciences. Special chapters are devoted to VIRTOPSY (a procedure for legal medicine) and FLINTS (a tool for the police). This is both an introductory book (possibly a textbook), and a reference for specialists from various quarters. © Springer Science+Business Media Dordrecht 2012.
KW  - Crime
KW  - Data mining
KW  - Surveys
KW  - Computational approach
KW  - Computer scientists
KW  - Computer techniques
KW  - Organisational aspects
KW  - Panoramic views
KW  - Police intelligences
KW  - Police investigations
KW  - Techniques and tools
KW  - Forensic science
M3  - Book
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - Belaroussi, R.
AU  - Gruyer, D.
TI  - Convergence of a Traffic Signs-Based Fog Density Model
PY  - 2015
T2  - IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC
DO  - 10.1109/ITSC.2015.132
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950272967&doi=10.1109%2fITSC.2015.132&partnerID=40&md5=c4dfb7b083695d75fd4cdea85152c037
AB  - Multiple-vehicle accidents caused by reduced visibility conditions occur during localized fog and often result in fatalities and injuries. The extreme variability in density, predictability, and location of the hazard further complicates the task of improving highway safety conditions. This paper presents a novel approach for estimating visibility condition using an onboard camera and a digital map encoding traffic signs informations. Ability to respond in a timely fashion to sudden local change in visibility conditions is specifically investigated. © 2015 IEEE.
KW  - Accidents
KW  - Highway engineering
KW  - Intelligent systems
KW  - Intelligent vehicle highway systems
KW  - Transportation
KW  - Visibility
KW  - Density modeling
KW  - Digital map
KW  - Highway safety
KW  - Onboard camera
KW  - Reduced visibility
KW  - Vehicle accidents
KW  - Visibility conditions
KW  - Traffic signs
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

TY  - CONF
AU  - John, V.
AU  - Nithilan, M.K.
AU  - Mita, S.
AU  - Tehrani, H.
AU  - Konishi, M.
AU  - Ishimaru, K.
AU  - Oishi, T.
TI  - Sensor Fusion of Intensity and Depth Cues using the ChiNet for Semantic Segmentation of Road Scenes
PY  - 2018
T2  - IEEE Intelligent Vehicles Symposium, Proceedings
DO  - 10.1109/IVS.2018.8500476
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056777877&doi=10.1109%2fIVS.2018.8500476&partnerID=40&md5=311c1dc179fe10800dca0d717af4ac36
AB  - Vision-based environment perception is an important research topic for autonomous driving and advanced driver assistance systems. Vision sensors, such as the monocular camera and stereo camera, are widely used for environment perception. The monocular camera provides the appearance information like intensity, and the stereo camera provides the depth information. The appearance and depth information are complementary, and their effective fusion would result in robust environment perception. Consequently, in this paper, we propose a novel deep learning framework, termed as the ChiNet, for the effective sensor fusion of the appearance and depth information for free space and road object estimation. The ChiNet has two input branches and two output branches. The ChiNet input branches contains separate branches for the intensity and depth information. For the output branches, the ChiNet contains separate branches for the free space and road object semantic segmentation. A comparative of the proposed framework with state-of-the-art baseline algorithms is performed using an acquired dataset. Moreover, a detailed parameter analysis is performed to validate the ChiNet architecture as well as the advantages of sensor fusion. The experimental results show that the ChiNet is better than baseline algorithms. We also show that the proposed ChiNet architecture is better than other variations of the ChiNet architecture. © 2018 IEEE.
KW  - Automobile drivers
KW  - Cameras
KW  - Deep learning
KW  - Roads and streets
KW  - Semantics
KW  - Stereo image processing
KW  - Stereo vision
KW  - Autonomous driving
KW  - Depth information
KW  - Environment perceptions
KW  - Learning frameworks
KW  - Monocular cameras
KW  - Object estimation
KW  - Parameter analysis
KW  - Semantic segmentation
KW  - Advanced driver assistance systems
M3  - Conference paper
DB  - Scopus
N1  - Export Date: 10 July 2025
ER  -

