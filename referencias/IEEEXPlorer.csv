"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Traffic Scene Object Detection Method Combining Deep Learning and Stereo Vision Algorithm","L. Li; M. Fang; Y. Yin; J. Lian; Z. Wang","Faculty of Vehicle Engineering and Mechanics, School of Automotive Engineering, Dalian University of Technology, Dalian, China; Faculty of Vehicle Engineering and Mechanics, School of Automotive Engineering, Dalian University of Technology, Dalian, China; Faculty of Vehicle Engineering and Mechanics, School of Automotive Engineering, Dalian University of Technology, Dalian, China; Faculty of Vehicle Engineering and Mechanics, School of Automotive Engineering, Dalian University of Technology, Dalian, China; Faculty of Vehicle Engineering and Mechanics, School of Automotive Engineering, Dalian University of Technology, Dalian, China",2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),"31 Aug 2021","2021","","","1134","1138","Object detection has been an important topic in the field of intelligent vehicle. The deep learning object detection is based on monocular vision without depth of the scene information and the detection of small objects in complex traffic scenes often has problems such as misdetection and omission, so the detection of small objects in traffic scenes is still a big challenge. In this paper, a method combining depth information with YOLOv5s object detection algorithm is proposed to improve the accuracy of object detection. Firstly, the depth information is obtained by the disparity images which generated by the end-to-end PSMNet network. Secondly, add an Attention Feature Fusion Module (AFFM) to YOLOv5s to improve the accuracy of small object detection. Finally, the depth information is fused with the object detection to reduce the probability of missed detection and false detection, and the distance information is also obtained. The experimental results show that the SUPER_YOLOv5s object detection algorithm combined with stereo vision can reduce the rate of missed detection and improve the detection accuracy.","","978-1-6654-3678-6","10.1109/RCAR52367.2021.9517460","National Natural Science Foundation of China(grant numbers:61976039,51775082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517460","","Deep learning;Intelligent vehicles;Conferences;Object detection;Feature extraction;Real-time systems;Stereo vision","","2","","11","IEEE","31 Aug 2021","","","IEEE","IEEE Conferences"
"Fast Traffic Sign and Light Detection using Deep Learning for Automotive Applications","H. Naimi; T. Akilan; M. A. S. Khalid","dept. of electrical and computer engg., University of Windsor, Windsor, Canada; dept. of software engg., Lakehead University, Thunder Bay, Canada; dept. of electrical and computer engg., University of Windsor, Windsor, Canada",2021 IEEE Western New York Image and Signal Processing Workshop (WNYISPW),"30 Dec 2021","2021","","","1","5","Traffic sign and light detections are core components of Advanced Driver Assistance Systems (ADAS) and self-driving vehicles. To this end, the automotive industry is widely exploiting computer vision (CV) and deep learning (DL) techniques. This paper presents a lightweight traffic sign and light detector by harnessing a single-stage, single-shot multi-object detector (SSD). For accelerating the inference speed of the detector, its original backbone, VGG16 is replaced by MobileNet V2 that expertly manages detection speed and network size. In autonomous driving, quicker detection performance with respect to the distance of an object is of particular interest, for a comfortable braking. However, farther distance makes the objects to be detected appear smaller. Unfortunately, the original SSD struggles to detect small objects. Thus, this work further optimizes the number of feature map layers of the SSD for the detection of small objects along with a better trade-off between detection precision and inference time. Experimental analysis confirms the effectiveness of the proposed model, which achieves 2 times (or more) faster detection time than the baseline SSD models and a competitive precision of 76.7%.","2471-9242","978-1-6654-3630-4","10.1109/WNYISPW53194.2021.9661284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661284","Computer vision;object detection;ADAS;deep learning","Deep learning;Industries;Computer vision;Analytical models;Computational modeling;Conferences;Detectors","","12","","25","IEEE","30 Dec 2021","","","IEEE","IEEE Conferences"
"A Survey of Deep Learning Approaches for Pedestrian Detection in Autonomous Systems","M. Sukkar; R. Jadeja; M. Shukla; R. Mahadeva","Department of Artificial Intelligence, Machine Learning and Data Science, Marwadi University, Rajkot, Gujarat, India; Electrical Engineering Department, Marwadi University, Rajkot, Gujarat, India; Department of Artificial Intelligence, Machine Learning and Data Science, Marwadi University, Rajkot, Gujarat, India; Department of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, Karnataka, India",IEEE Access,"9 Jan 2025","2025","13","","3994","4007","This paper surveys real-time object detection literature critically and analytically, focusing particularly on pedestrian detection for safe autonomous vehicles. It addresses the challenges in the domain, some of the sources of which are variations in age, gender, clothing, lighting, backgrounds, and occlusion. The paper reviews object detection algorithms after providing an overview of deep learning basics and main architectures of neural networks, followed by discussion on existing algorithms along with their strengths, weaknesses, and future research directions. There is a need for pedestrian detection datasets with further complex annotations and multi-source integration, which captures interactions between pedestrians and their surroundings. Incorporating advanced sensors, including LiDAR, infrared, and depth sensors, as the foremost means to enhance the detection capabilities in more adverse conditions, such as low-light situations and occlusion. However, architectures such as YOLO, SSD, and Faster R-CNN, which have led to current improvements in performance, still allow room for improving pedestrian detection accuracy. By filling in these insights and proposed solutions, the paper focus on the development of pedestrian detection technology, how it can be brought into a safer, reliable, real-world applicability towards the system of autonomous driving. All of these results point to continued innovation towards deep learning, multi-sensor integration, and developing datasets to achieve optimal performance levels in real world conditions for autonomous driving systems.","2169-3536","","10.1109/ACCESS.2024.3524501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10818658","Artificial intelligence;autonomous vehicle;computer vision;deep learning;pedestrian detection","Pedestrians;Computer architecture;Deep learning;Autonomous vehicles;Accidents;Reviews;Real-time systems;YOLO;Filters;Safety","","1","","97","CCBY","30 Dec 2024","","","IEEE","IEEE Journals"
"YOLO v8_CAT: Enhancing Small Object Detection in Traffic Light Recognition with Combined Attention Mechanism","X. Qu; Y. Zheng; Y. Zhou; Z. Su","College of Artificial Intelligence, Zhujiang College, South China Agricultural University, Guangzhou, China; College of Artificial Intelligence, Zhujiang College, South China Agricultural University, Guangzhou, China; College of Artificial Intelligence, Zhujiang College, South China Agricultural University, Guangzhou, China; College of Artificial Intelligence, Zhujiang College, South China Agricultural University, Guangzhou, China",2024 10th International Conference on Computer and Communications (ICCC),"2 Apr 2025","2024","","","706","710","This paper introduces YOLO v8_CAT, an advanced object detection model designed to improve the accuracy of small and challenging object detection in traffic light recognition tasks. Traditional object detection models, including earlier YOLO versions, often struggle with accurately detecting small objects and differentiating traffic light states (GREEN, RED, YELLOW, and OFF) due to limited feature refinement capabilities. YOLO v8_CAT enhances the baseline YOLOvS model by incorporating a Combined Attention Mechanism (CAT), which integrates channel and spatial attention paths to prioritize critical features. The channel attention path focuses on essential feature channels, while the spatial attention path emphasizes important regions, allowing YOLO v8_CAT to handle small and complex objects more effectively. Evaluated on the Bosch Small Traffic Lights Dataset, YOLO v8_CAT demonstrates significant performance improvements, achieving 84.5% accuracy for GREEN, 63.8% for RED, 21.5% for YELLOW, and 25.8% for OFF, outperforming YOLOv8 by notable margins. In particular, YOLO v8_CAT shows a 12.4% increase in accuracy for GREEN and an 8% improvement for RED detection. Additionally, YOLO v8_CAT maintains the high real-time processing speed of YOLOv8 at 1010 FPS. These results establish YOLO v8_CAT as a superior model for traffic light detection in autonomous driving, enhancing both detection precision and robustness without compromising computational efficiency.","2837-7109","979-8-3315-0707-7","10.1109/ICCC62609.2024.10941967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10941967","deep learning;object detection;YOLO v8;Traffic Lights detection","YOLO;Deep learning;Accuracy;Attention mechanisms;Computational modeling;Feature extraction;Real-time systems;Robustness;Computational efficiency;Autonomous vehicles","","1","","20","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"PE-SSD: Improved SSD for Small Object Detection","Z. Tang; L. Zhu; W. Wang; Y. Gong","School of Information Science and Engineering, Wuhan University of Science and Technology, WuHan, China; School of Information Science and Engineering, Wuhan University of Science and Technology, WuHan, China; School of Information Science and Engineering, Wuhan University of Science and Technology, WuHan, China; School of Information Science and Engineering, Wuhan University of Science and Technology, WuHan, China",2024 China Automation Congress (CAC),"13 Feb 2025","2024","","","383","389","With the flourishing development of deep learning, object detection technology has achieved encouraging results. However, small objects remain a major challenge in the field of object detection due to their lack of texture and contextual information caused by their small size. Despite such fact, we observed that their edges typically exhibit high contrast in relation to the background. Specifically, we innovatively employ a Laplacian image pyramid incorporated with feature enhancement modules rather than the traditional image pyramid, with the aim of more effectively capturing image details and edge information, given that the Laplacian pyramid has the ability to retain such information within its lower levels. Furthermore, we introduce attention mechanisms to optimize the feature fusion process, thus improving the utilization efficiency of multi-scale features. Taking SSD as the baseline, we propose a novel method called PE-SSD, which integrates the advantages of featureized Laplacian pyramid. Experimental results show that our method achieves higher accuracy in detecting small objects compared to traditional SSD. For inputs of size 512 × 512, the extensive experiments on the VisDrone2019 dataset demonstrates that our method achieves an average precision of 15.4%, surpassing traditional SSD networks by 1.2%. Specifically, The mAP_s score reaches 6.6%, which is 0.4% higher than the baseline network, indicating the effectiveness of our method in the field of small object detection.","2688-0938","979-8-3503-6860-4","10.1109/CAC63892.2024.10865724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10865724","Small Object Detection;Laplacian Pyramid;Feature Pyramid;Feature Fusion;Attention Module","Deep learning;Laplace equations;Image edge detection;Surveillance;Computational modeling;Object detection;Real-time systems;Computational efficiency;Standards;Autonomous vehicles","","","","36","IEEE","13 Feb 2025","","","IEEE","IEEE Conferences"
"Deep Neural Networks: Improved Object Detection for Autonomous Vehicles","E. Kelmendi; K. Jashari; D. Samanta","Department of CIT, RIT Kosovo (A.U.K), Rochester Institute of Technology - RIT Global Campus, Prishtina, Kosovo; Department of CIT, RIT Kosovo (A.U.K), Rochester Institute of Technology - RIT Global Campus, Prishtina, Kosovo; Department of CIT, RIT Kosovo (A.U.K), Rochester Institute of Technology - RIT Global Campus, Prishtina, Kosovo",2025 International Conference on Intelligent and Cloud Computing (ICoICC),"1 Jul 2025","2025","","","1","6","Deep neural networks may have the potential to revolutionize the way an autonomous vehicle perceives the world through methodologies such as object detection. This will require the interaction of neural networks of different kinds to equip the vehicle with capabilities to identify, for example, signage, images, and other entities associated with vehicles, thereby improving transport efficiency. According to many researchers, deep neural networks do not provide varied performance and low accuracy in identification of images. The study shows challenges that number mock image recognition neural networks thus trying to fill up the gap of integration of different neural networks with an autonomous vehicle system. General methods of the study consist of careful observation of the already existing literature, comparative analysis of deep learning networks, and interpretive figures and tables. As much of the study goes on, so many findings about how autonomous vehicles are added to neural networks for deception detection and optimization through the merging of different deep networks have emerged. Finally, the outputs of this paper constitute one of the very critical aspects of the automotive industry, the technology area of sensors, and artificial intelligence.","","979-8-3315-1302-3","10.1109/ICoICC64033.2025.11052045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11052045","Neural Networks;Autonomous Vehicles;Object detection;Artificial Intelligence;Image-to-Image Networks","YOLO;Deep learning;Technological innovation;Accuracy;Artificial neural networks;Real-time systems;Artificial intelligence;Autonomous vehicles;Automotive engineering;Synthetic data","","","","10","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"An Improved Lightweight Network for Real-Time Detection of Potential Risks for Autonomous Vehicles","X. Shen; V. V. Lukyanov","Faculty of Computer Science and Control System, Bauman Moscow State Technical University, Moscow, Russia; Faculty of Computer Science and Control System, Bauman Moscow State Technical University, Moscow, Russia",2024 International Russian Automation Conference (RusAutoCon),"4 Oct 2024","2024","","","583","588","In this study, we explore advancements in 2D object detection, focusing on the well-regarded KITTI dataset used extensively in autonomous driving research. We present YOLOv8, the latest iteration of cutting-edge single-stage detectors, and detail our network enhancements designed to boost its performance. Our method integrates the BiFPN (Bifurcated Feature Pyramid Network), optimizing the fusion of semantic and localization information crucial for precise object detection. We also add a specialized tiny object detection layer to effectively handle the challenges of small-scale object detection. To further improve our model, we employ extensive dataset augmentation techniques during training, which greatly enhance the network's robustness and generalization abilities. Additionally, we introduce the MCA (Multidimensional Collaborative Attention) module, a sophisticated attention mechanism that significantly improves feature extraction. Through thorough ablation studies, we validate the effectiveness of our enhancements, especially in detecting small targets within the KITTI dataset. This comprehensive research highlights the significance of state-of-the-art technologies like YOLOv8 and demonstrates the potential of our proposed modifications to advance the area of 2D visual object recognition. And detection","2836-614X","979-8-3503-4981-8","10.1109/RusAutoCon61949.2024.10694343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10694343","autonomous driving;object detection;KITTI dataset;YOLO networks;attention mechanism","Location awareness;Training;Visualization;Accuracy;Roads;Semantics;Object detection;Feature extraction;Real-time systems;Autonomous vehicles","","","","15","IEEE","4 Oct 2024","","","IEEE","IEEE Conferences"
"Expandable Spherical Projection and Feature Fusion Methods for Object Detection from Fisheye Images","S. Kim; S. -Y. Park",School of Electronic and Electrical Engineering Kyungpook National University; School of Electronics Engineering Kyungpook National University,2021 17th International Conference on Machine Vision and Applications (MVA),"19 Aug 2021","2021","","","1","5","One of the key requirements for enhanced autonomous driving systems is accurate detection of the objects from a wide range of view. Large-angle images from a fisheye lens camera can be an effective solution for automotive applications. However, it comes with the cost of strong radial distortions. In particular, the fisheye camera has a photographic effect of exaggerating the size of objects in central regions of the image, while making objects near the marginal area appear smaller. Therefore, we propose the Expandable Spherical Projection that expands center or margin regions to produce straight edges of de-warped objects with less unwanted background in the bounding boxes. In addition to this, we analyze the influence of multi-scale feature fusion in a real-time object detector, which learns to extract more meaningful information for small objects. We present three different types of concatenated YOLOv3-SPP architectures. Moreover, we demonstrate the effectiveness of our proposed projection and feature-fusion using multiple fisheye lens datasets, which shows up to 4.7% AP improvement compared to fisheye images and baseline model.","","978-4-901122-20-7","10.23919/MVA51890.2021.9511379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511379","","Machine vision;Image edge detection;Object detection;Feature extraction;Cameras;Distortion;Real-time systems","","1","","23","","19 Aug 2021","","","IEEE","IEEE Conferences"
"Applying the Improved YOLOv7 Algorithm to Improve the Real-time Object Detection and Obstacle Avoidance Capabilities of Smart Cars","L. Ding","College of Opto-Mechanical and Electrical Engineering, Zhejiang Agricultural and Forestry University, Hangzhou, China",2025 5th International Conference on Artificial Intelligence and Industrial Technology Applications (AIITA),"1 Jul 2025","2025","","","1559","1563","To solve the problem of missed detection of small targets and real-time obstacle avoidance of smart cars, this study introduces an improved YOLOv7 (You Only Look Once Version 7) algorithm. First, the channel pruning and quantization compression model is used, and the E-ELAN (Enhanced Efficient Layer Aggregation Network) module is optimized to enhance the fusion of small target features; DSNT (Differentiable Spatial to Numerical Transform) distance prediction, dynamic NMS (Non-Maximum Suppression) optimization and trajectory prediction are integrated to achieve efficient obstacle avoidance warning. The results show that the model has a small target mAP@0.5 (mean Average Precision) of 0.82 in the KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) dataset, an inference speed of 29.5 FPS (Frames Per Second) for traffic signs, and a 25.3% reduction in video memory usage, verifying its practical value in complex scenarios.","","979-8-3315-0976-7","10.1109/AIITA65135.2025.11047665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11047665","You Only Look Once Version 7;Intelligent Vehicle;Small Object Detection;Real-Time Obstacle Avoidance","YOLO;Adaptation models;Quantization (signal);Heuristic algorithms;Depth measurement;Real-time systems;Trajectory;Numerical models;Automobiles;Collision avoidance","","","","11","IEEE","1 Jul 2025","","","IEEE","IEEE Conferences"
"Railway Traffic Object Detection Using Differential Feature Fusion Convolution Neural Network","T. Ye; X. Zhang; Y. Zhang; J. Liu","School of Mechanical Electronic & Information Engineering, China University of Mining and Technology–Beijing, Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining and Technology–Beijing, Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining and Technology–Beijing, Beijing, China; School of Mechanical Electronic & Information Engineering, China University of Mining and Technology–Beijing, Beijing, China",IEEE Transactions on Intelligent Transportation Systems,"1 Mar 2021","2021","22","3","1375","1387","Railway shunting accidents, in which trains collide with obstacles, often occur because of human error or fatigue. It is therefore necessary to detect traffic objects in front of the trains and inform the driver to take timely action. To detect these objects in railways, we proposed an object-detection method using a differential feature fusion convolutional neural network (DFF-Net). DFF-Net includes two modules: the prior object-detection module and the object-detection module. The prior module produces initial anchor boxes for the subsequent detection module. Taking the initial anchor boxes as input, the object-detection module applies a differential feature fusion sub-module to enrich the sematic information for object detection, enhancing the detection performance, particularly for small objects. In experiments conducted on a railway traffic dataset, compared with the current state-of-the-art detectors, the proposed method exhibited significant higher performance and was more effective and more efficient than the other methods for object detection in railway tracks. Additionally, evaluation results based on PASCAL VOC2007 and VOC2012 indicated that the proposed method was significantly better than the state-of-the-art methods.","1558-0016","","10.1109/TITS.2020.2969993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978612","Railway traffic object detection;differential feature fusion convolutional neural network;prior module","Rail transportation;Feature extraction;Object detection;Safety;Radar tracking;Real-time systems;Detectors","","82","","31","IEEE","3 Feb 2020","","","IEEE","IEEE Journals"
"Real-time vehicle and lane detection with embedded hardware","J. Kaszubiak; M. Tornow; R. W. Kuhn; B. Michaelis; C. Knoeppel","Signal Processing and Communications, Magdeburg University, Germany; Signal Processing and Communications, Magdeburg University, Germany; Institute for Electronics, Signal Processing and Communications, Magdeburg University, Germany; Signal Processing and Communications, Magdeburg University, Germany; Research and Technology Information and Communication, DaimlerChrysler Aerospace, Stuttgart, Germany","IEEE Proceedings. Intelligent Vehicles Symposium, 2005.","12 Sep 2005","2005","","","619","624","For autonomously acting robots and driver assistance systems powerful optical stereo sensor systems are required. Object positions and environmental conditions have to be acquired in real-time. In this paper an algorithm based on a hardware-software co-design is applied. A depth-map is generated with a hierarchical detection method. A depth-histogram is generated by using the density distribution of the disparity in the depth-map. It is used for object detection. The object clustering can be accomplished without calculation of 3D-points, due to the almost identical mapping of the objects over the whole distance, within the histogram. A lane detection is applied by using a Hough transform. The suitability at night and the detection of small objects like bikers is proven.","1931-0587","0-7803-8961-1","10.1109/IVS.2005.1505172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1505172","","Vehicle detection;Hardware;Object detection;Clustering algorithms;Cameras;Laser radar;Optical signal processing;Sensor systems;Histograms;Real time systems","","14","","18","IEEE","12 Sep 2005","","","IEEE","IEEE Conferences"
"YOLO-CCA: An Encoder-Decoder Framework Vehicle Detector Based on Channel Attention","X. Liu; L. Liu; T. Liu","School of Civil and Transportation Engineering, Hebei University of Technology, Tianjin, China; School of Computer Science and Engineering, Guangxi Normal University, Guilin, China; School of Aeronautics, Northwestern Polytechnical University, Xi'an, China","2023 IEEE 5th International Conference on Power, Intelligent Computing and Systems (ICPICS)","5 Sep 2023","2023","","","742","747","Vehicle detection (VD), aiming at detecting the position of vehicles, can largely promote the development of connected and autonomous vehicles and intelligent transportation systems. Most VD algorithms cannot meet the demand of real-time road monitoring, due to some unsolved problems: (i) low accuracy for small vehicle targets that is demanding for real-time vehicle monitoring; (ii) high computational cost that is hard to deploy in edge devices. To solve these problems, this paper introduces a novel VD algorithm called YOLO-CCA inspired by channel attention technique. Efficient Channel Attention (ECA) and Encoder-Decoder modules are adopted into YOLO-CCA algorithm for VD. With the introduction of the Complete-IoU (CIoU) loss function, the convergence rate is also accelerated. YOLO-CCA algorithm is capable of capturing cross-channel information and maximizing it, which leads to high accuracy for vehicle detection, especially for small vehicle targets detection. Extensive experiments based on the UA-DETRAC public dataset demonstrate that YOLO-CCA outperforms the other four baseline algorithms, and improve 5.6% to the best baseline algorithm in small vehicle targets detection.","2834-8567","979-8-3503-3344-2","10.1109/ICPICS58376.2023.10235599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235599","Vehicle detection;Connected and autonomous vehicles;computer vision;Intelligent transportation systems","Training;Vehicle detection;Roads;Image edge detection;Transportation;Object detection;Feature extraction","","1","","18","IEEE","5 Sep 2023","","","IEEE","IEEE Conferences"
"SP-Pillars: An Efficient LiDAR 3D Objects Detection Framework With Multi-Scale Feature Perception and Optimization","T. Chen; Y. Yuan; B. Yin; Y. Liao","School of AI and Innovative Design, Beijing Institute of Fashion Technology, Beijing, China; School of AI and Innovative Design, Beijing Institute of Fashion Technology, Beijing, China; School of AI and Innovative Design, Beijing Institute of Fashion Technology, Beijing, China; School of AI and Innovative Design, Beijing Institute of Fashion Technology, Beijing, China",IEEE Access,"2 May 2025","2025","13","","74092","74106","In autonomous driving, achieving rapid detection of target categories and locations is a key technology. However, the data volume of radar point clouds is enormous, and processing efficiency becomes a limiting factor, so the balance between speed and accuracy is crucial. To address this challenge, this paper proposes a 3D object detection algorithm SP-Pillars that can effectively learn point cloud features. Firstly, a Pillar Feature Weighted Network (PFWNet) is proposed for processing point cloud information, which divides the point cloud into pillar structures and uses SPCV feature attention network to focus on its multi-level feature information. After feature extraction and dimensionality reduction, pseudo images are generated. Subsequently, before extracting pseudo image features, a multi-core perception network (PKINet) is introduced to further mine local contextual information and reduce computational complexity, enabling the backbone network to effectively learn features. The experimental evaluation results on the KITTI dataset indicate that the proposed algorithm is reliable and effective. Compared with other related algorithms, this algorithm exhibits excellent detection performance, slightly improving detection speed while maintaining high accuracy, meeting the requirements of real-time processing, and has important application value in optimizing autonomous driving technology.","2169-3536","","10.1109/ACCESS.2025.3564665","Research on the Cultivation Path of Compound Academic Literacy of Graduate Students in the Emerging Interdisciplinary Field of “Clothing Studies” of Beijing Institute of Fashion Technology, in 2024(grant numbers:J2024-YB14); Professional Teachers Artificial Intelligence Comprehensive Literacy Industry-University Collaborative Cultivation Project under the Background of New Engineering of Beijing Institute of Fashion Technology, in 2024(grant numbers:231103873221617); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10978003","3D object detection;autonomous driving;feature weighting;multi-scale;point cloud;residual structure","Feature extraction;Point cloud compression;Three-dimensional displays;Object detection;Convolution;Accuracy;Representation learning;Real-time systems;Autonomous vehicles;Computational efficiency","","","","62","CCBY","28 Apr 2025","","","IEEE","IEEE Journals"
"YOLOv8n-FAWL: Object Detection for Autonomous Driving Using YOLOv8 Network on Edge Devices","Z. Cai; R. Chen; Z. Wu; W. Xue","School of Electronic and Electrical Engineering, Zhaoqing University, Zhaoqing, China; School of Electronic and Electrical Engineering, Zhaoqing University, Zhaoqing, China; School of Electronic and Electrical Engineering, Zhaoqing University, Zhaoqing, China; School of Electronic and Electrical Engineering, Zhaoqing University, Zhaoqing, China",IEEE Access,"1 Nov 2024","2024","12","","158376","158387","In the field of autonomous driving, common challenges include difficulties in detecting small vehicles and pedestrians on the road, high computational demands of algorithms, and low accuracy of detection algorithms. This paper proposes a YOLOv8n-FAWL object detection algorithm tailored for edge computing, incorporating the following three improvements: (1) The Faster-C2f-EMA module is created, designed through the synergy of the FasterNet architecture and the concept of EMA modules, effectively addressing the challenge of suboptimal feature extraction for small objects. (2) The WIOU loss function is adopted to resolve the issue of imbalanced training samples. (3) The LAMP pruning technique is applied to reduce the model parameters and complexity, thereby enhancing the overall model accuracy. The experimental results show that compared to the baseline model, the proposed algorithm achieves improvements of 6.2% and 4.5% in the mAP@0.5, and 3.8% and 2.7% in the mAP@0.5:0.95, on the Udacity and BDD100K-tiny datasets,respectively. In addition, the model parameters we’re reduced by 49.2% and 46%. The model achieved real-time performance at 54 FPS, thereby advancing the development of autonomous driving technology.","2169-3536","","10.1109/ACCESS.2024.3480976","National College Students Innovation and Entrepreneurship Training Program(grant numbers:202310580013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10718290","YOLOv8n;autonomous driving;LAMP;FasterNet;EMA;WIOU","Computational modeling;Training;Autonomous vehicles;YOLO;Accuracy;Real-time systems;Performance evaluation;Feature extraction;Neck;Computational efficiency","","","","35","CCBYNCND","15 Oct 2024","","","IEEE","IEEE Journals"
"YOLOP-MVF: A Multi-Task Autonomous Driving Perception Detection Method Based on Multi Scale Feature Weighted Fusion","Y. Niu; J. Zhang","Department of Basic Sciences, Jilin University of Architecture and Technology, Changchun, China; Department of Public Instruction, Anhui Vocational College of City Management, Hefei, China",IEEE Access,"30 May 2025","2025","13","","91374","91383","To address challenges such as large-scale variations, background interference, and occlusions in multi-task autonomous driving perception, this paper proposes YOLOP-MVF, a multi-task detection framework based on multi-scale feature weighting fusion. The model integrates a sub-pixel 3D fusion module and a triple feature encoding module to enhance the representation of multi-scale features. A multi-scale convolutional attention-weighting mechanism is further introduced to adaptively emphasize critical spatial information. To improve feature extraction flexibility, deformable convolutions are incorporated, enabling dynamic sampling based on input characteristics. Additionally, the Powerful-IoU loss is employed to guide anchor box regression with adaptive penalty and gradient regulation, accelerating convergence. Experimental results on the BDD100K dataset demonstrate that YOLOP-MVF outperforms baseline models, achieving improvements of 1.2% in mIoU, 8.8% in accuracy, and 4.7% in mAP50, validating its effectiveness for robust multi-task perception in complex driving scenarios.","2169-3536","","10.1109/ACCESS.2025.3572331","Research on the Reform of Experimental Teaching of Mathematical Modeling in Applied Colleges and Universities through “Teaching and Competition Integration”(grant numbers:JGJX2022B56); Research on Medical Image Restoration based on Convolutional Neural Network (Scientific Research Project in Anhui)(grant numbers:2024AH050031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11008744","Autonomous driving;multitask learning;drivable area segmentation;lane detection;vehicle detection","Feature extraction;Roads;Object detection;Multitasking;Autonomous vehicles;Lane detection;Image segmentation;Convolutional neural networks;Computational modeling;Accuracy","","","","27","CCBY","21 May 2025","","","IEEE","IEEE Journals"
"Novel Pipeline Integrating Cross-Modality and Motion Model for Nearshore Multi-Object Tracking in Optical Video Surveillance","J. Ding; W. Li; L. Pei; M. Yang; A. Tian; B. Yuan","School of Information Engineering, Chang’an University, Xi’an, China; School of Information Engineering, Chang’an University, Xi’an, China; School of Data Science and Artificial Intelligence, Chang’an University, Xi’an, China; School of Information Engineering, Chang’an University, Xi’an, China; School of Information Engineering, Chang’an University, Xi’an, China; School of Information Engineering, Chang’an University, Xi’an, China",IEEE Transactions on Intelligent Transportation Systems,"29 Aug 2024","2024","25","9","12464","12476","Nearshore multi-object tracking (NMOT) aims to locat and identify nearshore objects. Most approaches accomplish this task using radar and remote-sensing technologies. In contrast, video data can describe the visual appearance of nearshore objects without prior information, such as identity, location, or movement. In this study, we introduce a cross-modality pipeline to address the four major challenges of NMOT. First, we propose introducing a cross-modality bi-attention transformer (CBT) manage the information interaction between RGB and thermal infrared videos effectively. This decoupling and guidance mechanism laid the foundation for our subsequent processes. Next, we integrate the outputs of the backbone with historical frames to extract crucial temporal features. Subsequently, we refine small object detection performance by employing multi-scale feature alignment (MFA). Observations are generated by the transformer decoder. To tackle challenges arising from extensive occlusion and interactions induced by waves in NMOT, we propose guiding modulation (GM), supplemented by low-confidence boxes and multi-point corner momentum (MCM) to facilitate association. Our approach is simple, online, and real-time, showcasing outstanding performance in benchmark evaluations. The open-source implementation of our work is available at https://github.com/Ding-JianGang/Cross-Modality-MOT-in-Nearshore-Environments.","1558-0016","","10.1109/TITS.2024.3373370","National Natural Science Foundation of China(grant numbers:51978071); Fundamental Research Funds for the Central Universities, CHD(grant numbers:300102249301,300102249306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10474332","Intelligent nearshore transportation;multi-object tracking;cross-modality;temporal context;multi-scale feature alignment;guiding modulation","Detectors;Feature extraction;Transformers;Target tracking;Intelligent transportation systems;Object tracking;Optical fiber communication;Video surveillance","","12","","50","IEEE","18 Mar 2024","","","IEEE","IEEE Journals"
"Airy YOLOv5 for Disabled Sign Detection","A. A. U. Rakhmonov; B. Subramanian; T. Kim; J. Kim","Computer Science and Engineering Department, Kyungpook National University, Daegu, South Korea; Computer Science and Engineering Department, Kyungpook National University, Daegu, South Korea; Dipvision, Daegu, South Korea; Computer Science and Engineering Department, Kyungpook National University, Daegu, South Korea",2023 Fourteenth International Conference on Ubiquitous and Future Networks (ICUFN),"7 Aug 2023","2023","","","869","874","Designated parking spaces for individuals with disabilities are only meant to be used by vehicles with proper handicapped signage. Real-time monitoring is necessary to ensure that only authorized vehicles are parked in these spaces and to prevent unauthorized vehicles from using them. First, this research proposes to replace the backbone of a baseline YOLOv5 model which has 9 blocks with 6 EfficientNet blocks with less parameters but still have a higher accuracy in detecting disabled signs among other signages on the windshield of cars. Second, to compensate for the loss of blocks we have included an attention mechanism before detection part in our architecture which allows us to focus on the important regions needed for the task. Additionally, we propose to use a better optimizer AdamW to prevent overfitting. Based on these improvements, we have created a new object detector named Airy YOLOv5. To evaluate the effectiveness of our proposed method, a dataset containing images of cars with disabled signage on their windshields will be gathered and labeled. Experiments using this dataset show that our model achieves a better F1 score of 0.67 with 5 percent less parameters compared to the baseline model.","2165-8536","979-8-3503-3538-5","10.1109/ICUFN57995.2023.10200853","Ministry of Education; K2; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10200853","depthwise separable convolution;disabled signage;small object detection;supervised learning","Space vehicles;Atmospheric modeling;Detectors;Real-time systems;Automobiles;Task analysis;Automotive components","","","","18","IEEE","7 Aug 2023","","","IEEE","IEEE Conferences"
"Enhanced YOLOv5s Model for Improved Multi-Sized Object Detection in Road Scenes","S. Sivanandham; D. Gunaseelan","School of Electronics Engineering, Vellore Institute of Technology (VIT), Vellore, Tamil Nadu, India; School of Electronics Engineering, Vellore Institute of Technology (VIT), Vellore, Tamil Nadu, India",IEEE Access,"1 Jul 2025","2025","13","","110110","110127","Detecting objects in complex driving environments is crucial for autonomous vehicles to navigate safely. However, this task becomes challenging when addressing scale variations, occlusions and diverse backgrounds. This paper proposes an enhanced YOLOv5s model for handling varying object sizes from small pedestrians and traffic signs to larger vehicles in road scenes. The proposed enhancement begins by refining the default anchor boxes using the percentile-based quantile method on the distribution of the bounding boxes and the adjustments to the convolution layers for enhanced feature extraction. Smaller kernel sizes and fewer channels are employed in the initial layers to capture fine-grained details, while in deeper layers, the number of channels is progressively increased to capture broader information that better represents larger objects. Furthermore, an efficient channel attention (ECA) mechanism is integrated into the backbone to prioritize key feature channels, thereby enhancing the model’s ability to detect overlapping and small objects. To improve the feature fusion process, a Multi-scale BiFPN block is integrated into the neck of the model. This combines fine-grained spatial details from the shallow layers with more abstract semantic information from deeper layers, enabling the detection of objects across varying scales. Experimental evaluations carried out on the IDD dataset reveal that the enhanced YOLOv5s model achieves a significant gain in prediction accuracy when compared with the original YOLOv5s. To mitigate the effect of class imbalance and improve generalization across varying object sizes, CutMix data augmentation is employed during training. It shows a 48% increase in mean average precision (mAP@0.5) and a 44% and 49% rise in precision and recall, respectively, with an inference time of 14.6ms compared to the baseline model. These improvements underscore the effectiveness of the proposed enhancements in addressing the challenges of detecting multi-sized objects in complex road environments.","2169-3536","","10.1109/ACCESS.2025.3582136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11048496","Anchor box refinement;attention mechanism;CutMix data augmentation;feature fusion;multi-sized object detection;road scenes;YOLOv5","YOLO;Feature extraction;Accuracy;Roads;Adaptation models;Real-time systems;Object recognition;Complexity theory;Pedestrians;Neck","","","","50","CCBY","23 Jun 2025","","","IEEE","IEEE Journals"
